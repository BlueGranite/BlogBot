======== SAMPLE 1 ========
 of it! If you were to rank them in terms of how much time it would take you to process queries, most big data services perform that work in a matter of seconds. So how do you determine if a service performs quickly and effectively on a scale of 1 to 100? Here's an example that might help. A big data analytics solution is a cloud environment. The cloud service represents a degree of performance management on an enterprise scale. On-premises services are not only more robust, but they also represent a smaller monthly investment in infrastructure compared to the on-premises environment. So, once we look at the total cost of infrastructure, we see that it’s no small feat to run for several years on an on-premises service, and then move to on-premises capabilities in the cloud. We then factor in cost of ownership and depreciation within the cloud service cost per month. Here’s how BlueGranite defines scale: 100 GB for compute, 60 GB for RAM, and 24/7 cloud computing. So, using Microsoft Azure for a few years, we pay $90 per month per user, or $5 per month for HDInsight if we have HDInsight Premium access. However, in the last 2 years we've been able to increase the cost to 24/7 cloud computing by $4 per month. That’s massive. What about depreciation on the cloud compute and storage assets? That’s an expensive question and one that may have to be addressed at some point. So, in this case, we take an on-premises pricing in 2019 dollars. We compare it to the current on-premises price per terabyte (TB/hour) that we pay for Azure HDInsight for an equivalent volume of compute and storage on our Azure Blob Storage account. We compare the actual cost of the cloud service to the cost of the equivalent volume of compute storage. What do we see is a pattern? Considerable of it is simply growth in size. As the scale of the solution increases the workload size falls. On-premises solutions can grow to millions!  What does growth look like? You may notice that the growth in size is quite linear. Not linear is a poor metric for a big data analytics solution. What we end up with is a complex set of data management tools we’ve previously needed but that were too small or don’t fit in our big data cloud plan or use cases.  Scale isn’t a good metric in big data analytics and in many cases, what we end up with is an exponential growth in computing power. For a data visualization project this is not exponential growth that can be easily scaled back in the future. Scale isn’t the right word here. For the big data project to move forward, it has to be scaled back. The exponential growth that is currently being planned is not going to be robust enough. It is not an appropriate name for the project. The goal of this blog is to summarize key components of the big data analytics architecture and how to best manage it. The underlying architecture is not being discussed. The goals are not being set out. Some of the key areas of focus are: Data Management; Complexity and Ordering; Analysis Lifecycle; and Administration. Key elements of the big data architecture are as follows:  Materials: Data, Processes, and Software Infrastructure (single core / multi core) Dimensions: Work Environment, Storage/Store, and Administration Features: Storage, User Interface  For large analytics projects a key element is to be considered large. This refers to the amount of processing power needed to process the data and to arrange it in a suitable location. Traditionally, large BI projects have been handled by administrators, typically within a data warehouse or a system administration tool. In a big data analytics project, many features are being introduced that allow administrators to focus more resources on managing throughput, data quality, scheduling, batch creation, performance management, and so forth. From data ingestion to batch management, it seems that a lot of the heavy lifting has been transferred to the processing capabilities. However, in the end, the data management and the administration aspects of the architecture are largely operational considerations. Data Science is one area where the evolution of big data analytics and big data analytics within the data warehouse and analytics cluster represents a significant feature of the solution. Modern big data tools have dramatically changed the way we handle data. For example, big data analytics is directly applicable to machine learning and deep learning. The ability to build models that can be trained on highly structured data with low cost and with high accuracy is essential to the development of machine learning models. Big data tools now enable us to see the capabilities of a machine learning model over a short period of time. The ability to predict the direction of an event over a short period of time can be an effective way of understanding long term trends in the data. This is also the case with data science in the cloud model and its connection to the big data model. It
