======== SAMPLE 1 ========
 the post-production workflow of the Azure SQL Database. Since this is an Azure-only project, some of the production data is not compatible with the new Streaming Datasource data models. For example, the employee list for the hospital was created using a Model created using SSAS Tabular, and the employee lists are stored using the Employee List API. The above JSON POST call returned a collection of lists that were compatible with the new Streaming model. Note: There is a limit of 200 lists. Each list has a URL that we can use to download additional files.  I'll cover each of the REST APIs in more depth in Part 2. To get the Streaming API, connect to your Synapse Data Warehouse on the Azure web service. In the Azure portal, search for qlsearch. You should see the results shown below.  Using the Streaming APIs, you can transform your existing ETL code into the pipeline for the Integration Services for your Big Data workload. Note: You’ll want to migrate your ETL to make use of the Streaming APIs first, to make sure that it does not include any require() /require(<TLSession>’);/or <TLSession>’ . Once you have the API objects ready, you can start adding events to your pipeline by creating one simple example case using a ProductId in myLogoActivity:  I used the following API in my example:  apiVersioning = (API Key - Like the one in my sample)apiCredentialsStringBody = json.stringify(app.QueryableCredentials)apiCredentialsString = json.convobate(app.QueryableCredentials)apiCredentialsCString = \"ProductId' != '1' && apiCredentialsString != '['+API_OBJECT_ID+'] AND 'ProductName' != '' && apiCredentialsString != 'https://metadata.tmdb.com/api-metadata' )endpoint = 'YHWH'  Now that I have the API keys, I can create a simple streaming table for each Product by creating a unique id using myAPI keys.  Then I can use the provided getter and setters and dapply() functions to pull in the id from my logs.  Lastly, I can use the provided setter to insert a JSON string into the id manually. Finally, I can use the stream method from the getter to pull in the ProductId and insert it into the flush method of my dataset.  These API calls are very similar to the functions provided in TensorFlow, and I would recommend buying the technical version of TensorFlow for TensorFlow to get the functionality. The key point here is that the TensorFlow developers know how to use the API and can make the required API calls in a R script. For example, I can use the functions in the Get Data function in R to pull in the ProductId and insert it into the flush method of my dataset. You can read more about R Data Flow in the TensorFlow webinar here. If you would like to learn more about Streaming APIs in R or Spark, Microsoft gives free trials a trial-free trial upgrade to all their products. For more information about R Data Flows and Data warehouses, check out my upcoming webinar on Spark Streaming in Azure. If you have other ideas for improving the performance of TensorFlow and Data Flows, please let me know!"
"199" "In Part I of this 2-part series, I've introduced Spark as an API and introduced a simple way to serialize data in Spark. In this post, we'll interactally introduce ourselves and introduce you to the project, while demonstrating the usefulness of Spark for data processing and big data processing.    At BlueGranite we use Apache Spark internally and help manage development of Spark applications by building and maintaining a standard distribution which provides the source code and utilities for development. After the unit testing and configuration steps, we can use the distributed storage engine to process our data and send it to the reporting audience. Let’s see how easy it is.   We’ll create an instance of Spark by running the Python script setup.py setup once and then create the file whatever you ended up with. You’ll want to make sure that your .sparkly sweat pattern is set to cream to reflect your new data.  To serialize data in Spark into DataFrames, we’ll use blob storage, a feature common to many other analytics products and services: distributed blob storage (DBS). Data in DataFrames can then be processed by implementing a method to zip them together to form a single data frame. One simple implementation of this is through the use of a custom loopback engine.  The code for this post is available here . To use the loopback engine, you’ll need to use a Databricks
