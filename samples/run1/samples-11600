======== SAMPLE 1 ========
 is a bit hard to describe in words, but if you’re familiar with drawing lines in the sand, you’ll be a master programmer. And if you are, then you have probably been taught that flow design is mainly about visual appealing and intuitive to some degree. But what if you wanted to visually communicate a clear goal? Well, you’re in luck if you’m actually familiar with a flow itself? Would you really want to rely on an esoteric tool like Power BI to orchestrate the visual interaction between the users? Enter Flow. Flow is a fully-functional, fully capable tool which provides a logical, well-defined path through data flows. It offers a database-as-a-service, and hence deserves its place in semantic layer over other data flow types on the analytic roadmap.  As the flow language gets more elaborate and complex, Flow will become less and less useful, but for now, heuristics and clustering still help. But the ability to easily convey meaning and anhedge while not overwhelming a user's attention span is significant. I would expect further improvement in this department as the sophistication of Flow grows the tool we use in our toolkits to rapidly creating analytic pipelines from categorical data continue to mature. In fact, I would expect more advanced capabilities continue to emerge from the fact that categorical data is by far the most important types of data for analytic analysis. One of the keys to successful analytic optimization is understanding the origins and objectives of various stateful processes. In actuality, quite often there is overlap in understanding on the aims for the distinct data sets to varying degrees. For instance, what is the objective cost for the project with respect to physical stores versus digital sales, or for other sales objectives? Is there a desire in the team to make business value equilibrate the data assets as the processes commingle in the digital realm, or is it an activity of working on a specific problem together? Often, key capabilities are operating at different scales (artificial intelligence, data science, medical device science, etc.), and ultimately, intentions are only as good as the times. Understanding the primary motivations for each activity is key to making efficient use of their potential. Understanding where they came from, how they are being used, and how to best capture and integrate these insights can help inform new or improved practices.  Understanding the downstream uses for the technologies is also particularly important. When one understands the high-throughput uses, downstream costs, and high-value areas of the technology, it helps to know which segments are most affected and to devise appropriate solutions to address those areas. Understanding which downstream use cases result in the least amount of costs and which, if any at all. Finally, since these are early insights, remediation activities can often be multiplexed and reconciliations often differ by deployment method and severity.  Be sure to Subscribe to our blog so that you don't miss some important technical information. If you want to know more about BlueGranite and the rest of SQL Server Management Studio, contact us today."
"223" "How can SQL Server performance degradation be evaluated? By simply asking what parts of the query do the databases tables fail on? How can failure identification be improved upon?  Performance degradation refers to the difficulty in classifying (and estimating) the number of rows processed per hour and the number of rows processed/failed to process per day.  In the data scientist organization above mentioned, I have a background as well as a passion for machine learning. With the right training and options, I can be confident of a model created with machine learning, when required, it can create additional performance degradation indicators. What is additional performance degradation? additional performance degradation is a commonly misunderstood term. Rather, it means that the machine learning or clustered load procedure used to create the dataset above can be reused or enhanced with other training data. In this example, a customer analytics report, we created a clustered load script using Blob storage, and trained the model using a specific dataset per customer to see how long it takes an analyst to calculate the correct answer. The total network bandwidth consumed in creating the report per day varies greatly depending on the customer, which data source the data comes from, and the number of analysts that consume the reports. The figure below shows the 5-day weighted average monthly cost per user associated with the dataset I created for an independent retail customer, and the cumulative average cost associated with each upgrade since the beginning of 2015. The data looks similar, however, because the data is drawn from a Consolidated Data File (available online for download here), and the customer data comes from a separate source, the annualized value of each upgrade has been: The cumulative value is: 5DSL (approx. $0.0007 per user)DLT (approx. $1.732 per user)CREATION FAGGED Monthly cost 59,749 $6,078,000 $5,768,997Shareholders48
