======== SAMPLE 1 ========
 so to move our production lines to new areas that lower costs.  A different approach is required to address what Prof Seong-hwan calls a 'shadow economy', in which cost management is critical. Costs associated with human resource development like development assistance, social assistance, and unemployment insurance are driving some of the \"missing\" development cost savings we see across many areas of business.  For example, social development assistance (also known as social security) is used so often that we forget that it is an annual cycle. That is, almost all recipients of the program and many recipients in the pwmode pwmode environment. Many organizations use unemployment insurance to lower operating costs related to uncompensated medical expenses. This helps us think ahead and strategize to avoid paying more in unemployment.  In a business context, the cost savings from better managing applications leads us to another strategy – moving to a cloud model – which reduces the operational costs and allows us to analyze and piece together new things like benefits savings and unused costs. Finally, we must consider what we can optimize to achieve in the cloud. Apache Spark is the subject of a recent Hadoop Manifesto. It lays out best practices to migrate to Spark like any other application or process and highlights the various advantages – from standard deployment methods (no configuration code, just a SQL host, Python and SQL Server together), to cloud services that are scalable and user-friendly. Hadoop has an incredible opportunity to further improve and realize its potential with cloud services if it commences behaving \"as expected\" in managing data assets in the service. As an Apache Spark pro, you get: Access to a wealth of visualization and analysis options Get blazing fast compute for fast work Whenever you need compute (e.g., in Hadoop mode) in your cloud service, you get blazing fast compute for work. Cloud services are inherently more complex and performant for tasks higher up the organization. This means that we should expect a certain level of performacling and geeky at the latter throughout the conversation, even when discussing specific compute activities. Prof Lee has done quite a few posts recently in response to perceived performance and workloads in his PTR pipeline. Let's see what Prof. Lee has to say about Azure and where to find it.  Prof. Lee: Azure is now second only to Cassandra in queries served times and even ahead of SQL Server MDS in cluster sizes. Azure is uniquely positioned against both Cassandra and Sqml3D in that it offers a unified data load service, dedicated compute nodes, and SMP (synthesized, polyglot latency) replicas. I.e., it can handle massively parallel data loads (DFM) with comparable performance to SMP, both optimized for data storage. SMP requires preparation for Hadoop, whereas Azure offers a BI environment that can tackle major data volumes and complex data genicures with ease. Biml is one of the many tools that recognizable as bi-annual activities, and Prof. Lee’s presentation delights me because of his deep knowledge of relational data and the strengths of Azure Data Factory.  Within the talk, a number of interesting subjects and deep dive-into-how-the-data-grant model fits into an already existing Azure-centric vision for Azure Machine Learning suite, including how it can be used to transform your enterprise data. Throughout the presentation, I asked my fellow Spark developers what would be their top Azure priorities over the next 2 years. Let me get to them. 1. Azure Databricks is a code-free experience. The software giant's Spark services are 100% program-less and provide a set of integrated Spark-based APIs for Spark-assisted data preparation, analytic modeling, data storage and data mining. The platform is purely R, and all analytic activities can continue without issues. Azure Databricks also allows data scientists to write scalable R code without impacting on the analytic experience of other Spark-assisted tasks. 2. No Spark-assisted development on Azure. Although Azure Databricks is R directly integrated with Spark, no R code can be built on the cloud-based platform. The platform cannot be used for operational testing of the analytic models used in the Azure ML project. It can only be used for operational analytic work, and that is contingent on the model being operationalized by an appropriate team of people within the Azure R ecosystem. The only way to ensure that an R R C Spark model can be used on Azure is to make it available on the platform only, and that means both code and R must be shared with everyone who needs it. Prof. Lee: Although most of the users of Databricks are familiar with Python, there has been an increase in the past few years in integrating the Python programming language with Microsoft R. This has led to an explosion of open-source R packages available, resulting in an increasing demand for software integrators to host open-source hardware experiments and training for customers. Microsoft has longed for the opportunity to integrate
