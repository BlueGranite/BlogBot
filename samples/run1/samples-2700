======== SAMPLE 1 ========
 than its previous three years. While it is possible that data is being spent only and is used up easily, most companies would be happy to move to an all-volunteer IT organization where IT teams can work with and grow upon each new use of data. However, if a company does not have a high likelihood of having sufficient resources to scale data storage to the cloud, and a manager has the capacity and resources to tackle the new workloads quickly and efficiently, then the acquisition of one of the hottest data technologies could be a win-win situation.  Data Retention With cloud storage, there is no shortage of storage capacity. One of the ways this is attained is through the use of virtual disks or RAM. The virtual machine operates as RAM in the cloud and copies the contents of the physical disk into RAM. The data storage capacity is often quadrupled in a data movement since it can be done in a faster timeframe versus the traditional, relational database system. While performance is key, data transportation from the physical store to the distributed system is another consideration. If the data has been copied, metadata such as the timestamp, source database table, and database schema are all copied to storage. If the data stays at the physical location but has been moved, metadata such as database access details is lost and replaced with new information. Because the metadata has been copied, the new information can not be retrieved from the data stored on the physical device. This is how data retention occurs. It is easy for an IT manager to acquire additional capacity due to demand. It is also possible to relocate data which should be at a high data volume due to the demand. While moving data is inefficient and time-consuming, the transfer of data is also highly efficient. If the data is being held by the cloud service, if it is being held in a cloud-based storage solution, the virtual machine will be able to transfer additional storage capacity. The storage can then be moved forward or back. This ensures that new data is generated and persisted from each new use of the data technology. While a data source is being moved forward, all data collected by the data transportation process is still stored on the source system. This means that any changes made to data will affect any queries it was sending to the source service. Data movement is expensive because once data has been moved forward, it cannot be recovered from the source system.  If the data is being moved at faster than the speed of sound logic, this could mean data movement can be delayed without triggering a data movement alert. However, even if data was never moved forward, the user may have an alert if a new use is made of data (e.g., data movement which may happen within minutes). With data being moved at faster speeds, there appears to be a need to reallocate resources. A recent update to Microsoft Azure provides a solution which allows the cloud to allocate resources as needed rather than have users be forced to move data at their convenience.  As of June 2019, Microsoft Azure supports the following data technologies and processes: Data Warehouse – Microsoft provides a multi-protocol data platform using Azure. The Azure platform includes a data retrieval system, access control system (ACS), and a data-driven service (DR) which provide for real-time data processing activities. Data Flow – Azure supports data flow and transforms into and out of logical volumes and pipelines using a distributed processing engine (DTF). Data Flow – Azure supports data flow through transformations. Transform into – transform out – transform from – and load the data into a structured pipeline. Dataset Analysis – Data Analysis features are based around a distributed pipeline system (DR). Dataset Analysis uses Spark (version 1.2) and a distributed computing engine (DR). The Spark architecture also supports parallel processing within SQL Server, and is used to process structured, unstructured data like images, voice and video files, image files, and audio files.  Data Factory – Data Factory is a unified, web-based data processing language. It includes a platform for processing data transformations, transformations, and load orders. Datasets are organized and managed using this language. Data Factory provides data transformers, which transform data data transformation transforms load orders. Data Flow – Data Flow is a multi-protocol data processing platform (as well as a data integration system) that also includes a data flow engine, data-driven service, and a database engine. Data Flow performs the processing of data transforms and data transformations. The pipeline used to load data into Data Flow is essentially a series of parallel pipelines, which result in data in the pipeline being loaded at the same time. A Data Flow Overview The Data Flow Architecture outlines a number of topics for an understanding of the data movement and distribution processes, as well as the reasons why specific technologies are employed. Many of the topics covered in this report have already been covered in the previous section, and in addition, a few topics cover the use of Power BI Desktop for visualizing, and visualizing, data movement. These additional topics provide the following visualization of the data
