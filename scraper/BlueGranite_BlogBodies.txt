"x"
"1" "Raw. Unfiltered. Data. The raw zone – it’s the dark underbelly of your data lake, where anything can happen. The CRM data just body-slammed the accounting data, while the HR data is taking a chair to the marketing data. It’s all a rumble for the championship belt, right? Oh, wait – we’re talking data lakes. Sorry. If the raw zone isn’t where data goes to duke it out, then what is the raw zone of a data lake? How should it be set up? First, let’s take a time-out to give some context. A data lake is a central storage pool for enterprise data; we pour information into it from all kinds of sources. Those sources might include anything from databases to raw audio and video footage, in unstructured, semi-structured, and structured formats. A data warehouse, conversely, only houses structured data. The data lake is divided into one or more zones of data, with varying degrees of transformation and cleanliness (see this video for more: Data Lake Zones, Topology, and Security). The raw zone is the foundation upon which all other data lake zones are built.  The raw zone is the original conception of a data lake. It takes the raw data, exactly as it appears in the source and captures it, but it’s not just a current copy of the raw data. Instead, it keeps every version of the raw data indefinitely. For all your sources. Oh yeah! That is a lot of investment in data storage. What do you get in return? You get three advantages: Auditability – Because you have an exact copy of the original data, you can look back at your data to confirm that the derived data in other zones is accurate. Discovery – With the raw data, a data scientist may identify new measures or attributes that can improve the data models used in day-to-day analysis. It ensures against the maxim, “You don’t know what you don’t know.” Recovery – If an attribute or calculation needs to be added or changed for successive zones, you can completely rebuild successive zones’ data with the new or changed data. Now that we have some context, let’s talk about what this raw zone does NOT look like. Simply put, if your data resembles the chaos of a wrestling match, you are doing this raw zone thing wrong. Despite how it sounds, dumping data into the raw repository with no organization is a one-way ticket to a data swamp; you’ll be mired trying to get any use from the data. You’ll be pinned by the weight of trying to retrieve the data. If it’s not a no-holds-barred free-for-all, what should a raw zone look like? What does the data look like? First, let’s look at it as if we’re standing on the top turnbuckle looking down at our opponent. The data lake is file-based storage, and that means we’ve got a directory structure. We need to stick it some place. We can leverage the location in our directory structure to give coherency to this raw data. At the top level, we use folders to demark each zone of our data lake. That’s only a start. Within the raw zone, there’s flexibility in how it’s organized, but there are a few bits of metadata that you should capture within the folder structure for your sources. These included the: Data Source – This is the logical name of the data source, uniquely identifying the source. This usually can be accomplished with naming conventions like “CRM” or “Finance Database”. Sometimes, though, when you are capturing from multiple similar or nearly identical systems, the data source might be represented by a folder hierarchy instead of a simple folder (e.g., region/server/database). Internal Structure – It’s always useful to capture the internal structure of your data source. For a traditional database, that includes capturing the schema and table name. For file-based sources, you’ll want to mirror the folder structure here. Include as much structure as the product provides, even if you’re not currently making use of it. A database may begin by only using the default schema but, as it matures, start migrating subsections to different schemas. Timestamp – Finally, because you’re capturing the same data as it changes over time, it needs to be marked with a timestamp. Embedding the timestamp in the folder structure allows you to keep the data untouched. That gives us quite a bit of metadata that we’re encoding right into the folder structure. Usually, we recommend layering the folders in the order we’ve presented: source, structure, then timestamp. This often makes the most sense in terms of governance and ability to transfer to other zones. Now that we’ve covered the overall structure, let’s get down to the mat and get technical for a moment. We’ve been talking about the raw data and how it’s pristinely copied from the source with no changes. When your source is a filesystem or set of files, you simply do a binary copy of the files. With a database, we want to extract the logical table data into a file that we can store in our data lake. So, the data extract/conversion is something to carefully consider. That’s right – we’re going to talk file formats. With data extracts, you’ve got several choices: CSV, JSON, Parquet, ORC, Excel, and more. Since you’re coming from a structured source, the ideal would be to choose a medium that can reliably transfer not only the data, but the structural metadata as well. Compression will be a benefit for network transfer and storage volume too. Those two considerations point you to Parquet or ORC, and they are substantially similar. Choose the one that best integrates into your workflow. If you structure your raw zone correctly and choose sensible file formats, you will be building the foundation layer for a successful data lake. As you import your raw data from your sources, you’ll build up a treasure trove of information available to be cleansed and structured into new analytic insights. And maybe, just maybe, you’ll win that data championship belt. Does your data leave you slammed? Check out BlueGranite’s free eBook “Data Lakes in A Modern Data Architecture” to discover the benefits of a data lake and to learn whether it’s right for your organization. Questions about how a cloud-scale approach can help you maximize the benefits of your data? Check out our Modern Data Platform resources, or give us a call today.  "
"2" "The most appropriate way to deliver Power BI content to your users is not always apparent. With Power BI Pro licensing, most people view your reports and dashboards in the Power BI Service, but that is not always the case. Power BI Premium and most tiers of Power BI Embedded deliver dedicated capacity in the Power BI service to help with additional embedding scenarios.     In every case, Power BI is very versatile. You may see reports embedded in other Microsoft applications or your own custom application without even knowing you’re accessing Power BI!       As you try to navigate different embedding scenarios with Power BI, you’re exposed to different capabilities, terms, and license options. It’s a confusing landscape, and bringing clarity to everything is difficult. Even terms that appear clear at face value such as internal and external are not as straightforward as you would think.    Terminology You’ll Encounter  There are many variables that could help you decide whether you need Power BI Premium or Power BI Embedded, and here are some terms that come into play:     Audience You may have internal or external users who need to view your Power BI content. Sometimes, you have both. Sometimes your external users can be guests that function as internal users. It’s complicated, so read on.    Access Scenario Your audience may view content in the Power BI Service web application, embedded in a Microsoft application such as Microsoft Teams or SharePoint, or embedded in a custom application that you create (or a combination of any of these).    Product Lines Power BI Embedded (A SKUs) is primarily used to embed in custom applications with external users only, and where you do not need to authenticate with Azure Active Directory. Power BI Premium (P SKUs and EM SKUs) allows you to embed in custom applications but also in Microsoft Teams and SharePoint. Power BI Premium P SKUs are the only ones (without a Pro license for users) that allow you to use the Power BI Service application for users in your AAD tenant while also embedding anywhere.    Licensing  You will encounter choices for Power BI Pro or Power BI Free. Pro gives you publishing rights in addition to viewing rights. Free gives you viewing rights, but only when your content is hosted in a workspace with dedicated capacity through either Power BI Premium or Embedded.     What’s the Difference Between “User Owns Data” and “App Owns Data”? What about “Embed for your Organization” and “Embed for your Customers”?  While you’ll often see different scenarios labeled as “User owns data” and “App owns data” in documentation, these terms can be confusing. The terms are principally related to authentication, but what is meant by “user” and “app”?     User owns data means that users authenticate through Azure Active Directory. You may see this scenario also labeled as Embed for your Organization.  App owns data means that your custom application handles authentication. You may see this scenario also labeled as Embed for your Customers.  Even in an “App owns data” scenario, you would still need at least one Power BI Pro license to publish and embed content from a workspace in the Power BI Service. Embedding could occur with a user or a service principal.    What also makes these terms sometimes misnomers is that they are not one-size-fits-all, universal phrases for all scenarios. Depending on your technology selection, for example, you could have a custom application for internal users or a combination of internal and external users that is documented as “App owns data”/”Embed for your Customers”. You could also invite external users into your organization as guests in a “User owns data”/”Embed for your Organization” scenario, where these external users could access a Power BI service app or workspace. Exceptions like these make your individual solution’s needs important when it comes to architecture and licensing.      What about Azure B2B?  With Azure B2B, you effectively bring external users into your organization (Azure Active Directory tenant) as guest users. This enables a “User owns data” scenario for these external users. For various embedding scenarios, guest users invited into your tenant through Azure B2B are considered part of your internal organization.      Power BI Embedded – A SKUs  Power BI Embedded is the only family of SKUs used for embedding that you purchase through Azure instead of Microsoft 365. While many of same capabilities are unlocked from a technology standpoint as Power BI Premium, you are limited to \"App owns data\" authentication in custom applications and do not get use of the Power BI Service web application or the ability to embed in Microsoft Teams or SharePoint. Microsoft's typical messaging around A SKUs is for ISVs and external customers, and we maintain that focus in the diagram below. There are some circumstances where you may consider A SKUs for internal usage though.        Power BI Premium – EM SKUs  With EM SKUs, you typically embed in organizational scenarios such as Microsoft Teams or SharePoint. Organizational users do not get access to the Power BI Service web application. EM SKUs are not as frequently considered in conversations about Premium versus Embedded due to that limitation. The situation has to be just right to consider EM SKUs for embedding in “User owns data” scenarios without jumping to full access to the Power BI Service with P SKUs.         Power BI Premium – P SKUs  With P SKUs, you cover all use cases. From an audience perspective, your internal users could use the Power BI service web application at www.powerbi.com, you could embed in Microsoft Teams or SharePoint in a “User owns data” scenario, and you could embed in a custom application in an “App owns data” scenario. Your external users could be invited into your tenant with Azure B2B for “User owns data” development, or they could remain external for “App owns data” development.         Quick Decision Summary  If you consider general scenarios before digging too deeply into your own nuances, here’s a quick summary that may help in most situations. Your particular requirements may affect the final decision-making process though. While the details of your exact usage should be considered, the following approach acts as general guidance.     User Audience    Scenario    SKU Selection    Internal users only    View in Power BI Service    P    Internal users only    View in Microsoft Teams or SharePoint    P  EM    Internal users only    View in custom app    P EM   External users only    View in custom app    A   Mix of internal and external    View in custom app    P A   Mix of internal and external    View in Microsoft Teams or SharePoint    P (with external users as guests through Azure B2B) EM (with external users as guests through Azure B2B)    Mix of internal and external    View in Power BI Service    P (with external users as guests through Azure B2B)   Mix of internal and external (Option A – single SKU)    Internal view in Power BI Service while External view in custom app    P   Mix of internal and external (Option B – multiple SKUs)    Internal view in Power BI Service while External view in custom app    P (for internal) A (for external)      Still Perplexed?  There are many variables in this discussion, and we’ve only touched on licensing considerations. BlueGranite can help you plot a course that’s consistent with the targeted needs of your business. Reach out to us today to setup a conversation. "
"3" "BlueGranite has enjoyed several opportunities recently working with state and local governments to help enable modernization of their systems with Power BI to support everyday operations. Like many, these agencies working to serve their communities are also seeking to realize the benefits of modernization: advanced capabilities, potentially delivered at much lower costs than those of existing, aging solutions.  Yet they face unique challenges, and this article takes a technical dive into how to meet one such challenge. Usage Auditing – Logins Power BI helps government agencies manage their data for increased operational efficiency, often at lower cost than existing solutions.  As with all systems that manage sensitive data, government agencies are required to track and capture system usage details to ensure that the security and integrity of citizens' data is maintained.   Detailed analysis of internal and federal standards applicable to the government agencies reveals that information to be captured for auditable events should include the: Date and time of the event. Component of the information system (e.g. software or hardware) where the event occurred. Type of event. User/Subject identity. Outcome (success or failure) of the event. Additionally, the standards direct that agencies shall retain audit records for at least one year, and that once the minimum retention time period has passed, agencies shall continue to retain audit records until it is determined they are no longer needed for administrative, legal, audit, or other operational purposes. To ensure the integrity of systems and sensitive citizens' data, user logins are among some of the most fundamental events to track. Solutions Power BI Desktop Power BI Desktop is a complete data analysis and report development application that is used on workstations.  It includes functionality to connect to many different kinds of data sources and then shape the data into integrated models.  It also includes functionality to analyze, format, and display the data in many kinds of interactive visuals.  A separate Report Builder application extends Power BI to include pixel-perfect, paginated reports. Power BI Cloud Service Once data connections and reports are developed on workstations, Power BI software allows data modelers and report authors to publish and share their works in the Power BI service. Each user is required to log in to a cloud-enabled account to publish to the Power BI cloud service. Government agencies have the option to subscribe to an exclusive Government Community Cloud that is a comprehensive suite of advanced cloud hosting, cybersecurity, and information management technologies that caters to public sector needs for additional security and U.S. data residency, as required by federal regulation. Power BI Logins Power BI has its roots in Microsoft’s suite of cloud-enabled office productivity tools, Office 365.  To maintain identity integrity across its many components, user authentication with Office 365 is implemented as a centralized service. Therefore, each “login” is an instance where the user provides credentials to access an app or service within Office 365. Logins<U+202F>are each recorded in the Office 365 Audit Logs and are available for reporting. Depending on an organization’s security settings, user sessions resulting from a single login can span extended periods of time with users navigating into, away from, and back to apps and services without requiring re-authentication. This is especially true when “Keep me logged in” options are enabled. Retrieving Login Events Power BI is part of the Office 365 composite application consisting of numerous apps and services. Activities are tracked within each app or service and then centralized into a Unified Audit Log for all Office 365 apps and services. Activities are stored in the unified log for a limited time and must be extracted and preserved long-term by security administrators in an external repository using an available REST API.  Three REST APIs are available to retrieve Power BI activity events for long-term storage: Office 365 Management REST API The Office 365 Management REST API can be used to retrieve login events across all Office 365 apps and services. As described earlier, these events pertain to users’ access to Power BI based on administrative settings. Events up to seven days old can be retrieved;   these might be delayed up to 48 hours before appearing in the unified log for retrieval. Access to the Office 365 API requires Office 365 Global Administrator privilege. The API is called from a scripting language (such as PowerShell) using an OAUTH authentication pattern. This API support a large volume of events while offering limited options for retrieving a subset of the data. Exchange Online Rest API The Exchange Online REST API can be used to retrieve login events across all Office 365 apps and services. As described earlier, these events pertain to users’ access to Power BI based on administrative settings. Events up to 90 days old can be retrieved, these might be delayed up to 48 hours before appearing in the unified log for retrieval. Access to the Office 365 API requires O365 Global Administrator privilege. The API is called from a PowerShell script using a convenient cmdlet. This API offers many options for retrieving a subset of the data (such as only logins) but is less able than other API options to support a large volume of events. Power BI Management REST API The Power BI Management REST API can be used to retrieve Power BI events solely but doesn't include login events. Events up to 30 days old can be retrieved; these might be delayed up to 30 minutes before appearing in the activity log for retrieval.  Access to the Power BI REST API requires Power BI Administrator privilege.  The API is called from a PowerShell script using a convenient cmdlet.  This API retrieves only Power BI activity events and offers additional options for retrieving subsets of the data, while also supporting large volumes of events over other API options. Due to the nature of Office 365 activity log processing, usage of an available REST API will require an approach to handle late-arriving events. These are events that can take up to 48 hours to appear in the unified audit logs from which the available REST APIs retrieve activity events. Given that the REST APIs do not offer a method to identify and select only these late-arriving events, only periods older than 48 hours are guaranteed to contain all events. Periods retrieved less than 48 hours old may need to be retrieved again to ensure that they contain any late-arriving events. For example, a daily process may choose to retrieve activity for the past three days, accepting that data within the past 48 hours may be missing some late-arriving events that will be included in subsequent daily process re-runs. Implementation To demonstrate how Power BI can meet these challenges, the Exchange Online REST API can be used to retrieve login events. This API has a convenient cmdlet (Search-UnifiedAuditLog) that can be called from a PowerShell script, making it easier to implement and maintain than the Office 365 REST API. A basic PowerShell script to retrieve today’s logins:   Install-Module -Name ExchangeOnlineManagement -Scope CurrentUser$conn = New-PSSession -ConfigurationName Microsoft.Exchange `                      -ConnectionUri \"https://ps.outlook.com/powershell/\" `                      -Credential (Get-Credential) `                      -Authentication Basic `                      -AllowRedirectionImport-PSSession -Session $conn -CommandName Search-UnifiedAuditLog -AllowClobber$todayUTC = [System.DateTime]::UtcNow.ToString('yyyy-MM-ddT00:00:00.000')$response = Search-UnifiedAuditLog `                  -RecordType AzureActiveDirectoryStsLogon `                  -SessionCommand ReturnLargeSet `                  -StartDate ([datetime]$todayUTC) `                  -EndDate   ([datetime]$todayUTC + [timespan]\"23:59:59.9999999\") `                  -SessionID $todayUTCRemove-PSSession -Session $conn  Some comments on the basic script: Office 365 Global Administrator credentials can be used for the connection to Exchange. More limiting options are possible but require custom configuration. The StartDate and EndDate parameters must be on the same day UTC. The ReturnLargeSet session command enables the cmdlet to return the maximum number of events (50,000) for the interval. For very large organizations where there may be even more events than this in a single day, then the data would be retrieved in smaller increments. For example:      -StartDate ([datetime]$todayUTC + [timespan]\"00:00:00.0000000\") `    -EndDate   ([datetime]$todayUTC + [timespan]\"11:59:59.9999999\") `          -SessionID \"$todayUTC 00:00:00.0000000\"   and       -StartDate ([datetime]$todayUTC + [timespan]\"12:00:00.0000000\") `      -EndDate   ([datetime]$todayUTC + [timespan]\"23:59:59.9999999\") `      -SessionID \"$todayUTC 12:00:00.0000000\"  Each call to the Search-UnifiedAuditLog cmdlet returns an array of objects with key fields:  The array of objects returned may contain one, some, or all of the events found in the search.  Because of this, the call to the API must be repeated using the same SessionID until all records have been retrieved (hence why the SessionID should uniquely identify the interval).  The records may be returned out of order. To complicate matters, the API self-limits the span and pace of activity.  There are varying limits on the number of sessions that can be established in a short time frame, as well as varying limits on the number of records that can be retrieved in a short interval.  When these varying limits are exceeded, the API will “reset” the stream of activities returned. These varying limits only rarely disrupt the smooth retrieval of events from the API because the limits are fairly generous.  One strategy to manage these disruptions would be to try to avoid them altogether through introduction of purposeful delays in the pace of calls to the API.  However, this strategy unnecessarily increases the overall time it takes to retrieve all records – this can be a problem for large government agencies where the total number of activities can be very large and take an extended time to retrieve.  Because the disruptions are rare, a better strategy is to carefully monitor the sequence of calls to the API to detect when disruptions occur and then restart the retrieval.  A sample script to accomplish the monitoring, detection, and recovery under API self-limits:   #retry loopdo {  if ($retry) {    Write-Host '   Service API throttled/timeout ... retrying ...'    $retry = $false  }  $resultCount = 0 #the number of results to retrieve  $results = @{}   #accumulator for results  #accumulate results loop  do {    #get a batch    $response = Search-UnifiedAuditLog ...    #check results    if (!$response) {      #should be $null only if search found no records; otherwise there was a problem      $retry = ($resultCount -ne 0)    } else {      #should return index 1 only in the first batch; otherwise there was a problem      $retry = (($response[0].ResultIndex -eq 1) -and ($resultCount -ne 0))    }    if ($response -and !$retry) {      #so far so good, set target number of results and accumulate unique results      $resultCount = $response[0].ResultCount      foreach ($result in $response) {        $results += @{ $result.ResultIndex = $result } #build hash table of unique results      }    }  } until (($results.Keys.Count -eq $resultCount) -or $retry)} while ($retry)  Finally, the API appears to sometimes duplicate activity events that happen to occur at the top of an hour.  For example, an activity that happens at 01:00:00 sometimes will be returned by the API twice, each with the same unique event ID!  While this duplication persists, the final step to retrieving events from the API is to eliminate duplicates based on the unique event id. Summary Technology modernization delivers greater capability for less cost than existing, aging solutions. State and local governments face unique challenges in tracking and capturing system usage details to ensure that the security and integrity of sensitive data is maintained. This is especially true for very large government agencies that have many users and much activity. Power BI can meet these challenges by providing functionality and features that help these agencies meet tracking challenges and realize modernization goals."
"4" "When city government begins to leverage data, it can spur transformations that maximize resources, support people and infrastructure, boost education, streamline city services, and so much more. Generally, the most challenging part of a data analytics project is figuring out where to start. Are data available? Is there a good question to answer? Municipalities are all very different – industry, presence of a college or university, weather, and more can vary. But some challenges affect almost all: areas lacking critical internet connectivity, effective allocation of scarce budget resources, and generating adequate revenue fairly. As these are also issues where data is often easily available, nearly every municipality can begin using technology today to analyze them.   Internet Connectivity Connection to the internet, especially during the time of COVID-19, is critical. Those who work from home rely on an internet connection, as do students trying to learn remotely. Unfortunately, not everyone has the same access to the internet. This can be due to very limited or no or service provider options, or a lack of affordable offerings. In its annual American Community Survey, available at data.census.gov, the U.S. Census Bureau asks questions about internet access and makes data available down to the census-tract level. You can quickly discover which census tracts have the lowest internet connectivity, either through exporting data or reading data in via API. Add in an additional variable – poverty – and you can then understand if there is a relationship between poverty and lack of internet in a household. As school districts, cities, states, and the federal government debate investment in broadband expansion across the country, this data can inform your municipality’s potential need to invest and partner in internet expansion and affordability. Being armed with data about where your community needs to narrow the gap can help when applying for grants or lobbying elected officials. The Power BI dashboard below shows broadband connectivity rates and poverty rates in Syracuse, New York. The darker red areas are census tracts with lower connectivity rates and higher poverty. In the scatterplot, each point represents a census tract in Syracuse and compares poverty rates and broadband connectivity. Notice the relationship where the poverty rate rises as the connectivity rate falls. There are many reasons for this, and the relationship is likely not causal, but it is a finding worth researching more in your own municipality.   Using Power BI, the entire dashboard can be filtered, so if you want to find more information about a specific census tract, all you need to do is select a tract from the map, scatterplot, or table. Budget  The annual budget is a municipality’s lifeblood. It shows the intention of where a municipality intends to invest scarce resources, and ultimately, is the best way to understand priorities and values. For an analyst seeking data, budget information is generally available in a machine-readable format (especially if you make friends with the budget director), and oftentimes the information is readily accessible online. Austin, Texas, makes both budget and expenditure data available on its Open Data Portal. The dashboard below, built using Power BI, is an example of how such information can be visualized to make it more digestible and understandable. Instead of a long PDF budget document with lists of dollar amounts and categories, this dashboard easily shows which departments receive the most money in the budget, how much of that budget departments have spent so far this year, and, by building a hierarchy of budget items, a user could easily see how much is budgeted and expended in different funds (capital versus operating budgets, and more). Because budget documents are often long and filled with tables of numbers, they may not be read by many people; but since they are so important in the way a municipality operates, making those documents more accessible can be key to increasing community engagement.   Assessment Data Behind every municipal budget is the revenue funding it. Municipal governments raise revenue in a variety of ways, one of which is through property taxes. Those taxes are generally associated with property assessments; here is another place where you should be able to access data to embark on an important analysis. Analyzing data by property type and city neighborhood is a good way to begin exploring assessment inconsistencies and working toward accuracy. A joint study released June 2020 by university economists highlighted property assessment inequities across the U.S. Prior to its release, newspapers were undertaking their own analyses to suss out assessment gaps, such as this Syracuse.com | The Post-Standard investigation. Using often readily available data, cities can undertake their own community property assessment explorations. With Power BI, you can load in and filter data by neighborhood, property, or land use type. The dashboard shown below, with data available at data.syrgov.net, aggregates all property assessment data by city neighborhood and also by assessment section – as defined by the city of Syracuse. In it, you can see which parts of the city have higher average assessments. The next step of the analysis should then look at how the assessments compare to sales prices, but this dashboard alone can offer a lot of value to an Assessment department.  Finding the right questions to tackle when building data-driven analyses in government can be difficult – both because there is so much to focus on, and finding data that is straightforward to work with can be challenging. But asking the right questions, and using data and analytics to delivery clarity and answers, can lead to data-driven city management improvements. The examples above – all using data that is often available internally in an easy-to-use format, or publicly, can offer important understanding and inform potential policy change. Interested in learning more? Visit BlueGranite’s Modern Business Intelligence page to learn how technology can deliver value to your municipality, or download our free eBook exploring how to implement a successful self-service BI program.  "
"5" "Azure Data Factory (ADF) is one of the most useful services in the Microsoft Azure modern data platform. Using ADF’s seamless user interface, you can design and orchestrate complex data movement and transformation from just about any source. This is extremely valuable for data-rich companies that need to manage large ETL processes in both the on-prem and serverless spaces. One of the challenges of scaling ADF for larger implementations is monitoring its execution. The larger your process becomes, the harder it can be to monitor.   Now don’t get me wrong, there have been noticeable enhancements from ADFv1 to ADFv2 in terms of monitoring features, however the UI still has its limitations. First, you can only see 100 items per page, which is great, unless your ETL performs multiple actions on hundreds of tables (as is usually the case). Secondly, wouldn’t it be nice if you could filter through more than just timeframe, status, or pipeline name? What if you wanted to find pipelines with long execution times, create aggregates, or drill down and slice using even more parameters? Lastly, you might be able to get by with simple pie or Gannt-style bar charts, but what if you wanted to add your own visuals to see the executions in a way that makes the most sense to you? My solution is to leverage the flexibility and simplicity of the comprehensive Power BI business analytics suite to create a fully customizable report for monitoring your ADFv2 processes. This post will show you how this can be accomplished using the Azure Data Factory v2 REST API to query your data factory via Power BI. Prerequisites To make API calls using Power BI, you will need to create a Service Principal App with Contributor Access, then authenticate to the ADF Service using the App’s ID and Secret Key. There are two ways to create the Service Principal and I will go over each of them. The first is to use the Azure CLI: Execute the following command in the Azure CLI to create the Service Principal: az ad sp create-for-rbac --name ServicePrincipalName Your new Service Principal will be created with Contributor access by default. If you would like to check the role assignment, you can run the following command in the CLI: az role assignment list --assignee APP_ID It is quite possible that your personal account does not have the permission to create a Service Principal at the subscription level. In this case, the second approach is to create an App registration in the Azure Active Directory UI, then assign it to a role at the resource group or ADF level (as long as you have Contributor access to these resources): Navigate to Azure Active Directory for your subscription. Select App registrations to create a New registration. Choose a Service Principal name (you don’t need to worry about the redirect URI in this case). Click Register. After the application and Service Principal have been created, you need to manually assign a role to the application. Assuming you don’t have access to assign the role at the subscription level, let’s assign it at the resource group level. Select the resource group that houses your ADF instance. Navigate to Access control (IAM). Choose + Add at the top, then Add role assignment. Select Contributor from the Role dropdown. Search for the Service Principal you created in the last section. Click Save and the permissions should be set up just as they would be if you used the CLI method. Regardless of your method for creating the Service Principal, you will need to give it a new application secret to allow it to make API calls programmatically. Return to App registrations under Azure Active Directory. Select Certificates & secrets then choose New client secret. Set a description and duration for the secret. Click Add. Once the secret has been generated, it is important to copy it to your clipboard immediately and keep it on hand because we will need it for the next section. NOTE: It is also a good idea to store the new application secret in Azure Key Vault or somewhere your application can retrieve it, otherwise you won’t be able to view it later. For more information, see https://docs.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#create-a-new-application-secret. Making the Connection Congrats! You successfully set up the Service Principal and its permissions. Now you have a means for accessing ADF anonymously using a dedicated Service Principal. Next, we will go over the process for using Power BI to authenticate to the data factory and query your pipelines. Open Power BI Desktop and select Get Data. Select Blank Query from the data source options and click Connect. Click on Advanced Editor in the Home ribbon. Paste the code below into the editor window and replace the highlighted sections with your Azure Tenant ID, Subscription ID, Resource Group Name, and ADF Name. Retrieve the Application (client) ID from the App’s Overview page in Azure Active Directory and replace the remaining highlighted fields with your Service Principal’s App ID and secret (which you should have saved somewhere). let  //Set variables for authenticating to Azure using Service Principal and making API request  tenantID = \"<Azure Tenant ID Here>\",  subscriptionId = \"<Azure Subscription ID Here>\",  resourceGroupName = \"<Resource Group Name Here>\",  factoryName = \"<Data Factory Name Here>\",  apiVersion = \"2018-06-01\",  appId = \"<Service Principal App ID Here>\",  clientSecrets = \"<Service Principal Application Secret Here>\",  uri = \"https://login.microsoftonline.com/\" & tenantID & \"/oauth2/token\",  res = \"https://management.azure.com/\",  today = DateTime.LocalNow() as datetime,  prev = Date.AddMonths(today, - 1),  startDt = DateTime.Date(prev),  startDtText    = \"'\" & Text.From(Date.Year(startDt)) & \"-\" & Text.From(Date.Month(startDt)) & \"-\" & Text.From(      Date.Day(startDt)    )      & \"T00:00:00.0000000Z'\",  endDt = DateTime.Date(today),  endDtText    = \"'\" & Text.From(Date.Year(endDt)) & \"-\" & Text.From(Date.Month(endDt)) & \"-\" & Text.From(      Date.Day(endDt)    )      & \"T00:00:00.0000000Z'\",  //Obtain Authorization token for Service Principal  authBody = [    grant_type = \"client_credentials\",     client_id = appId,     client_secret = clientSecrets,     resource = res  ],  authQueryString = Uri.BuildQueryString(authBody),  authHeaders = [#\"Accept\" = \"application/json\"],  auth = Json.Document(    Web.Contents(uri, [Headers = authHeaders, Content = Text.ToBinary(authQueryString)])  ),  token = auth[access_token],  //Build request URL & Body using variables  url    = \"https://management.azure.com/subscriptions/\" & subscriptionId & \"/resourceGroups/\"      & resourceGroupName      & \"/providers/Microsoft.DataFactory/factories/\"      & factoryName      & \"/queryPipelineRuns?api-version=\"      & apiVersion,  reqBody = \"{ \"\"lastUpdatedAfter\"\": \" & startDtText & \", \"\"lastUpdatedBefore\"\": \" & endDtText    & \" } ] }\",  //Make API Call to query Data Factory  reqHeaders = [#\"Content-Type\" = \"application/json\", #\"Authorization\" = \"Bearer \" & token],  result = Json.Document(    Web.Contents(url, [Headers = reqHeaders, Content = Text.ToBinary(reqBody)])  )in  result   The first section of the code above sets the necessary variables and makes a call to the Microsoft API to obtain an authorization token. This token is then passed to the next section of code which queries the Azure Data Factory API using the request body. You may notice that we are setting the start and end dates in the body to create a window for the last 30 days. There are other useful fields that you can filter on in the request body (such as “PipelineName”), but for demo purposes I decided to query the entire data factory for the last 30 days. For more information on the capabilities of the ADF REST API you can find the Microsoft documentation here. 5. Finally, click Done and your M query will be saved.  NOTE: Once you save your M query, Power BI will ask you to set privacy levels for the data in this dashboard. It is a good idea to consult your IT department or data governance team to determine the recommended settings, but if your data factory is not deemed sensitive then you can check the box to ignore privacy restrictions on the API results.  Power BI will call the API to query the data for your ADF pipelines, but there are a couple more steps we need to take to turn the JSON result into readable data. Click List in the query preview window. The first time you connect to the Azure API, Power BI will ask you to select your authentication settings. I have found that it is best to authenticate at the Subscription scope, that way you can use the API to connect to other resources within the subscription without changing your settings. To set the scope to the subscription level:Click Edit Credentials. Choose the third option from the top in the dropdown to select your specific subscription. Click Connect. Once this is done, you will need to add a few more steps to parse the JSON and transform the API response into a table in your data model:  Click To Table in the Transform ribbon. Click OK. Click the icon at the top right of the column to expand the JSON records into unique rows and columns. Uncheck the box Use original column name as prefix. Click OK. Repeat steps 3-5 on the parameters column to expand the ADF pipeline details. Click Close & Apply to add the query to the data model. Hooray! Now you have a functioning data source that can seamlessly load pipeline execution data from ADF to the Power BI data model. But wait a minute… there are only 100 records. Didn’t the ETL execute more than 100 pipelines last night? What you’re seeing is a result of the API’s limitation for returning more than 100 records in the JSON response. But have no fear, there is a workaround that involves invoking a Power BI function to get all the records. Creating the API Function Whenever an API call to Data Factory returns more than 100 records, the JSON response includes a “ContinuationToken” field which can be passed as a parameter in a successive API call. If you do not change any of the other parameters (e.g. date windows, pipeline names, or other filters), you can add this token to the body of another API request and get the next batch of records. Doing this recursively, using the next ContinuationToken from each response, will give you the full dataset. To make these recursive API calls, you just need to wrap the API call in a simple Power BI function that takes the ContinuationToken as a parameter and loops through the code, merging each batch together until you have a full master dataset. Once you create the function, you can add a separate M query to invoke the function and execute the loop whenever the report is refreshed! Create a new Blank Query in the editor. Right-click the query and select Create Function. Click OK to create a function with no parameters. Name the function “pipe_runs_recursive” and select OK. Open the function in the Advanced Editor and paste in the code below which allows the Continuation Token to be passed to a subsequent API call (don’t forget to insert the values for the Tenant ID, Subscription ID, Resource Group Name, Data Factory Name, and App ID/Secret variables just like before):let  pipe_runs_recursive = (optional contText as text) as list =>     let      //Set variables for authenticating to Azure using Service Principal and making API request      tenantID = \"<Tenant ID Here>\",      subscriptionId = \"<Subscription ID Here>\",      resourceGroupName = \"<Resource Group Name Here>\",      factoryName = \"<Data Factory Name Here>\",      apiVersion = \"2018-06-01\",      appId = \"<Service Principal App ID Here>\",      clientSecrets = \"<Service Principal App Secret Here>\",      uri = \"https://login.microsoftonline.com/\" & tenantID & \"/oauth2/token\",      res = \"https://management.azure.com/\",      today = DateTime.LocalNow() as datetime,      prev = Date.AddMonths(today, - 1),      startDt = DateTime.Date(prev),      startDtText        = \"'\" & Text.From(Date.Year(startDt)) & \"-\" & Text.From(Date.Month(startDt)) & \"-\"          & Text.From(Date.Day(startDt))          & \"T00:00:00.0000000Z'\",      endDt = DateTime.Date(today),      endDtText        = \"'\" & Text.From(Date.Year(endDt)) & \"-\" & Text.From(Date.Month(endDt)) & \"-\" & Text.From(          Date.Day(endDt)        )          & \"T00:00:00.0000000Z'\",      //Obtain authorization token for Service Principal      authBody = [grant_type = \"client_credentials\", client_id = appId, client_secret        = clientSecrets, resource        = res],      authQueryString = Uri.BuildQueryString(authBody),      authHeaders = [#\"Accept\" = \"application/json\"],      auth = Json.Document(        Web.Contents(uri, [Headers = authHeaders, Content = Text.ToBinary(authQueryString)])      ),      token = auth[access_token],      //Build request URL & Body using variables      url        = \"https://management.azure.com/subscriptions/\" & subscriptionId & \"/resourceGroups/\"          & resourceGroupName          & \"/providers/Microsoft.DataFactory/factories/\"          & factoryName          & \"/queryPipelineRuns?api-version=\"          & apiVersion,      reqBody =         if contText = null        then \"{ \"\"lastUpdatedAfter\"\": \" & startDtText          & \", \"\"lastUpdatedBefore\"\": \" & endDtText          & \" }\"        else \"{ \"\"lastUpdatedAfter\"\": \" & startDtText          & \", \"\"lastUpdatedBefore\"\": \" & endDtText          & \", \"\"continuationToken\"\": '\"          & contText          & \"' }\",      //Make API call to query Data Factory      reqHeaders = [#\"Content-Type\" = \"application/json\", #\"Authorization\" = \"Bearer \" & token],      result = Json.Document(        Web.Contents(url, [Headers = reqHeaders, Content = Text.ToBinary(reqBody)])      ),      //Get continuation token from previous call if available       contToken = try Record.Field(result, \"continuationToken\") otherwise null,      //Run function for each new continuation token      CombinedResults =         if contToken = null        then {result}        else List.Combine({{result}, pipe_runs_recursive(contToken)})    in      CombinedResultsin  pipe_runs_recursive  Open the Blank Query you created in step 1 and enter the following code to trigger the function and add your data to one big result set: let  Source = pipe_runs_recursive(null)in  Source   The function takes the Continuation Token as a parameter, but this parameter is optional. When your query runs it will trigger the function passing null as the token for the first set of 100 records, then it will pass any token it receives to subsequent calls. You will still need to expand the JSON like we did earlier, but once that’s done you will have a functioning query that returns the entire result set to your data model without the 100-record limit. Click Close & Apply to save your queries and look at your new Power BI data model! Once you have the connection set up to get raw pipeline data, you can start creating personalized measures and dimensions to visualize your data. Some things I found helpful were building a date picker to filter the last 30 days of runs down to more manageable slices. I also created measures that calculate Execution times in hours and minutes, which could help you see the big picture in terms of average ETL execution or long-running pipelines. I provided some quick examples below, but for more in-depth data visualization tips and tricks, consider attending one of our Power BI “Dashboard-in-a-Day” webinars, or subscribing to our YouTube channel.  While Azure Data Factory’s comprehensive integration and orchestration capabilities offer data transformation at cloud-scale speed, Power BI simplifies data visualization and interaction. Whether you’re considering streamlining and modernizing your data platform, or want to further explore Azure’s robust capabilities, check out BlueGranite’s complimentary Modern Data Platform Fundamentals webinar. Using our unique BlueGranite Catalyst for MDP, we guide companies through digital transformations from start to finish. Contact us today to discover how we can help! "
"6" "While data is the lifeblood for many successful organizations, others are still striving to realize a data-led transformation. In today’s world – where data is constantly generated, sent, and received – harnessing and leveraging it in ways that drive business forward can be a massive undertaking. Creating reports and dashboards to explain and utilize critical data is work that has traditionally fallen on IT departments –  but too often it creates bottlenecks, stretching IT teams thin as they tackle other projects.  If these challenges sound familiar, fear not: self-service business intelligence (BI) may be exactly what you’re looking for. Enabling business teams and users to slice and dice data in ways that provide them meaningful insight is often critical to success. An expert-led strategic adoption of the right self-service BI platform can profoundly reshape enterprise. While the possibilities are nearly infinite, supporting business users and IT, encouraging ingenuity and discovery, and giving you an edge over competitors are among the top self-service BI benefits. Let’s look a little deeper at the potential of each. #1 Enable the Business A common complaint with traditional BI implementations is that often they do not provide business users with the insights they need to drive the business forward. Maybe the operational landscape has changed and reports that were once valuable are outdated, or developers have not been able to make requested updates and modifications to the product. These BI solutions may be cast to the side, turning what was once a promising tool into an expensive source of frustration for end-users.  Possibly the greatest benefit of self-service BI is the way in which it enables business users, empowering them to create their own BI solutions as data becomes democratized across the enterprise. This is not to say that IT is completely removed – but rather that, with a strong data governance plan in place, it can step back from creating BI solutions and begin to shift focus to maintaining data accessibility and integrity. Without the need for IT to be involved in the day-to-day operations of your analytics, they are freed to tackle other high priority tasks, like rolling out new features that improve your customers' experiences. As business users are empowered to solve problems by leveraging enterprise data, IT is able to ensure that users are employing validated data. With bottlenecks removed, BI begins to move at the speed of the business. #2 Create Citizen Data Scientists Once a realm solely for developers, robust self-service BI platforms, like Microsoft Power BI, enable anyone to create and distribute powerful analytics solutions. Embracing modern BI’s low-code and no-code tools lowers the barrier to entry, equipping more business users with the ability to take action. Such tools allow even nontechnical users to easily and securely access data to gain insights. When business users begin creating applications designed to enable their own data-led decisions, they become “citizen data scientists.” Need to create a quick visualization of sales numbers for the board meeting this afternoon? No need to call IT – you can do it yourself right on your laptop! Empowering business users to act quickly and creatively fosters cross-department collaboration that often leads to bonus business insights. #3 Boost Competitive Advantages What good is your business data if it doesn’t help move you forward? Whether you are looking to streamline operations, reduce costs, or stay competitive in the marketplace, data is key. Solely having data is not enough – you must be able to leverage it to take action. Self-service BI puts data in the hands of people who are most familiar with the business and its intricacies, enabling data-driven business decisions. An operations analyst can assess detailed logs from production line equipment and create interactive visualizations that assist in identifying production inefficiencies. Or a marketing director can quickly visualize the effects of marketing campaign on unit sales, without asking an analyst to run a report. Modern BI equips departments that best understand their own business challenges with the capabilities to tackle them. Summary: Self-service BI Plans Drive Gains A well-considered adoption of the right self-service BI tools can be a boon to your business, empowering users to become citizen developers of data analytics solutions, reducing the strain on IT teams, and fostering collaboration across teams and departments. Solid data, a robust BI platform, and rich, interactive analytics can speed your organization toward a successful future. Visit BlueGranite’s Modern Business Intelligence page to discover how a tactical self-service BI deployment can revolutionize operations.  If you are looking for implementation guidance or have questions, BlueGranite’s team of expert data professionals is ready to help! Feel free to contact us any time."
"7" "Analytical Maturity Key to Solving Business Intelligence Puzzle  The world of Business Intelligence (BI) tools is constantly evolving, which can make evaluating a BI solution seem daunting. Organizations have different needs, often based on where they are in analytical maturity. Rather than talking about which BI tool is right tool for you, this article explores how to evaluate different tools to determine the best fit for your organization. As we dive into key evaluation criteria, we will explore which pieces are most important for organizations, based on their analytical maturity. We will cover four different areas: Data Capabilities, Analytics Capabilities, Implementation and Maintenance, and Cost.   Data Capabilities Every BI tool starts with data; no matter which you choose, it must take data from somewhere and do something with it. Different tools have different capabilities when it comes to accessing, refreshing, and manipulating data. Empowering your company with the right deep-insight tools can boost engagement and drive revenue. If your organization is starting to explore analytics, standard data connectors and data preparation abilities take priority. If your organization already has significant analytical capabilities, the speed of data delivery and data scaling likely rank highest. Here are some capabilities you'll want to keep in mind during your search: Standard Data Connectors: These are the connections to data sources available to your BI tool right as it is installed, and may include both standard, out-of-the-box connectors, and the option for custom connectors for data sources not covered by what’s shipped with the product. Data Preparation Abilities: BI tools often come with the capacity to apply data preparation steps to data received before it is processed into the data model. Having robust data preparation abilities can take a significant portion of the data cleansing and put it on your analysts, rather than your data professionals. Speed of Data Delivery: This refers to the speed at which data is automatically pulled from data sources and surfaces in the tool. It involves the frequency of refresh (how many times per month, week, day, hour, minute, or second) and the speed of processing and loading relevant data. Data Scaling: This relates to how a tool adjusts to handling increasingly larger sets of data. Some tools have hard limits (data-size limits), others have computational limits (no data limits, but performance issues make them impractical to use), while others can scale up with infrastructure adjustments. Analytics Capabilities Now that you have ensured you can access your data, the next question to ask yourself is what can this tool do with your data? When you hear about all of the things a tool might be able to do, keep in mind who in your organization will actually work with the tool – both on the development side and the consumption side. If your organization is newer to analytics, you likely will not have many resources dedicated to full-time development with such tools, and interactive dashboards will probably be new to your report consumers. Accordingly, visualizations and ease of use will be key to a successful implementation. If your organization has significant analytical capabilities, calculations and advanced analytics will be far more important to success. Let's explore these criteria further: Visualizations: The ability to successfully visualize data is likely one of the most important Business Intelligence tool capabilities. Visualizations must be clear and visually pleasing to be effective. A robust library of available visualizations and/or a way to create and use custom visualizations is critical in a BI tool. Ease of Use: There are two main groups of users for any BI tool: report developers and report users. It is important to consider ease of use both for those using it to create reports and those using it to review reports. Calculations: This refers to a Business Intelligence tool’s capacity to apply a layer of calculations over a data model, and the complexity of those calculations. It is important to consider how similar this may be to other data languages already used by your report developers. Advanced Analytics: As the analytics world continues to move deeper into artificial intelligence and machine learning, how does this tool enable those abilities and/or integrate with other tools that can do the heavy lifting? Implementation and Maintenance Knowing what a tool can do with the data it can access is critical, but you also need to understand how the BI tool surfaces analysis to your organization. If you are looking for a tool that can launch your analytics journey, this topic might seem like overkill, but it is critical to think ahead when it comes to how this tool will be used by your organization. Although full adoption of a BI tool may take a significant amount of time, branches of use can spring up very quickly, potentially posing risks if you are not prepared. Each of these topics only becomes more important as your organization’s analytical maturity increases. Security: How does the BI tool handle security with accessing reports and/or data points within the reports? It is important to ensure that the right people have access to the right information. Governance: This is an extremely important topic, especially when considering how to scale up the usage of a BI tool to an organizational level. How can the BI tool help ensure governance policies as more report developers publish data to more areas of the organization? Administration: What administrative capabilities does the tool offer? This one can be a double-edged sword, as there is often a point where the increase in administrative ability becomes too onerous without a dedicated administration effort. Infrastructure Needs: BI tools often mix cloud-based and on-premises resource use. It is important to know how the tool can maximize your resources, especially as data models grow in size and computational complexity. Cost The last area to consider when evaluating a BI tool is often one of the most important – cost. For organizations that do not have a significant analytics presence, implementation and soft costs should be a significant part of the decision-making process. For organizations that are looking to advance their analytics presence, scaling costs will be extremely important to understand. Implementation Costs: The costs to start using a tool usually include the purchase price of the software and/or hardware, and any licensing costs for BI tool users. The licensing costs may be applicable to report users, as well as report developers. Soft Costs: Although implementation costs are usually fairly clear, BI tool soft costs can be very hard to quantify. How much time will training on this tool take away from the report developers and report users? Are there any resources available to reduce these soft costs to your organization, such as live help, significant documentation, official trainings, etc.? Scaling Costs: As your organization scales up the use of the BI tool, how will your costs increase from a licensing, infrastructure, and soft cost perspective? Careful evaluation of your organization’s current needs — and how those might evolve in the future — can save money and time. We hope these key criteria can help. If you’re exploring Microsoft Power BI’s high performing, interactive data and analytics capabilities, visit BlueGranite’s Modern Business Intelligence page to discover a wealth of information on how this tool delivers enterprise-wide value.  "
"8" "In some of my previous posts, I've talked about using the Databricks Runtime for Genomics for scaling up common bioinformatics analyses using Apache Spark. Today, I want to highlight a rather new package that further enhances our ability to perform genomics-based workloads in Azure Databricks: Glow. Project Glow began out of a partnership between the life sciences team at Databricks and the at Genetics Center at Regeneron.   About Glow Glow is an open-source and independent Spark library that brings even more flexibility and functionality to Azure Databricks. This toolkit is natively built on Apache Spark, enabling the scale of the cloud for genomics workflows. Glow allows for genomic data to work with Spark SQL. So, you can interact with common genetic data types as easily as you can play with a .csv file.  Learn more about Project Glow at projectglow.io. What's Included? Glow already includes easy-to-use functions for reading and writing common file formats like VCF, BGEN, Plink, or GFF3. In addition, there are tools for performing the following secondary and tertiary analyses: Secondary Analyses Tertiary Analyses  Perform variant quality control Perform liftOver genomic conversions Perform variant normalization Split multiallelic variants Prepare genomics data for machine learning   Parallelize common bioinformatics tools with Transformer Utilize Python statistics libraries Perform GWAS regression tests  Learn more about the features of Glow here: https://glow.readthedocs.io/ Why Do We Need Scale? Genome-wide association studies (GWAS) correlate genetic variants with a trait or disease of interest. These types of studies are effective in the identification of particular mutations and how they affect the disease in question. Traditionally, these analysis are performed by bioinformaticians and genetics and workstations, which have a limit in their processing power. As genetic sequencing becomes cheaper and more prevalent and as study cohorts have increased in size to millions, there is a need to robustly engineer GWAS to work at scale. Luckily, the Azure cloud, Apache Spark, and Databricks are built for just that! Demo Video In the following video, I give a quick overview of some nice features from the Genomics Runtime in Azure Databricks and how to get started using the Glow package.     "
"9" "In part one of my Power BI Performance series, I talked about the importance of being able to understand the overall architecture of Power BI so that you could better understand where your performance problem may lie.  In this post, it's much more simple because we are going to focus on the performance of your visuals.  This is going to get a lot more detailed but that is to be expected because we are rolling up our sleeves and really digging into the material now.  In all honesty, I probably should have lead with this section because this is usually how you are first going to experience performance problems occur in Power BI.  Let's face it, Power BI is all about visualizing data so that you can gain new insights about your data.  If your users are staring at spinning dots instead of visuals, they aren't learning anything and their razor thin attention spans are at risk of being grabbed by something else.  Websites, apps, games - it's all about how fast you can get information to the user to grab their attention and hold it.  How fast is fast?  I spent a good chunk of my early career in managed hosting (the cloud before the cloud) and the rule of thumb we lived by was an eight second load time.  Eight seconds is a lifetime now and if you are still using that as your benchmark, you are frustrating your users.  It's much more reasonable to expect users to start complaining if they are routinely experiencing load times over three seconds. Anyone who has ever worked in IT is familiar with the problem here though, one user's slow is another user's normal so how do we make this conversation about speed more objective than subjective? Enter the Power BI Performance Analyzer View.  This view in the Power BI Desktop tool was released in May 2019 and it provides the definitive tool for understanding how your visuals are performing.  To access this view while in the report view of Power BI Desktop, first open a blank page in Power BI Desktop, then select 'View' from the ribbon and choose the Performance Analyzer icon from the ribbon.  This will open a new section in Power BI Desktop shown below:  Opening a blank page first will allow you to have control over what visuals you want to capture performance data from.  Click on 'Start Recording' and you are ready to start testing load times for your visuals.  Select the page that you want to analyze and the tool will collect information on how long your visuals took to render.  As an example, I am going to go through in detail how to use the results from the Performance Analyzer to understand the performance of your visuals.  I downloaded the sample PBIX from the Power BI Documentation at Microsoft.com - https://docs.microsoft.com/en-us/power-bi/create-reports/sample-datasets and I will use the visuals from the Net Sales report in the screenshots that follow. I am going to walk through how I would approach looking at the performance of this visuals on this report and show what we can learn from the data that the Performance Analyzer gives me. With the PBIX file open, I created a blank page and put it next to the Net Sales report in the bottom page navigator so that I could save the PBIX, close it, then open it again and have it start on the blank page.  Doing this lets me get the Performance Analyzer started and recording without having loaded any visuals into memory.    So, with the PBIX reopened and on the blank report page, I start the Performance Analyzer and select 'Start Recording'.  I browse to the Net Sales report page and it captures the timings for the visuals. Information overload right?  The first thing I do is click the arrow next to Duration (ms), and select the options to Sort By - Total Time and leave the default sort type to Ascending.  This gives me the output in a way that I can easily see the slowest visual and how long that visual is taking to render.    Let's take a look at the last visual, 'Category Breakdown' and see what we can learn about this visual that took the longest to run (794ms).  If I click on 'Category Breakdown', it will highlight the visual on the Power BI Desktop canvas so that I can easily determine what visual's metrics are highlighted.  Clicking on the plus sign will expand the entry (as shown in screenshot) and show details on how long the DAX took to run, how long the visual took to render, and a mysterious heading named 'Other'.  If you hold your mouse over 'Other', it will explain that the 'Other' category captures time that was spent waiting in a queue or waiting for the system to finish with other visuals.    Clicking on 'Copy Query' will allow you to copy the DAX query for that visual to your clipboard.  This means that you can then copy the DAX code to DAX Studio and execute it there as well.  DAX Studio has built in tools that will let you dissect the execution of your DAX code so you can really drill in on what is running slow. More on this in Part three of the series. Selecting 'Refresh Visuals' will allow you to refresh the visuals on the page to create another capture of the times it took to execute.  Note that Power BI Desktop will cache content so after the first time you load the visuals so when you select 'Refresh Visuals', you may notice that its a bit faster after the first load.  To get around this, stop the performance analyzer and start the recording again, then select 'Refresh Visuals'.  To go expert mode, attach DAX Studio to your Power BI instance and use the 'Clear Cache' setting in DAX Studio to clear the cache between refreshes. Knowing how to use the Performance Analyzer view will really help you start to quantify what is running slow and what 'slow' means.  Performance analysis is often an art form and there are no easy answers as to how to improve performance but getting a benchmark is a great start. Let me leave you with some tips and tricks that I have collected for visualization performance. 1. Be smart about the grain that you choose for your visuals.  Use a design pattern that shows aggregated, or rolled up data at a high level on your more popular reports.  Use the 'Drill Through' feature to allow those few users who really need to dig down into the data to do so on less commonly used report pages. 2. Remember that more visuals means more DAX Queries analyzed.  If you have a slow page, you can see this as you will see visuals sometimes render one at a time. 3. Resist the urge to duplicate Excel Spreadsheets in the Table or matrix visual.  Just because you can, doesn't mean that we should.  Again, aggregate, roll-up and leave the high grain, spreadsheet visuals to drill through pages. 4. Pay attention to visual interactions.  If you have a lot of visuals and don't need them to interact or cross filter each other, disable the interactions between them. 5.  The capability to sync slicers across pages is cool, but understand that there can be performance implications if you are syncing a large amount of slicers across multiple pages.  Try disabling the synchronization to see if performance improves. 6. DAX has some behind the scenes optimizations that it will do to try and reduce the number of round-trips that have to be made to retrieve data from the storage engine.  Just like SQL has the query optimizer, DAX is always watching queries to see if it can combine them to get the data in less requests.  If you use a bunch of visuals like cards on your page, the DAX engine won't be able to combine those requests because each card is a single request.  If you combine your cards into a single visual, perhaps using the multi-row card or the Table/Matrix visuals, you give Power BI a chance to combine requests.  You can see how Power BI performance can be complicated: in step 3 I tell you not to use the Table or Matrix visual but here I mention that its use could improve performance.  To learn more about this internal engine optimization, go check out Phil Seamarks blog post on DAX Fusion at https://dax.tips/.  Armed with the Performance Analyzer view, you can really start to get an idea about what the slow visuals are that are slowing down your pages.  If it's not one of these issues above causing the slowness, then what?  Odds are you are now in the unenviable position of having to optimize your custom DAX measures that are driving those visuals.  Optimizing DAX truly is an art form but if you have followed the tips from the first article in this series and the tips that I have included above aren't helping either then you need to look for part three in my Power BI Performance series where I start to grind down into the details of optimizing your DAX code."
"10" "Last week at Build, Microsoft announced that Azure Synapse Analytics is moving to Public Preview. For most of us, this represents the first real chance to get hands on with the truly new elements of Azure Synapse since the exciting announcements at Ignite (which I wrote about 6 months ago). In fact, I've been excited about the potential of Azure to truly transform the rules of the game with the advent of true, cloud-scale analytics for some time now.   Over a year ago, Azure SQL DW, the origin of Synapse Analytics, had established itself as the clear top performer in providing traditional data warehouse technology with the provisioning, scale, and performance of the cloud. With the release of the updated feature set into Public Preview, we are seeing the next phase in industry leadership as we move towards a unified experience, integrated services, and a platform that truly delivers insights for all! Not long ago, we used this simplified (trust me, I know!) slide to provide a high level overview of a Modern Data Platform. If you noticed it, Azure Synapse Analytics in this diagram is really just the initial re-brand of Azure SQL DW, as the real transformation of Synapse's capabilities remained in the realm of announcements and private previews until last week…  Now granted it's only now in Public Preview (and subject to continued improvements!), Synapse is redrawing the landscape dramatically: In fact, for some applications, the column on the right can be omitted as Synapse provides integrated Spark notebooks and integration or Azure Machine Learning for ML/AI as well as native Power BI modeling and reporting capabilities! The new Azure Synapse Studio interface alone has dramatically improved the experience of numerous roles within any organization that runs on data. The Modern Data Platform: not just logical anymore! For the last few years, the Modern Data Platform has been a logical concept that, in application, required the stitching together of numerous disparate technologies with various types and states of user interface, security integration, languages, and experiences in order to make everything deliver on its promise. It worked - one of the benefits of using Azure is Microsoft's ongoing ability to provide service integration and security - but it was somewhat more challenging than it needed to be for more basic scenarios in order to bring a full spectrum of capabilities to the most complex scenarios. In technology at least, c'est la vie, non? But Azure Synapse Analytics is making a clear statement… equal parts unified experience (Synapse Studio), service integration (ADF, Power BI, AMLS, etc), and net new services (serverless/on demand SQL, Spark pools, etc), we can expect a far more streamlined technical landscape as we implement modern data platform solutions for organizations moving forward. Organizations exploring the expansion of their analytical capabilities with Spark have a minimal-friction access point with Synapse, and a more unified focal point to build their teams and skillsets around as they look to leverage the capabilities of Azure. A look towards the future Much of the Public Preview has been closing some loops on capabilities and interfaces that I personally have been excited about for the past 6 months, but one feature in particular that was not previously on my radar is worth calling out. Initially, it is very limited just now, but the pace of change is both rapid and inevitable, and this is one to keep an eye on! Hybrid Transactional Analytical Processing, or HTAP, implemented in Synapse as Azure Synapse Link, is not a brand-new concept. SQL Server has been blurring the lines between Transactional and Analytical data with column-store indexes and in-memory transactional tables for years, and the HTAP acronym is not owned by Microsoft. However, what we see in the Synapse Preview is a cloud-native HTAP implementation that combines Azure Cosmos DB with Synapse Analytics – it’s built for cloud-scale analytics and may well represent a significant paradigm shift that reduces costs and improves accessibility for powerful AI and BI across a wide range of scenarios. This one is going to be fun to explain to my less technical friends, as my own excitement will make it harder to slow down and help connect the dots on why this is just so cool…In case this is still feeling a bit fuzzy on what it means, here is a nice picture from Microsoft that highlights the new Synapse Link allowing Synapse to reach across to Cosmos DB and directly interact with the available analytical store in that system. No data movement/transformation to get in the way… just a much broader canvas and faster tools to use when working in the analytics studio! (source: https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link) Although this presently only works with Cosmos DB, the concept will certainly extend to additional data repositories in the future. This has the potential to dramatically reduce the ETL/ELT workload for certain data sets while dramatically increasing the real-time or near-time analytical capabilities surfaced by Synapse – and I for one am excited for that future!"
"11" "The financial services community has a long and storied history of computer-controlled record keeping. For many years, IBM mainframes and their applications were the solution of choice for large-scale, online transactional processing systems. While these mainframe systems served users’ transactional needs well, they did not keep pace with their users’ analytical needs. Organizations found mainframe-sourced data expensive, inaccessible and difficult to use. Mainframe vendors charge a premium for their systems and their operation. The storage and processing power required for large-scale analytics is cost prohibitive compared to modern, cloud-based tools. Mainframes have a fixed capacity, so customers must pay for a system based on peak demand. In contrast, Azure has a pay-as-you-go model, allowing customers to scale systems up or down according to their needs.  Mainframe systems pose many problems to IT professionals:  Some mainframes require specialized knowledge to access and extract data for analysis in modern tools. Users might need to use antiquated console development tools and query environments to analyze data. Mainframe databases use obscure, coded names for tables and columns. Modernizing to Azure gives organizations easy data access, user-friendly interfaces, and more convenient development tools. Moreover, it closes the potential skill gap as users and developers of all levels of experience are empowered to extract and analyze data. Azure also allows organizations to leverage powerful self-service business intelligence tools and cutting-edge AI platforms that provide a competitive advantage. Self-service tools like Power BI allow users to easily organize and present data without developer support. In fact, it allows out-of-the-box analytics and visualizations that are virtually impossible to recreate with legacy tools. Who Wants EBCDIC in a Unicode world? At BlueGranite, one of our clients was struggling with this very issue. They only had indirect access to their mainframe data, meaning they could only access the data encapsulated in thousands of preformatted reports. Unfortunately, those reports often didn’t give them exactly what they wanted. To compensate, some users had created in-house applications that would scrape the data they needed from these report outputs. Obviously, the existing reports were not ideal for meeting the demands of today’s data-driven world. To solve this problem, BlueGranite first designed a solution to continuously import these thousands of report files into an SQL database. But we soon realized that with such varied data, a traditional ETL approach would require constantly maintaining ETL code.  Something else was needed — the situation called for a metadata-driven approach. With a few bits of metadata maintained by the client, we designed a solution that allowed them to dynamically modify their data export process without developers or custom code. Then, based on that metadata, the solution would automatically create everything necessary to import new or updated reports, which dramatically reduced the need for maintenance. For the users, the data could now easily be reached by querying a much cheaper, more modern database. The technically savvy, like those who created those data scraping applications, were happy to import the data from there and flush, filter, slice and dice the data to their hearts’ content. That’s great for the technically savvy, but what about those who live and die in Excel? For the legions of Excel crunchers, an Excel front-end seemed most appropriate. This solution delivered the data where the analysts felt most comfortable.  With easy access to the data, they too can filter, slice, and dice the data to their hearts’ content Scaling and Looking Forward Our first iteration used on-prem tools to prove out the technology stack and user tools. While we were building that first iteration, the client’s network and security team vetted Azure’s modern data platform services for compliance with their standards and procedures. We demonstrated that the technology stack and user tools met their complex reporting needs, and that it eliminated time-consuming developer turnaround. When it was time to scale the solution, we wanted to use Azure’s elastic scalability and pay-as-you-go model, as it required fewer upfront costs than scaling out an on-premises solution. Throughout this period, the security team and networking team had grown more comfortable with Azure. Azure had also introduced a few new security features that complied with the client’s need for private networking. With these factors in mind, it made sense to try a migration to the cloud. Despite having to navigate the intricacies of tight security measures and a new ETL architecture, the company’s new Azure solution offers nearly unlimited scaling and the beginnings of a data lake to boot. It also laid the foundation for a Power BI implementation that will allow even richer self-service BI solutions in the future. How Can We Help? With a solution that helps our client reduce maintenance, improve access to information and lower costs, their business is in a better, more sustainable place.  In these times where the future becomes less certain, two things become increasingly critical: Clear visibility and attribution of resources for data & analytics workloads Elasticity to scale up and down according to need while minimizing overspend At BlueGranite, we can help you gain that insight with our highly trained team of data professionals and implement cost-conscious, scalable solutions. To see how we can help you optimize your data infrastructure, get in touch today!"
"12" "If you take your car to the mechanic, throw your keys on the counter, tell them your car is performing poorly and walk out, odds are the problem won't get fixed.  For the mechanic to be successful, they need to know the answers to the following questions:  Which part is performing poorly? When did the problem start? Has there always been a problem? Is there anything that makes it work better? Does it only malfunction under certain conditions? These questions can also be applied to any performance problem in any system — and Microsoft's Power BI is no exception. But if you'd like to improve your Power BI performance, you first need to understand the major parts of Power BI and how they work together so you can better isolate the issue and quantify what your performance problems are.  What are the components of Power BI?  If you are creating reports, your first exposure to Power BI is usually through the Power BI Desktop, Power BI's free integrated development environment that gathers, transforms and models data for the reports you create.  Hidden under the hood is a full-featured instance of SQL Server Analysis Services Tabular Engine, which hosts the in-memory data models you create. Speaking of data, Power Query Editor wrangles and mashes data that is built in Power BI desktop, allowing you to import and transform data from various data sources using either a graphical interface or manually written scripts in the Power Query Formula Language (otherwise known as M).  Data can either be imported into a data model that is stored in your data file, or you can use a feature called Direct Query to connect to data stored in a different model such as Azure Analysis Services to build reports without pulling all of the data to your Power BI desktop instance. Once you have built your model and your reports, you publish these reports to the Power BI Service, which has two tiers of service: Power BI Pro and Power BI Premium. Improving Power BI performance Now that we have talked through the general Power BI system components, let's talk performance!  The scope of this blog will cover import models (where data is imported to Power BI Desktop and built into a data model) in the Power BI Pro service tier.  Power BI Premium and Direct Query performance tuning will not be included in this blog post, but if there is interest in those areas, please let us know. In part I of this performance series, we will look at improving performance in your model, the heart of an import Power BI report solution. Turn off auto date/time Hands down, reducing the model size will provide the biggest return on investment for hours spent optimizing Power BI performance.  Tabular model speed and efficiency come from loading data into RAM and working with it there. Thus, the smaller your model is, the faster it will be.  Even faster than RAM is the L1 and L2 CPU Caches in today's modern CPUs. To put it simply: smaller is better. My very first step when I look at a new model is to see if the user has Auto Date/Time enabled in Power BI Desktop.   Power BI documentation describes this feature as allowing report authors to easily use calendar time periods to drill down into your data without you having to build out a specific Date dimension.  This sounds like a great idea at first, but its implementation means that for each date column used in your data, a hidden date/time table is created in your model.  With a reasonably large amount of data at a daily grain, you can easily start to chew up a lot of memory on these temporary date tables.  How much memory, exactly?  A recent customer model that I reviewed went from 860mb to a mere 122mb — or a 7:1 compression — just by turning off this feature.  Important note: If you haven't built your own date/time dimension tables, turning  off Auto Date/Time will break your reports — but that just gives you all the more reason to set up your own dimension tables now. By the way, since these are hidden tables, you won't even know that Power BI is creating them in the background unless you connect with Dax Studio, so you could be running a lot of bloat that you don't even know about. To see if Auto Date/Time is enabled, Select File from the ribbon and choose 'Options and Settings - Options' as shown.    Once the options screen comes up, scroll down to the 'CURRENT FILE' section and look for 'Time Intelligence' as shown.  Make sure that the box is unchecked.  Check this one first, you won't ever be able to do anything else with such a high rate of return as this step. Remove unnecessary columns Our next performance tip is also easy to do: get rid of ALL the columns you have pulled in that you are not using.  Even if it's one of those columns that you think you might use later, do yourself a favor and get rid of it.  Smaller is better, remember? If you aren't using the data, get rid of it. Eliminating these columns could make a huge difference when it comes to compression of the data model and things like optimizing sort order. Tabular databases are built to be stored in memory, so the less data you have stored, the better.  Tabular databases shift the IO pain from disk to CPU & RAM and, let's face it, CPU and RAM speed increases have far outstripped disk IO performance.  This means that you have to be able to store large amounts of data in RAM or CPU cache. To do this, tabular databases such as the Vertipaq engine in SSAS Tabular and Power BI use highly efficient compression algorithms to store the data.  The details of the algorithms are outside the scope of this blog post, but simply put: You get better compression with lower cardinality (i.e. unique) data If the engine can find a way to sort the data to increase its compression, it will do that.  For example, if the Vertipaq engine can replace a commonly occurring string or number with a smaller variable, it will do so to save space.  Columns that have highly unique data like DateTime columns or identity columns should be removed if they are not being used because the Vertipaq engine will struggle to find ways to compress those columns. Build efficient models It's easy to forget why we build reporting models in the first place.  When you design a typical database, you attempt to: Organize or normalize the data in a structure that reduces redundant data Enforce relations between the data structures that align with the business logic behind the data. Oftentimes, the emphasis with databases is on writing high amounts of data and saving space to limit disk IO.  To this end, data in these databases is often highly normalized to reduce redundancy and provide structural integrity.  For reporting models, however, the design specifics are different.  Reporting models are usually not updated as much or in real-time, and the focus is on reading large amounts of data quickly so that it can be aggregated and displayed.  For the data models used in Power BI Reporting, star schema models are the gold standard.  Star schemas have fact tables and dimension tables, and when diagrammed, the dimensions typically align with the fact tables giving it the star shape shown below:    There is a closely associated model design referred to as the snowflake model where the dimensions can be normalized into further sub dimensions to reduce the redundancy of the data.  You end up with a diagram that looks more like this:    From a performance perspective, the reason a snowflake schema is not recommended is that each of those sub dimensions represent an additional JOIN statement in your queries.  This presents difficulties in Power BI, where instead of simply joining a fact table to a dimension table, a snowflake schema creates as many joins as you have sub dimension tables — for example, two joins in the diagram above.  Since joining these tables together hinders performance, the less you have the better. The immediate question, then, is why you wouldn't use anything other than one large, flat table (think of a spreadsheet) without breaking out fact and dimensions?  While that technically might work, you can run into other problems with that design. A large flat table will be harder for your users to navigate if they want to make reports from your model or even understand how it is put together.  Adding another fact table in the future that uses the same dimensions will be more difficult. The wider the table, the harder it will be for Vertipaq to find its ideal sort to compress that data.  Several performance documents from Microsoft also seem to indicate that the internals of DAX and the Vertipaq engine will run best against a star schema model. Recap So, that’s part one of my Power BI Performance series.  We talked about model size and design because that’s usually the place where you really can make the maximum impact on performance with the least amount of work— so it's best to start there.  In later blog posts in this series, I will look at the performance of your visuals and reports, as well as what not to do as a Power BI Developer if you want quicker refresh times and a better IDE experience in Power BI Desktop. Struggling with Power BI Performance issues?  Join our weekly Power BI Office Hours to learn how we can work with your business to help you be more successful with Power BI."
"13" "Sample data is critical for learning data systems and techniques, developing proofs of concept, and performance testing. And while sample datasets are easy to find, a limitation shared by most of them is that they are small. This is fine in many cases, but if you really want to evaluate a big data system, you’re going to need big data. Unfortunately, big datasets are difficult to find. One solution to this problem is the TPC-DS benchmark dataset. Instead of having to download and move this data to the desired location, it is generated using software provided by the TPC (Transaction Processing Performance Council).  The TPC-DS dataset has some important advantages; the first being that it is variable in size – supporting datasets up to 100 terabytes (TB)!  Another big advantage is that the data is modeled as multiple snowflake schemas, with fact and dimension tables having realistic proportions. This makes the dataset representative of typical data warehouse workloads. A nice summary of the TPC-DS benchmark can be found here. Databricks spark-sql-perf Library You can run the data generator as is from the TPC (dsdgen) on your personal computer or other machines, but the features are limited and it’s difficult to impossible to generate data at the larger scales on modest hardware. This is where the spark-sql-perf library from Databricks comes in handy. The spark-sql-perf library allows you to generate TPC-DS data on a Databricks cluster size of your choosing, and provides some important added features, such as: Additional file storage formats, such as Parquet File partitioning Database creation with optional statistics collection With Databricks, you can use a powerful cluster of machines to generate the data at any scale, and when you’re done you can terminate or delete the cluster, leaving the data in place. Generate Data The BlueGranite GitHub repository tpc-ds-dataset-generator contains everything you need to generate the data except a storage account. Below are a few sample results from generating data at the 1 and 1000 scale.  File Format   Generate Column Stats   Number of dsdgen Tasks   Partition Tables   TPC-DS Scale   Cluster Config   Duration   Storage Size   csv   no*   4   no   1   1 Standard_DS3_v2 worker, 4 total cores   4.79 min   1.2 GB   parquet   yes   4   no   1   1 Standard_DS3_v2 worker, 4 total cores   5.88 min   347 MB   json   no*   4   no   1   1 Standard_DS3_v2 worker, 4 total cores   7.35 min   5.15 GB   parquet   yes   1000   yes   1000   4 Standard_DS3_v2 worker, 16 total cores   4 hours   333 GB  * Attempting to generate column stats with csv and json both resulted in error. Explore the Data Let's take a look at how the data can be used for demo purposes. In this example we'll query the same data stored as both uncompressed delimited and as Databricks Delta. The cluster has four Standard_DS3_v2 workers. First let's query the delimited data. The query is simple, but it involves a 418 gigabytes (GB) fact table that contains 2.9 billion rows. %sqlUSE tpcds001tbadlsgen2;SELECT date_dim.d_year,SUM(store_sales_delimited.ss_quantity)FROM store_sales_delimitedINNER JOIN date_dimON store_sales_delimited.ss_sold_date_sk = date_dim.d_date_skGROUP BY date_dim.d_year This query took 12.72 minutes. Now let's convert the data to Databricks Delta, which stores the data as parquet. %sqlUSE tpcds001tbadlsgen2;DROP TABLE IF EXISTS store_sales_delta;CREATE TABLE store_sales_deltaUSING DELTALOCATION '/mnt/adlsGen2/tpc-ds/SourceFiles001TB_delta/store_sales_delta'AS SELECT * FROM store_sales Parquet is highly compressed, and the data now sits at 141 GB. Let's run the same query against the data stored as Databricks Delta. %sqlUSE tpcds001tbadlsgen2;SELECT date_dim.d_year, SUM(store_sales_delta.ss_quantity)FROM store_sales_deltaINNER JOIN date_dimON store_sales_delta.ss_sold_date_sk = date_dim.d_date_skGROUP BY date_dim.d_year This time the query took only 1:35 minutes, which is 9.4 times faster! This is just one example of how it's helpful to have big datasets for testing. If you'd like to generate big datasets for yourself, head over to the BlueGranite tpc-ds-dataset-generator repository on GitHub to get started! Dive into Databricks Interested in how we’ve put Azure Databricks to use for others? Visit our Databricks resources collection to discover how we’ve used it to implement predictive maintenance to cut operational downtime; explore a retail analytics product dimension load using Databricks, or watch our webinar to learn how Databricks eases AI in the cloud.   "
"14" " Right now it seems like time is accelerating while data is trailing behind. If you are like me, then you have been voraciously reading the news and latching on to any new information that comes out. But uncertainty still looms large. As a Data Scientist and an Economist, I am accustomed to big data sources that are high-volume, high-variety, and/or high-velocity. Now the problem has reversed. Reliable economic indicators are few and far between while individuals, organizations, companies, and governments are all trying to understand how their own local economy is affected. In this data drought, I will show how BlueGranite uses Power BI to leverage historical employment data with early indicators to measure county-by-county unemployment risk. Power BI Interactive Report    How High Can Unemployment Go? Put bluntly, the economy is not doing well amid the COVID-19 pandemic. Companies are reducing hours, furloughing, or outright laying off workers. Social distancing is changing both the ability of people to work as well as demand for goods and services. Economists at the University of Chicago recently estimated that 63% of jobs in the US (representing 56% of wages) cannot be performed from home [1]. However not all of those 63% of jobs are at risk. But how many are? Who and where are the most affected? And by when can we know? Since the lag in official government data is long, I will consider the first two questions using already available information. One early indicator on the health of the labor market is the initial unemployment claims from the US Employment and Training Administration. This reports on a weekly basis the number of employees recently laid off who applied for unemployment benefits. It is an imperfect measure since it excludes many groups, such as those with informal jobs, gig workers, the self-employed, and those who choose not to apply or simply do not know they are eligible. (Note: This list varies by state and the recent CARES Act passed by Congress has changed some requirements.) Regardless, the surge in unemployment claims beginning mid-March is both intimidating and unprecedented. Initial Unemployment Claims (weekly) click here for live report Despite the aforementioned caveats, there were over 30.3 million applications for unemployment benefits submitted in the six-week spike shown above. Evaluating this with the 205.6 million working-age population in the US means that approximately 14.7% of working-age adults have applied for unemployment benefits [2]. This is a staggering number when compared to the unemployment rate that peaked at 10% during the Great Recession and 25% during the Great Depression. On May 8th the Bureau of Labor Statistics confirmed that April unemployment did in fact reach 14.7% [3]. A glass-half-empty person will note that unemployment may get worse next month, but a glass-half-full person will point out that this hopefully a short-term low point before the economy bounces back. Regardless, we should expect higher unemployment to persist throughout the year since business economists forecast the unemployment rate in Q4 of 2020 to still be around 9.5% [4]. The Industry Most Affected These are unprecedented times and organizations need better data in order to make decisions. Instead of waiting months for granular data to be made available, we can instead estimate the population that is at risk of becoming unemployed. The sector most affected, Leisure and Hospitality, makes up a sizable share of the US workforce. Employment by Sector click here for live report This single sector of course does not represent all jobs at risk – far from it. Instead it represents the most salient of industries affected by the pandemic in terms of lost employment. In March 2020 alone, nearly 16.4 million US employees within the Leisure and Hospitality sector lost their jobs followed by an additional 7.7 million losses in April [5]. Change in Employment from Previous Month (March 2020)  click here for live report Your Local Economy Although the above numbers are useful, they only represent estimates at the national level. If we can measure the size of this group by county, then we can estimate the share of the workforce within Leisure and Hospitality that is likely to become unemployed. Without recent information available, I use Q2 data from 2019 to build a map in Power BI to visualize county-level data across the entire United States. The different shades represent a county’s share of Leisure and Hospitality employees relative to the national average. It shows additional county-level information about this sector such as the average weekly wage of a worker, total quarterly wages, total number of establishments, and total number of workers. County-level Employment Data, Leisure & Hospitality Sector (Q2 2019)  click here for live report The map provides some clear insights into the geographic distribution of an industry hard-hit by the current pandemic. In my own state of Michigan, I can see the most at-risk counties are Mackinac and Keweenaw with Leisure and Hospitality employment at 316% and 308% of the national average, respectively. This makes sense given that Keweenaw County is known for its outdoor recreation, including the state’s only national park, while Mackinac County includes the highly visited tourist destination of Mackinac Island. This is in sharp contrast to the remote and sparsely populated counties of Baraga and Missaukee in Michigan with a share of employees in the Leisure and Hospitality sector of only 37% and 32% of the national average, respectively. The Need for Business Intelligence Visualizations like this can help businesses estimate how affected their nearby customers will be. It can also help local governments anticipate local needs as well as how local unemployment can affect their tax base. As data continues to come in, it is now more important than ever for organizations to use modern business intelligence tools to make sense of these numbers.   [1] How Many Jobs Can be Done at Home? https://bfi.uchicago.edu/working-paper/how-many-jobs-can-be-done-at-home/ [2] Working Age Population: Aged 15-64: All Persons for the United States https://fred.stlouisfed.org/series/LFWA64TTUSM647S [3] Bureau of Labor Statistics Employment Situation Summary (April 2020) https://www.bls.gov/news.release/pdf/empsit.pdf [4] April 2020 National Association of Business Economists Flash Outlook  https://files.constantcontact.com/668faa28001/765a4afd-4ed3-4ea0-b98b-ae631771042b.pdf [5] Bureau of Labor Statistics Employment by industry, monthly (April 2020) https://www.bls.gov/charts/employment-situation/employment-by-industry-monthly-changes.htm    "
"15" "Power BI Report Server (PBIRS) is a superset of SQL Server Reporting Services (SSRS), itself a mature BI technology that has been a key component of the Microsoft BI stack for 15+ years. The main difference between the two platforms is that PBIRS brings with it Power BI reports, providing a modern BI experience to on-premises implementations. As is the case with any enterprise BI product, careful planning and consideration of how the technology will be leveraged should be made up-front. This will help avoid speed bumps during implementation and ensure a smooth ride as you roll out your BI and analytics solutions. Whether you’ve been working with PBIRS from day one, or are starting to use the platform for the first time, the goal of this post is to showcase tips and techniques that you can turn around and apply in your own environment. Additionally, for those of you considering a hybrid Power BI implementation (Power BI service and Power BI Report Server), we will compare specific features of each platform below.    This post will cover five specific tips. The topics are listed chronologically based on how a typical implementation normally proceeds.  Configure Your Report Server Understand, Plan, and Implement Folder Level Security Leverage Analysis Services for Reusability Simplify Administration Using PowerShell Monitor Your Report Server   Configure Your Report Server After installing your report server, you should consider reviewing some of the default configurations. We’re not referring to configurations found in the Reporting Services Configuration Manager, but rather the advanced properties you can tweak from within SQL Server Management Studio (SSMS). For many implementations, the default values of these properties will be fine. However, there may be some specific use cases for when you would want to change a setting. The advanced properties in PBIRS are akin to Power BI Service’s tenant settings in the Admin portal. First, to view the advanced properties dialog, you need to connect to your report server from SSMS. From Object Explorer select Reporting Services. Enter your credentials to connect to your report server. Right-click on your instance name and select Properties. The Server Properties dialog box will appear; select Advanced on the left-hand pane.   From the advanced properties menu you’ll notice a whole host of options you can toggle. Below are three specific properties that we have modified for some of our customers’ implementations. EnableMyReportsThis setting is disabled by default, but many organizations prefer to enable this. The feature will provide every report server user with their own dedicated folder on the report server. This is similar to the My Workspace concept that users have within Power BI service. Enabling My Reports is a great way to encourage self-service BI. ExecutionLogDaysKeptThis setting allows you to toggle how many days of report execution history are stored in the underlying ReportServer database. The default is 60 days. Increasing this value will collect more metadata, thus allowing you to analyze adoption over time. We talk about how you can monitor your report server in tip #No. 5. EnablePowerBIReportExportUnderlyingDataThis setting tells PBIRS what level of detail users can export data from a visual. In our experience, exporting underlying data can lead to major performance bottlenecks if you’re not careful. This is especially true if the underlying model is large (both in terms of data and attributes/measures). For this reason, we encourage customers to disable this setting. Most of the time users will simply be happy to export the raw data that they see within the visual they are exporting from. Understand, Plan, and Implement Folder Level Security When it comes to storing content like Power BI and paginated reports, Power BI Report Server and Power BI service are very different. Within Power BI service, we have the concept of App Workspaces and Apps. App Workspaces serve as collaborative sandboxes for teams to develop Power BI content in unison. Apps can then be created to publish the content to a broader audience of read-only consumers. These concepts do not exist in Power BI Report Server. Instead, we can create (and secure) folders. Folders within Power BI Report Server (and SSRS) conceptually behave like folders within a file system. Folder-Level Security can be applied to restrict access to all content within the folder. Also, much like a file system, a folder hierarchy can be created. This is different than the flattened nature of App Workspaces within Power BI service. However, whether you are hosting reports in the service or PBIRS, careful planning must be done up front to properly secure your content. Typically, it makes sense to create folders for different departments or subject areas (e.g. Financials, Marketing, Sales). In PBIRS, you can also define security on individual items ((e.g. a single Power BI report), but we typically steer customers away from that practice. In larger, enterprise deployments, it is not uncommon to have hundreds of reports. Securing each one individually would be a nightmare to maintain. Thus, the primary reason we advocate creating subject-specific folders to secure multiple related reports for a subset of users. In most cases, we also advise customers to stick to a flattened folder-structure. This not only makes securing folders easier to maintain, but it also logically adheres to the flattened structure of App Workspaces in Power BI service. This could simplify lifting and shifting your PBIRS content to Power BI Service should you ever explore that option in the future. Lastly, it’s a best practice to use AD groups as opposed to individual AD accounts when granting access to your user base. Much like securing folders over individual items, leveraging AD groups significantly simplifies the maintenance of your implementation.  The root folder (Home) with four folders displayed.    While it is possible to create a folder hierarchy, we typically advise customers to create a single level of folders for transparency and maintainability.   Leverage Analysis Services for Data Re-usability Whenever we are asked about best practices related to Power BI Report Server, this is one of the first ones we mention. Repeat after me, “I will not create silo-ed reports!” While multiple reports can share the same dataset within Power BI service, in Power BI Report Server, each Power BI report can only reference its own data model. For example, if you are anticipating 50 Power BI reports in your environment, there would also be 50 individual data models under the hood, one for each report. For this reason, we are big advocates of leveraging Analysis Services in tandem with PBIRS. Analysis Services is a mature technology that enables the creation of in-memory, enterprise semantic models. In other words, Analysis Services can provide both performance and a single version of the truth. When using a live connection to an Analysis Services tabular model, Power BI acts as a visualization layer only (i.e., you will only see the report canvas, no modeling or data tabs while authoring reports in Power BI Desktop).  Analysis Services (SSAS) models encourage data reusability and consistency across reports.  While fine for self-service BI scenarios, avoid Power BI data models in enterprise scenarios as the underlying data model cannot currently be reused for other reports.   Hybrid Use Case Analysis Services is a great choice if you already have an investment in SQL Server, and its BI components. However, if you are implementing Power BI Report Server as part of a larger, hybrid implementation with Power BI Premium, then you can also leverage datasets that reside in Premium capacity as your reusable data models. You can establish a live connection from your Power BI, paginated, and Excel reports to a Premium dataset as if it were an Analysis Services model. First, you must ensure that your Premium capacity has Read enabled under the XMLA Endpoint setting. Next, you need to obtain the Workspace Connection for the dataset you want to connect to. This is needed to build your connection string in the next step. Finally, you can establish a connection by selecting the following settings: Type Microsoft SQL Server Analysis Services  Connection String Data Source = “<Power BI Premium Workspace Connection>”; Initial Catalog = “<Power BI dataset name>”   Sample connection string for a Power BI Premium dataset. Simplify Administration With PowerShell This is a topic that we’ve blogged about previously. We’re big fans of leveraging the ReportingServicesTools PowerShell module to streamline different deployment activities. The module will work with both PBIRS and standard SSRS.   For instance, we have leveraged this module to perform tasks such as: Exporting reports created by analysts to integrate them into our Git repository. Deploying reports to different report servers (Dev --> Test --> Prod). Displaying a basic inventory of our report server. Monitor Your Report Server Our final tip is all about monitoring report adoption, visualizing usage trends, building an inventory of your report server, and identifying any gaps in security. Power BI Report Server does not have an out-of-the-box monitoring solution. However, the underlying ReportServer database contains all kinds of useful metadata just waiting to be analyzed! NOTE: Important! Develop reports against the underlying ReportServer database at your own risk. The underlying database schema could change with a new release of the product. Within the ReportServer database there are many views and tables that the platform uses to store its metadata. However, when it comes to inventorying and monitoring your report server, there are some primary tables/views to start with.  Object Name   Object Type   Description   dbo.Catalog   Table   Role-playing table that stores metadata for all the different content types found in the report server. This includes: Folders, Power BI, Paginated, and Excel reports.   dbo.ExecutionLog3   View   This view contains execution history for your reports. The view has some built-in CASE statements to provide additional attributes.   dbo.Users   Table   This table has a record for every AD user and AD group that has logged into your report server. This is useful for tracking who is doing what on your server.   dbo.Subscriptions   Table   This table contains records for both paginated report subscriptions and Power BI data model refreshes. This is useful for monitoring what tasks are being automated, and whether those tasks are running properly.    If building your own custom monitoring solution sounds tedious, or isn’t your cup of tea, no worries! Below you can download a Power BI template file for a plug-and-play monitoring solution that we have used for several customer projects. The report requires that your report server be on the January 2020 release or later. To leverage the report, simply enter in your own Report Server instance and database name. The queries will then connect to your own report server’s database and ingest its metadata!  Click here to download the Power BI Report Server Monitoring Template.   When you open the companion PBIRS Monitoring.pbit file, you will be prompted to supply your own server and database name.    The summary page of our PBIRS Monitoring report.    A sampling of just some of the ReportServer tables you can bring into your Power BI model.   BlueGranite can optimize your investment in Power BI with our Catalyst for Modern BI. To learn more about the key pillars of successful deployment and adoption addressed by BlueGranite’s Catalyst approach, register for our upcoming Power BI Office Hours. Check out additional Power BI blog posts from our team at BlueGranite.  "
"16" "Visual personalization has arrived in Power BI, and it is a significant feature that expands viewers’ data exploration capabilities.  For users accustomed to pivot tables and charts in Excel, Power BI now has a similar experience for rapid visual exploration built-in. Users do not need to have access to edit the report or have Power BI Desktop installed to take advantage of this feature either. Personalization is available in the Power BI Service for any end user once enabled for a report.  How To Personalize Visuals BlueGranite guides you through the basics of personalizing visuals in this video:      Does Personalization Work with Custom Visuals? Users do not have the ability to add new custom visuals, but any custom visual that the report author has imported into the report is available when personalizing. It does not matter whether the visual is actively used on the report or not. This means that users can extend their exploration well beyond Power BI’s core visuals.   Can We Format Personalized Visuals? Visual personalization currently only allows users to change the visualization type and the data in a chart. Basic formatting such as changing colors or adjusting axis options are not available. Even the visual size cannot be changed, so either start by personalizing a larger visual or get used to expanding out to Focus Mode to explore the data.   How Can Viewers Save Personalized Visuals if They Cannot Edit and Save Reports? The easiest way to save visuals is to use Personal Bookmarks. Once a user modifies a visual, save that view as a personal bookmark in the Power BI Service. Personalized visuals will not persist if a viewer simply closes and re-opens a report. They will appear when a personal bookmark is saved as part of their personalization effort though.   Can Viewers Share their Personalized Visuals? Once a user personalizes a visual, they can pin the visual to a dashboard in the Power BI Service. From there, it is available for anyone with access to that dashboard.   What About Excel Pivot Tables and Pivot Charts? Does this Replace Excel Data Visualization? Personalizing visuals in Power BI is a native substitute and extension of how many people use basic pivot tables in Excel. It is a powerful alternative in scenarios where users employ Analyze in Excel or export their data to Excel and only want to explore data in a pivot table. It is doubtful that visual personalization will overtake Excel pivot tables anytime soon. This feature--while a powerful exploratory tool--will not replace outside analysis and customized Excel reporting altogether. As more users are educated and aware of personalization and its potential, however, it may replace some reliance on Excel for basic pivot scenarios.   What Is the Impact of Hiding Implicit Measures in Data Models?  An accepted practice in Analysis Services and Power BI data modeling is to create basic explicit measures with a default aggregation and hide the underlying implicit measure. An explicit measure uses DAX. One common reason to hide implicit measures is that they do not work as expected in Excel pivot tables. Although a field may be numeric, Excel does not recognize the implicit measure as such and does not allow it to be added as a value to aggregate. Explicit measures are required in this scenario. With explicit measures, users cannot override the selected aggregation when personalizing visuals. For example, a typical explicit measure is a basic sum of a numeric field. If an end user who is not the data modeler wants to view the average of that field instead of the sum, they would either need to request that a new explicit measure be created for them in the data model or ask that the underlying implicit measure be made visible. From an exploratory perspective, implicit measures combined with Power BI’s visual personalization are far more powerful in the personalization scenario than explicit measures that only consist of a predefined aggregation. With a single implicit measure, users who may not have access to change the underlying data model can still explore variations in the data without having to request and wait for an update to the dataset that includes another explicit measure for them. Implicit measures allow users to easily see variations of a numeric field such as sum, average, minimum, maximum and more—now bundled into a convenient user experience with personalization. End users themselves can dynamically change the default aggregation and explore and pin different views with that measure. In the case of personalized visuals now native to Power BI, however, the common practice of hiding implicit measures prohibits the feature from reaching its full potential. One advantage of visual personalization is to put powerful exploratory ability into the hands of end users without having to rely on upfront changes to the data model. Handicapping Power BI for the convenience of external tools like Excel or to reduce perceived clutter in the model is not advantageous for this new native feature but will of course continue.   Get Started with Personalization Personalization is available at the report level. Once enabled, you can personalize any visual in the report and pin the result to a dashboard. Enable personalization in Power BI Desktop in Options / Preview Features for now. Alternatively, enable it in Power BI Service in the report Settings area. Once this option is available, hover over a visual and select the Personalize this Visual icon to get started.   "
"17" "The Pain, the Gain, and the Plan for Modern BI I’ve recently been writing about considerations for successful Power BI deployment, and it has become a 2-part blog.  In Part 1, the focus is on the external environment and its impact on both organizations and individuals that may be considering the adoption of Power BI as a modern business intelligence tool.  In Part 2, we’ll turn the view inward – recognizing the pains presented when a modern BI tool is missing, reviewing the benefits available with success, and then reviewing the various stakeholders and the key considerations that need to be addressed in order to realize the full potential of a Power BI deployment.  Recognizing the Opportunity Oftentimes it can be as useful to look at the risks or downsides of not taking an action as it can be to consider the potential promise, especially when determining the priority and return on investment of a given initiative.  In Part 1, I discussed some external factors that may guide organizations (and individuals) to adopt and embrace the data and analytics capabilities of modern business intelligence, but let’s look at three sources of pain or categories of risk that can be apparent when an organization lacks a modern business intelligence platform. The video below covers both Part 1 and Part 2 of this topic.  The portion that aligns with this post begins at 15:38.      The top-down risk is characterized by a lack of vision & strategic alignment between enterprise services - especially between data management, business intelligence, and artificial intelligence.  This can result in a variety of challenges, including multiple technology stacks with increased demand for custom integrations, out-of-sync progress as new BI and AI tools lack data, or as data availability outpaces the demand across individuals or teams.  Siloed solutions may be common, with specialists sourcing their own specialized tools, and business units making SaaS purchases that do not line up well with the enterprise architects’ vision for tool deployment. The mostly technical risk comes from deficient tooling – lack of scalability, uniformity, or agility to adapt to new data sources or use cases.  We may also see outdated tools being stretched to satisfy newer requests, resulting in longer development cycles and an over-tasked report development team, especially in organizations with an expanded demand for data but lacking the tools and/or governance to enable proper self-service.  Some tools do not adapt well to the variety of data sources presented, including the ability to hit various cloud repositories and API calls in order to gain access to critical data for analysis. The myriad of challenges created in these scenarios could be the topic of several additional blog posts, but a common trait when tools are lacking is the proliferation of Excel; even when various reports or systems provide access to data and reports, analysts and advanced users are focused on being able to get the data extracts into Excel to satisfy requirements not provided for in the existing reporting solution. And finally, the bottom-up risk of disengaged users!  This piggybacks on the preoccupation with Excel mentioned above and can arise due to complexity/heterogeneity of tools, outdated interfaces, and a general lack of collaborative features.  Disengagement can be exacerbated in a primarily WFH environment where organizations may no longer rely on getting everyone into a war-room for collaboration or stroll over to their go-to allies in IT to provide access to data or reports.  In this scenario, tools that deliver individual agency and automated processes are necessary to be successful.  Now if any of the previous sections were reminiscent of pains or problems you’ve experienced within your organization in recent memory, take heart! These and more challenges can be overcome and the benefits of successful deployment can dramatically improve the work experience for the vast majority of workers!  Before we get into describing the impact and value of successful Power BI deployment, I do want to make one thing clear: the very best technology in the world can fall flat if it is not used and trusted by an engaged user base.  Some of these outcomes describe the benefit to the right processes as well as the right technologies – and work best when they are enacted in concert.  So here we go: Engaged Users: enjoy a seamless experience when accessing both managed and ad-hoc data, with a consistent end-user experience that promotes trust and familiarity.  Empower various groups to collaborate and socialize the usage of data and analytics to processes and workloads outside of IT or reporting. Agility & Innovation: Respond rapidly to new analytics demands with robust self-service and ad-hoc capabilities that compliment enterprise managed process. Encourage experimentation and augmented analytics with integrated AI and Machine Learning capabilities.  Bring agency and opportunity to improve processes and elevate organizational intelligence about all aspects of the business. Business Productivity: Single tool & support for both enterprise and self-service reporting tools with quick, intuitive development process for data & report professionals and simplified hand-off from business analysts to professionals to enable governance where needed.  Full support of delivery mediums gets the right data and analytics capability to the right audience, regardless of method of access or sophistication. Scalability & Reliability: Develop with confidence in the reliability and security of the industry leading tool that is fully supported by a complement of enterprise tools for data management and AI development.  Roll out the appropriate levels of governance to maintain accuracy and confidence while empowering innovation to expand into new data domains or technical capabilities.  Now some of that sounds like a commercial for Microsoft Power BI, and it’s a great tool certainly. But the reality is that a modern business intelligence tool should provide for all those benefits when successfully deployed and coupled with the right process-level support to deliver digital transformation.  There are some great individual analytics tools out there, but few if any can compete with the big-picture, holistically-minded deployment approach outlined here.  Gartner awards Microsoft as the runaway winner in both the ability to execute as well as the completeness of vision, and those measurements are critically important when you get out of contrived bake-off comparisons and into real-world deployments! Planning for Success The vision of a data-centric organization is appealing, and consensus has shown it is a top priority for organizations.  However, the vast majority of organizations struggle to achieve the benefits, in some cases, even after paying the large price tags associated with the tools and technology of the trade. So how do we avoid the pitfalls? How can we chart a course towards success? The answer is anything but simple, and there is no one-size-fits-all, but there are some clear critical areas of focus, and we’ll review with more detail below. Who are we doing this for? It’s a simple question, but it’s also a bit of a trick question. There are multiple key audiences and stakeholders when building organization capabilities, deploying wide-spread tools, and taking aim at digital transformation. Let’s consider a few key audiences and their primary concerns and needs when deploying Power BI.  Business Leadership & Enterprise Architects: Leadership & Technology Landscape Alignment. Admittedly, this might be an odd grouping here, but bear with me. The right level of sponsorship and vision is critical for any digital transformation initiative and the push for such change, the vision that inspires the work, must come from the right level to propel the organization over the hurdles and bring all the other stakeholders together.  The EA teams should be concerned with achieving the vision of the executive leadership in a managed and intentional way, and this is where alignment with other business and technology leaders needs to come together.  Finance and accounting teams need to get on the same page as the BI managers, who themselves need to be working in parallel with data platform leadership.  And a properly functioning modern data platform touches every internal system as well as external sources of data from all areas of the business, so to say this can be a diverse and very busy set of stakeholders is an understatement!   The executive sponsor must share the vision that rallies the disparate stakeholders and those impacted to move in the same direction and appreciate the value of the transformation. The EA function must help align the data strategy with BI and reporting while ensuring that every department and the embedded analysts are provided for. Data & Analytics Leadership: Technical Deployment & Management. The technology leaders responsible for obtaining and deploying various tools and platforms are an obvious inclusion – but let’s point out that many BI tools over the last 5 years have made it their focus to promote themselves as “self-service” tools, and heavily down-played the importance of these IT leaders.  I have long maintained that this tends to be folly in almost every organization, promoted by software vendors to close a quick sale with a business stakeholder who may be frustrated at the speed (or lack thereof) of their supporting IT department.  That being said, I do believe that the frustration that led to the appeal is very real, and any modern solution must take into account the balance between agility and governance, and establish guidelines for ownership of reporting and analytics that empower both efficient managed reporting as well as agile and innovative analytics from both technical and non-technical audiences.  This is supported by a need to ensure robust security and governance processes and controls are in place as appropriate, and that the platform itself can be monitored and managed to provide consistent performance and functionality. BI Practitioners and Analysts: Solutions & Capability Enablement. Last but certainly not least are the actual users of the BI tool!  As the consumers of data, this represents the largest audience, and the most diverse – spanning levels of professional report developers and BI practitioners along with business analysts, budding data scientists, and managers and workers from almost every aspect of the business.  These audiences need access to training and support resources.  They benefit from showcase examples that offer up patterns and best practices to follow as new assets are built.  Training should be appropriately tailored along a tiered path of depth and expertise so as not to overwhelm the lightweight users while also provide the enablement resources for innovation and experimentation from more advanced users.  Professional report developers need access to a proper tool set for managed assets, performance management, and scalability and maturity options that help ensure the tool set can grow with the data volume and complexity of the use cases. With these primary audiences in mind, and their respective wants and needs as participants in the organizational change towards becoming a data-driven organization, we can group the principal activities and areas of responsibility that enable success.  At BlueGranite, our experiences with hundreds of clients (we've been Power BI fans for awhile... seriously), in various states of Power BI use, have highlighted some key pillars to support deployment and adoption.  These pillars are grouped by domain, and the associated primary audience that they help address, and collectively help provide a holistic perspective to deploying Power BI and maximizing the benefit!  It is important to highlight that each of these pillars themselves contain multiple activities and considerations.  While the audiences and their objectives may be vastly different, with well north of 100 different client engagements across various aspects of Power BI deployment, we made an effort to capture the types of activities we were being asked to do, and the key considerations that our team of experts identified that enabled a path towards successful deployment and adoption.  We invested additional time to work through these considerations in light of each pillar and identifying additional unmet or unasked needs that organizations can expect to encounter while tailoring a deployment plan for their own organization and environment.  We also noticed an unintended side effect: simply by grouping these pillars, we’ve allowed collaboration among our own team to develop their own expertise within the domains and considerations that they have the most experience in or feel most passionate about! That last images admittedly covers a lot of ground and it’s probably too small to read through in detail, but it’s important to note that very few organizations need to consider every single item on those lists. Analysis paralysis is real, and it can be deadly to any initiative intended to spur innovation to over-complicate the details of governance, monitoring, strategy planning in general.  “Plans are worthless, but planning is everything” Taking a page from General Dwight D Eisenhower, we can move forward in an agile fashion knowing that each consideration we choose to undertake has been worked out in a broader framework that will let us address additional challenges and considerations as the need arises!  And we can certainly benefit from retrospection on the rough plan that we have seen lead to success for multiple medium sized deployments (roughly 250 – 1,500 Power BI users).  Here we can see 4 key topics, and a rough timeline in how they can be successfully addressed. This is not comprehensive, and depending on the deployment approach, may be followed by additional activities to scale various elements to additional business units.  But here’s what remains consistent:  Strategic Planning – this does not require a massive project charter or Gantt chart detailing every single step along the way, but we also do not want to proceed blindly. Getting the environment set up right, aligning to enterprise data management functions, and understanding the likely plans for governance and ownership will all help ensure that the early solutions and adopters do not get painted into a corner, or adopt early habits that veer too far from the best practices that the organization seeks to establish. Adoption Pilot(s) – although Power BI is easy to get into and use, it also can be tremendously sophisticated. Getting some quick wins that solve high-visibility and high-value analytics problems while also helping establish patterns, palettes, and practices for others to model against can be critical! This also ensures that efforts in the next section have content immediately to help manage, to prevent the perception gap between starting an adoption and seeing the benefits! Enterprise Deployment – Identifying and implementing the supporting infrastructure and controls necessary for at least the first few months of expected usage is critical. No one likes to stabilize and build out the right support mechanisms once the plane is in-flight – it’s always best to address them before things get off the ground! At a minimum, we know that decisions around Premium and Gateways are critical, and a clear understanding of security needs and data movement strategies will pay-off later. In addition, we believe that monitoring, specifically user & adoption monitoring, should not be overlooked. Understanding the impact of the deployment, tracking adoption metrics, and helping with targeted support and training are all necessary to ensure we end up achieving the goals of the initiative. Support & Training – providing the right support for the various levels of users – in terms of training and other resources – is straight forward. But there are some ways to improve the efficacy of these efforts! Making sure that training is modular and accessible is key – this helps various users engage with the resources they need without having to invest extra time in features they may not be utilizing yet. In addition, being deliberate with the groups that engage in structured training – paying attention to their department and level of sophistication – can help ensure that such sessions are as productive as possible while also investing in advanced practitioners who can advocate and assist others on each team. Finally, monitoring the adoption of the platform as well as specific assets (datasets or reports) can help provide visibility into where additional training or support is necessary while highlighting the benefits enjoyed by those who are taking advantage of Power BI!   So if you've made it this far with me, let me recap (and thanks for hanging in there!).  The potential of digital transformation exists across many domains, but the core concept remains the same: letting technology transform our business processes and capabilities to operate better in a wide range of current and future scenarios.  When external events such as the COVID-19 pandemic and associated economic recession exert tremendous pressure upon operating 'as usual', we have an opportunity to lean in to the efficiency gains available from becoming a data-driven organization.  One foundational opportunity to do just that is to look at deploying and adopting Microsoft's Power BI platform as an enterprise-wide tool, and enabling self-service analytics to become a widespread capability.  In pursuing successful adoption of Power BI, there are many stakeholders and considerations to address, and I've taken the time to highlight some of the findings collected across dozens (and dozens!) of Power BI engagements with organizations of varied sizes, industries, and levels of data maturity. At BlueGranite, we're continuously learning and improving our understanding of how to help organizations maximize their investment in Power BI, and we see it as a critical component of successfully harnessing the value of data, and charting a path towards Artificial Intelligence and automation.  It is our firm belief that most organizations will need to journey this path in order to survive and thrive in the current volatile economic climate and into the foreseeable future. Helping organizations deploy and adopt Modern Business Intelligence solutions is core to our vision, and it's an area we are excited to continuously invest in, learn about, and share!  "
"18" "The novel SARS-CoV-2 coronavirus has thrust us into novel times. Many foundational norms – social, economic, education, civil – have been turned upside down. Lives are disrupted. Health is at risk. Hope and fear mingle in equal parts. But we are a resilient society, and ultimately we will pull through.  As we collectively grapple with the daily challenges of these uncertain times, we must also look to what comes next. Unfortunately, many among us haven’t that luxury. Our heroic health care providers and first responders must maintain focus on the here and now. But those who can look to the future should be working to assure that what comes after the crisis will honor their sacrifices. How Data Science is Taming COVID-19 When we say that the COVID-19 disease is caused by the novel SARS-CoV-2 coronavirus, novel expresses that we have not encountered this particular virus before. In lieu of that direct experience, we turn to our experience with other coronaviruses and viruses in general to inform our responses. This alternate experience is codified in various historic epidemiology models for infection, hospitalization, and mortality. Starting with these alternate models, we conservatively calibrate them with initial parameters that drive disease progression projections based on observations of the disease from other countries. Then, over time, we continue to refine the parameters and select the most meaningful models as we learn more and more about this new disease. One of the benefits of this approach to modeling COVID-19 is that our leaders can make decisions based on the best information that they can get from these models before the models have been perfected. Actions from these decisions alter real world disease progression, so model refinements must take these changes into account also. Although each generation of the models is short-lived, over time the leading generations become more and more accurate and predictive. The results of the cyclic process by which the COVID-19 models are perfected can be illustrated by this example showing the increased precision of their estimates of total deaths over time:  In the graph, the information box shows the dates of each analytics generation and their increasingly more accurate predictions. How COVID-19 is UnTaming Data Science In the last decade, there has been an explosion of analytics embedded into everyday processes. Resource management systems automatically place orders when inventory analytics detect that current (or anticipated) holdings fall below a certain threshold. Human resource analytics assess and report the turnover risk of key personnel (like nurses). Remote monitoring analytics assess biometric telemetry to predict near-term outcomes for patients with complex cases. Emergency department utilization analytics detect rising patient loads in real-time, triggering staff surges to meet increasing demand. Condition management systems scan populations for tell-tale signs of precursors to chronic diseases. Analytics are deeply woven into the fabric of health care delivery. Across the globe, these carefully crafted analytics are dutifully sounding alarms as they detect that operations are currently outside normal limits. However, their warnings go unheeded and alarms are summarily silenced because the world they were made to monitor and evaluate has been greatly altered by COVID-19. Their assessments no longer apply to the current state of the processes that they inform. Many of these analytics were developed and are maintained using a traditional machine learning methodology consisting of five-step cycles:  Under this process, the effectiveness of the analytics is based on the foundational premise that the past is a good indicator of the future. Consequently, analytics can be less effective over time as conditions change due to the (hopefully) positive impact of the analytic or for other external reasons. This is true for many pre-COVID-19 analytics: the cases used to train and validate the analytic models is not indicative of the current state, thus they are producing erroneous responses that do not lead to the intended increases in the effectiveness of operations or outcomes. Many organizations must simply “work around” these current disruptions to their operational analytics, waiting for the time when we are past the immediate crisis and things will return to “normal”. But the Big Question is: What will “normal” look like then? Are the lessons we learn from this crisis likely to introduce lasting, structural change to core processes? Increased telemedicine utilization would alter time and place of care delivery. Preparedness policies would change how we provision for large-scale events. Communications channels could expand and increase in responsiveness. Advance detection algorithms would provide more advanced notice that events were upon us. Many of the changes may be an acceleration of existing trends, while others might be completely new paradigms for care delivery. It is a unknown how the many pre-COVID-19 analytics will fare in the “new normal”. While some will be unaffected and others require only minor updates, many may no longer be fit for their original purpose – a “mass extinction event” for analytics driven by rapid and drastic environmental change resulting from COVID-19! Many organizations may struggle to remedy the many simultaneously mal-operating analytics due to their number and complexity. And while the situation persists, organizations and those they serve will suffer both the original and emerging inefficiencies that led them to adopt these analytics in the first place. Adaptable Analytics – Survival of the Fittest Taking a lesson from biological systems, species that can adapt to changing circumstances will thrive while species which are less adaptable will not. This is true also for analytics – those which can adapt more readily to changing circumstances will be more effective in improving operations and outcomes. How can analytics be made more adaptable? In the Traditional machine learning methodology, analytics are monitored for continued effectiveness and periodically replaced as their effectiveness wanes. The cycle time to detect and address reduced effectiveness of analytics can itself contribute to overall reduced effectiveness. A proven variant of the Traditional machine learning methodology, Continual Learning, can be used to create analytics that are more effective and resilient in changing circumstances. Continual Learning Analytics Traditional machine learning methodology uses serial cycles of model development to implement successive generations of an analytic. Continual Learning methodology expands on the Traditional methodology as follows: Re-modeling of analytics is continuous and automatic Training and Testing data sets for each generation are a combination of prior generations’ historic and live cases from Monitoring Multiple generations of an analytic can be live simultaneously, with configurable case distribution logic determining which cases are processed by which generation(s) The current generation in development is virtually in continuous “pilot” mode since its responses to live data are assessed whenever it is (re-)validated Older generations are soft-retired by case distribution logic that considers Monitoring results to determine that cases are no longer processed through older generations Continuous Learning analytics are developed using parallel machine learning cycles:  One or more previously published, live generations are selected to process live events, while a current generation in development extracts both historic and live cases from prior live generations to support Continual Learning. Thus a typical journey for a live event through the Continual Learning analytic is: An event is routed by a requesting process to the analytic for insight The event’s contextual details are considered by the analytic’s configurable case distribution logic to select which generation(s) of the analytic will process the event, and then a selected generation processes the event to add a calculated insight computed using the analytic’s models The event and calculated insight are returned to the requesting process for further action After some time, the outcome of actions taken based on the calculated insight will be evident. Then: The actual outcome of the event is gathered from operational systems by the analytic’s Monitoring logic The event, its prior calculated insight, and actual outcome are staged into an organized case The case is assessed by the analytic’s Monitoring logic for effectiveness and reported After sufficient cases have been staged: The current generation in development considers each case’s contextual details and may select the case for Training or Validation for the current generation in development based on its age and relevance Continual Learning Illustration When the nature of cases changes rapidly, analytics implemented with Continual Learning can be much more accurate than analytics implemented without. The example below depicts daily mortality rate projections using two custom COVID-19 analytics developed in Power BI:  Each of the two analytics accumulates expected mortality across 2,637 independent county-level mortality models to project overall mortality. One analytic uses a Traditional machine learning methodology (orange line), while the second uses Continual Learning (dark blue line). The Continual Learning analytic is autonomously re-modeled daily and its projection is consistently closer to the actual mortality (light blue line) than is the projection from the Traditional machine learning analytic that is re-modeled weekly. For the specific values called out for 04/09/2020, the difference is most dramatic. Summary COVID-19 reminds us that models are only as good as what goes into them. And as real-world conditions change, our analytic models must be updated to reflect those changes if they are to be effective. The widespread impact of COVID-19 has impacted many analytics, creating a need for widespread updates. These updates are neither free nor easy, and many organizations will lack the resources to make timely updates to them all – especially as we are in a period where they may require ongoing update as COVID-19 continues to alter real world conditions. So our finely tuned processes that have come to rely on our valuable analytics may be chaotic for a time. An important lesson for analytics managers and machine learning practitioners is that in order to mitigate the impact of changing conditions on our analytic-enabled processes, we should strive to make our key analytics change-ready through Continual Learning so that they easily adapt to changing conditions."
"19" "In this post, we'll review an example of SIR modeling on Azure, using the COVID-19 Hospital Impact Model for Epidemics (CHIME) model, developed by this team at Penn Medicine Predictive Analytics.  We reworked this model for a BlueGranite healthcare client, customizing it and deploying it to Azure so it could be used by hospital staff across their system. While we didn't create the CHIME model, we helped our client make enhancements to the model with UI customizations, integrate automated data inputs from their hospital systems, and then deploy the application on Azure. Before we dive into the details of the CHIME model, we'll get everyone up to speed on how epidemiological models like this work. There's a little math in this post, but I'll walk you through it.      The CHIME model as well as some other COVID-19 models that are floating around these days, are variations of what is called an SIR model. SIR models are compartmental models that help us model disease progression over time, and what this means by compartmental model is that there are three kinds of buckets that someone could fall into – you’re Susceptible, you can become Infected, and then hopefully you are soon Recovered.     These models try and see what the movement between these groups are, which are called transitions. The model shows how people flow through this graph, so to speak, over time. For those of you who haven't seen differential equations in a while, these equations are helping to describe the change in the number of susceptible people over time, how that flows into the number of infected people, and then how does that flow into recovered.  You may have seen some charts that look like this, where you have a certain percentage of the population that is susceptible and then that starts to go down as the number of infected people go up. Then as infected people recover, you can see that the chart flips. There are built-in assumptions that once someone has recovered, they are no longer susceptible to be infected again, which is true for SARS-CoV-2, for the short-term at least. We know that this virus is an RNA virus and can mutate like the yearly flu. Each year we experience a similar flu, but it has a mutated a bit, which enables it to evade your immune system's memory of last year's version. This is the reason why you need a flu shot every year.  *A side note - once you are susceptible, you may get infected, and then once you get through the infection phase, then you may recover. Some epidemiologists treat the “R”  in SIR as \"removed\" instead of \"recovered\", and \"removed\" means they're just removed from the system – they are removed from the equation, so to speak. This could either be that they are recovered and no longer susceptible, but it could be that they are moved out of the population or they died. While this is a little macabre, it is the way some models capture the change over time.  These types of models can give us a couple different curves that measure slightly different things such as: the number of daily admissions into a hospital the total number of people in the hospital with COVID-19 (which we call the census view) or the proportion of the population that fits into one of these SIR states over time Flattening the Curve In the news, we have been hearing for several weeks now that we have to \"flatten the curve\". You might be wondering what actually goes into the curve itself! In epidemiology, we have a ratio called R0 or “R naught”, which is the basic reproductive number of a pathogen. R0 defines how many people we would expect that one person might infect. A pathogen that isn't so bad has an R0 of less than one, meaning every person that's infected may infect one person or fewer. Then there are some pathogens that travel through the air (like measles), and the R0 could be 10 or higher. That means that for every one person that's infected they might infect 10 people, which is really, really bad, and that's how exponential growth gets out of hand really fast. When we talk about R0, it is calculated by looking at the time for the population of infected people to double - what is the exponential growth rate of the population. With mitigation measures like social distancing or better hygiene, we can reduce this growth rate. \"Flattening the curve\" is just a simple way of saying reduce this R0. In epidemiological terms, we're talking about reducing R0 into Rt. Rt denotes the reproductive number after you do some sort of mitigation.  In this example above, 30% social distancing mitigation cuts the R0 of 3.59 down to the Rt which is 2.51. That means that we go from infecting over three people each, to just between two and three each. For this particular population, it also puts the peak of the infected curve to the end of May. Next, let us see what happens if we bump up the amount of people that are social distancing; that is to say, the percentage of the population that follows the rules and stays home (hopefully all of us!).  If we move social distancing adherence to 80% (ramping it from 30%) and keep everything else the same – other rates, severity measures, population, etc., - we actually cut the R0 from 3.59 down to an Rt of 1.81. This is much more manageable. It does not change the fact that it still exponential growth, but what ends up happening is that the exponential growth is not nearly as severe. In this CHIME model, at 80% social distancing, we are seeing that a given infected person might infect one or two people. This also pushes the peak of the curve out to late June instead of late May and reduces the severity of the peak when it gets here (from 120 people down to 53 people at the peaks, respectively). The critical importance of flattening the curve is that it helps to not stress out our health care system. Side note: flattening the curve does not solve the problem by itself. While flattening the curve helps to not stress out our health care system, it extends the duration of the problem.  However, it also gives us more time to develop better treatments, tests, and vaccines. This is what we see happening across the U.S. and the world today. Enhancing the CHIME Model The original version of the CHIME was built by researchers at the Predictive Healthcare Team at Penn Medicine. Their original model is completely parameter-driven, meaning that you plug in all of these numbers in a Python script in the app's files and then it builds the curve based off of these numbers. Our client wanted the models to be a bit more dynamic and driven off of actual data. So, we modified the CHIME functionality to pull in a stream of data with their actuals – total beds, ICU beds, and ventilator capacity, along with actual daily admissions and counts for each. In the video for this post, you can see the blue line is the actual number of hospital admissions that the client is seeing. Now the parameters in the CHIME model are taking into account the hospital system’s actual data, so that the physicians (who are not epidemiologists, nor are they mathematicians) can easily go in and see the model and results tailored to their own scenario. The customized web application has most of the numbers the doctors need, so they do not have to change as many factors and turns as many knobs to perform analysis. They can see the hospital system’s up to date admission numbers compared to the predictions in the model. They can dial up or down social distancing and other assumptions to see the impact on bed, ICU bed, and ventilator demands versus capacity. Plus, they can now manually change the doubling rate to see how it affects the curve and the R0. Deployment on Azure The application itself is written in a web framework called Streamlit. Streamlit is a Python framework that's kind of like Django or Flask, but built simple modeling tasks with visual outputs, and it's what the original CHIME model was written in from the U Penn researchers.  Our client needed the model customized quickly, so that is what we used for the project.     Shown above is the overall deployment pathway. The application is written in Streamlit locally, and includes different functions to create the charts, the actual SIR model itself, and the logic to make the UI elements for users to play with. There is a lot of functionality that's built in there, and then it's all wrapped up as the Streamlit application. For this project, we then containerized the application in a Docker Container, which captures all the dependencies and everything that is needed to run that web application. We then deployed this to the Azure Container Registry, which further deploys to Azure websites. Now this application is hosted in Azure in our client’s Azure tenant. On the data side, the client wanted to start pulling in actual data on a daily basis to drive more accurate modeling. We started by pulling the data from a local flat file that I had to manually update. To \"productionalize\" the data ingestion, the client set up a flat file extraction package using SQL Server Integration Services (SSIS)  that queries a local database and writes out a CSV file daily. The package then uploads it to an Azure Storage container. Finally, the CHIME app pulls in the data directly from Azure Storage. With this approach, the client doesn’t have to manually update the data – the SSIS job will automatically do this daily extract for them. What is interesting about this is how easy it was to build the container and deploy it to Azure. We had built containers before, but had never deployed to an Azure Website, especially not a Streamlit application. Oddly enough, it was very straightforward. In fact, it was so easy that I reached out to my co-worker at BlueGranite, Josh Fennessy, to confirm that I was not going crazy. The back end of the solution was very simple. There are just a few resources that get deployed. There is the storage account that holds the data. The app registry that holds the container registry that holds the container images. The app service plan, which manages the virtual machine that the app is hosted on, and then the web app itself.  The web app is where the physicians and administrators at the hospital can log in and see the results of the model, or tweak the parameters.    Within the model in the web app, it shows admissions along with lines we added that show bed capacity, ICU capacity, and ventilator capacity. It helps inform hospital staff and administrators when they may exceed capacity, based on current social distancing guidelines, at least based on the CHIME model. Key Takeaways A key takeaway from this is that there are all sorts of models! As we like to say in the business, \"all models are wrong, but some are useful\". There are news stories, politicians, and business leaders across the country saying that “we’re going to reopen the state” at a certain point. In my professional opinion as a data scientist, biologist, and through my work with infectious diseases and other coronaviruses, they will probably reopen much too early to try and get things moving again with the economy. As this happens, we may see another bump up in these models, and I expect these models will continue to be adjusted as we collect new data. What we may see is an apex in current curves across states and regions, and then we'll see another hump later this year (though this isn't modeled in CHIME or any other popular SIR-based model that I've seen so far). While the future hump will likely be smaller, this is still a problem for those at heightened risk as we have not made as much progress with vaccines and treatments as could be hoped, though we are making advancements with better and more widespread testing. Stay tuned for adjustments to CHIME, and probably brand new models, that will have to take into account up-and-down adjustments to social distancing as policy changes and new cases increase. Other Resources University of Pennsylvania Medicine Chime Model: https://penn-chime.phl.io/ University of Washington IHME Model: https://covid19.healthdata.org/united-states-of-america WBTV News Interview on COVID-19 Modeling: https://www.wbtv.com/2020/04/13/limited-data-makes-it-tough-model-coronaviruss-spread-experts-say/ "
"20" "Change in the time of Coronavirus I have been with BlueGranite for quite some time, and most of that time has been wrapped up in Microsoft technology in the Data and Analytics space.  It was my focus, but also a source of excitement. Recently, I got to share some of my passion for a different facet of applied technology – the potential for organizational change that can come with it! When I started putting together this presentation in mid-February 2020, the intent was clear: we wanted to talk about successful deployment and adoption of Power BI and its ability to deliver Digital Transformation – more on that in a minute.  But no one knew how dramatically the pace and patterns of nearly every organization would change by the time we presented in late March!  I spent some time reading, thinking, and reworking some of the content to reflect the new operating environment that exists as the US and in fact, the entire world, deals with the COVID-19 pandemic. I believe that now more than ever, the capabilities of a modern analytics-driven organization are critical to both survive and thrive in the changing landscape in which we operate.     Digital Transformation But back to Digital Transformation.  The People, Process, and Technology framework has been around for quite a while.  In fact, it was published in a paper in 1965 by a professor at Stanford University named Harold Leavitt.  He came up with the model for organizational change, but at the time it had 4 components rather than the common 3.  Eventually, Structure and Task were generalized to Process, and the People, Process, Technology model widely used today was born.  A couple of years ago, I heard a presenter from Microsoft posit that, in order to deliver true Digital Transformation, we must start to think about the unification of Technology and Process.  Digital transformation, itself defined as a re-imagining of business in the digital age that seeks to deliver change driven by technology but centered around People and Process.  Indeed, Technology has become so pervasive (and when done well, equally intuitive), that it could and should have a fundamental impact on process.  It is by applying that mindset to the award-winning technology in Power BI that we have developed the approach and material we leverage at BlueGranite and is the subject of this post. Microsoft’s domains of Digital Transformation:  The Environment we find ourselves in – and how Organizations and Individuals can react Ok – so 2020… where do I even start?  We’re almost done with Q1 and it feels like it’s already been a long year. Anyone reading this will already be fully aware that this has been the year of the Coronavirus – the pandemic that is COVID-19.  For some practical analysis of the situation, check out this recent blog from Dr. Colby Ford – one of our leading data scientists here at BlueGranite!  It has generated one of the most significant economic collapses in history and introduced us to the term “social-distancing”.  Potential recession had already been on the horizon, and the anxiety and disruption of the virus has hastened and amplified its arrival.  We have seen massive disruptions in almost every corner of the globe, and to every industry.  In developed nations, and especially the United States, we have seen dramatic increases to Working From Home (WFH) models hastily spun up, which have resulted in tremendous strain on existing operating models and technology platforms.  In a related post, Leo Furlong (one of our Principals) takes a look at how to maintain productivity while WFH – worth a read!  As of this writing, we are starting to see unprecedented spikes in unemployment, and the government-led stimulus beginning to fight back. Here’s an interactive view of the sheer size of the COVID-19 pandemic’s unprecedented expansion in the last few weeks:      And what should we be preparing for in Q2? The back half of the year? I will point out that I am in no way an economist, but many economists and financial analysts are explicitly forecasting a dramatic 2nd quarter.  With unemployment spiking into the double digits (12%-30%, depending on who you read) and an unprecedented decrease in annualized GDP of between 30% and 50%, it seems likely that - while not nearly as extreme or panic-driven - Q3 will continue with a more traditional economic recession pattern, but what’s interesting is what comes next.  As the markets and the economy more broadly stabilize, and start to resume a growth trajectory, it is likely that data & analytics will be the primary fuel driving us forward! Fed Reserve Bank of St. Louis President James Bullard predicted the US unemployment rate may hit 30% in Q2 due to coronavirus, with an unprecedented 50% drop in GDP But it is not all doom & gloom.  In fact, Bullard went on to say that while this Q2 impact will be intense and extreme, it is, at least in part, intentional.  We have chosen to slow down economic activity to help control the pandemic.  Former Fed chairman Ben Bernanke likens the scenario to a natural disaster more than the Great Depression.  And both believe that the downward trend can be unwound quickly. So while recession may be inevitable, the panic and anxiety fueled extreme reactions are likely to be temporary, and if you take a step back, a common thread appears in most of the writings around our best ways to deal with the crisis: data.  Numerous outlets, specialists, epidemiologists, and prognosticators talk about how the key to successful navigation in trying times is access to accurate and plentiful data.  Health organizations and governments share and analyze charts and graphs to determine the patterns, the points of inflection, and the likely predictions.  We have shared a “flatten the curve” data visualization to help everyone understand the importance of social distancing, and perhaps started to appreciate just how important access to accurate and comprehensive data is when making decisions… So, an opportunity lies amongst all the volatility and challenge – a newly awakened hunger and appreciation for data! But I must make something quite clear here: we’ve been shifting towards a data-driven mindset for quite a few years.  A study completed 4 years ago highlighted that the vast majority of organizations knew they needed to make data the heart of everything they do, and yet most of them failed to do so.  The consensus on the importance of data to literally fuel the future continued as data was compared to oil – in fact, dubbed “the new oil” – our planet’s most valuable resource.  And much of the message has shifted to AI in recent years, but what is AI except the technological potential of data (and of course, incredible advances in computation power and storage capacity)?  So the drum beat has been sounding for years, but even last year, CIO.com published an article that highlighted just how far we still were from transforming into data-driven organizations, this by-most-accounts essential activity to remain relevant and to survive and thrive in the modern economy.  What was the blocker?  Why were organizations not making the transformation that was seemingly so obvious and imminent?  Perhaps operating amidst a decade long bull market resulted in the simple fact that success itself was the enemy of efficiency and innovation. As a Rockefeller (David, as it were) once added when quoting Plato, “If necessity is the mother of invention, discontent is the father of progress.”  And suddenly, with the raging pandemic and economic recession, we find ourselves awash in both necessity and discontent. Even in early January, while corona viruses were still relatively obscure to most, 2020 was preparing to continue an already established trend of an increasingly tech savvy and data-aware workforce, from all walks of business.  And data-fueled technology has been poised as the accelerant, ready to transform the way we live and work! Microsoft has established a significant edge and leadership role with focus on the ‘democratization’ of technology, especially data, BI, and AI tools.  And this doesn’t mean that the tools participate in overblown campaigns and flawed elections before ultimately proving to be ineffective and inauthentic to the platforms they ran on – no, it means that these tools and their capabilities should be accessible to everyone.  Frequently shared wisdom to remaining relevant during a recession is to focus on skills and qualifications – improving the capacity with which one can perform their professional responsibilities - and increasingly the most relevant upgrade for almost any job-skill is to become more data-driven, analytics-minded, or AI-enabled.  The path out of the pending recession, the very ‘future of work’, will be spurred by massive efficiency gains as non-IT workers learn how to reinvent their own processes with the additional capacity of data-fueled analytics and AI.  Although it may be rare to see organizational and individual trends align so well, the path forward is neither paved in gold nor easy.  Our markets will likely see shifts out of whole industries, and entirely new models and operating patterns may never return to their BC (before COVID-19) look and feel.  Some workers will be left behind, by choice or by capacity.  And technology is not a magic wand to be waved, it requires a concerted effort and thoughtful approach in order to deliver on its promise.  Some organizations may already be down an inefficient path towards becoming data-driven, and others will not come to it with the proper multi-faceted perspective.  Once the decision has been made, or at least is being explored, to pursue the type of digital transformation that is possible with a Power BI Deployment, it helps to move forward with eyes wide open, and to benefit from the learnings of numerous organizations who have already embarked upon this journey. Please continue to our follow up piece, “The Pain, the Gain, and the Plan for Modern BI”."
"21" "Severe acute respiratory syndrome coronavirus 2 (aka SARS-CoV-2) is the virus responsible for Coronavirus Disease 2019, more commonly known as COVID-19. COVID-19 has affected all of our lives and it's all the news can talk about these days. However, in this world of misinformation, I wanted to provide a bit of insight into what we do and do not know about this pandemic and what that means for us all long term. In this post, I will address some of the pathogen statistics, new scientific developments, and where you can go to get data if you want to do your own analysis.  My recent research focuses on more common pathogens like E. coli superbugs or malaria, though I worked on MERS (another coronavirus) in 2016. My colleagues and I published the research paper Spread of Middle East Respiratory Coronavirus: Genetic versus Epidemiological Data in the Online Journal of Public Health Informatics. What's interesting about this work is that we highlighted the discrepancies between transmission data shown from phylogenetics versus what the World Health Organization's (WHO) Disease Outbreak News was reporting. Though this research was published in May 2017, the presence of conflicting or incomplete data still haunts us today with this current outbreak. I'm working with my fellow data and AI specialists at BlueGranite on using multiple COVID-19 data sources together to paint a more complete picture of the disease. This includes a chatbot to help state and local governments respond to COVID-19 questions and other work coming soon. To start, I thought I would get everyone up to speed on this virus itself and what efforts are being done to combat it.   Pictured: 3D Structure of SARS-CoV-2 Spike (S) Protein [5] Disclaimer: Do not take this blog post as medical advice.Consult your physician before treating yourself or others for this disease. A Little History Coronaviruses are not new nor are they necessarily rare. The most common strains in the Coronavirinae subfamily only affect mammals like bats, cats, and camels (oh my!), though there are 7 strains known to affect humans, including some that cause common human colds. In fact, most of us have lived through two other coronavirus-based epidemics, obfuscated under the names Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS) in 2002 and 2012, respectively. Note that SARS-CoV-2 is not a direct descendant of SARS. This is a bit confusing, but it is named this way due to shared genomic properties from SARS and because they share a common ancestor. Epidemiology What makes SARS-CoV-2 interesting is the diversity in its pathogenicity. That is, how differently it affects people. Today, according to the CDC, the reported overall mortality rate for COVID-19 sits at somewhere between 1.8% and 3.4%, but this varies drastically based on other factors like age and existing conditions [1]. Country Number of Cases Mortality Rate USA 51,914 1.30% Canada 1,739 1.44% Mexico 370 1.08% UK 8,081 5.22% Italy 69,176 9.86% China 81,848 4.02%  Source: WHO COVID-2019 situation report 65 (Data as of March 25, 2020) [2]    In the initial report from the Chinese CDC following the Wuhan outbreak, there are a lot of statistics that are still being reported [3]. For example, \"The virus affects men more than women.\"  from the report that there is a 2.9% mortality rate in men vs. 1.7% in women. Statistically speaking, there is a lot of bias in the sampling of infected individuals, which may be skewing these percentages. In this example, ~30% of the individuals tested in this study work in the service or farming industries, which may be male-dominated jobs, therefore making the gender breakdown of those tested unrepresentative of the general population. A problem with many of the reports on this disease is in the lack of broad testing. We see a mortality rate and severity reports based only on a population of individuals that are already experiencing symptoms (plus a couple of celebrities and politicians that got their hands on a test). We think that far more individuals may be infected, but have little to no symptoms. If this is true, this reduces these severity and mortality statistics tremendously, but increases the prevalence numbers. From Testing to Treatments It's no secret that one of the hurdles we in the US are working through (aside from the lack of masks and ventilators) is the lack of testing kits. Current existing tests work by swabbing the back of the nasal cavity and then sending this swab off to a lab for manual processing by technicians using reagents to positively identify the presence of the virus. This can take days, which is a huge diagnosis bottleneck in understanding the spread and prevalence of this disease. (Not to mention the accuracy of this method isn't exactly ideal either.) In other diseases (including malaria, strep, the flu, and MERS), rapid diagnostic tests have been developed that allow for test results to come back in minutes or hours. These tests operate in a similar manner to at-home pregnancy tests in that a small blood or fluid sample is taken and there are proteins (or other chemicals) that bind to a particular target in the sample and can give a quick and accurate result. This is a current push by a couple of research groups that are racing to develop an accurate test and get it out to market quickly.  Example Alere BinaxNOW™ Diagnostic Test for Influenza   As for prevention and treatments, this is where it gets tricky (and super interesting). There are two main options here: prevention of infection using vaccines or treatment using drug therapies. Vaccines Today, a few companies are working on a vaccine for SARS-CoV-2. Two novel approaches you should know about are from Moderna and Inovio. Moderna's work is quite interesting as they are using messenger RNA (mRNA) to help your own immune system learn about a specific viral protein that SARS-CoV-2 uses and enables your body to protect itself against the virus without ever having seen it. (Super cool!)  Learn more about Moderna's vaccine work here. Inovio's work is similarly interesting as they are using a DNA injection to invoke the production of customized monoclonal antibodies. Antibodies can serve as ID tags for your immune system to know when something isn't right. So, your immune system can better recognize the coronavirus and get to work on clearing it out. In contrast to traditional vaccines, this also enables your body to prepare for the fight against SARS-CoV-2 without ever having been infected with it.  Learn more about Inovio's technology platform here. Note: SARS-CoV-2 is an RNA virus and is expected to mutate quite often (although the data is not sufficient at the moment to get an accurate mutation rate measure). As with yearly flu shots, we should expect continual vaccine updates as the virus will likely change over time. Treatments Viruses work by using particular proteins to enter your cells and then reproduce by using the cell's machinery to rapidly make the parts that make up the virus. A good practice to try and stop the virus is to use a combination of drugs that combat multiple different mechanisms.  Example methods: Stop the virus from attaching to or entering our cells Stop the virus from hiding from our immune cells Stop the virus from assembling properly in our cells Stop our cells from making the viral proteins correctly Stop the virus from invoking such a severe response from our bodies (fever, mucus, inflammation, etc.)  Currently, computational biologists (like myself) are working to evaluate proper drug combinations that both reduce the infection while trying to mitigate the likelihood of developing drug resistance. We're working on this by looking at the individual genes that SARS-CoV-2 has and comparing this to treatment pathways that have worked in the past for other viruses.   SARS-CoV-2 Reference Genome Visualization from NCBI. View interactively here.(Figure from Gordon et al., 2020)    A Little Hope In epidemiology, there's a notion of an \"inflection point\" in the lifespan of a pandemic. When infections begin to spread, they usually spread exponentially up until a certain point where there are not enough individuals for the pathogen to spread to. While we don't know the true number of cases of COVID-19, we do have a decent idea about the number of cases that require medical care. Thus, our social distancing is limiting the spread of the virus. According to a report by the Imperial College COVID-19 Response Team in London, the US may not reach this inflection point until June '20, but that our isolation practices (social distancing) may be helping tremendously.  Projected deaths per day per 100,000 population in the United States [4].   Flattening the curve isn't the solution to COVID-19, it's just holding off the spread as to not continue to overwhelm our healthcare system. It will be the development and distribution of adequate tests, vaccines, and treatments that will return our lives to normal. In the meantime, stay at home!   Source Link [1] CDC - Severe Outcomes Among Patients with Coronavirus Disease 2019 (COVID-19)   https://www.cdc.gov/mmwr/volumes/69/wr/mm6912e2.htm#suggestedcitation  [2] WHO COVID-19 Situation Reports  https://www.who.int/emergencies/diseases/novel-coronavirus-2019/situation-reports  [3] China CDC Weekly - Epidemiological Characteristics of an Outbreak of 2019 Novel Coronavirus Diseases http://weekly.chinacdc.cn/en/article/id/e53946e2-c6c4-41e9-9a9b-fea8db1a8f51 [4] Imperial College London - Impact of non-pharmaceutical interventions (NPIs) to reduce COVID19 mortality and healthcare demand https://spiral.imperial.ac.uk/handle/10044/1/77482 [5] 3D Protein Structure of SARS-CoV-2 Spike Protein (from Wrapp et al., PDB: 6VSB) https://www.rcsb.org/structure/6VSB  "
"22" "Many enterprise-wide business intelligence initiatives meet an unfortunate end: lots of money, time and resources are spent with nothing tangible to show for it. Or worse, the new solution just adds layers of complexity instead of removing them — and key talent still do not have access to the data they need.  What exactly went wrong? To paraphrase Leo Tolstoy’s famous quote, “All happy families successful BI adoptions are alike, each failed one has failed in its own way.” At BlueGranite, we’ve learned a thing or two about successful BI adoption after working with businesses across numerous industries, from Fortune 500 companies to small mom and pop chains, and we want to share our expertise with you. Here are seven tried-and-true tips from other successful business intelligence adoptions you can’t miss: 1. They had buy-in. There is no BI without Business For any BI Initiative to be successful, it needs to involve or be driven by the business! Even if the early seeds are planted by IT, these initial stakeholders must work to secure support within the company before progressing past initial tests and proof of concepts. Leadership and C-level executives in particular are the best champions. They are familiar with the political landscape, have the ability to allocate resources and can create visibility for the project.  Achieving business involvement in the planning stages and decision-making process is crucial in obtaining organization-wide adoption. In addition, they prove to be a needed balance in preventing IT from getting carried away. Because, let’s face it: if left unchecked, IT will probably pick the newest and flashiest technology for the sake of it being new and cool. 2. They did their research In many organizations, data resides in disparate legacy systems. A clear plan on how these data sets will be integrated with the new technologies is a must. Users throughout the organization should be able to utilize the new systems and tools to perform their current job activities. This must be thought through and addressed prior to making any binding decisions. Proofs of concepts to the rescue! Most vendors will provide funding incentives and consulting support in the early discovery stages.  Use their help to test connectivity to key resources, analyze costs and learn as much as you can.  Word of advice: do not base your decision on promises and ‘soon-to-be-released features,’ especially if a feature you identified as critical for your user-base is currently missing from the solution. 3. They had a big plan but started small  To continue with our literary theme: “A goal without a plan is just a wish,” said Antoine de Saint-Exupéry, and we could not agree more. Having a detailed but flexible implementation plan with defined milestones is key, as is knowing and preparing for risks with sufficient resources (both human and monetary). Having the right level of resources allocated to the governance, architecture, security, monitoring, user education and support will set you light-years ahead.   We cannot stress enough the importance of having a core team with defined roles and responsibilities to see this through. Insider tip: Identify quick wins that can deliver the most value for the business in the shortest time, and implement them first. Evaluate what went right and what could be improved upon, and adjust your plan accordingly prior to organization-wide rollout. Not sure where to start? Do not be afraid to use consultants! Vendors will provide you a list of trusted implementation partners, but do not hesitate to shop around, ask for previous client references and do your research! Consulting partners have lived through a fair share of BI Implementations, both successful and not, and bring experience and valuable outside perspective into the mix. 4. They had a safety net The best governance is the one you cannot see. You’ll need to institute rules and policies that: Make sense for your organization Are easily accessible Remove any ambiguity for the user while steering them in the right direction. If you do not make these decisions now, users will make decisions for you. So make sure you do the work now to establish roles and responsibilities, evaluate data access policies and license distributions, and set up necessary system-level controls. Any BI initiative — self-service types, in particular —can turn into an onion (lots of layers to peel, and there is a chance one will cry in the process) due to initially underestimating the complexity involved. To avoid this, think about different types of deployment mode possibilities and the levels of support and controls they each require. Insider tip: The key to success here is finding the right-for-your-business balance between user freedom and flexibility with administrative oversight. 5. They defined what success looked like Success looks different for everyone, so you’ll need to define what success means for your business before implementation. Here are some questions that can get you started: Establish measurable adoption indicators: Is it the number of users completing training and switching to new tools for their daily work activities? Is it automation of manual and recurring processes so time can now be spent uncovering insights that showcase exceptional value your company delivers to your customers? Is it consistent positive survey feedback from users? Set up and track measurable user experience indicators: Can the CFO access critical real-time data within 5-10 seconds all in one place, instead of relying on static, outdated reports? Are reports optimized for most frequently used content delivery methods? Do these reports follow best practices on accessibility? Insider tip: Established management and monitoring processes can be used to generate data for most of these metrics and KPIs. See how this can be done for Power BI 6. They made communication a top priority People respond to changes better when they know the reasoning behind them, and know that their opinion matters and their voices are heard. Let users know what/when/why/how things are changing and what they should expect. It helps to hold town hall meetings where everyone can ask questions and receive answers.   Emails are great communication tools, but don’t shy away from using tools like Teams, Yammer or Slack during the implementation as these encourage collaboration and improve efficiency for issue discovery and resolution. Strategic and timely communication can also help discover and address these common adoption hurdles: Lack of buy-in across departments – seeing successful roll-out and hearing about their experience directly from the Finance team can help encourage HR and Marketing Security and data quality concerns (valid or not) Duplicative effort repeated across business units Uncertainty on how to get started and access resources  7. They did not leave anyone behind Do not overlook the importance of user resources; everyone learns better with a bit of help! The internet has a lot of information out there, but how much valuable on-boarding time and money could be saved if all users were provided a list of curated resources and had access to a dedicated support center with expert help? Your implementation consultant partner can help here, as well; they can assume the role of support hotline and deliver training sessions until you are able to fill that gap with your own resources and establish a working Center of Excellence. Here at BlueGranite, we found a lot of value in collaborative quick starts where new report development is led by a subject matter expert on a client side while we provide technical guidance and advice on best practices in using the tool, modeling and report design. Insider Tip: Do not limit user resources to report author support! Key decisions makers most likely will not be creating solutions and building reports, but will need to be able to consume the data. Education of the consumer base is equally as important! Here are just a few ways this can be done: In-person demos Recorded walk-through videos How-to documentation along with dedicated data dictionary and report information pages Feature comparisons between legacy systems and the new tools We hope that these observations and recommendations are useful and can help you prepare for a journey ahead and avoid a failed implementation! If one of the tools you are considering is Power BI we encourage you to explore our Power BI Adoption offer and we will be thrilled to help you!   "
"23" "Inspired by a conversation we recently had with our COO, my colleague (and Microsoft MVP), David Eldersveld, tweeted a question. “What is the TOP reason you use Power BI Premium today”? The variety of responses David received were fantastic. Below is a summarized list of some of the responses he received. It’s more cost effective when you hit a certain number of read-only users (i.e. 500+) Larger datasets Greater refresh frequency Embedded reports Linked and Computed entities in Dataflows The features above certainly aren’t an exhaustive list of all the benefits of purchasing Power BI Premium. In the sections below, we’ll discuss three other features of Power BI Premium that you should consider if your organization is implementing, or already using, Power BI service.  Honorable Mention – XMLA Endpoints Okay, I lied. We’re going to talk about four features, because I simply couldn’t disregard the ability to connect third party applications to Power BI datasets via XMLA endpoints.  What is the XMLA endpoint? Power BI datasets leverage the same Vertipaq engine that Analysis Services tabular models are based on. While Analysis Services’ purpose has always been to serve as the enterprise semantic layer for different reporting tools (i.e. Excel, SSRS, Power BI), Power BI datasets could only be leveraged by Power BI reports and Excel. That is, until Power BI Premium introduced the XMLA protocol for workspaces and datasets that reside in dedicated capacity.  These XMLA endpoints allow client tools to establish a read-only connection to your Premium workspaces and datasets. This means that the following client tools can all connect to your Power BI dataset: SQL Server Management Studio (SSMS)  DAX Studio Power BI Desktop  Power BI Report Builder  And even Tableau The ability to connect third party applications to Power BI Premium workspaces and datasets is a giant step toward Microsoft’s long-term vision of Power BI Premium becoming a superset of Analysis Services. This feature should only improve in the coming months as the ability to also allow write operations are now in public preview. You can check out the announcement here!    Connecting to a dataset that resides in a Power BI Premium workspace from DAX Studio Top Three Features   1. Paginated Reports The first Power BI Premium feature we selected was the ability to leverage paginated reports in Power BI service.  What are Paginated Reports?  Paginated reports have been a key component of Microsoft’s Business Intelligence offering for 15+ years. Paginated reports are commonly referred to as “SSRS reports”, since they originated in SQL Server Reporting Services (SSRS). Paginated reports are a great compliment to Power BI reports. Whereas Power BI reports tend to be more visual and interactive in nature, paginated reports are great for showing tables of data that can be heavily formatted, while also spanning hundreds of pages. Power BI Report Builder Power BI Report Builder is a client tool that can be used to author paginated reports. Power BI Report Builder can be downloaded from the download menu within the Power BI service (it’s called Paginated Report Builder in the menu). If you’re already familiar with Report Builder for SSRS or PBIRS, then you’ll feel right at home with this client tool.       The Power BI Report Builder interface.   Connecting to a Power BI Dataset There is a reason we called out the XMLA endpoint as an honorable mention. When you are authoring a paginated report, the first thing you’ll want to do is connect to a data source. This is a necessary first step before you can even build your report’s dataset(s). Power BI Report Builder makes this process seamless. Shown below, you can see the dialog that appears. Simply sign-in to your Power BI account and a list of available workspaces and datasets you can connect to will appear.    Power BI Report Builder allows you to easily connect to a Power BI dataset via the XMLA endpoint The ability to connect your paginated reports to an existing Power BI dataset encourages both re-usability and a single version of the truth. It’s a best practice to build robust data models (datasets) that can be used by multiple reports and dashboards. Shown below, you can see we have four reports all leveraging the same Power BI dataset. Two of the reports are paginated.  Not a premium feature, but the new Lineage view of Power BI allows you to easily see your workspace dependencies. It is shown here to illustrate that we have two paginated reports referencing a single Power BI Premium dataset. Sample paginated report.   Feature Parity with Power BI Report Server The Power BI team is working hard to reach feature parity with paginated report functionality we’re already accustomed to in SSRS or PBIRS. You’ll discover that the vast majority of features you’re already familiar with work in Power BI Premium. However, you should understand the current limitations of paginated reports within Power BI before moving forward with your implementation.  2. AI Workload Previously, we provided a tour of AI features available in the Power BI ecosystem, as well as an in-depth look at Power BI’s AI Insights from Cognitive Services. If you haven’t seen those posts, please give them a read as they go into greater detail than this section will.  When you purchase a Power BI Premium SKU, you unlock the AI workload, which provides additional AI insights during your data preparation tasks. These AI insights are powered by Azure Machine Learning and Azure Cognitive Services. There is no added cost to your organization to leverage the AI workload since the use of the underlying Azure services is already bundled into the price of Power BI Premium.  In addition to the AI insights during query preparation, the AI workload also unlocks AutoML, which provides a machine learning interface within Power BI service. AutoML is essentially machine learning with “training wheels” which allows analysts to create, train, and apply ML models to their dataset directly in Power BI service.  3. Deployment Pipelines (coming soon) Oddly enough, none of the responses to David’s tweet mentioned Deployment Pipelines. The most likely reason is because Deployment Pipelines still won’t be available for preview for several months. That’s right, the last feature we selected is one that isn’t even in preview yet, but that should indicate how much of a “game changer” we think Deployment Pipelines could be. Deployment Pipelines were first introduced to the community last November at the Ignite conference. Deployment Pipelines will significantly improve the deployment process of Power BI assets across different environmentally defined workspaces (i.e. development, test, production). Even more, Deployment Pipelines will be baked into Power BI service as a new user interface. This feature should be welcomed by any Power BI administrator or developer who has bemoaned the need to leverage the API or PowerShell module to facilitate deployment tasks. If you’re not currently familiar with Deployment Pipelines, we encourage you to check out the Ignite announcement video here.  Wrap Up Many Power BI customers with less than a few hundred users immediately dismiss Power BI Premium due to its price tag. However, Power BI Premium can bring many value-add features to your Power BI implementation that could immediately justify the cost. In addition to its cost-effectiveness for larger implementations, support for larger models, and ability to leverage third party applications, we highlighted our three favorite Premium features: Paginated Reports AI Workloads Deployment Pipelines BlueGranite can optimize your investment in Power BI with our Catalyst for Modern BI. To learn more about the key pillars of successful deployment and adoption addressed by BlueGranite’s Catalyst approach, register for our upcoming Power BI Office Hours. Additional links: Additional Power BI blog posts from our team at BlueGranite. "
"24" "As companies are moving to remote/virtual work in droves because of COVID-19, many people are becoming virtual employees for the first time.  Entire departments and companies are learning how to be a virtual team in a trial by fire. To help you out, BlueGranite has compiled a list of ideas/concepts to help you be effective virtually.  Every company is different, but these concepts have worked well for us in our decade of experience as a remote company.   1. Have a Video Conference Tool and Get Comfortable with It A good video conferencing tool is imperative for virtual work. BlueGranite uses Microsoft Teams, but there are many great tools out there; as long as everyone is on one platform and is actually using it, you’re on the right track.  As everyone gets started, make sure you  leverage the video and audio capabilities of your tool.  The ability to actually see your coworkers helps folks transition and keep meeting participants engaged.  Since this is new, it may take a while to work out the kinks. Not everyone will have a PC camera, microphone, and/or a headset right away.  Encourage employees to do test runs with their “setup” prior to getting on real meetings. While you get going, remember to , pardon the interruption – no, not the show on ESPN —but your coworkers.  As we all make life changes and perform our civic duty to self-quarantine and help control the pandemic, be patient that your coworkers will have background interruptions.  It will level off as everyone gets used to their routine.   Finally, video conferences can take up a lot of internet bandwidth.  Make sure the teenagers aren’t ruining your virtual meetings by chewing up your bandwidth.  It helps to enable Quality of Service (QoS) rules on your home router to make sure your work tools are given priority.   2. Connect Socially, but Virtually Many people struggle making social connections at work without in-person interaction, but they natively know how to do this in their personal lives through texting and social media.  Encourage your team to adopt some of these virtual networking concepts in your new workplace. It helps to get employees chatting about fun stuff as well as work.   At BlueGranite, we have Channels in our chat platform for sharing Family Fun, Watercooler Talk, I’m new, and Industry News.  We also have a weekly staff meeting to discuss a collection of topics including wins, learnings, and strategy.   3. Have a dedicated work area if possible Considering everything going on right now, this one is the hardest.  If possible, have a dedicated workspace separate from normal living space. Create the mental border that says, \"When I'm here, I'm working.\"  This can be challenging with kids at home doing virtual schoolwork and spouses/significant others also working from home. Consider lighting, internet connection, and ergonomics – you may consider sitting and standing throughout the day as standing is better for your back.     4. Track Tasks and Initiatives using a Virtual Tool Use a tool to keep track of initiatives and tasks so everyone can see status.  This could be done in an Excel spreadsheet or a more formal tool like Microsoft Planner, Monday.com, or Trello.  Software development teams should continue using their familiar tools to track sprints like DevOps, Rally, or Jira.  To keep things moving along, try having a daily call to discuss how you are tracking towards your goals and what is holding you up.  If you’ve never heard of SCRUM or Agile, regardless of whether you are IT or business, I’d encourage you to check it out: “Scrum: The Art of Doing Twice the Work in Half the Time”   5. Track Your Time This one might be the weirdest for most people.  Try tracking the time you spend actually working and discount time for taking breaks and stepping away. Take notes on what you worked on as it is very difficult to reflect on a past week and recall the information.  Many consultants at BlueGranite use time tracking apps like Toggl to keep track of our time and activities.   You may also end up working less during regular business hours and more during non-regular business hours as you increase your flexibility to take care of your family and other obligations.  This is ok, and you and your co-workers should be patient. If you will be stepping away, make sure to block your calendar and update your status to reflect what you are doing.     Use Power BI for Self-Service Data Analysis This wouldn’t be a true BlueGranite blog if we didn’t mention data and analytics.  As more and more companies go remote, the self-service capabilities of Power BI are crucial in empowering everyone to do their jobs efficiently.  The Power BI Service enables safe and secure viewing of data from anywhere in the world with an internet connection. You can: View existing reports and dashboards Tap into existing datasets in the Power BI Service, and Creating new reports right in your internet browser. With Power BI Desktop, you can access cloud data sources from home and on-premise data sources using your VPN connection.  When you are finished with your models and analysis, you can publish and share your reports to the Power BI Service even when remote.  Making traction on strategic initiatives remotely can be a challenge. BlueGranite consultants know how to execute projects remotely and successfully from design/prototyping, building out solutions, and driving adoption. If you need help starting or completing a Data or AI project, BlueGranite can help.  "
"25" "My colleague David Eldersveld recently wrote a blog post showing how to leverage a subset of Microsoft Cognitive Services in Power BI, via AI Insights. AI Insights allows you to detect the language and sentiment of text, extract key phrases from text, and tag images. As David pointed out, enhancing Power BI data models with AI Insights can be beneficial, but you must have Power BI Premium to take advantage of this added functionality.  Fortunately, you can still perform sentiment analysis for free without Power BI Premium if your organization is using SQL Server 2017 or higher. To do this, you will need to use a special stored procedure that leverages R or Python to access a pre-trained model to score your data. Like any stored procedure that returns a data set, it can then be used as a data source for your Power BI data model. Benefits of the Pre-Built Sentiment Model Sentiment analysis can be extremely useful across multiple verticals. Some possible scenarios for using the pre-built sentiment machine learning model to enhance Power BI data models include: Retail: Add an attribute to the customer dimension table that shows the sentiment score for each customer, based on their reviews. Insurance: Add an attribute to the insured dimension table that shows the sentiment score for each insured person, based on historical call logs associated with them. Retail: Add an attribute to the product dimension table for each product, based on the reviews for that product. While not by any means an exhaustive list, the above gives you an idea of how this feature could be applied. There are also some technical benefits associated with SQL Server Machine Learning Services that your IT team might love, including: Secure data scoring environment: Security can be a concern when performing data science. Data is often moved from a secure database to a less secure area when data science tasks are performed. With the addition of Machine Learning Services in SQL Server, security is no longer an issue – it allows you to perform data science tasks in one of the most secure locations in the enterprise: the database. Easily add ML scoring to your ETL process: Since scoring is done inside a stored procedure, it can be accessed in the same way that you traditionally accessed stored procedures during ETL – via SQL Server Integration Services (SSIS), Azure Data Factory, or, as we illustrated in this example, Power BI. Operationalized data scoring: SQL Server Machine Learning Services gives you the option to store your models in a SQL Server table, making them more easily accessible to your R or Python stored procedures. This makes operationalizing your models much easier than historical methods. Efficiently score big data sets: One of the historical pitfalls of data science was not being able to score data because it was too big to fit into memory. SQL Server Machine Learning Services gives you access to functions that enable you to overcome that problem. With SQL Server Machine Learning Services, you can score data in chunks, enabling you to score data too big to fit into memory. Ready to see how it works? Let’s dive in with an example.  The Required Steps   1. Verify that SQL Server Machine Learning Services is installed. This example requires several installations on your instance of SQL server: SQL Server Machine Learning Services Pre-trained models As of this writing, there are two pre-trained models available: one for sentiment analysis and another for image classification. This example focuses on sentiment analysis. Both of these installations are freely available to the on-prem version of SQL Server 2017 and later. For more information on how to install these on your instance, reference this article for SQL Server Machine Learning Services and this article for pre-trained models. 2. Start defining the special stored procedure. For this example, we have used a data set of simulated comments that are warehoused in this GitHub repo.  Let’s start by creating an R stored procedure that will use the pre-trained model built by Microsoft to score the comments. We will name the stored procedure [dbo].[getSentiments_R]. We need to declare four variables for this stored procedure:  @RScript, @Query, @InputDFName, and @OutputDFName. Here is the T-SQL used to define the variables: DECLARE @RScript nvarchar(max);DECLARE @Query nvarchar(max);DECLARE @InputDFName nvarchar(128) = 'dfInput';DECLARE @OutputDFName nvarchar(128) = 'dfOutput'; The @RScript variable will hold the R script that will do the scoring. The @Query variable holds the SQL statement representing the input data to be passed to the R script for scoring. The @InputDFName and @OutputDFName hold the names we will use for the input and output data frames, respectively in the R script. If we don’t specify names, the default names will be InputDataSet and OutputDataSet.  3. Set the values of the @Query and @RScript variables. Next, we need to set the @Query variable with the string that represents the T-SQL for our input dataset. We also need to set the @RScript variable to hold the R code that will be used to perform the sentiment analysis. The T-SQL script for the @Query variable in this example is as follows: SET @Query = 'SELECT [id], [text] ' +             'FROM [dbo].[SentimentData]' It is a simple SELECT statement that grabs the data from the [dbo].[ SentimentData] table needed for the R script. Next, we define the R code needed to score the data. Here is the code:  SET @RScript = 'dfInput$text = as.character(dfInput$text)  sentimentScores <-    rxFeaturize(        data = dfInput,        mlTransforms = getSentiment(                                         vars = list(SentimentScore = \"text\")                                      )    ) sentimentScores$text <- NULLdfOutput <- cbind(dfInput, sentimentScores)' The script is only four lines long because the pre-trained model does the heavy lifting. In the above R script, the first line changes the text field in the dfInput data frame to a character data type. By default, R converts any character-based fields into a data type called a factor when creating data frames. Factor fields are used for categorical data.  Underneath the hood, the data is stored using a method similar to the dictionary encoding used in Microsoft’s tabular engine. Each unique element in the field is replaced with an integer that serves as an index, and a map is created that maps the index to the unique element it replaces. This technique is beneficial because integers are more efficient to work with and have a lower memory footprint than long strings. Factors are good for data analysis, but not for the sentiment analysis we are doing. We need access to the actual text in the text field, and that is not easily done when the text field is stored as a factor. Converting the text field to a character data type alleviates this problem. The next line (actually the next four, due to formatting) performs the sentiment analysis. The workhorse functions are the rxFeaturize() and getSentiment() RevoScaleR functions. RevoScaleR is a R package of R functions built to overcome many of the challenges of working with big data sets. The rxFeaturize() function enables us to access data that has undergone a machine learning data transformation via MicrosoftML. It specifies the machine learning transformation that is being performed in the mlTransforms argument; which, in this example, is a getSentiment() transformation. The vars argument in getSentiment() is used to specify the fields in the dataset that we want to score. If you specify a named list, as we did in this example, then the name of each element in your list represents the name of the column that will hold the sentiment score, and the value represents the field you want to perform the sentiment on. Note: You can perform sentiment analysis on multiple fields at once. You just need to add an element to your named list using the method previously described.  The getSentiment() function will return a numeric value between 0 and 1 for each sentiment analysis it performs. The closer to 0 the value is, the more negative the sentiment, and the closer to 1 the value is, the more positive the sentiment. 4. Use the sp_execute_external_script special stored procedure to execute the R script.  EXEC sp_execute_external_script      @language = N'R'     ,@input_data_1 = @Query     ,@input_data_1_name = @InputDFName     ,@output_data_1_name = @OutputDFName     ,@script = @RScript A special stored procedure named sp_execute_external_script will execute our R code. This stored procedure is used to execute code written in languages other than T-SQL inside SQL Server. This stored procedure supports three languages as of this writing:  R, Python, and Java. The parameters you need to supply depends on the R script. In our case, we need to supply five parameters. @language is used to specify the language we are using @input_data_1 represents the T-SQL code needed to create the input dataset.(@input_data_1_name represent the names that the R script will use to refer to the input data frames@output_data_1_name) represents the names that the R script will use to refer to the output data frames@script will hold the R script that does the sentiment analysis 5. Use WITH RESULT SETS to define the output returned by the sp_execute_external_script. We add the T-SQL code below to the end of our sp_execute_external_script: WITH RESULT SETS (     (      [id] [bigint],      [text] [varchar](8000),      [score] [float]     )) This code is used to add data types and names to the output. Without this code, the resulting dataset will not have column names, and SQL Server will determine the data types. This can cause problems when fetching data from this stored procedure in Power BI. The WITH RESULTS SETS clause prevents this issue by allowing us to explicitly define the name and data type of each column. 6. Call the stored procedure from Power BI. Below is the entire T-SQL script needed to create the stored procedure: CREATE PROCEDURE [dbo].[getSentiments]ASBEGIN     DECLARE @RScript nvarchar(max);     DECLARE @Query nvarchar(max);     DECLARE @InputDFName nvarchar(128) = 'dfInput';     DECLARE @OutputDFName nvarchar(128) = 'dfOutput';     SET @Query = 'SELECT [id], [text], [likes] ' +                               'FROM [dbo].[SentimentData]'      SET @RScript = '          dfInput$text = as.character(dfInput$text)      sentimentScores <-         rxFeaturize(             data = dfInput,             mlTransforms = getSentiment(                                              vars = list(SentimentScore = \"text\")                                          )         )       sentimentScores$text <- NULL      dfOutput <- cbind(dfInput, sentimentScores)      '      EXEC sp_execute_external_script            @language = N'R'           ,@input_data_1 = @Query           ,@input_data_1_name = @InputDFName           ,@output_data_1_name = @OutputDFName           ,@script = @RScript        WITH RESULT SETS (            (             [id] [bigint],             [text] [varchar](8000),             [score] [float]            )        )END After you execute the T-SQL script inside of SQL Server Management Studio, the [dbo].[getSentiments] stored procedure will be added to your data base. Open up Power BI then go to Get Data > SQL Server to get to the form that you will use to add the T-SQL code needed to execute the [dbo].[getSentiments] stored procedure. Below is a image of the form:  Configure the form by populating the Server text box with the server name, the Database text box with the database name, and be sure to expand the Advanced options to expose the SQL statement (optional, requires database) text box. Place the following T-SQL code in that textbox: EXEC [dbo].[getSentiments_R] Click the OK button to execute the stored procedure and expose the output to Power BI. Here is what the output looks like in our example:    You can either add the resulting dataset to the data model as is, or you can make further transformations to the data using Power Query. In this example, an obvious transformation would be to create a categorical field based on the score field that labels each text as positive or negative. We can say that any score greater than or equal to 0.5 is positive, and any score less than 0.5 is negative. Feel free to use a different threshold as needed.  I hope you are half as excited as I am about leveraging SQL Server Machine Learning Services to enhance your Power BI data models. In this post we used R, but this GitHub Repo contains the information you need to do the same in Python. Be sure to subscribe to BlueGranite’s blog to get future blog posts written by the talented team at BlueGranite!"
"26" "For many years, Microsoft offered a simple set of Business Intelligence tools. That all-purpose suite featured a relational database, an ETL (extract, transform, and load) tool, an OLAP (online analytical processing) database, and a reporting tool. There were a handful of additions, but the core was this simple, monolithic toolset. One person could master it and provide world-class BI solutions. Today’s technologists have a wealth of options at their disposal. These abundant new offerings hold exciting possibility, allowing enterprising solution architects to tailor each design to today’s business demands.  Flexible Offerings Replace Multi-tool Approach When formulating their Azure strategy, Microsoft took a different approach. I think they found inspiration in the established trades – builders, plumbers, mechanics, and electricians. Each tradesperson relies on a toolbox. Take the mechanic’s toolbox, for example: a huge chest, with drawer after drawer of instruments. One of those drawers will hold dozens of wrenches and sockets of varying sizes – the opposite of a one-size-fits-all tool. Each instrument accomplishes a single, specific task. The chest will also hold lots of specialized tools that aren’t used often, but that can transform a few nearly impossible feats to easily managed tasks. In any given assignment, a mechanic might use a dozen or more tools to finish a job. Microsoft’s rich cloud assortment gives technologists the same capabilities – offering tools to design tailored solutions for complex needs.  The Microsoft Azure and Power Platform ecosystems – the data equivalent to the mechanic’s toolbox – feature a mass of options compared to on-premises offerings. Microsoft’s database offerings in Azure are a great example. Rather than featuring a single Azure database that does everything, the company offers multiple products designed to excel with different workloads. There’s the traditional relational database in Azure SQL Database (and Azure SQL managed instance), or the massively parallel version in Azure Synapse. Cosmos DB, Databricks or Hadoop-based offerings are some of the more exotic options. Then there are the OLAP databases with Azure Analysis Services and Power BI. The story is the same with ETL: there’s Azure Data Factory, Power Automate (formerly Microsoft Flow), and Databricks. Other categories are rich with choices, too. Superior Instruments Afford Sophisticated Solutions These Azure tools all offer more refined, specialized abilities compared to their on-premises kin. Continuing in the database vein, Azure SQL Database is the relational SQL engine migrated to a cloud environment. While it works for many types of workloads, it best suits OLTP (online transaction processing) workloads. If you are working with a massive data warehouse, you would probably look at Azure Synapse (formerly Azure SQL Data Warehouse) instead. If you need document storage, you would look at Cosmos DB. Designing a solution around the workload would not be new to solution architects. In fact, veterans of on-premise architecture would probably ask the question, “Which tool should I be using? Azure SQL Database or Azure Synapse?” But if we find ourselves asking this as an either/or question, we need to take a step back. In the cloud, the solution might need both tools to operate best. Architects should instead be asking, “Which suite of technologies can work together to handle our needs?” Let’s get a little more concrete with an example. Suppose we are creating a high-volume data warehouse in Azure. We’ve estimated that the data volume we need to support would work well with Synapse. But, there’s a problem – we have lots of control and audit data that doesn’t work well in a massively parallel environment. It’s small but important to the success of the project. When we architects ask ourselves, “Which tool should I use?”, we might use Azure SQL Database on the hyperscale tier. It will scale up and scale out. It doesn’t have any size limitations, unlike the other tiers of Azure SQL Database. But is it the right solution? It may be, but more experienced cloud architects among us will have an “Aha!” moment and ask, “Why not both?” Our scenario might allow for control and audit data to be separated from the bulk of the data. If so, it may make more sense to store the small data in the standard SQL engine and load the bulk dimension and fact data into Synapse. Then we can play each database engine to its strengths, rather than handicapping our solution simply to dump all our data into a single bucket. Complex Data Requires Creative Solutions The flexibility of Azure here allows for this new way of approaching a problem. Because the primary cloud structures are virtualized, we can now mix and match technologies in a way that would be prohibitively expensive and difficult to integrate in an on-premise environment. Play with the building blocks. Mix and match. Embrace the diversity that the cloud enables. Visit our Modern Data Platform solutions page for more information and resources. Or contact us if you have any questions or would like to discuss your organization's needs."
"27" "When it comes to machine learning in Microsoft Azure, there are two main contenders for running your experiments: Azure Machine Learning Service and Azure Databricks. In this post, we will discuss the strengths and capabilities of each service and why you might choose one over the other for all or part of your machine learning workflow. We know that the machine learning workflow and lifecycle doesn't only include training models - there's data prep and performance tracking and, when you're finished modeling, there's also the process of putting your model to work.   Azure Machine Learning Service Azure Machine Learning Service (AMLS) is Microsoft's homegrown solutions to supporting your end-to-end machine learning lifecycle in Azure. AMLS is a newer service on Azure that's continually getting new features. Currently you can use either the Python SDK or the R SDK to interact with the service or you can use the Designer for a low-code foray into machine learning.  AMLS includes functionality to keep track of datasets, experiments, pipelines, models, and API endpoints. Plus, you can easily provision notebook virtual machines, training clusters, and inference clusters right from the site (and, of course, from the SDK).   AMLS allows users to use virtually any machine learning package with the service and also includes functionality for automated machine learning (AutoML) and hyperparameter tuning (HyperDrive). To get started, visit ml.azure.com. Azure Databricks If you've been a reader of the BlueGranite blog for any amount of time, you know we never go too long without talking about Azure Databricks. (See our other Databricks-related content here.) Databricks is a unified analytics platform that allows teams to tackle everything from data platform projects to data science solutions using Apache Spark.  One of the biggest benefits, in my opinion, around Databricks is the interface that allows for easily cluster creation, data management, and user collaboration while coding. That is, users can work together in the same notebook, but one person can be writing R in one block and another can be switching back and forth between Python and SQL somewhere else. This environment really makes it easy for users to work on data preparation for machine learning projects by being flexible in the language that's being used.  Apache Spark prides itself with being the state-of-the-art \"embarrassingly parallel\" system for data processing, but it also has a pretty impressive machine learning library known as MLlib. This highly-scalable library allows for distributed machine learning model training on very large data in a cluster environment. To learn more about Azure Databricks, click here. What About Deep Learning? A hot term you often hear in the AI space are \"neural networks\" and the use of deep learning frameworks to train them. Whether you're a TensorFlow troubadour or a PyTorch pro, both AMLS and Databricks can support these workloads. Both services allow you use to GPU-enabled virtual machines to train neural network models. In AMLS, your experience in training neural network models will be quite familiar to your normal, local training. In fact, if you use Keras, TensorFlow, Chainer, or PyTorch, there are easy-to-use estimators in the AMLS SDK to help you along. To learn more about Deep Learning in AMLS, click here. In Databricks, however, since the underlying Spark engine isn't being used while your models are being trained on the GPU(s), you might find it less cost-effective to run your deep learning experiments in Databricks versus AMLS. (This is mainly due to the fact that you're paying a little extra for DBUs that you're not taking advantage of.) One benefit to deep learning in Databricks is the use of Horovod, a distributed training framework that works with TensorFlow, Keras, PyTorch, and MXNet. This is especially useful if your model or your data are too large to fit in memory on a single machine. To learn more about deep learning in Databricks, click here, or for information on distributed training, click here. But Wait! I Need To Track My Experiments! As I mentioned before, AMLS was built with this in mind from the ground up. Once you set up an experiment, AMLS will keep track of the individual runs of that experiment, including any metrics you want to keep an eye on.  While AMLS is designed around keeping track of everything from machine learning experiments to pipelines and models, Databricks won't be left out. If you want to use Databricks as the place to train your models, but you want to use AMLS to track your results, no problemo! Since the Python SDK is just a series of libraries, you can install them on your Databricks cluster and still take advantage of AMLS. Alternatively, though, Databricks users can use MLFlow as the tracking engine inside of Databricks. This is an open source package that come pre-installed and enable in the ML runtime versions in Databricks.  Source: https://databricks.com/blog/2018/06/05/introducing-mlflow-an-open-source-machine-learning-platform.html A big benefit of MLFlow, which is seemingly simpler/less-featured than AMLS, is that MLFlow will automatically track your MLlib experiment runs with zero configuration!   Let's Talk Operationalization... Once you have finished modeling and you have a model that you are ready to use the model in production, you might want to serve up the model as a callable API. This works well for use cases where you don't have some sort of batch scoring need, but you need your data scored ASAP. AMLS touts super simple deployments. Within a couple clicks (or lines of SDK code), you can deploy your trained model to a Azure Container Instance or Azure Kubernetes Service-based container, complete with a URI that can be called to score your incoming data. This can even be configured with API keys or tokens. You can further customize by configuring a custom Docker image for niche use cases. To learn more about how to deploy with AMLS, click here. As for Databricks, you have 2 options: Take the model out of Spark and operationalize using AMLS or use MMLSpark to make an distributed web service inside your Spark cluster. MMLSpark is a package by Microsoft that allows Spark to handle a streaming workload of data from an API endpoint. This process is great for complex data that need to be processed prior to being run through a trained model. Since the web service is distributed, there API can take full advantage of the nodes of the cluster, making this fast for heavier workloads. To learn more about Spark Serving, click here. Who Wins? In short, it depends. Spark and Databricks surely wins the battle for scalability, especially in your data preparation step of the machine learning process. Plus, Spark's MLlib is also highly scalable and works well on huge datasets. However, AMLS' new UI is really nice and the ease-of-use around the tracking capabilities, model interpretability, and model deployment are top-notch. Plus, for deep learning workflows, AMLS just seems a little easier to get going (not to mention, a little more cost-effective). My oversimplified recommendation? Use Databricks for your heavy lifting (data prep and modeling on large datasets) and use AMLS for tracking, machine learning on normal datasets, deep learning on GPUs, and operationalization. For your machine learning practice, the correct choice might be a blend of both. Since Databricks can use AMLS as the experiment tracker and as the place to deploy models as APIs and since AMLS can use Databricks as a compute target, the cooperativity between these services is obviously a strength that allows both services to shine! Want to learn more? BlueGranite is a top Azure Databricks partner, winning 2018 U.S. System Integrator Partner of the Year award for Databricks. We're also an elite Microsoft partner, helping clients build and deploy modern data platform, modern BI, and machine learning & AI solutions using Power BI and Azure data services. We'd be happy to help you explore Azure Databricks and Azure Machine Learning further.  Contact us today to find out how!"
"28" "Introduction Gateway Performance Monitoring is a frequently overlooked but important part of Power BI governance. Many people think that as long as all data refreshes are successful through the gateway, and the gateway status is online, there is no reason to bother setting up a full-fledged monitoring and reporting process. Instead, they rely on occasional health checks and performance counters. Some administrators may run a Port Test, or open a log file or two during the monthly update process or when unit testing.  But when errors start to appear the Power BI Service, refresh queries begin to time out, and unhappy users and developers come knocking on your door, the ability to answer questions like the ones below becomes very handy: Is there a problem with throughput on one specific gateway node, or an entire cluster? Is there an issue with a particular data source? Are there any particularly inefficient queries that need to be optimized to improve user experience? How do we define inefficiency for DirectQuery (DQ) and Refresh activities? Are there too many scheduled refreshes running at the same time? Can they be better spread out throughout the day until a new node is added to the Gateway Cluster? Even better is having the ability to predict potential throughput problems and simply scale gateway clusters up – or out – before troubles arise. By answering these questions, you can analyze Power BI usage and begin planning ahead : What are the most frequently accessed data sources? What proportion of traffic is DirectQuery, and what is Import? This can inform whether gateway nodes need to be more Memory or CPU heavy. When does the gateway have more activity? How does this change as deployment progresses and user counts grow? What are the overall trends in the gateway throughput? In this blog post we describe the solution developed by BlueGranite; explaining how to gather necessary gateway log data, and providing a detailed overview of the report we designed to leverage it. We mention alternative options, and help you make sense of the data and tools made available by Microsoft. Hopefully this will inspire you to enable Gateway Monitoring in your organization! Gateway Logs: What They Are and Where to Find Them In June 2019, Microsoft made Gateway Monitoring much easier via the addition of structured logs that can be enabled in the configuration file, and by providing a starter Gateway Performance Power BI report template to visualize the results. These logs give full access to the information, minimizing the need to pull counter data from IT management systems, like the Microsoft System Center Operations Manager (SCOM); or run the Windows Performance Toolkit and dig through dust-covered gateway info and error log files; or pull refresh error messages from the Power BI Service. Enabling structured Gateway Monitoring logging doesn’t add any significant load to the server, so that’s another win. As of January 2020, there are four log files, and these are the main things you need to know to follow along: Query Execution – Information for each individual query that was issued to an on-premises data source. It also has data about a parent Request ID. For example, user interacting with a visual in DirectQuery mode or user running on-demand dataset refresh each issue a series of various queries with two distinct Request IDs. Query Start – This one is essentially an extension of the Query Execution log with information about Query Start Time in Coordinated Universal Time (UTC) and base64 encoded full Query Text. In the context of the questions we are aiming to answer, this is pure gold! Query Execution Aggregation – Query Execution data aggregated for a predefined period grouped by data source, query type (DQ or Import Mode Refresh) and Query Status (success or a failure). System Counter Aggregation – Base performance data on CPU and Memory counters for a predefined aggregation period, grouped by counter. It captures data about system, gateway, and mashup engine. As of the time of this writing, no information is available regarding the report name or dataset name that these queries were executed from. However, we can establish a link to Usage Auditing activity logs via the Request ID for on-demand data refreshes as a consolation prize. See our companion blog post explaining Usage Auditing and Tenant Inventory done right! There are a few additional parameters related to these logs that can be controlled in a main configuration file, such as aggregation period span, log file size, and number of log files of the same type to keep. It makes sense to change the last two from defaults when a centralized extraction and reporting process is established. Connecting to These Data for Reporting For organizations with less complex or voluminous data, a process to copy all log files from each respective gateway machine to a central location (file system or a data lake) may be preferred over the more robust architecture defined below. However, in the long run, we recommend implementing a full-fledged solution to extract and store these files in a designated Power BI Monitoring Database, coupled  with additional Usage Auditing and Tenant Inventory data. The example architecture for this process is shown in the image below:    Actionable and Effective Gateway Performance Report Design Now that we are ready to consume the data and gain insights, what are the next steps to consider? In June 2019, Microsoft released a report template, which is a great place to start to get data loaded and processed. There have been some great improvements to the template over the past several months, but in its current state, organizations would still have a lot of work to do to add more insightful, actionable visualizations, and a more intuitive report layout. The Gateway Performance Report template, developed by BlueGranite, uses Microsoft’s as a starting point but expands and builds upon it to provide an actionable, easy-to-navigate, and use, report. The most notable features include: A defined Gateway Node dimension, and classification of Gateway Objects by Cluster and Environment, such as Test and Production. An optional toggle switch between default UTC and desired local time zone. Parameters built specifically to track down long-running DirectQuery queries (threshold is in seconds) and long-running refreshes in Import mode (in minutes). Normalization of CPU counter data. The ability to run a comparison to a user-defined time period (also normalized) so that last week’s performance can be compared to last month, last quarter, etc. Actionable, easy-to-understand visuals. A full-fledged Data Dictionary to help user experience. Concise grouping of Data Source Types The Data Model for the report has the following structure:  Primary Fact tables are the actual log files (please note that we chose to incorporate the Query Text field from the QueryStart log into the Query Execution Stats and not bring the entire log into the model).  Primary dimensions are Gateway Nodes, Query Type, Query Status, and Data Sources, along with standard Time Intelligence ones.  Inactive Relationship between Dates and Previous Dates is used to set up comparison period visuals. In this example, the client also wanted the Time Zone switch toggle to display data in local time. We accomplished this using role-playing Time and Date Dimensions, and a disconnected Time Zone Control table. Every Measure was configured with a SWITCH() DAX. Additional time zones can be added to accommodate users in different global locations. The first page of the report provides insights into CPU metrics with options to display charts and comparisons for max or average counter value within the aggregation period, and during times when actual query activity was occurring. All CPU counters are normalized by the number of CPU cores, so that the maximum value will not exceed 100%. A warning line is placed on the line chart and alerts can be set up in the Power BI Service to email responsible team members.  The environment variable is added to the ‘Filters on all pages’ pane to allow users to quickly switch between Production and Test Clusters:  The Memory Counters page is set up very similarly to CPU.  In this particular case, a responsible IT team member may want to take a look at NODE B  to see why average memory usage is slightly, but consistently, higher.  It could be something simple as a different verbosity level setting enabled in a core configuration file. The Query Throughput Page is our main knowledge gold mine, providing insights into the type of queries run, their total counts, and frequency.  By looking at the Average Daily Query Throughput column chart, with a filter set to display only Refresh values, one can see the busiest times. Conditional formatting can be set to notify report viewers when these values exceed a certain threshold. In the visual below, it looks like very few daily refreshes are set to 9 a.m. local time. When a new set of reports are published to the service, this time slot can be recommended.  If the Queries Run by Gateway Node tree map visual does not show a fairly even split among the load on all gateway nodes in the cluster (with the load balancing set to on in the Gateway Admin portal) – Houston, we’ve got a problem! The next level in the hierarchy will display Data Source Type Groups, which are derived from the initial Data Source Types category provided in the Microsoft template by folding duplicate types, so that SQL+Web and SQL+Web+Web+Web sources will show in the same category. Actual data sources are another level down.  “Wait a minute!” one may say. “I thought cloud sources did not go through the gateway; where are those Web ones coming from?” The web sources you see here are either internal URLs or references to files stored on SharePoint Online with an Organizational-level privacy policy, so when a query is issued to a SQL database  on-prem, using data from an online source, one will see both sources listed in the logs. It did not make sense to apply the same criteria to analyze DirectQuery and Refresh activity; hence, we designed two separate pages of visuals to highlight what matters most. Looking at the key cards, one can quickly see how many queries and requests there were, the average query and request execution times (in this case, a report-interaction level), if there were any failures, and what, exactly, the errors were.  For DirectQuery we are tracking performance in seconds. We wanted an easy way to find both queries and their parent requests taking longer than a certain time to execute. The Seconds Threshold parameter will filter the table visuals to display just the queries over an established length. From there, a detailed Drillthrough page is set up to show all corresponding queries, data sources, execution times, and full, decoded Query Text, which is our best partner in this detective work.  In the example above, we do not see any queries taking longer than 10 seconds to execute, however we can see some requests taking that long, due to a larger number of shorter queries running together. We can advise report designers to disable unnecessary visual interactions, or check whether there are other optimization techniques we can employ. For refresh activities that are usually performed over larger datasets, and queries and transformations that would not have been possible in DQ, we are thinking in minutes for our threshold parameter. Data Processing duration also comes into play here.  Finally, we added a Data Dictionary  to help users make sense of the data.  We hope our insights can encourage Gateway Performance Monitoring in your organization, and inspire report design. BlueGranite is committed to continuously evolving our solutions. Contact us today – we welcome feedback and are eager to hear about any other visuals or report design features that could be useful to you! For more information and resources on Modern Business Intelligence and Power BI deployment, check out our Catalyst for Modern BI resource page. Good to Know It is worth a reminder that the picture will not be complete without also establishing the Usage Auditing and Tenant Inventory reporting mentioned earlier, and described in our companion blog post. These can help you find unauthorized personal gateways, unused data sources, and maintain full access lists. In addition, please keep in mind that it is still possible for scheduled refreshes to fail on the service, but not show errors for these requests in the gateway logs. In cases with complex models, using both cloud and on-premises sources when refresh is executed, the cloud source can time out or throw an error that will fail the refresh, while all gateway log activity completed by that point in time for this same request will show successful queries going through to sources located on-premises. Additional Links Instructions on how to set up Gateway Logging and the Microsoft Gateway Performance Power BI template download are available here."
"29" "Hosted in the beautiful San Francisco from January 27th to the 30th, the rstudio::conf(2020) kicked off with two days of training followed by two days of jam-packed session on everything R. Every year, this conference grows and welcomes more and more statisticians, data scientists, researchers, viz experts and more. This year, the conference attendance jumped to an impressive 2,242 people! In this post, I want to recap some common themes that I heard at the event along with some cool packages I learned about along the way.   RStudio is now a B Corporation At the keynote on day 1, RStudio CEO, J. J. Allaire made the announcement that RStudio, Inc. is now RStudio, PBC, a Public Benefit Corporation. This new structure allows for the company to focus on open source development of software and put their mission and other stakeholders on equal footing with shareholders. It will be interesting to see how this change allows RStudio to balance the creation of professional products (like RStudio Server and RStudio Connect, which they sell to companies) with the continuation of making amazing open source products that benefit us all. To me, the most interesting part is the open annual reporting of RStudio's contributions to benefit everyone, which can be seen at bcorporation.net/directory/rstudio. Read more about their change to a B Corp here. Cool Packages Many of the sessions included speakers talking about their development of innovative packages for a variety of purposes. Here are just a few that stuck out to me as either really useful or just plain fun... vctrs Provides size and type-stability to vectors and avoid undesirable behavior when mixing different S3 types.  vlbuilder Create interactive Vega-lite graphics in R.  tidymodels  A suite of packages that includes a core set of packages for modeling and statistical analysis using the grammar and data structures of the tidyverse. Includes recipes, parsnip, and tune, which are awesome packages for data preparation, ML modeling, and tuning, respectively.   brickr Create 2D and 3D LEGO® art and generate real step-by-step instructions for building your brick art in the real world.    Asynchronous Programming In addition to the packages listed above, I attended quite a few talks around scalability in R. Out of the box, R is a serial, scripting language.  Due to the hard work of some developers, we can now use packages that allow R to complete tasks asynchronously. In other words, we can do scale our code by doing multiple tasks at once. Previously, this was done using the foreach package along with a package like doParallel to parallelize iterations of a task. Henrik Bengtsson's package, called future, provides a very simple and uniform way of evaluating R expressions asynchronously on whatever resources the user has available. This is accomplished by using the new %<-% operator and giving the package a \"plan\" of how to execute.  library(future)plan(sequential)# or plan(multiprocess), plan(cluster), etc.y %<-% {     x <- 22 * x}    One complaint programmers have about distributing your code is that it's difficult to tell how much work has been completed while you're waiting. Bengtsson's solution to this is the progressr package, which will allow for overall progress of a task to be returned to the user, even when distributing the task across CPU cores or machines in a cluster.  library(progressr)slow_sum <- function(x) {p <- progressr::progressor(along = x)sum <- 0for (kk in seq_along(x)) {Sys.sleep(0.1)sum <- sum + x[kk]p(message = sprintf(\"Added %g\", x[kk]))}sum}with_progress(y <- slow_sum(1:10))[1] |===================== | 40%    Lot's of Shiny For those of you who don't know, Shiny is a web development framework for R. It allows for R programmers to create responsive web applications easily using R and all your favorite packages. Traditionally, Shiny works by rendering a Bootstrap page complete with UI elements, graphics, and the works. If you've ever created a Shiny app before, you've probably used the shinythemes package to quickly give your app some color and style. However, if you wanted to completely change all the styles of a Shiny app (for example, to match your company's brand standards), you have to write custom CSS. Now, thanks to the bootstraplib package, you can easily make global variables in your R code to finely tune your style. And there's even a theme customizer to interactively try out a new look.  Lastly, developers often create Shiny apps to provide an easy-to-use interface into a particular type of analysis, data, or visualization. Let's take my application for example: StrainHub. In a nutshell, StrainHub is a phylogenetic tools built in Shiny that allows researchers to build transmission networks from metadata. In the screenshot below, a transmission network is generated using a dataset of Hepatitis C isolates. (Specifically, a phylogenetic tree + accompanying country metadata.) While this application is a great interactive tool for epidemiologists or public health researchers that don't want to use R to generate the visuals, Shiny apps can be a bit ephemeral in the sense that reproducing this output (for research transparency and publication purposes) requires that the user have the exact data and analyze it using the same version of StrainHub in the future to guarantee the same result. This poses a problem if there are code changes down the line. Welcome, shinymeta! The shinymeta package provides tools for auto-generating R code that captures the logic from your Shiny app. In other words, a user can now interact with your app and then generate a code and output bundle of the results. This is PERFECT for researchers who want to use someone's app and want to publish on the results of the app as this allows them to create reproducible artifacts from the analysis in Shiny. As a researcher myself, this was one of the coolest things I saw at the conference! R + Microsoft = <U+2764> In previous years, you may have noticed Microsoft's focus on the use of Python in Azure Machine Learning Service (AMLS). However, as of November 2019, Microsoft has released an R SDK for AMLS. For more information about the R AMLS SDK, see: https://azure.github.io/azureml-sdk-for-r/. Also, if you're an Azure Databricks aficionado, you already know you can take advantage of the R API for Spark (SparkR) or you can use RStudio's sparklyr package for a more dplyr-like experience. For more information about R in Azure Databricks, click here. Resources You can find the recordings of all the conference sessions here: https://resources.rstudio.com/rstudio-conf-2020 ...and Emil Hvitfeldt has curated a list of the slide links, etc. here: https://github.com/EmilHvitfeldt/RStudioConf2020Slides"
"30" "There are plenty of reasons to monitor your organization’s Power BI adoption. At any given time, you need to know what content has been published, who is accessing that content and how often, whether users are interacting with content through a desktop, mobile device, or through the Power BI Mobile App, and – perhaps most importantly – what actions your users are performing in the Power BI Service.  If you have ever tried to make sense of the usage metrics exposed within Power BI, you have probably noticed inconsistencies or missing data when compared to your Office 365 Logs or the new Power BI activity log. This can happen for a few reasons: Power BI usage metrics rely on the user’s browser. Tools like ad blockers or pop-up blockers can prevent data from being properly transmitted from the user’s browser session. Inconsistent network connections can cause data loss. Certain types of events are not captured by the Power BI usage metrics but are captured in the Office 365 audit log. Activities may be over-counted in the Power BI usage metrics if the user does a lot of refreshing of their browser. BlueGranite has created a solution within our Catalyst for Modern BI that leverages the Office 365 Management APIs to extract and store all available Power BI audit data. Simultaneously, a separate process retrieves data through Power BI Admin APIs to enumerate tenant inventory. Two Power BI reports – Tenant Inventory and Usage Audit – make use of these data to answer those questions above that all Power BI Administrators have. This blog will provide a 50,000 foot view of the technical solution, and show a closer look at how audit data is used to answer critical Power BI Adoption questions about assets and use of the Power BI Service. Keep your eyes peeled for a companion blog detailing our Power BI Gateway Monitoring solution coming soon! Overview of the Data Extraction and Storage Process  Sample architecture for BlueGranite’s Tenant Inventory and Usage Auditing solution. In the above architecture diagram, data from the Office 365 Audit logs is retrieved through PowerShell scripts authenticating via an Azure Active Directory (AAD) App and stored in a Data Lake or File System. Additional scripts write only the Power BI-related audit logs to a database on a SQL Server, which can be either in the cloud or on-prem, parsing the original JSON along the way (with a few exceptions that are handled in the reporting layer of the Usage Audit report). Additionally, a series of PowerShell scripts perform calls to the Power BI Admin REST APIs, authenticating via an AAD-only Power BI Administrator account. These data are used to enumerate the existing Power BI tenant inventory, including Workspaces, Datasets, Reports, Dashboards, and Users. Just as with O365 log data, these parsed JSON data are stored in the database on a SQL Server. Finally, data models and reports are created in Power BI Desktop to give Power BI Administrators insights into their tenant. Tenant Inventory Report The Tenant Inventory report can help Power BI Administrators answer essential questions about their Power BI adoption, like: How many Test, Production, and Personal Workspaces have been deployed? What are the assets associated to each of those Workspaces? What permissions do my users have in each Workspace? The data model for the Tenant Inventory report is as follows:  Workspaces are the “parent” data element, with the other inventory components existing as “child” elements. For example, Datasets are identified by the workspace in which they are housed; Users are identified through the Workspaces to which they have access. The exception is for Reports, which have an ID joining them to both a Workspace and a Dataset. A Data Dictionary table is included to allow report consumers to see the different attributes and their definitions. The report itself breaks down each element into its own report page, beginning with Workspaces. In this example, additional DAX is used to identify which Workspaces are the Personal “My Workspace,” which are considered Test, and which are considered Production (categorized in this report as “Workspace Level”). This allows Power BI system administrators to see how many of each type of Workspace exist, as well as the additional data elements – Datasets, Reports, Dashboards, and Users – associated to each Workspace. One caveat regarding Users: only users in V2 Workspaces – those decoupled from O365 groups – can be retrieved with their Workspace association through the Power BI Admin APIs. V1 Workspaces are represented by the “Group” Workspace type in these reports. In other words, we know when V1 Workspaces exist, but we can’t see who has what permissions within that Workspace.  A drillthrough page (not pictured) lists each data element per Workspace by name. Users are also identified as either individuals or AD Groups, and their role within the Workspace is identified: Admin, Member, Contributor, or Viewer. Next, the report identifies Datasets within the tenant. Counts of Datasets by Workspace Level; Workspace Type – where PersonalGroup includes “My Workspace”; Group, those associated with an O365 group (the “old” Workspace experience where Workspaces are tightly coupled with O365 groups); and Workspace (the “new,” decoupled Workspace experience); and which user configured the Dataset. Here, we have also tagged if the Dataset is Refreshable, either through a DirectQuery connection or through a scheduled refresh through a Gateway.  The next element is Reports. Similar to the Datasets page, counts of Reports by Workspace Level and Workspace Type are provided, as well as the number of reports associated to each Dataset. They are also listed in their respective Workspaces in a table visual.  Dashboards are the final asset detailed in the Tenant Inventory report. Similar to previous elements, Dashboards are counted by Workspace Level and Type, and listed within their parent Workspace.  Finally, we come to Users. In the screenshot below, we see that there are slicers to examine Workspace membership for a specific user, or through the different Workspace Levels. A duplicated count of users is given in the column chart to illustrate the distribution of roles access across the tenant. The Matrix visual on the bottom left of the report page lists the Workspace and Access for each user.  Usage Audit Report The Usage Audit report can help Power BI Administrators monitor Power BI Adoption in their organization by answering the who/what/when/how of their Power BI tenant: What content are users interacting with? What are they doing? When are users accessing the Power BI Service? How are they sharing or distributing content to colleagues? What is the most critical content based on frequency of access? Who has access to each Workspace? The data model for the Usage Audit report is as follows:  As you can see, the model is a little more complicated than the Tenant Inventory solution. Here, Audit Data is our primary fact table. Supporting dimensions include a boilerplate Date and Time, Apps (referring to Power BI Workspace Apps), and the data elements from the Tenant Inventory solution – Workspaces, Datasets, Reports, and Dashboards. The Gateway Datasource Users, Sharing Information, and Dataset Refresh Schedules are all born of unparsed JSON fields originally contained in the Audit Data table. The ID Bridge is exactly that – the link between Audit Data and those unparsed elements. Finally, to the right, you’ll see the Data Dictionary for Audit Data, as well as hidden tables which act as disconnected dimensions allowing report users to toggle time zone and rolling average attributes (Time Zone Control and User Averages Control, respectively). Data from the O365 Admin logs and Power BI Admin APIs are stored in UTC. Columns representing local date and time were added to the PowerBIMonitoring SQL database. The Time Zone Control table acts as a toggle to display measures in either UTC or Local Time. Additional time zone information could be added to support local time reporting for various global regions. The first page of the Usage Audit report details the activities being performed in the Power BI Service. In the example below, more than 37,000 unique events distributed among 295 distinct users have occurred in the tenant since monitoring was enabled in July 2019. We can see the peaks and valleys of use (activity tends to dip on the weekends!), the top five activities being performed in the Power BI Service, and additional visuals showing change over time. Right off the bat, we can see that both the use of the Power BI Service is increasing in this tenant, as are the number of users.  Activity Name filters on all pages allow users to see all or specific activity types throughout the report. Their seledtions are displayed in the card on the upper right corner of the report page. All the screenshots here display “All Activities” as no distinct selection has been made. The next page has a similar concept, but focuses on users rather than activities:  Here again we can quickly see how the number of users has increased over the past five months. We also have funnel charts detailing how users are accessing the Power BI Service, whether through their desktop or phone, and which provider they use when accessing through the Mobile App. Having information about the avenue through which users are interacting with the Power BI Service can help administrators understand if additional support or development may be needed. For example, if a high volume of activity is through the mobile app, report developers may need to ensure that their solutions are designed optimally for mobile. A User Details page breaks this information down further, showing specific activity counts for individual users, allowing Power BI Administrators to understand exactly what is happening in their tenant and who is performing those operations. Additionally, measures were written to show the number of days that have passed since a user’s last activity in the Power BI Service. This is particularly powerful information, as perhaps users who do not frequently interact with the Service do not need a Power BI Pro license (if you have Pro licenses in your organization).  The next set of pages in the Usage Audit report enumerate the number of activities and distinct users for the different assets in the Power BI Service – Workspaces, Datasets, Reports, Dashboards, and Workspace Apps. Drillthrough pages provide additional details for specific assets and users. The example below is the primary page for Workspaces:  Many organizations choose to discourage the use of the Sharing feature in the Power BI Service in favor of content distribution through Workspaces and Workspace Apps. Sharing is an activity type that is recorded in the O365 Logs and detailed in the Usage Audit report. Administrators can follow up with any users who may appear to be “oversharing” content.  As you can see, there are many avenues to explore and many questions to answer once you have access to the complete data needed to monitor your deployment to the Power BI Service. Whether you are just beginning your journey into using Power BI or have a long-standing implementation, BlueGranite can help you understand how the tool is being used in your organization. BlueGranite can optimize your investment in Power BI with our Catalyst for Modern BI. To learn more about how our Catalyst Framework speeds deployment and adoption of Power BI contact us today. Additional links: Additional Power BI blog articles from our team at BlueGranite. A complete list of activities audited by the Power BI Service is available here.  Power BI Admin APIs are listed here.  "
"31" "Project Cortex, Microsoft's latest software announcement, puts Knowledge Mining features at your fingertips. Announced at Microsoft’s recent Ignite 2019 conference in Orlando, Florida, Project Cortex is an artificial intelligence-driven engine that tags and organizes disparate data across your Microsoft 365 ecosystem (and other connected systems). Or, as Microsoft CEO Satya Nadella said in his Ignite Keynote, Project Cortex “converts data to knowledge.”  But what exactly does that mean? Cortex uses artificial intelligence (AI) to surface relevant information by tagging documents using Managed Metadata Services and LUIS, Microsoft’s language understanding tool that extracts user intent and key items from natural language. Once it surfaces information, Cortex sorts these knowledge assets into AI-generated Topic Pages and Topic Cards, gathering them in a Knowledge Center (more on that later).   Surfaces, Curates, and Condenses Key Information With so many places to store digital assets in Microsoft 365 – Teams, OneDrive, Outlook, SharePoint, and more – it can sometimes be difficult to remember where you saved something. Trying to find that project estimate? Is it on SharePoint? Maybe the Teams channel? Perhaps it’s in OneDrive for Business? Powered by AI, Project Cortex can mine documents for you; finding key words, appropriately tagging files, and fostering easy access. Let’s explore what that access looks like. With Project Cortex comes the advent of Topic Cards and Topic Pages. When browsing a file in Outlook, Word, SharePoint and Microsoft Teams, Topic Cards will pop up when you hover over a Project Cortex-identified key word. That might be a project name, client name, company initiative, or departmental procedure. The Topic Card provides a brief summary of the subject, points to associated people or subject matter experts, and offers key documents to consider reviewing. From the Topic Card you can step into the Topic Page, which provides a more detailed overview, resources of interest, people, and a spiderweb of related topics that might be relevant. All Topic Pages are housed in the centralized Knowledge Center.   Outshines Typical Wiki Pages Though the Topic Page may sound like a typical wiki page, an important difference is that this page is built and updated by Project Cortex's AI engine. Traditional wikis – which must be researched, created, and updated – can be labor intensive. Here, AI-driven Project Cortex does the heavy lifting, though humans still play an important role: Topic Cards and Pages function most effectively when curated and enhanced by subject matter experts. Project Cortex’s success lies at the intersection of humans and AI. Where AI does the difficult work of surfacing relevant people and resources, humans best shape and refine the information. Features Secure Permissions Sets, Deep Connection Ability Admins out there concerned about maintaining control over who can access which resource can rest easy: Project Cortex uses the same permissions sets as the rest of 365. If someone doesn't have access to a document on a SharePoint site, they also won't have access when that same resource is presented in a Topic Card or Topic Page. Amplifying Project Cortex’s massive potential is its ability to connect, using the new Microsoft Search connectors, to content in third-party repositories and systems. Currently supported connectors are Windows File Share, ServiceNow, Server Database, Intranet Websites, MediaWiki, and Salesforce. It will be exciting to see the creative ways these third-party connectors are used to create robust, truly enterprise-level knowledge centers that span platforms. Exploring Project Cortex Possibilities Project Cortex is rich with potential. Let's think about onboarding a new employee. Starting a new job can be overwhelming for numerous reasons, but one hurdle all new employees face is learning internal and sometimes industry-specific jargon and acronyms. With Project Cortex, each time such language shows up in a Teams chat or Outlook email, a Topic Card can provide a brief overview, and quickly help the new employee understand the reference. It’s also not unusual for organizations to reshuffle resources midway through projects, meaning staff can find themselves tackling a job designed and partially executed by someone else. The time in which it takes someone to familiarize themselves with the particulars of a projects can directly influence its progress. A Topic Page serves as a centralized resource; an easy place to house the project overview, work done to date, key resources, and an explanation of relevant personnel and their roles.  Connecting your people to the content and knowledge they need can spur valuable insights and effective actions. It can also cut costs. Based on U.S. Bureau of Labor and Statistics data, Microsoft estimates a $1 billion dollar revenue company could save nearly $9 million by reducing time for new hire effectiveness by just one week. When technology can increase effectiveness and decrease costs, it's time to take a serious look. Getting Started If you’re already thinking of opportunities where Project Cortex can reshape your business practices, you aren't alone. The possibilities to apply Project Cortex to streamline knowledge transfer and collaboration abound. If you’re ready to dive in, Project Cortex is now in private preview (you can apply here); it is scheduled for public release in the first half of 2020.  Once you have access to Cortex, you’ll likely want to begin in a limited section of your organization's 365 environment (where you’re confident of the security permissions in place) so you can perform some micro-testing. Administrators can choose which sites are available for \"mining\" and which aren't – allowing you to ensure that your security groups are configured correctly. After deciding what part of your ecosystem is enabled for Project Cortex, you can provide taxonomy term sets, acronyms, or even just a list of project names. This gives Cortex a foundation for generating Topic Pages and Topic Cards as it learns what kind of information is key to your organization. Looking to learn more about the latest news in advanced analytics and AI? Subscribe to BlueGranite’s blog today.  Or, check out our other articles and information regarding Machine Learning and AI.  If you have any questions or are interested in exploring Knowledge Mining for your organization, please contact us today."
"32" "Power BI’s integration with and reliance on Office 365 has been a source of great strength in many areas. In other areas, it has been a cause of frustration. Obtaining usage data from the Office 365 audit log is one area that can be complex yet critical to providing a data-driven measurement of Power BI adoption.  The complexity usually stems from organizational security or politics rather than technical aspects. Power BI usage data has traditionally been difficult to get since it requires elevated access rights beyond the Power BI service administrator role. With the Power BI activity logs released in December 2019, service administrators no longer need to try to obtain permissions apart from their existing rights. Any user with the Power BI service administrator role can access the new activity logs either through the API directly or using PowerShell.  What is this activity data, and how is it valuable? The audit/activity log data contains details for every interaction that users in your tenant have with the Power BI service (powerbi.com). Activities such as viewing reports, publishing apps, modifying gateway data sources, changing workspace security, and dozens of others have records broken out by user and timestamp. Using this data, organizations not only know who does what and at what time. You can move beyond a simple audit trail to measure how well Power BI adoption is progressing at your enterprise. In this case, adoption targets for a group’s collective number of touchpoints can be compared to the actuals obtained from the logs—even down to the individual object level. Using the logs in this manner by combining actuals to targets, BlueGranite often finds underutilized reports or other opportunities to improve adoption. Reducing complexity while improving security BlueGranite has helped many organizations large and small with Power BI deployments, and auditing for adoption is a common topic. Depending on organizational policy, some security teams feel comfortable providing the O365 access and others do not. Why? Many organizations do not feel comfortable providing elevated access to Power BI service administrators because it grants access to read logs from other Office 365 projects. In other organizations, service administrators might already be admins for some of these other products, so there is less of an issue. Every business is different. Even though the data can be filtered down to only Power BI activity, the raw O365 data contains usage for other products in its Audit.General category. For example, with the Office 365 audit log, admins see data from Microsoft Teams, Yammer, and other products in addition to Power BI. With the new Power BI activity log feature, admins no longer mix log data with those other products. How does the Power BI activity log differ from the Office 365 audit log? From a Power BI perspective, the Power BI activity log contains the same data you receive in the Office 365 audit log. This data is simply prefiltered for Power BI usage so that only the Power BI data is available to Power BI service administrators. According to Microsoft, the name was changed to help differentiate the two methods of obtaining the data. In this manner, you can have discussions about Power BI auditing and know that the source will be the traditional O365 audit logs versus the newly available activity logs. This helps avoid confusion, and anyone involved in getting data can immediately know by using the term activity log that it pertains to the new access method. Activity Logs for New Deployments Any new Power BI deployment project should consider the new activity logs to avoid additional complexity and any security concerns surrounding elevated permissions. Note that there could be overlap in efforts at larger organizations where Office 365 admins may already be collecting this data for reasons outside of a specific Power BI rollout. In that case, the O365 admins may plan to make filtered audit log data available so that Power BI adoption could be tracked without a new dedicated effort from the Power BI deployment team. Transitioning Existing Deployments What if you already have a Power BI auditing solution built on the Office 365 audit logs, and that works smoothly? In that case, nothing should have to change in the short term. Remember, the new activity log is a different way to access the same data. While not likely, however, simply watch for any possible differences that may appear in the future. Longer term, it may be worthwhile to shift to the dedicated Power BI activity log in order to simplify training and maintenance for Power BI service administrators by reducing your solution’s complexity. For more details of the new activity log, watch this recording of BlueGranite's Power BI Office Hours dated January 2020.        Let’s Roll(out) As mentioned previously, BlueGranite has helped businesses of all sizes with Power BI deployment and adoption. We look forward to implementing the new activity log in our auditing solutions because of the simplified access and reduced complexity. If you’re interested in learning more about how we can help accelerate your Power BI adoption, contact us today."
"33" "I’ve gotten quite a few questions over the past few months about the new Power BI Key Influencers visualization. While many of my colleagues and clients really love the concept and the visualization, others are struggling to understand what exactly it is and how it works. I’ve done some research over the past few weeks on the underpinnings of the Key Influencers viz and have come up with some interesting results.   The example that follows resulted from my work helping an education client analyze student data. In their case, they wondered what effects certain variables had on their senior cohort’s Admissions Index - a measure used within higher education to help qualify prospective students. After computing a few descriptive statistics for the cohort, we began throwing variables into the Key Influencers visualization to see what kind of answers we could come up with. The client was amazed by the visualization and the fact that they now had AI at their fingertips. And then we got the inevitable question: “What do these numbers mean?” The visualization itself has two tabs: Key Influencers and Top Segments. These two tabs operate by way of different models. Key Influencers uses regression models, while Top Segments uses Decision Trees. To make things just a little more complex, Key Influencers also uses two types of regression, depending on the situation; Linear Regression for continuous variables, and Logistic Regression for categorical variables. Models Used in Key Influencers Visual in Power BI   Today we’ll be sticking strictly with the Linear Regression model as our example, as it’s the most straightforward to understand. (For more info on Linear Regression, see my post on Simple Linear Regression in Power BI.) In a nutshell, Linear Regression works by plotting two variables – x and y, or input and output, or independent and dependent – against each other, then calculating a trend line that is the best fit for the model. Without getting into the math, the typical method is to use Ordinary Least Squares, which attempts to find the trend line with the least amount of total distance, or error, between the line and the actual data points. The output of the model is the linear equation.  The one thing that we need to take away from this equation is b. This number is the coefficient of the input. This number describes the relationship between the input and output: b is equal to the amount that y increases for every 1 increase in x. If you recall from your early math days, this is the Rise over Run. In the example we’re using, plotting GPA Average against the Admissions Index results in a coefficient of 26.8217. So, for every 1 point increase in GPA we’ll see a 27 point increase in the index. Now that we understand how to get our coefficient, let’s make it a little more complex and add in multiple inputs like we see in our Key Influencers visualization.  Mathematics aside, we now have multiple coefficients for our multiple inputs.    There’s one problem with this. The scale of these inputs can be (and usually are) completely different. Below I’ve plotted four of our dimensions as histograms to demonstrate. (More on histograms and how to make them in Power BI). Notice how each histogram has its own scale on the x axis. It wouldn’t make sense to compare the coefficients from these inputs when one ranges from 0 to 5, while another ranges from 0 to 20, and yet another from 15 to 40.      To get around this issue, the Key Influencers visualization normalizes the dimensions using their Standard Deviations. In our example, the standard deviation of Composite ACT scores is 4.56 and the standard deviation of GPA Average is .91.  Using these numbers, in combination with their respective coefficients, allows Key Influencers to report the output variable on the same scale, so we are now comparing identical values in our visualization. And we can see from our example below that, according to our model, Unexcused Absences have the greatest impact on a student’s Admissions Index, followed by GPA and Credits Earned, then by Composite ACT scores.    One last thing to note: this visualization uses a proprietary version of the Linear Regression algorithm called Fast Linear. You will not be able to get an exact match between Key Influencers and a standard Linear Regression model using Excel, Python, or R. The coefficients adjusted for standard deviation should follow a similar pattern to the visualization, though. As we’ve demonstrated, at its core, Key Influencers is a visualization of multiple linear regression. Visualizing this type of model has historically been impossible; statisticians skip the visual and report only the model output. This, in my opinion, is the most impressive attempt at visualizing these types of models that I’ve seen to date. It does take a little work to interpret, but the resulting usefulness is limitless, given proper usage. In our example today we’ve seen how we can get insight into what drives student success in education by using Multiple Linear Regression via the Key Influencers AI visualization. This simple example can help educators understand the influences behind learning metrics and home in on students that may be having issues and get them on the correct path to a good education. To reiterate my comments in my previous post, Linear Regression has an endless number of uses. It can offer insights into our budget trends. We can analyze the effect of marketing on sales and profits. Or it can clue a company in to how raising prices may affect a consumer’s buying habits. Insurance companies can also use this technique to assess risk between customer demographics and insurance claims. Next time we’ll discuss some of the pitfalls of Linear Regression, and by extension Key Influencers, and explore solutions to avoid them. Explore more of BlueGranite’s Machine Learning & AI experience here to learn how we can help your organization maximize Power BI’s expansive capabilities."
"34" "Our staff’s passion for technology and collaboration drives BlueGranite’s success. Through the Meet Our Team blog series we introduce you to some of the many people behind our reputation as a trusted analytics partner. Today we’re introducing Josh Smarrella, a BlueGranite industry account lead. Josh partners with clients to strategize targeted, successful modern data platforms and machine learning solutions around specific customer niches.  A graduate of Indiana University’s Kelley School of Business, Josh majored in business management and human resources. He thrives on innovative collaborations and solving complex data challenges. Outside of work, he’s dedicated to his family and mastering Brazilian jiu-jitsu. The martial art requires steady discipline, which he credits with influencing every aspect of his life, including his career. While he has contributed to various management leadership associations throughout his education and career, Josh recognizes some of his earliest experience – with one of the nation’s largest quick-serve dining chains – in helping shape his approach to business, as well as his commitment to community and customer service. “My very first job, at age 14, was with Chick-fil-A,” Josh said. He worked there through high school and college, originally hoping to franchise his own restaurant and serving in nearly every restaurant leadership position along the way. When he was a Director of Leadership Development, Chick-fil-A awarded Josh its Leadership Scholarship award – an education assistance honor bestowed on employees who demonstrate professional achievement, are actively involved in serving the community, and who receive a recommendation from someone in a leadership role with the company. “It was a really cool experience,” Josh said. “My 10 years with the organization greatly shaped and molded me into the business professional that I am today.” Since then, Josh has held roles allowing him to refine his expertise in leveraging data to further positive customer connections and experiences, making him perfectly suited to BlueGranite’s client-first, collaborative ethos. “I am thankful to be part of an organization that encourages personal and professional growth, teamwork, and work-life balance,” Josh said. BlueGranite’s emphasis on collaboration and open dialogue foster the partnerships he thrives on – those that deliver accelerated, cutting-edge solutions and cultivate long-term relationships. Josh said the tenure of his fellow BlueGranite account leaders speaks volumes. While sales roles tend to have a high turnover rate, “You do not see that at BlueGranite; I truly believe that is due to our culture.” He and his wife, Hannah – along with kids Zeke, Paisley, and Lakelyn – also have a passion for travel. While their roots and home are in Indiana, they love taking road trips across the U.S., traveling 8,000 miles and visiting 10 new cities in 2019 alone. Read more here about BlueGranite’s commitment to innovation, or contact us today to discover how our partnership services can help you do more with your data."
"35" "The key to building a data driven culture: Timely insights As digital transformation in business continues, data storage and analytics are becoming increasingly critical to growth in any industry. A data-driven culture, one which enables data-driven transformation, starts with considering how to unlock data trapped in divisional and system data silos, how to ensure data privacy and security, and how to deliver insights cost-effectively and with ease. At BlueGranite we believe that kind of data storage and analytics should be available to all, and digital transformation should be within reach for any company willing to invest in its data. Microsoft's newly announced Azure Synapse Analytics offers limitless analytics through integrating data warehousing, data storage, data engineering, data transformation, Machine Learning modeling, and business intelligence solutions. There is too much in Azure Synapse to explore in one blog post, so today let's just focus on dynamic duo of Data Warehousing and Power BI. Power BI gives users the ability to deliver insights, by creating interactive visualizations from the data gathered, sorted, and stored in a data warehouse. Having these two capabilities under one roof in Azure Synapse give you potential to transform your business through data.  “The biggest barriers aren’t related to technology; they’re all the other issues surrounding the transformation, from being a traditional legacy business to becoming a data-driven organization,”-Randy Bean, CEO at NewVantage Partners. Microsoft's Azure Synapse Analytics wide-ranging services leads to fewer bottlenecks and better business outcomes over time. According to a Harvard Business Review Analytics Services pulse analytics strategies survey, 91% of global executives say effective data and analytics tactics are essential for business transformation; 55% of organizations say data silos and data management difficulties are the biggest challenges to these strategies; and 51% of executives rank self-service analytics for business users at the top of their investment priorities. Here’s why you should consider Power BI and data warehousing in Azure Synapse Analytics to drive transformative change. Improved Analytics Deliver Tangible Business Benefits Better, more timely analysis allows for stronger decision-making and improved outcomes. Data warehousing and Power BI drive data analysis leading to increased revenues, decreased operations and supply chain costs, and faster entry into new markets. These tools also allow both IT staff and business users to be more productive, by spending less time wrangling data and more time analyzing it and planning accordingly. \"The average time savings is 1.75 hours per week. The total risk-adjusted cost savings in effort over three years is $4.9 million.\" -Forrester Consulting’s Total Economic Impact of Microsoft Analytics study, commissioned by Microsoft. Cost Effective Solutions On-premise infrastructure has a high cost in terms of both money and effort. In the past, data analytics solutions have often cost companies more than they were willing to invest for implementation and maintenance. Azure Synapse Analytics with Power BI has both a lower risk and lower monetary cost for implementation, deployment, and maintenance. By moving to Azure for warehousing, storage and compute, an applications manager in the retail pharmacy industry said the \"TCO (total cost of ownership) has been cut in half and our performance is three times faster. That comes out to six times improvement in performance per dollar.\" Empower Users A move to Azure Synapse Analytics with its BI and data warehousing offerings, puts data in the hands of more users, giving them the ability to create their own analyses and valuable reports, increasing the speed of change, and business outcomes. Power BI’s dynamic capabilities empower users to share data – to which they have secure, organized access – in a rich and meaningful way. In the survey referenced above, Forrester Consulting interviewed four Microsoft customers who use Azure Analytics and Power BI solutions, as well as 364 companies currently using a mix of Microsoft Azure and Power BI. Every company interviewed reported improved analytics since moving to Microsoft, and all were pleased with the ease of integrating data sources and the increase in performance. Surveyed companies also saw improved data democratization (the process of making data available to those who need it, rather than just analysts) and results that led to meaningful improvements in overall company performance.    Increased Flexibility  The wide array of capabilities across Azure Synapse Analytics often lead to added business benefits outside those that drove the initial adoption. Once these solutions are in place, organizations find it easy to add additional data sources or initiate new analytics projects. As new Azure features are released they can be incorporated into the ever-advancing tool set, increasing capabilities as technology progresses. Leveraging Azure for analytics also enables  artificial intelligence and machine learning opportunities! Security and Privacy  Azure is known for being the most secure cloud for analytics. Donald Farmer, a well-respected thought leader in the data industry, said the Azure platform \"offers by far the most comprehensive set of compliance and security capabilities of any cloud data warehouse provider.\" Microsoft has also recently added Dynamic Data Masking, which limits sensitive data exposure by masking it to non-privileged users, and Data Discovery and Classification, which classifies, labels, and further protects data, to automatically help protect sensitive data, further enhancing data security and privacy.   Taken together, Azure Synapse with Power BI provides a best-in-class platform for all aspects of the Modern Data Warehouse:The security and flexibility of Azure data warehousing automatically scales up and down to meet your needs with the best cost/performance measurement on the market Enterprise-grade Spark and SQL engines give you powerful processing solutions across a full range of use cases Power BI’s visually rich business analytics capabilities securely deliver custom insights to the right audience, in the right format, on any data Insight from BlueGranite's COO: These days, a lot of attention is being paid to data growth, and rightly so. By 2025, IDC predicts we will have more than quadrupled the amount of data in existence from 33 ZBs to 175 ZB since 2018! And technology has broadly risen to the challenge with scalable cloud solutions like Azure that put price, performance, and scale into an attractive and manageable service. However, an area that I increasingly like to focus upon is highlighted by playing on the words \"data growth\". Sure data growth is impressive, but the growth of data as an asset, in terms of who pays attention to it and leverages it, may be just as significant. Looking back over my own career in data and analytics, the shift in awareness and understanding of the domain has been flat out amazing. Even as data sources and types evolve and grow, the number of jobs, roles, and activities that incorporate data to drive better performance has exploded as well. That same IDC report states: \"By 2025, 75% of the world’s population will have at least one data interaction every 18 seconds\". That's amazing. These days, jobs in almost every corner of every sector are collecting and analyzing data in order to be more productive, more competitive, and more efficient. The benefits of pairing scalable data storage & management technology with an intuitive and user-friendly interface for all data consumers simply cannot be overstated. Azure Synapse Analytics with Power BI unlock performance advancements, security, and insights for businesses, and help to develop a data-driven culture that will propel an organization forward and foster transformational growth.  For more information on Azure, Power BI, or advancing the digital transformation of your company, contact BlueGranite today. "
"36" "I recently had the pleasure of attending the Artificial Intelligence (AI) & Machine Learning (ML) in Power BI track at the Microsoft Power BI Bootcamp for partners. The two-day track was a deep dive into the various AI and ML capabilities currently available in Power BI. Partner attendees got to share the room with members of the Power BI product team over the two days. It was really fun to be able to hear directly from the team why features were designed a certain way, and even offer up feedback of how we think the platform could be further improved. The Power BI team has been working tirelessly to integrate AI capabilities into traditional BI scenarios. Because Power BI is constantly evolving, it’s easy to miss, or be unaware of, specific features. The goal of this post is to provide a consolidated overview of AI and ML capabilities currently available across the Power BI platform. However, since I don’t claim to be a data scientist, we’re going to focus primarily on features that were developed with the analyst and end user in mind.   1 - Different personas in Power BI. Data scientists can add Power BI to their tool belt with R, Python, and Azure ML integration. Whereas, BI developers, analysts, and end users can leverage features aimed to improve their analyses. We're going to focus on these features throughout this post. The Power BI Platform Before we dive into specific AI features, let’s take a 50,000-foot view of the complete, end-to-end Power BI platform. Microsoft MVP and BlueGranite alumna Melissa Coates has a fantastic diagram that displays all of the various components across the Power BI platform. The key takeaway here is that Power BI is much more than a “visualization tool”. Power BI is a mature, modern, enterprise BI platform. Shown below, I took Melissa’s same diagram, but I wanted to focus on specific areas of the platform where we’ll find unique AI features. In the subsequent sections, we’ll touch on each feature in greater detail.  2 - Notice how AI is being integrated into data visualization, data preparation, and data modeling components. Power BI Components Conceptually, I like to think of most Power BI authoring components falling into one of three categories: Data visualization Data preparation Data modeling The Power BI team is investing heavily in incorporating AI capabilities into all three of these categories. We’ll define each category, and the AI features within them, below. Data Visualization With Power BI we use visuals to easily interpret and analyze our data. Power BI now contains three explicit AI visuals, which can be identified by the light bulb icon. Additionally, we can also leverage various AI features baked-into traditional visuals, such as bar and column charts. Decomposition Tree The Decomposition Tree is the latest AI visualization developed by the Power BI team. This is a highly interactive visual that allows you to decompose (break down) a measure by various attributes across different dimensions. This visual can be used for ad hoc, exploratory analysis to understand your data. Or, it can be used to perform root-cause analysis, which is enhanced by the built-in AI functionality. For example, when drilling into the tree, you can choose “High value” or “Low value”. Selecting one of these tells the visual to perform an “AI split”, which will then identify the next field to drill into and display the highest or lowest value, respectively. The Decomposition Tree is currently in public preview, so we can expect many enhancements in the coming months.  3 - Decomposition Tree is the latest addition to the AI visuals in Power BI. It was one of the most requested features for a long time. Key Influencers The Key Influencers visual is a feature that we previously blogged about. Key Influencers is Power BI’s first AI visualization. The visual has seen many improvements since it was initially made available in public preview in February 2019. Key Influencers is a great visual to use when you want to take the guesswork out of analyzing the drivers behind a condition or metric. Check out how we improved our Employee Retention Showcase report to identify the key drivers that affect employee flight risk.  4 - Key Influencers even works on continuous values! Shown above, we're analyzing what influences home sale price increases in a specific county. Q&A Visual The Q&A visual was released in October 2019 as part of the larger Q&A overhaul. Q&A functionality was already available in Power BI reports via a button, but now we have a fully functional visual that behaves and acts like any other visual in Power BI. That means features like applying report themes and cross-filtering will also apply to the new Q&A visual! You can leave the visual in a “Q&A state” so end users can continue to ask their own questions when using your report. Or, if you like a particular visual that was created by asking questions, you can turn the Q&A result into a standard visual by clicking on the icon in the upper right-hand corner. 5 - Shown above are two Q&A visuals side-by-side. One has created a map visual for us based on the natural language query. The other has been left in its default state, with some potential suggestions to start your analysis with. Also released in October 2019, is the Q&A Tooling interface. The new interface allows you to perform tasks such as: Reviewing questions users have asked the underlying dataset Teaching Q&A to understand questions and terms e.g., you can provide translations for both nouns and adjectives teaching Q&A terms in the interface will create synonyms behind the scenes  Managing terms and definitions that you’ve taught Q&A  6 - The new Q&A Tooling interface, which is still in public preview. Insights The last feature of data visualizations we’ll mention is “Insights” which allows AI-powered analysis within traditional visuals we’re all already familiar with. Explain Increase/Decrease This feature can be used to have Power BI identify and explain the increase or decrease between two data points. For example, perhaps you see a large increase in sales between two years. Rather than performing a manual analysis to explain the increase in sales, Power BI can perform that analysis for you!  7 - Having Power BI explain the increase in sales between two data points. Find Where the Distribution is Different This feature is especially useful if you want to easily identify sub-populations in your dataset where the distribution of data is different from the analysis you performed. This feature takes a lot of the guesswork out of your analysis. Data Preparation Data preparation refers to the ETL (extract-transform-load) processes that consolidate, transform, and enrich your dataset. From a Power BI perspective, we’re talking about Power Query within Power BI Desktop, and Power Query Online within Dataflows. Important: Please note that the features that we’ll discuss in the Data Preparation section all currently require Power BI Premium, and the AI workload enabled within your premium capacity. The reason for this is because Power BI is leveraging Azure services, such as Azure Machine Learning and Azure Cognitive Services, under the hood. You will indirectly pay for these features by purchasing Power BI Premium. Just think of them as another perk of purchasing premium capacity! Automated Machine Learning (AutoML) in Power BI AutoML is a feature of Power BI Dataflows that enables users to create, train, and invoke machine learning models. The best part? This is done 100% inside of Power BI! AutoML was designed with the business analyst in mind. I like to refer to AutoML as “Machine Learning with training wheels.” I say that in the most endearing way possible, of course. What I enjoy about AutoML is that it enables business analysts, or even traditional BI developers (like myself), to learn the basics of machine learning, while simultaneously building a model to perform useful predictions. AutoML is now generally available! It currently supports the creation of Binary Prediction, General Classification, and Regression models. We can expect additional model types, such as Forecasting, in the future. We can also expect the ability to export ML models created in Power BI to Jupyter Notebooks in the future. This could be especially useful for analysts, or developers who want to prototype functionality in Power BI, but then hand it off to a data scientist to really refine and standardize the model.  8 - You create ML models within a dataflow. The model is 100% developed within Power BI. Cognitive Services Within Power Query, we can enrich our dataset with prebuilt-AI functions from Cognitive Services. The Cognitive Services functions available to us in Power Query are features that BlueGranite’s David Eldersveld previously blogged about. The four functions currently available are extremely easy to use and can provide additional insights in a matter of seconds. In addition to what David previously blogged about, we can now also access these functions in Power Query within Power BI Desktop!  9 - A look at the new \"AI Transforms\" section in the Power Query ribbon within Power BI Desktop. You’ll still have to log in with an account that has access to a Power BI Premium capacity for this to work. A great use case for these functions is to perform text analytics. Examples include leveraging functions that enable sentiment scoring and key phrase extraction on text data, such as: User reviews Social media feedback Every line spoken from your favorite TV show <U+0001F60A>  10 - Key phrases, from our friends at Dunder Mifflin! Data Modeling Data modeling in Power BI is the process of designing in-memory, enterprise grade, semantic models. In layman’s terms, it allows you to build an experience that enables analysts to build their own reports on top of the model. A well-defined data model also allows Power BI to easily auto-generate insights for you, via features like Q&A and Quick Insights. Q&A Q&A allows users to ask natural language questions against their data model. Q&A has long been a selling point of the Power BI Service, as it has served as a key differentiator when compared to other vendor BI platforms. Q&A functionality recently underwent major improvements that became available in the October 2019 Power BI release. We already mentioned the new Q&A visual and Tooling Interface above. Quick Insights Quick Insights is probably the simplest AI feature to leverage in Power BI. It literally involves clicking on your dataset and selecting “Get quick insights”. Power BI will then run various algorithms to search for, and identify, trends in your data. This is the part where having a well-designed data model will help Quick Insights yield more useful results. Once Quick Insights has completed its algorithms, you can click “View Insights” and determine if you would like to keep any of the visuals provided by pinning them to a dashboard. Reach out to us! Are you excited about any of the features mentioned above, but aren’t quite sure where to start? Reach out! If you want to know more about artificial intelligence capabilities in Power BI or Azure, BlueGranite provides services across Microsoft's Modern BI, AI, and Data Platform tools. Contact us today!"
"37" "For the past several months, I’ve been working on a project in the primary and secondary (K-12) education space – my first in that domain. In a lot of ways, I’ve found the experience to be quite unique when compared to some of the other industries that I’ve worked in, and I wanted to write this as a follow up to a Solution Brief that BlueGranite did on the subject, and to share some of my thoughts and experiences stemming from my time working in the education field – as it’s been a journey that I’ve found to be singularly interesting, often challenging, and ultimately, very rewarding.   To help set the stage a bit with some technical specificity – this project is about building a data warehouse in SQL Server from the ground up, incorporating three primary domains of data from within a fairly large school district with approximately 30,000 students:  student data, financial data, and HR data.   With the data structured and consolidated in one place, Microsoft’s Power BI is then used to provide insights to staff throughout the district using sophisticated analytical logic, powerful aggregation capabilities, and centralized web-service hosting and deployment for ease of collaboration and consumption. Holistic Process, Structured Data Are Key Something I’ve written about in the past (if I can be permitted to go ahead and climb atop my soap box right out of the gate) is that properly structuring data is absolutely paramount for an analytics project – and education is no exception;  if anything, it only underscores the importance.  This isn’t to say that the data we worked with started off unstructured (though it can in some cases, as with sentiment data drawn from survey feedback), or that insights can’t be drawn from raw data, just that the state and structure of our source data is seldom ideal to an analytics practice – both in terms of what is most performant in the analytics tools being used, but also with regard to what is most conducive to imparting meaning and insight to our data consumers.  So how is such an important thing attained?  Aside from the technical particulars of what constitutes a “proper” data structure for analytics – which represents a body of nuanced information far greater than what can be placed into a blog format – I’d like to focus more on the holistic process that needs to be established as a prerequisite to building an analytics solution.  In summary, that process is about garnering understanding, which itself is contingent upon something far more interpersonal and organic than technical:  the establishment of a dialogue between those who work within the day-to-day processes which generate our intended source data, those who are tasked with collecting and structuring that data, and those who will become consumers of the analytical insights which are the essential output of the project.   This represents no small feat of coordination, and often defies some manner of technological “silver bullet” for remediation.  In education, a lot of data is generated by human beings entering data about other human beings; there is margin for error, bias, and assumption.  There are contextual qualifications, such as assessment scoring,  that do not fit neatly into universal schemas.  So it is absolutely critical that the processes which generate source data are understood fully. This is the foundation to any analytics solution, and like anything, the soundness of the foundation represents the stability of everything built on top of it.  As an analytics practice matures, the necessity becomes more apparent than ever.  Advanced analytics, like those on offer in Microsoft’s Azure AI suite, absolutely require a high degree of confidence in the data inputs that they draw upon. The process by which data is structured for analytics is also one in which such confidence is built and standardized across an organization. Breaking Down Siloes Boosts Shared Goals  If those tasked with building and maintaining an analytics practice draw the foundation of their understanding from those that oversee the generation and curation of source data, then we can say that the building plans are drawn from the perspectives of those that will be consumers of the data.  This is the source of our big picture, and the sophisticated logic that crystalizes it.  This is the perspective that provides the basis for breaking down the silos in which our source data is often stored, and the impetus for finding value in the exercise itself.  In education, every facet of a school district’s operation bends ultimately to a single common goal:  improving student outcomes.  This means that a truly holistic view involves not just the data about students and education directly, but also about the district’s function across the board. This includes everything from how budget is allocated to program management efficacy – from staffing of open positions to credentialing and qualifications of those staff.  In terms of structuring data for analytics, this particular process is often referred to as “conforming”, or tying disparate sources of data to a single common form for the sake of deeper, more complete insight. As a technologist, I’ve always been drawn to the way in which new tools help facilitate old tasks.  Or, more specific to Business Intelligence, how new technologies can ease the turning of the gears of discovery and implementation which drive an analytics practice along its path to greater maturity and, by extension, greater insights.  However, as a consultant, I am also fascinated by the more organic nature of problem solving, and the elemental facets of such which seem to defy the passage of time – largely I think because they stem from our strengths and weaknesses as human beings.  There’s a certain resonance to that when working in the education space, which is fundamentally about advancing human understanding – an endeavor which requires a certain measure of due diligence,  an open dialogue among those with something at stake, and a spirit of breaking down barriers if it is to succeed. We Build Solid Foundations BlueGranite can help your organization implement a holistic, thriving analytics practice, too. Whether you’re considering an Azure-based modern data warehouse, or how to put AI and machine learning to work for your enterprise, we can help. Contact us today to learn how we can implement a strategic analytics framework tailored to your needs."
"38" "The BlueGranite team has a lot to be thankful for when working in the data and AI space. Whether we are working to support our clients, our team, or building relationships with our partners (Microsoft, Databricks, and others), we are constantly learning and growing as an organization. As 2019 draws to a close, we’d like to take a moment to reflect and express our thanks to the industry, the community, and the people in our organization.  We decided the best way to show our thanks would be to ask around and get feedback from our experts. Here are some fun and interesting insights from the BlueGranite team on what they are most thankful for: Matthew Mace, CEOI’m thankful for our clients who put their trust in BlueGranite and our team, sharing their vision and goals and providing opportunities to do the work we love – building data and analytics solutions on the Microsoft platform!    Amy Ford, Director of People & CultureI am thankful and honored to work with such an amazing brilliant team who is constantly amazing me with all the incredible solutions they build.  I have enjoyed the year growing our team and building stronger relationships with them as well as our Partners like Microsoft and Databricks. I am looking forward to an exciting 2020. Daivd Eldersveld, Solution ArchitectI’m thankful for the opportunity to develop in ecosystems like the Power Platform and Microsoft Azure. I immediately feel productive without worry about infrastructure since it's handled by Microsoft in the cloud.  Leo Furlong, Principal I am thankful for BlueGranite's continued investment to bring the entire company together every year to discuss vision and strategy for data and AI.  #staffretreat     Sawyer Nyquist, Staff ConsultantThis year I am incredibly thankful for the opportunities given to me. Those opportunities come from a tremendous employer in BlueGranite, from clients who invite us to participate in solving engaging problems, and from an ever evolving technology space that keeps me learning every day.   Olya Musokhranova, Solution ConsultantI am thankful for all the incredible people I met since starting at BlueGranite earlier this year.      As we celebrate more than 20 years in business, we at BlueGranite are most thankful for you; the sustained opportunity and support from our collaborators, clients, and affiliates helps us continue to flourish. We’re excited to discover what lies around the corner in the progressive world of big data and analytics. We’re just as excited to master those potential new advancements, while staying at the top of trusted current solutions, to continue making the most of your data. Keep up on the latest in the data and analytics industry by subscribing to our blog. Have questions or want to chat with an expert?  Contact us today. We’re here to help!"
"39" "Artificial Intelligence (AI) is changing how business works across industries. AI is driving efficiency in resource management and manufacturing, changing the hospital care landscape for both patients and providers, and helping financial services meet shareholder, customer, and regulator needs. Retailers benefit from AI’s predictions and supply chain management improvements, and it can help the public sector increase the speed and quality of government services.   While the incredible scope of Artificial Intelligence and Machine Learning capabilities can seem daunting, Microsoft Azure Cognitive Services puts them in reach of every developer. BlueGranite has had the pleasure of not only watching these services unfold and evolve, but also of incorporating Cognitive Services into our solutions to streamline our clients’ processes and provide them new opportunities. We know these tools can be a lot to take in, so we created an eBook detailing the five primary Cognitive Services – Speech, Language, Decision, Search, and Vision. In it, we illustrate AI’s potential to inform new products, simplify operations, empower employees, and more deeply engage customers, users, and students. Each section of our free eBook gives an overview of tools that infuse apps, websites, and bots with intelligent algorithms to see, hear, speak, understand, and interpret user needs through natural communication methods to transform business. Download the free eBook HERE. If you are already familiar with Cognitive Services and want to learn more about case studies that use these tools, we can help there, too. Our team put together a whitepaper covering each cognitive service offered, real-world examples, and a look into Cognitive Service labs that are still in preview. We showcase custom vision services, text analytics, QnA, search tools, and much more! You can find the complimentary whitepaper HERE. AI drives business evolution, and we don’t want any of the organizations that keep up on our content to miss out. Organizations that strategically deploy cognitive services can work more efficiently, safely, and sustainably, and deliver more engaging, immersive experiences. If you have any questions or feedback, please feel free to contact us. We hope you enjoy our whitepaper and eBook."
"40" "As the decade that brought us the Internet of Things and connected devices comes to a close, companies are awash in data. For many, the struggle to manage, store, and make sense of exponentially expanding records is real – especially when it comes to unstructured information. Breakthrough knowledge mining tools, such as Microsoft’s Azure Cognitive Search, can now turn unstructured data into searchable records. At BlueGranite, we’re already putting these capabilities to work for key clients; organizations that once had to dig deep through documents (some collections span decades and number in the millions) in sometimes fruitless searches. This on-demand, fast access to previously buried information is transforming operations across industries.   Unlock Fixed Data Picture a warehouse – physical or digital – packed with files and documents, or PDFs, JPEGs, and blobs. Now imagine the ability to not only immediately explore the contents within that warehouse, but to also surface exactly what you’re looking for, within moments, using just a key word and the click of a button. It’s happening. And we’re using AI-powered search methods to do it. Driven by Azure Cognitive Services – tools that give your tech human-like interaction abilities – evolving Cognitive Search fact-extraction capabilities are akin to having a full-time expert team analyze limitless documents to leverage key data. And not just any team; a multilingual team that can translate and analyze text in more than 50 languages, that can perceive sentiment and attitudes, and that can detect key phrases and values. Knowledge Mining: Limitless Potential  In the eye-opening study by Harvard Business Review (HBR) Analytic Services, “KNOWLEDGE MINING: The Next Wave of Artificial Intelligence-Led Transformation,”explores just how far knowledge mining has come, and how organizations can best use these tools, whether to support employees and enhance operations, or to strengthen customer engagement and reinvent offerings. Whatever your end goal, today’s ability to consume and process archives of unstructured data, quickly and consistently, is unprecedented. The report highlights the abundant potential in automated content comprehension, including: Cost savings Competitive advantages Streamlined processes Greater efficiency Increased compliance When report authors surveyed more than 500 HBR readers to determine the need for such tools, 82% said that trying to leverage unstructured information from things like PDFs, images, and audio files was time consuming and difficult. Though this this data frequently holds business-critical information, the majority of participants still rely on time-consuming manual methods, ripe with the possibility for error, to get the job done: “someone scanning, interpreting, annotating, cutting, and pasting.”   Business Use Case  The HBR review includes global engineering company Howden among its four highlighted case studies; a firm with an engineering portfolio spanning nearly two centuries. Much of the company’s unstructured data is more than valuable – it’s essential to daily operations. Mixing new ideas with trusted technology required frequent access to knowledge and diagrams stored among paper files, and outdated digital and visual resources – a time-consuming process, where it was critical to uncover easy-to-miss risks and anomalies to prevent future delays and quality issues.Harvard Business Review Analytic Services explains Howden partnered with a knowledge mining advisor to design a solution that overcame these challenges. They now upload relevant documents to a system that searches for technical terminology to immediately identify country, code and building rules; possible risks; or conflicting plans. Engineers can now make notes directly in the documents for current and future collaboration. This unmatched access to unstructured data is transforming Howden operations, as well as customer service. Technical Use Cases Scanning reams of paper documents and running them through the OCR (optical character recognition) API is an excellent knowledge mining entry point for organizations. In this scenario, a scanning vendor is used to convert paper documents into digital form, where Azure services can pick it up and store it in Azure storage to be indexed and made available for end-user access by Azure Cognitive Search. Cognitive Search has a new knowledge store feature (currently in public preview). Knowledge store is a table-based representation of your indexed, unstructured data. For all intents and purposes, the knowledge store is a database and can be accessed with any tools that can access a traditional database. Organizations can now query previously unstructured data using reporting/querying tools, data analytics tools such as Power BI, or even use it as a source for training machine learning models. Adoption Success While knowledge mining is still among emerging capabilities, the HBR piece covers four successful case studies spanning worlds of finance, engineering, healthcare, and contract management. The early adopters all followed best practice recommendations to ensure implementation success, including: Consider the goal you need to accomplish first, then build your system to accomplish it. Get top-down buy-in and give stakeholders across departments a seat at the planning table. Partner with a knowledge mining specialist, who can tailor a solution specific to your needs, and jump-start your initiative. Think Big, Start Small Watching this mind-blowing tech evolve, and now implementing and managing solutions driven by it, the thing that continually stands out for our team at BlueGranite is the potential some of these systems have to change the world. Wondering how it can potentially change your organization? Contact us today to discover how we can put your data to work for you."
"41" "At the recent Ignite 2019 conference in Orlando, Florida, Microsoft outlined its vision to continue extending Power BI’s enterprise capabilities. Even with Power BI Premium, deploying Power BI in an organization often means working around concerns such as model size limitations, vague deployment procedures, and more.  With upcoming features announced and demoed at Ignite, Microsoft is much closer to completing its enterprise vision for Power BI, and balancing the needs of both the self-service and enterprise audience. Whether you work on a business team or in IT, there are benefits for everyone, available now and coming soon to Power BI.  Deployment Pipelines BlueGranite works with a variety of organizations of all sizes to deploy and manage Power BI. Many corporations prefer a formalized deployment process, which is often built into their enterprise tools. By comparison, Power BI is immature in this area. Deployment Pipelines will overcome the current gap by allowing a clear and easy path through different environments for development, test, and production. Instead of an incomplete process for manually publishing content into different workspaces or having to immediately jump into the API or PowerShell, deployment becomes a straightforward procedure in the Power BI user interface. BlueGranite consultant Olya Musokhranova praises Deployment Pipelines and sees positive impact in streamlined deployment strategies: “Currently any enterprise-level Power BI adoption project requires developing a set of company policies on publishing, testing, and distributing content, such as naming conventions for Test and Production Workspaces, usage of Workspaces, and Apps for content distribution, etc. The new features will make these initial discussions with both BI and IT teams easier.” Expect to see Deployment Pipelines in public preview this spring. Data Protection, Sensitivity Labels, and Monitoring A common problem for organizations is safeguarding sensitive data both in and outside of a tool like Power BI. Often, explicit rules for compliance and auditing guide security efforts. Terms like PII (Personally Identifiable Information) and PHI (Protected Health Information) have become commonplace in organizations that store personal data, corporate financial data might not be appropriate for all levels of an organization, etc. Beyond current protection levels, like workspace security or row-level security, what can be done to safeguard data in Power BI? What happens if someone exports sensitive data to Excel, after which, it is anyone’s guess where it may end up? One of the primary strengths of Power BI is its integration with other tools in Microsoft’s ecosystem. Power BI integrates well with products across the Power Platform, Office 365, and Azure. In order to better protect data, you could use Azure Key Vault to “bring your own key” for encrypting your data at rest. In addition, users can now rely on sensitivity labels created in Microsoft Information Protection, and governance and monitoring of sensitive data through Microsoft Cloud App Security. For an example with sensitive data and Cloud App Security, a healthcare organization could label data as Protected Health Information. It could block downloads completely based on sensitivity, device, location, or other criteria. Alternatively, it could allow exports associated with a sensitivity label that maintain a level of protection even in what has always been a tenuous Excel-focused distribution scenario. Administrators can also monitor how the data is being distributed in Cloud App Security, and this view will also soon come to Power BI’s admin portal. Most of these capabilities are available today in public preview. Larger Tabular Models Josh Crittenden is excited about breaking through the current ceiling to enable larger model sizes in Power BI Premium: “Microsoft has stated numerous times that Power BI will contain a superset of Analysis Services functionality. Allowing for large, enterprise-size models is a big step toward that goal. It's also another selling point for Power BI Premium. Previously, customers would have a costly decision – whether or not to purchase both AAS (Azure Analysis Services) and Power BI Premium – but in many cases, Power BI Premium can now be the sole purchase.” In Power BI Premium, models will soon be able to jump well beyond the current 10GB limit to 400GB. Eventually, Microsoft expects growth into multiple terabyte territory with planned “extended memory” features. Lineage View Until now, Power BI has relied on a simple List view of reports, datasets, and other content within a Workspace. Lists unfortunately cannot easily display the oft-intricate web between multiple data sources, datasets, and reports. In particular, with the advent of dataflows and shared datasets across workspaces, it became difficult to see the end-to-end view from initial data sources to final Power BI content. In preview today, the Lineage view, for both workspace content and dataflows, helps contributors more easily trace the path from original data to what viewers see in Power BI. This view overcomes confusion and helps teams more clearly see dependencies between different objects. Enhancements to Dataflows Power BI Dataflows are an increasingly common way for users to consolidate and transform data for personal or organizational reusability. With the Power Query Online version of the same Power Query used in different Microsoft products, such as Excel and Power BI Desktop, the Dataflows UI and experience still requires some maturity. With over 50 new enhancements, that experience of creating dataflows in the Power BI service at PowerBI.com will soon become much closer to the more complete functionality of using Power BI Desktop! Azure Synapse Analytics  While Power BI could always connect to Azure SQL Data Warehouse, the recent announcement of Azure Synapse Analytics is exciting. Azure Synapse Analytics is not simply a rebranding of Azure SQL Data Warehouse. It’s an explosion of new capabilities that unite data warehousing, data lakes, AI, and BI. Power BI is more tightly integrated into the product, where you can build reports directly on top of your data—at cloud scale and performance. Synapse Analytics is currently in private preview, and we are eagerly awaiting the ability to work with it publicly. Conclusion It’s an exciting time to be working with Power BI as it grows into an even more capable tool for enterprises. Many of the traditional gaps from an enterprise IT perspective are being closed, and the product team is working hard to make sure that Power BI is the best analytics tool for both self-service and large corporate scenarios. If you would like to learn more about how BlueGranite can accelerate your journey with Microsoft Power BI, contact us today or join our free monthly Power BI Office Hours."
"42" "Whether you have a mature higher-ed data analytics solution or are in the planning phase, you likely consider the needs of your executives and administration when deciding who needs data access. And why not? These decision-makers must determine what strategy will best steer your institution. This post addresses other potential consumers within higher education settings that can directly benefit from having access to an institution’s data.     Faculty Faculty members fill many roles, including instructor, advisor, and mentor.  Putting certain data directly in their hands can enhance their influence on student success.  Course-based data: Certain aggregate measurements can help faculty plan, or adjust instruction, making it easier for instructors and students to succeed. How is this class performing compared to others this term or in previous terms? What qualitative measures can be seen in my enrollments for next semester? What tutoring needs might be anticipated?  Should I plan for additional office hours? Student-based data: Seeing a more complete picture about each student allows faculty and teaching-support interventions, when required, to try to prevent poor performance, withdrawals, or failing grades. How is this student performing compared to others? What measurable differences may influence performance?  What changes can be made to boost success? How is this student using the learning management system to collaborate? What is this student’s course load for this term? Is this their first full-time term?  Do they commute or live on campus? Students Data and insights presented to students can directly impact their behavior and learning outcomes.  A student usually receives direct feedback on their performance throughout the semester from grades on homework, presentations, and tests.  While students have the option of talking with their peers to see how they are doing comparatively, a student-centered dashboard can make that information even more transparent.  Combined with other data – and suggested actions – this information has even more power. Class performance data How does my current grade average compare to other students? How does my attendance impact my grades? Would better attendance result in better grades? How much do other students use office hours? How much do other students attend study sessions? What resources are available to improve my grades? How have these resources impacted other students? Finance data What is my estimated financial aid for the upcoming year? What impact would more/fewer courses have on my tuition costs?  What are my options for payment plans, loans, or scholarships? How much does it cost to live on campus versus commuting? How do upcoming tuition increases affect my out-of-pocket costs? Can I add an on campus/off campus job to my schedule? Degree selection and attainment data What courses are remaining in my current degree plan? What is the impact of changing my degree? Adding a minor? What are the careers of recent graduates with my degree? General Public Chances are your institution publishes data to its website, in newsletters, or other media.  And likely this data is a single point in time, and could have taken weeks, or longer, to prepare.  Consider what other key metrics can highlight your institution’s successes and its future potential. Such metrics might include incoming class attributes, career placement, graduation rates, and alumni giving.  These are points of pride that can garner interest in your university from prospective students, transfers, alumni, and community leaders. BlueGranite can help you realize an agile, decentralized, data-driven strategy to enhance student and faculty experiences that boosts long-term institutional success.  Our expertise in higher education, advanced analytics, and modern data platforms position us as a trusted partner in implimenting limitless Azure Synapse Analytics – the evolution of Azure SQL Data Warehouse, or deploying Power BI for reports and dashboards to drive your organization.  Check out our Solution Briefs page to explore some of the many ways BlueGranite helps schools and students succeed."
"43" "The intuitive Mapping Data Flows (MDF) in Microsoft’s Azure Data Factory (ADF) hit general availability in October. The easy-to-use Mapping Data Flows tool empowers users to quickly design ETL processes that transform data in the cloud, at scale.  Under the hood, Mapping Data Flows uses Spark-powered Databricks clusters. Spark is a cluster-computing framework used to process large amounts of data. Don’t know how to code in Spark? Don’t worry; while it’s helpful to know some of the internals of Spark when doing more advanced data flow optimizations, you don’t have to write any code to create your Mapping Data Flows. On execution, Azure Data Factory Mapping Data Flows are compiled to Spark code for you.  Mapping Data Flows is similar in look and feel to SQL Server Integration Services (SSIS). If you’re coming from an SSIS development background, Mapping Data Flows is a lot like the Data Flow tab. ADF Pipelines are a lot like the Control Flow tab. An ADF Pipeline allows you to orchestrate and manage dependencies for all the components involved in the data loading process. ADF Mapping Data Flows allow you to perform data row transformations as your data is being processed, from source to target. Users can maintain Mapping Data Flows source control from right within the Azure Data Factory user interface. You can also integrate with GitHub and Azure DevOps Git repositories. Once you link your repository to your ADF project, you’re able to create and switch between branches from right within the ADF interface. This fosters easy collaboration among your data engineering team. Side-by-Side View: MDF vs. SSIS Below are side-by-side comparisons of ADF Mapping Data Flows and SQL Server Integration Services. The functionality is exactly the same between the two. First we’ll compare SSIS Control Flow to the ADF Pipeline. Our use case is a standard scenario – we’re loading a flat file from Blob storage into a table in Azure SQL Data Warehouse (now Azure Synapse Analytics). Walking through the SSIS Control Flow/ADF Pipeline: First, resume the Azure SQL DW instance. Here, the instance is paused during non-loading times, so we don’t incur additional costs. We then execute Mapping Data Flows to load the dimension tables. After all the dimension tables have loaded, we execute Mapping Data Flows to load our fact table. Lastly, once all the data has loaded, we pause our Azure SQL DW instance. Example of the SSIS Control Flow tab for loading our data mart tables:  Example of the ADF Pipeline for loading our data mart tables:  Now let’s look at one of the Mapping Data Flows “LoadFactInternetSales”. Again, this is a very common scenario where we’re loading a flat file that contains our internet sales data. We do a simple transformation on the data and load the records into our data mart. First, we’ll read in a CSV file from Blob storage that contains internet sales data from Microsoft’s AdventureWorks sample database. Then we’ll do a transformation on the product code column, where we’ll cleanse and parse out the product code. After that, we’ll join the cleansed product code column to our dimension product code column to get the product surrogate key from our product dimension. Lastly, we’ll insert the records into our FactInternetSales table. Example of SSIS Data Flow tab for loading the FactInternetSales table:  Example of ADF Mapping Data Flows for loading the FactInternetSales table:  As you can see, both the SSIS Control Flow and Data Flow look very similar to the ADF Pipeline and Mapping Data Flows. What Sets Mapping Data Flows Apart While there are a lot of similarities between SSIS and ADF Mapping Data Flows, the latter brings exciting new features that don’t exists in SSIS. Schema Drift – This creates the ability to ingest data when the source schema is unknown or changes. Do this by checking the Allow schema drift box at the source, then adding a derived column pattern afterwards to search for a specific column or perform any cleansing of columns as they process. If your source schema changed in SSIS, it would error out, creating a waterfall of changes that needed to be addressed. Derived Column Patterns – This feature allows you to specify a pattern to be used on the data as it’s being processed. One such pattern could be something like “replace all NULL values with an empty string if the value is a string data type”. Previously, you would have had to define that logic for every single column, which is tedious and time consuming, especially when you’re dealing with many columns. Upsert – You can perform an “upsert” operation in ADF Mapping Data Flows. An upsert will update records that already exist in the destination and insert records that are new. This simplifies the process and is very handy when loading your data mart tables. Debug Mode – Allows you to view your data as you develop your pipeline. This is especially useful when working with some of the more complex transformations to ensure the results of those transformations meet your expectations. Turning on debug mode starts up a cluster that it uses for your ADF Mapping Data Flows. It takes a couple of minutes for the cluster to spin up, but once it’s running you can keep using that same cluster from data flow to data flow. An important thing to note is that when running in debug mode, the default row limit is set to 1000 for each source. That means that you are only getting a sample of the data. This can cause confusion when trying to troubleshoot an inner join when the sampling of the two sources happen to not have any of the same values for the join. Luckily, you can change the default row limit to match the size of your data if needed. Should You Use It? If you’re looking for a rich user interface that allows a drag-and-drop ETL experience for your modern data platform solution in Azure, then Mapping Data Flows is a great option. There isn’t much of a learning curve if you’re coming from an SSIS background. It’s a code-free experience, so you don’t need a heavy coding skill set to get started. To read more BlueGranite insights on the differences between Azure Data Factory and SSIS, check out this two-part post from Merrill Aldrich. BlueGranite helps organizations realize their full potential through technology. Sign up for our blog and be the first to get our updates on data management breakthroughs, the modern data platform evolution, and breaking business intelligence and AI news."
"44" "At the beginning of 2019, I wrote about how Cloud Scale Analytics would change the game in the data-sphere, and specifically that actual capability would catch up with hype and deliver impact. With Azure Synapse Analytics recently unveiled at Ignite, Microsoft has unified the vision, and set a course for the future with a seamless set of technical capabilities and unmatched potential!  Azure Synapse Analytics – what exactly is it, other than a cool sounding reference to the connectivity between nerve cells in our brain? The simple answer is that it’s phase next of Azure SQL DW. As such, if you head into your Azure portal to provision an Azure Synapse Analytics deployment, it will look and feel pretty much exactly like Azure SQL DW did last week, but with the new name. That being said, it also signals the evolution of the platform into a much bigger, more powerful, and more seamless experience across all data and analytics needs in the cloud… To be cute, \"this is gonna be big\".  Azure Synapse Analytics combines data acquisition, preparation/transformation, querying, visualization, and AI enablement across structured, streaming, and unstructured data; it effectively unifies the foundation of SQL Server with Spark-enabled big data and streaming data processing together with data flows, Power BI, and Azure Machine Learning… all while bringing unprecedented flexibility to tailor and scale the capacities to exactly what you need, when you need it. That may sound great, and if the technical intro sounds intriguing, stay tuned for a deeper dive into the platform from additional BlueGranite team members! But if you are asking yourself… why do I want Azure Synapse Analytics, then consider that the ultimate goal of Cloud Scale Analytics is to enable digital transformation through data, and this new platform offers a single technical solution to a nearly limitless set of business opportunities. BlueGranite works in many industries where Azure Synapse Analytics will drive advances in digital transformation. In our experience with Retail, organizations are implementing Customer 360 initiatives which require a flexible data layer to capture data from internal, external, and personal sources with varying volumes and structures, quickly and accurately piece together the consolidated view of that data, light up Artificial Intelligence to further enrich data and predict outcomes, and then deliver insights to the people who can implement action and drive new outcomes. Prior to Azure Synapse Analytics, that might have required multiple practitioners integrating dozens of technologies, services, and solutions in order to realize the value, which could lead to failed or stalled initiatives. With Azure Synapse Analytics, retailers can spend less time architecting technology, and more time improving marketing efficiency and increasing pricing & promotional effectiveness! Now let’s take a closer look at how Azure Synapse Analytics delivers on the vision of Cloud Scale Analytics. Cloud Scale Analytics – a quick refresh: what does it mean? – or – “defining the vision”First, the break-down: Cloud – Operating Expense (OpEx) model versus Capital Expense (CapEx); ease of provisioning; ease of deployment; modernization of available services; speed of iterations and pace of change Scale – Across all major analytics workloads: storage, processing and compute, and serving; scale of access/deployment; and availability Analytics – BI modeling; data visualization; machine learning/data mining; artificial intelligence and cognitive analytics And then the combination: Cloud Scale Analytics – Now, let’s zero in on what these words mean together. If we move past the core IT services that underpin all of this – to wit, storage, processing and compute (and serving, to the end users) – with either Data or Analytics as the focus, we arrive at a more refined set of capabilities that belong under this term: A Modern Data Platform that includes the Data Lake (comprised of scalable storage for Structured Data, Unstructured Data, Semi-structured Data, Streaming Data) Analytical Modeling capabilities (that can aggregate and query billions of rows of data with scalable compute) Interactive Visualizations, and the ability to train and process Machine Learning models over multi-threaded, distributed compute clusters Conveniently, Cloud Scale Analytics quickly encapsulates the technical capabilities that BlueGranite leverages in helping our customers harness the value of data – it describes a ‘tool’ built perfectly for digital transformation! We’ve built a team of experts at applying the individual technologies to maximize their impact, and a strong partnership with Microsoft to furnish the tools of our trade. And already, Microsoft has been delivering on this vision with Azure SQL DW, delivering unmatched price and performance (and of course, price:performance!) along with ease of deployment and clean interoperability with other tools. Together with complementary Azure services like Azure Databricks, Data Factory, and Power BI, Azure SQL DW has already become the bedrock upon which data and analytics capabilities are built in the cloud. And when reviewing Microsoft’s product details for Azure Synapse Analytics, we can see the evolution ahead of us.The promise of Limitless Scale coupled with “On-demand query” builds upon workload importance & isolation to allow flexibility and cost control previously unheard of. Integrations contained in Powerful insights support and empower the dramatic increase in citizen data consumers (both data scientists and self-service BI analysts) by seamlessly expanding access to all types of data in industry-leading tools like Power BI and Azure Machine Learning Services.The tools and capabilities that provide a Unified experience mean that data developers and engineers can provide expertise and support demanded by the growing audience of data consumers, upgrading existing skills and capabilities with new capability and efficiency. And backed by Microsoft’s commitments and decades of expertise with SQL Server, Azure Synapse Analytics can promise the truly Unmatched security that today’s organizations demand. Closing thoughts: For any team of technical experts, the pace of change in technology can be both scary and exciting. But digging in deep with Azure Synapse Analytics has been remarkably exciting, and not at all scary! Sometimes new tech can be described as a ‘revolution’, but that type of disruption often precipitates chaotic times… and this feels more like an ‘evolution’ where every aspect becomes more efficient and more powerful. We’re witnessing the unification of the full set of technologies and capabilities powering true cloud scale analytics, and the future looks very bright indeed. Interested in partnering with BlueGranite to bring Azure Synapse Analytics to your organization? Start here to learn more about our experience and insights regarding Modern Data Platform solutions, and let’s deliver the value together!"
"45" "Microsoft’s cloud-based, scalable Azure Machine Learning (ML) service speeds development and deployment of data science projects. In this demo, we’ll use the Azure Machine Learning SDK for Python to train a simple machine learning model and deploy it to the cloud service to then get predictions from our model.   Before beginning, you’ll need an Azure subscription (create one free here). Then you’ll need to create an Azure Machine Learning service workspace resource in your tenant. Once you spin up the workspace, you will also need to create a Notebook VM, then open a Jupyter notebook to code in. Each of the code segments below are blocks of code inside of the notebook I created for this demo. We will cover three major sections in our walkthrough: 1) setting up the development environment, 2) training the predictive model, and 3) deploying the model as an Azure ML service – each step has several sub-steps. Set Up the Development Environment We’ll begin by setting up our environment and getting it ready to train an experiment. There are five essential steps to getting our environment ready for an experiment: Initialize the Workspace Initialize an Experiment Create a Datastore Create or Attach a Compute Resource Environment Set Up Initialize the Workspace This step imports the base Azure ML packages into our Python code and connects us to the workspace we created in the Azure portal. The print lines are, of course, just a check to make sure that we’ve imported and are executing the code properly. The important lines of code in this section are the importing of the azureml.core package, which contains all of our SDK functions, and the initializing of the Workspace object. Initialize an Experiment Next, we’ll initialize our Experiment. Let’s give it the name that we’ll save it as. Then we’ll create an Experiment object that links the workspace to the experiment for us to use later. Create a Datastore In this step, we need to create a folder in our workspace in which to write out our code in some later steps. Any custom Python scripts used in Training or Scoring the Model should be added to this folder after it is created. This is also the folder where we’ll want to write out any file outputs (Training, Scoring, YAML, etc.) Create or Attach a Compute Resource The code below is one of the most basic variations on creating a Compute Cluster that I’ve found in my research. This version creates a cluster using the VM size and Max Node options from the Azure ML Compute Cluster interface. This section of code is expandable beyond what we’re showing here and is where much of Azure ML service’s scalability is based. You can choose from dozens of options in the Azure Cloud to spin up various types of clusters. These clusters also come preloaded with your environment, whether it’s Anaconda, Spark, or TensorFlow, to name a few. You will need to add any packages that aren’t preloaded into the environments, which may take a little trial and error, but you won’t need to create a Python Environment from scratch. Once this code completes, a Compute Cluster will appear under the Compute section in the AzureML Workspace resource in the Azure Portal. Environment Set Up In this step, we need to import our Environment package from the azureml.core, if we haven’t already done so. We must also initialize the Environment object for later steps. If you’re not adding any packages that aren’t preinstalled on the Compute Cluster, you can skip the “adding packages” steps. I would, however, advise leaving in the step that saves the YAML file to your workspace, as you may need it for later deployments. Also, be aware that some packages are not registered with Anaconda and may need to be loaded from PIP (the package installer for Python), or some other resource. I’ve added a Conda and a PIP example in the code below (conda installs and manages packages from the Anaconda repository.) Train the Model This section describes the three steps involved in training and executing an experiment in the Azure ML Service: Create the Training Script Submit the Training Job to the Compute Cluster Register the Model Create the Training Script The training script is essentially the guts of the Azure ML Service. Every other section in this post is Azure ML SDK code. This section is based almost entirely on the code you are using to produce your AI model. I recommend writing this section outside of your Azure ML service deployment; doing so allows you to develop and test your code without the overhead of developing the Azure ML service deployment at the same time. That being said, for this code to work inside of the Azure ML service, the last few lines that output the pickle file are mandatory. The pickle file is what is used to register the Model inside of the Azure ML service. Submit the Training Job to the Compute Cluster In this step, we’ll create an object to run our script against the Compute Cluster we created earlier. The object needs the script directory and the training script to initialize. We’ll then run configuration functions against the object to set the target Compute Cluster and the Environment we will use in the training. And lastly, we will execute the configured run script job. When this code section completes, an Experiment will show up in the Azure portal in your Azure ML workspace resource. Also, assuming your Experiment has a pickle file output, that file is created here. And we can watch the execution of the Training with the following code. Register the Model This last step in Training the Model takes the earlier-mentioned pickle file, output in the training script creation and execution, and registers it in the Azure ML service workspace. Once this code completes, a Model will show up in the Azure portal. Deploy the Model as an Azure ML Service This last section describes how to deploy the Model and create a web service that can be used for scoring new data. Create the Scoring Script Deploy the Azure Container Instance Test the Deployed Service Create the Scoring Script The scoring script is used to return predictions from your registered model. This script has two mandatory sections and one optional section. The scoring script requires an INIT function and a RUN function to be defined. The INIT function connects the scoring script to the Model we deployed in the previous section. The RUN function executes the predict function from whatever package you used to do your training. For example, in the scikit-learn package the predict function is called predict(). Other packages may use a differently named function to do the same thing. The optional third section defines a schema for the inputs and outputs for scoring. In some software this schema is a requirement. For example, in the webinar BlueGranite did on this subject, we to connect to the Model and get predictions. Power BI dataflows require creation of a schema. As most other requests to this API will not require the schema, it can be skipped. Deploy in an Azure Container Instance Now that we have a trained model and the ability to score data input to the model, we need to wrap the scoring script in a container and deploy the Container as a Service (CaaS) to the Azure ML service. In this example, we’ll deploy the service as an Azure Container Instance (ACI). This first section of code adds some metadata to the container configuration. And the second section of code creates the Image and deploys it as a service. When this section completes, the ACI will show up under Images in the Azure ML service user interface, and the web service will appear under Deployments. Test the Deployed Service We can test our deployed service, using test data in JSON format, to make sure the web service returns a result. Additional Resources Think of this tutorial as a basic framework; a starting point to developing your own Azure Machine Learning deployments tailored to your company’s needs. We also have a couple of other blog posts at BlueGranite that add to what we’ve discussed here. To see a webinar in which I give a more complex demo of Azure ML capabilities, click HERE. Andy Lathrop, a BlueGranite principal, wrote a blog post describing how we used Azure ML to build a personalized marketing model using the Microsoft Recommenders GitHub code: https://www.blue-granite.com/blog/introduction-to-personalized-marketing-with-azure-machine-learning And be sure to also explore this blog post, by BlueGranite Senior Consultant David Eldersveld, a Microsoft MVP, on using Azure ML deployments in a Power BI Dataflow: https://www.blue-granite.com/blog/enrich-power-bi-data-with-ai-insights-from-cognitive-services David’s post makes use of both Azure’s Cognitive Services and Machine Learning. The code in this post was developed using the sample code included in the Azure ML service and the Microsoft Recommenders GitHub repository: https://github.com/microsoft/recommenders And you can find copies of the Jupyter notebooks I used to create this post and the webinar demo here: https://github.com/datascinerd/AzureML-Examples Want to find out more about how Azure ML can fit into your advanced analytics strategy? Contact us today to learn more."
"46" "Now more than ever before, leaders in higher education are using data to make smarter decisions that improve their institutions, as well as students’ experiences. Often, these data-driven decisions are led by Institutional Research departments, who are focused on student outcomes, retention, attrition, and other academic metrics. However, widening the analytical lens to include data generated at the beginning of a student’s lifecycle, when they are first identified as a lead, can produce more efficient, effective marketing teams, and streamlined enrollment efforts.   The first step to improving marketing and enrollment outcomes is defining what the conversion pipeline is, and what the key components to measure and manage are. Important conversion milestones in the early phase of the student’s journey are: Lead creation First response – when the school responds to the lead (Speed to Lead) First contact – when the lead responds to the school Application start date Application completion date Conversion from applicant to admitted student Conversion from an admitted student to an active, revenue-generating student Critical questions and metrics are: How many leads convert at each step in the pipeline? What content does the lead see and interact with before converting, both on the website and in contact efforts? Are there any affinity-generating activities the lead attends (campus tours, webinars, events, etc.)? How long does each conversion step take? How do the above vary across attributes like degree level, program, student demographics, enrollment team member, etc.?Sample conversion funnel created in Power BI After the student has completed this first lead generation activity it’s important to have a well-defined, well-managed, and well-measured contact strategy. Speed to Lead – the time from when a lead first completes a form to the first moment the enrollment team has the chance to contact them – is the first key metric to measure. Conversion rates are generally higher when leads are contacted immediately. Understanding when inbound leads are created, as well as the time it takes for your enrollment team to respond, can help inform: Staffing decisions; after all, someone needs to be available to respond to inbound leads. What auto-response to craft for leads received off-hours. How many new leads can one enrollment team member reasonably handle in their queue? If Speed to Lead times are slow, perhaps consider shifting workloads or adding additional resources. The third step in the conversion pipeline is the First Contact, when the student responds to outreach attempts from the enrollment team. This may happen during the initial, first response from the enrollment team, or may happen after subsequent outreach attempts. Many schools have defined workflows for their contact strategy, made up of a combination of emails, phone calls, and SMS messages. Knowing which outreach attempt produced results, and which the student received prior to the contact event, can inform messaging content. The next points of conversion are for the student to begin their application (if that was not their original lead source) and then to complete that application. This is a key step in the enrollment journey: understanding the time from lead generation to applicant conversion can inform timing of marketing campaigns, and knowing the amount of time/effort spent on outreach and communication with the student can be incorporated into Cost per Lead metrics. The next step in the conversion pipeline, the status change from applicant to admitted student, can be either passive or active, depending on what – if any – materials or fees the student has outstanding. Here again is another opportunity to measure the number of contacts between the student and the school, and the time it takes to complete this conversion step. Finally, an admitted student will convert to an active, revenue-generating student. At this point, we can analyze the information the student received prior to enrollment to help understand their preparedness; did they receive content regarding tuition, financial aid, academic support resources, etc.? A school can incorporate these data points from the CMS or CRM into a predictive model for student success. Here again, understanding the time it takes to convert to an active student is critical to successful campaign planning. There is also opportunity to measure conversion by enrollment team member to identify areas of strength, and occasions for growth and coaching. BlueGranite can help you optimize lead conversion and enrollment strategy at your educational institution. Whether that solution starts with building a new Azure SQL Data Warehouse, incorporating predictive analytics and machine learning, or using Power BI to build engaging, interactive reports and visualizations, we are here to help."
"47" "In retail, the art of the deal is far more complex than offering the lowest price. Retailers need deep knowledge of their customer’s behavior, products, value propositions, cost structure, platforms, and sales channels to boost success and cultivate customer loyalty. Developing and maintaining a winning position requires mastery of modern technologies married with pragmatic business strategy. BlueGranite’s proficiency in both gives our clients the competitive edge.   Thanks to the digital era, retail has undergone a wave of innovations. While we see sensational articles in the news proclaiming the death of retail, all you have to do is look around and you’ll see that consumption is in full swing. Retail isn’t going anywhere, it’s just rapidly shifting shapes. Record numbers of consumers now conduct in-depth, online product research across multiple devices prior to making a purchase. Accessing and understanding the vast amounts of numbers and data points the customer research and purchase process generates, and using it to inform operations, is now critical to retail success. Relying on data produced at various stages of the retail journey is nothing new; good retailers have kept track of cost of goods sold for centuries. But even the greatest bookkeepers have been forced to focus on measures in their rearview. Looking at historical data aggregations for sales trends, decision-making was often left to the lucky few with a “God-given” gut feel. Modern technology, through the proliferation of sensors, mobile devices, and cloud scale data storage and processing power, is producing an explosion of data. The ability to stream, aggregate, store, analyze, and ultimately automate prescriptive actions or remediation has turned retail into predictable game. Key advances in cloud platforms, like Microsoft Azure, enables retailers of every size to store massive amounts of data, measure every single data point, and to focus on the single most important areas. While retail has seemingly become a science, there’s still an art to mastering the techniques required to win in today’s digital era. At BlueGranite, we’re focused on helping our retail clients modernize their practices and technology through the use of cloud scale analytics on Microsoft Azure. Our data-driven platform designs and retail Insights-as-a-Service capabilities give users at every skill level the ability to maximize insight from their data.  hbspt.cta._relativeUrls=true;hbspt.cta.load(257922, '071780ef-2d03-4664-abfc-25a980e73237', {});      If you’re struggling to wrangle fast-moving data, mine your data in real-time, find trends, and feed it all back, whether to decision-makers, apps, or customers, BlueGranite can help. Our trusted experts envision, design, and implement modern retail analytics platforms that help our clients do more than dominate their category; we can optimize your customers’ path to purchase, help you achieve personalized marketing offers, increase your operational effectiveness, and build true loyalty to your brand. Contact us today to learn more, or check out some of our retail solutions here."
"48" "Report development is iterative. Every good report goes through multiple phases of the development cycle. Developers try different ways to display data, uncovering new insights and incorporating user feedback until they have a production-ready report. As Power BI evolves and adds new features, even reports in production might benefit from another phase of development. This was the case for BlueGranite’s “Employee Retention – Organizational Flight Risk” report.  The original report, detailed here, showcased how Power BI features like drillthrough, buttons, and bookmarks can be used to investigate predicted levels of employee flight risk.  With the advent of Power BI’s built-in AI visuals, it was time for us to take another look at this report and give it some updates. The predictions used in these reports – the original and updated version detailed in this post – come from a machine learning model built outside of Power BI by one of our data scientists.  The model predicts the probability of an employee leaving as “Low”, “Moderate”, or “High” using historic turnover data. If you’d like to learn more about the process of developing a predictive model for employee flight risk, check out this post. The original report does a fine job of showcasing data, allowing users to explore the model through visuals and slicers to gain insight into the factors affecting flight risk. So why bother updating it? You might be questioning, too, if the data driving the report has already been run through an AI model, what additional insight could these Power BI visuals add to the report? The answer is simple: Power BI’s AI visuals, specifically the Key Influencers visual and the built-in Insights capability, deliver faster, deeper insight into the root causes of important outcomes. In the original report, users could explore different fields and data points and their effects on flight risk using slicers and visual interactions. This method of exploration often relies on hunches and instincts, utilizing trial and error to eventually identify key flight risk drivers.  Power BI’s AI features take guesswork out of the equation to offer a better method for exploring data like this. In both the Key Influencers visual and Insights feature, Power BI analyzes data for you, exploring many variables and categories at once, then displaying the categories and data points that have the biggest impact on the outcome being predicted; in our case, flight risk. This not only delivers faster insight but also helps to ensure that all the important flight risk  drivers are discovered. Here's a link to the live Power BI Showcase featuring this employee retention report that you can explore on your own. For an overview of the makeover, check out the page-by-page walk through below. Page 1: Workforce Overview Like many Power BI reports, the first page, “Workforce Overview”, gives a summary of the dataset. It introduces users to the data in our model and displays high-level metrics. Looking at the first page, users can see that Northside is the location with the most employees categorized as high flight risk. Through visual interactions, users discover that although Northside has the highest number of high flight risk employees, Huntington Park has the highest percentage of high flight risk employees, and a much smaller number of total employees.  To explore this further, users can click the “High” button to view a bookmarked version of the first page, configured to display only high flight risk employees. If they select Huntington Park in the bar chart, they will see that at Huntington Park, the job with the highest flight risk is Registered Nurse II and the department with the highest flight risk is Emergency Room.  At this point in exploration, the user has a better picture of the overall employee population, an awareness of flight risk at specific locations, and is starting to get an idea of who is at risk of leaving. However, to effectively act upon this insight, users need a deeper, more specific understanding of who is at risk of leaving the hospital and most importantly, why. Enter the Key Influencers visuals. Pages 2-3: Key Influencers Pages On the next two pages, developers separated fields across two Key Influencers visuals. For this dataset, we found that separating fields between visuals in this way made insights easier to digest and more targeted for specific actions. Each of the visuals analyzes and ranks fields and data points for their effect on flight risk and displays the most influential ones. The first page, “Key Influencers – Personnel Data”, shows fields and data points relating to HR-type data, like Hourly Wage, Age Group, and Tenure. The second page, “Key Influencers – Work Environment Data”, explores data relating to an employee’s day-to-day work environment, like Patient-to-Staff Ratio and Recent Performance results. Both visuals include Job Title as a field that is analyzed for flight risk alongside the other fields, as it is key to understanding which patient populations fall into the various risk categories. Looking at “Key Influencers – Personnel Data”, users start to get a better understanding of factors affecting flight risk. Each bubble represents a value or range of values from one of the fields analyzed and uses machine learning to quantify how much it increases the likelihood of being categorized as either high, moderate, or low risk. As the users click each bubble, results of the analysis are displayed graphically on the right comparing other values in the field.  Users can also change the drop-down to view what influences employees to be categorized as moderate or low risk. From the results, users can see that using PTO (paid time off) and scoring high on a recent performance review can decrease an employee’s flight risk.  Within this visual, users can also click “Top Segments” to see which segments, or combinations of fields, have the most impact on flight risk. The “Key Influencers” tab of the visualization shows the impact of individual fields, while Top Segments illustrates effects of multiple variables.  From this view, when users click a bubble, they can see more information about it, as shown below. For example, on the “Key Influencers – Personnel Data” page, the 30.8% bubble in Top Segments represents employees who are between the ages of 20-29, and are newly licensed RNs, and have been in their position for under a year. All these factors combine to greatly increase flight risk. Approximately 31% of employees who have all three of these attributes fall into the high flight risk category, whereas only about 15% of the overall employee population is classified as high risk.  On the “Key Influencers – Work Environment Data” page, users can see additional data points that affect flight risk. For example, for the factor selected above, users can see (below) that employees who have not been assessed for Recent Performance are 8.63 times more likely to fall into the high flight risk category, on average, compared to other values of Recent Performance. It gives users the same ability to interact with the visual and explore different segments, flight risk categories, and data points that the first Key Influencers page does. Fully exploring both the major influencers and top segments identified gives users a complete picture of the major contributors to flight risk.  Final Page: Explore the Results Armed with the insights and knowledge gained from previous pages, on the final report page, “Explore the Results”, users can explore independently using the Filter Pane or the “View High Risk Employees Only” button to select and view specific employee populations.    This page has a table that breaks down flight risk and some of its factors by both Facility and job. It also has bar charts that the developer decided were the most relevant and show the biggest and most actionable differences in distribution between flight risk categories. Driving from an initial bar chart that did not end up in the final report, the developer right-clicked on the visual, selected “Analyze” and “Find where this distribution is different”.  Power BI runs analysis and displays its findings in a window. From here, the developer can see which factors affected the difference in distribution between flight risk categories within the employee population the most and choose which visuals, if any, to add to the report. This feature is also useful for initially identifying fields to include for analysis in the Key Influencer visuals.             The developer decided to add and format three visuals from Power BI’s Insights to the final report page that show the difference in distribution for employees who have not had a Recent Performance assessment, are in the 20 to 29 Age Group, or who are Newly Licensed RNs compared to the entire employee population. These insights are actionable and have appeared continually throughout investigation of the data model. At this point in exploration, end users have an idea of specific employee populations that tend to be high flight risk and the factors contributing to that risk that may need attention. With the dawn of new AI capabilities and visualizations in Power BI, getting fast and complete insight into what’s driving certain metrics is easier than ever before. It might be time for your organization to take a look at existing reports in need of enhancement, like we did here, or to start thinking about exciting new reports that can leverage these features. In a data-driven, fast-paced world, time is of the essence, and getting a comprehensive picture of your data more quickly with Power BI is a major win. If you need help with AI, Power BI, or data in general, reach out to us at BlueGranite. We’d love to help."
"49" "In this installment of BlueGranite’s Knowledge Mining Showcase, a series showcasing how the human-like reasoning capabilities of Microsoft’s Azure Cognitive Search can maximize insight from all kinds of data, we are going to learn how to integrate one of Microsoft’s newest cognitive services, Form Recognizer – a comprehensive AI-powered document mining tool – into Azure Search to create a powerful discovery solution.   First, let’s review Azure Search’s revolutionary capabilities. The intelligent, cloud solution-as-a-service is reinventing knowledge mining – the practice of using cognitive search to extract facts from unstructured data. Azure Search recognizes and extracts text and identity from images; can discover key talking points in text; and can identify and classify people, places, and things from text and images. Trailblazers are using it to discover and leverage important, enterprise-driving data. By incorporating Form Recognizer with the dynamic Azure Search tool, we can pull key-value pairs from forms that can then be searched as metadata, adding another layer to Search’s capability and sophistication. Say we need to comb thousands of invoices to uncover those associated with a specific company – let’s call it “Acme Inc.” Surfacing those with Azure Search is a breeze. But we’re going to further complicate things; let’s assume that, in addition to invoicing Acme Inc., our company also resells its products. However, we only want to surface invoices in which we’ve billed Acme Inc. Adding Form Recognizer to Azure Search allows us to do just that, rather than showing all associated billing (such as invoices where we’ve charged others for Acme Inc. products).   The fast, accurate Form Recognizer service uses machine learning to deliver precise, consistent results; results that once required data science know-how. However, Form Recognizer is currently only available in limited-access preview. To try out our tutorial below, you’ll need to request Form Recognizer access by completing and submitting Microsoft’s form here. If Microsoft’s Azure Cognitive Services team approves the request, you'll receive an email with access instructions.  We’re going to review and elaborate on this Microsoft “Analyze form sample skill for cognitive search” training module. You can follow along this exercise simply by reading, but if you’d like to take a more hands-on approach, you’ll need: An Azure subscription Access to the Form Recognizer resource Microsoft’s Visual Studio 2019 or a recent version of a C# compiler A REST API tool, such as cURL or Postman (we will be using Postman) Clone the GitHub azure-search-power-skills repository locally on your machine Create a Form Recognizer Resource When you're granted access to use Form Recognizer, you'll receive a “Welcome” email with several links and resources. Use the \"Azure portal\" link in that message to open the Azure portal and create a Form Recognizer resource. In the Create pane, provide the following information:  Name   A descriptive name for your resource. We recommend using a straightforward format, such as MyNameFormRecognizer.   Subscription   Select the Azure subscription that has been granted access.   Location   The location of your cognitive service instance. Different locations may introduce latency, but have no impact on the runtime availability of your resource.   Pricing tier   The cost of your resource depends on the pricing tier you choose and your usage. For more information, see the Cognitive Services API pricing details.   Resource group   The Azure resource group that will contain your resource. You can create a new group, or add it to a pre-existing group.    Important Normally, when you create a Cognitive Service resource in the Azure portal, you have the option to create a multi-service subscription key (used across multiple cognitive services) or a single-service subscription key (used only with a specific cognitive service). However, because Form Recognizer is a preview release, it is not included in the multi-service subscription, and you cannot create the single-service subscription unless you use the link provided in the Welcome email. For this example, I created the following Form Recognition service:  Note that I used the Free tier, the name of my service is BGFormRecognizer, and the Endpoint is https://bgformrecognizer.cognitiveservices.azure.com Train the Form Recognizer Model To begin training our Form Recognizer model, we'll need a set of training data in an Azure Storage blob. We need a minimum of five filled-in forms (PDF documents and/or images); we can also use a single empty form with two completed forms. The empty form file name must include the word \"empty”. (See Microsoft’s “Build a training data set for a custom model” for tips and options if you are putting together training data). For this example, we’ll use the sample forms included in the aforementioned GitHub azure-search-power-skills repository. You’ll find these under the SampleData folder, right off the root folder. 1. Create a blob container in an Azure Storage account and upload the five SampleData folder forms into it.  In this example, I’ve uploaded the five sample forms into a blob container named “form-recognizer”, under a storage account named “kmdatasource”. The blob container access level is set to Public – container. We wouldn’t do this in a production environment, but rather would use a shared access signature (SAS). For simplicity’s sake, we’ll keep it public for this example. 2. We’ll train the Form Recognizer model using an API call in the following format: Request:POST https://[form recognizer end point]/formrecognizer/v1.0-preview/custom/train Headers:Content-Type: application/jsonOcp-Apim-Subscription-Key: [form recognizer subscription key] Body:{    \"source\": \"[blob storage end point]\"}             Where:  [form recognizer end point]   The end point of your Form Recognizer service; you’ll find this on the Overview and the Quickstart tabs for your Form Recognizer service.   [form recognizer subscription key]   The API key of your Form Recognizer service; you’ll find this on the Keys and Quickstart tabs for your Form Recognizer service.   [blob storage end point]   The end point of the blob container where you stored your sample forms.              Using the Form Recognizer service and blob container I created earlier, my calls look like this: Request:POST https:// bgformrecognizer.cognitiveservices.azure.com/formrecognizer/v1.0-preview/custom/train Headers:Content-Type: application/jsonOcp-Apim-Subscription-Key: 32-bit hexadecimal value Body:{    \"source\": \"https://kmdatasource.blob.core.windows.net/form-recognizer\"} 3. Train the model using our correctly formed POST API call. When I run the train API call with all of the above settings, I get a 200 OK response with the following body:  {    \"modelId\": \"794f2366-045a-4fbd-81cb-8af57f541e6f\",    \"trainingDocuments\": [        {            \"documentName\": \"Invoice_1.pdf\",            \"pages\": 1,            \"errors\": [],            \"status\": \"success\"        },        {            \"documentName\": \"Invoice_2.pdf\",            \"pages\": 1,            \"errors\": [],            \"status\": \"success\"        },        {            \"documentName\": \"Invoice_3.pdf\",            \"pages\": 1,            \"errors\": [],            \"status\": \"success\"        },        {            \"documentName\": \"Invoice_4.pdf\",            \"pages\": 1,            \"errors\": [],            \"status\": \"success\"        },        {            \"documentName\": \"Invoice_5.pdf\",            \"pages\": 1,            \"errors\": [],            \"status\": \"success\"        }    ],    \"errors\": []} Take special note of the “modelId” value returned. We will be using that value as we progress. Validate Your Trained Model Now that we’ve trained our model, let’s validate it by returning some key-value pairs from the sample data. We’re going to do that by making another API call in the format of: Request:POST https://[form recognizer end point]/formrecognizer/v1.0-preview/custom/models/[modelId]/analyze Headers:Content-Type: multipart/form-dataOcp-Apim-Subscription-Key: [form recognizer subscription key] Body:form: [form to validate]type: application/pdf             Where:  [form recognizer end point]   The end point of your Form Recognizer service; you’ll find this on the Overview and the Quickstart tabs for your Form Recognizer service.   [modelId]   The Id of the model returned when you trained your Form Recognizer model in the previous step.   [form recognizer subscription key]   The API key of your Form Recognizer service; you’ll find this on the Keys and Quickstart tabs for your Form Recognizer service.   [form to validate]   The actual form to validate against the model; this is not a link to the file. In Postman, you are prompted to select a file.    Using the Form Recognizer service, blob container, and trained model I created earlier, my calls look like this: Request:POST https:// bgformrecognizer.cognitiveservices.azure.com/formrecognizer/v1.0-preview/custom/models/794f2366-045a-4fbd-81cb-8af57f541e6f/analyze Headers:Content-Type: multipart/form-dataOcp-Apim-Subscription-Key: 32-bit hexadecimal value Body:form: Invoice_3.pdf   // Uploaded from my local drivetype: application/pdf When I run the train API call with all of the above settings, I get a 200 OK response with the following body: {    \"status\": \"success\",    \"pages\": [        {            \"number\": 1,            \"height\": 792,            \"width\": 612,            \"clusterId\": 0,            \"keyValuePairs\": [                {                    \"key\": [                        {                            \"text\": \"Address:\",                            \"boundingBox\": [                                57.4,                                683.1,                                100.5,                                683.1,                                100.5,                                673.7,                                57.4,                                673.7                            ]                        }                    ],                    \"value\": [                        {                            \"text\": \"1111 8th st.\",                            \"boundingBox\": [                                 … cut for space                            ],                            \"confidence\": 0.86                        },                        {                            \"text\": \"Bellevue, WA 99501\",                            \"boundingBox\": [                                … cut for space                            ],                            \"confidence\": 0.86                        }                    ]                },                {                    \"key\": [                        {                            \"text\": \"Invoice For:\",                            \"boundingBox\": [                                … cut for space                            ]                        }                    ],                    \"value\": [                        {                            \"text\": \"Alpine Ski House\",                            \"boundingBox\": [   … cut for space                            ],                            \"confidence\": 1.0                        },                        {                            \"text\": \"1025 Enterprise Way\",                            \"boundingBox\": [   … cut for space                            ],                            \"confidence\": 1.0                        },                        {                            \"text\": \"Sunnyvale, CA 94024\",                            \"boundingBox\": [   … cut for space                            ],                            \"confidence\": 1.0                        }                    ]                },        … cut for space} Now we’ve validated the model and can see that the Form Recognizer service is correctly picking up key-value pairs for the model we trained. Next, let’s look at integrating our trained Form Recognizer model into Azure Search. Create the Search Service Now we need to create an Azure Search service. If you are unfamiliar with this process, please see the previous blog post on Azure Search in our Knowledge Mining Showcase series. I created an empty Azure Search service called bgforrecognition-search. For now, don’t create an index, data source, or skillset. We first need to create an Azure Function App to call our Form Recognizer service. Create an Azure Function App to call the Form Recognition service Go to your local repository that you cloned from azure-search-power-skills. Open up the PowerSkills.sln file in Visual Studio 2019. Make the Vision/AnalyzeForm project the default project. It should look something like this:  There are only two files we are concerned with at this point: the AnalyzeForm.cs file and the launchSettings.json file under the Properties folder. First, let’s look at the launchSettings.json file.  You’ll want to replace the value for “FORMS_RECOGNIZER_API_KEY” with the key from the Quickstart or Keys tabs from your Form Recognizer service in the Azure Portal. Replace the “FORMS_RECOGNIZER_MODEL_ID” with the modelId value that was returned from the train API call you made earlier. In the AnalyzeForm.cs file you’ll need to update the static string formsRecognizerApiEndpoint variable with the end point of the Form Recognition service you created.  Note the section right under where our settings variables are being set:  You can update the fieldMappings to return any of the key-value pairs you want for your forms. The first value in the dictionary is the Key from the form itself. The second value is what it will be mapped to and exposed to Azure Search as.For this exercise, we’ll leave the mappings as they are.Let’s test out our function app and make sure it works. You can run this locally and the Azure Functions Core Tools will launch to give you the API call you need to make.  Copy the URL for the call and create a new request in Postman. You’ll see a README.md file in the AnalyzeForm project. That file contains a sample post body that we can use to test the function. {    \"values\": [        {            \"recordId\": \"record1\",            \"data\": {    \"formUrl\": \"https://github.com/Azure-Samples/azure-search-power-skills/raw/master/SampleData/Invoice_4.pdf\",                \"formSasToken\":  \"?st=sasTokenThatWillBeGeneratedByCognitiveSearch\"            }        }    ]} My setup in Postman looks like this:  Send the request and you should get back the results shown in the README.md file. Now, we need to deploy our function app to Azure. Right click on the AnalyzeForm project and select Publish. You should see a screen that looks like this:  Select the Azure Functions Consumption Plan option and then press the Publish button. You’ll then see a screen like this:  Fill out the fields on the left side of the screen. Azure will give you a default Name that isn’t very descriptive, so you may want to change it. The function app needs blob storage to store metadata, so create a new blob container, or select one that already exists. Go ahead and press the Create button. This can take a while, don’t worry that it’s not working. Once the function app has successfully deployed, you should see it in the Portal under App Services.  Now that our function app connection to our Form Recognition service is configured and deployed, lets configure our Azure Search service to use it. Update the Search Service Now, we’ll take the search service that we created earlier and update it to make use of our trained Form Recognition model. You will need the Service Name and the API Key for your search service to make the following API calls. The Service Name is simply the name of the Search Service; you can get the API key from the Keys tab on the search service portal page. We’ll give some examples on the API calls to create these search resources. For a full description of how to create these resources, please reference the BlueGranite Knowledge Mining Showcase article here. Create the Datasource Using the blob container and search service I created earlier, my call looked like this: Request:POST https:// bgformrecognizer-search.search.windows.net/datasources?api-version=2019-05-06 Headers:Content-Type:    application/jsonApi-key: 32-bit hexadecimal value    // From Search Service keys Body:{    \"name\": \"pdf-datasource\",      \"description\": \"pdf files for searching.\",      \"type\": \"azureblob\",    \"credentials\":    {\"connectionString\":      \"DefaultEndpointsProtocol=https;AccountName=kmdatasource;AccountKey=r/Ue6aX23+OqOt0SxmZhvzt97xrVjiki8JyhLrlTW59QyAnppsdE1fEQFMA63xxxxxxxxxxxxxxxx;EndpointSuffix=core.windows.net\"     },      \"container\": {\"name\": \"form-recognizer\"}}  Create the Skillset The skillset is how we add all cognitive services to Azure Search. It defines how to process data from the search using external tools and functions. You can find more information on creating skillsets here. I used the following API call to create the skillset for my search (that uses the function app we created to integrate the Form Recognition model we created). Using the blob container and search service created earlier, my call looks like this: Request:POST https:// bgformrecognizer-search.search.windows.net/skillsets/formskillset?api-version=2019-05-06 Headers:Content-Type:    application/jsonApi-key: 32-bit hexadecimal value    // From Search Service keys Body:{    \"name\": \"formskillset\",    \"description\": \"Skillset for Form Recognition\",    \"skills\": [        {           \"@odata.type\": \"#Microsoft.Skills.Custom.WebApiSkill\",            \"name\": \"formrecognizer\",            \"description\": \"Extracts fields from a form using a pre-trained form recognition model\",            \"uri\": \"https://formrecognition.azurewebsites.net/api/analyze-form?code=d4MaIrcb2PP7pmIMqC7E8UcjK2zEhEJxle1ivtyi3xxxxxxxxxx==\",            \"context\": \"/document\",            \"batchSize\": 1,            \"inputs\": [                {                    \"name\": \"formUrl\",                    \"source\": \"/document/metadata_storage_path\"                },                {                    \"name\": \"formSasToken\",                    \"source\": \"/document/metadata_storage_sas_token\"                }            ],            \"outputs\": [                {                    \"name\": \"address\",                    \"targetName\": \"address\"                },                {                    \"name\": \"recipient\",                   \"targetName\": \"recipient\"                }            ]        }    ],    \"cognitiveServices\": {        \"@odata.type\": \"#Microsoft.Azure.Search.CognitiveServicesByKey\",        \"key\": \"29261034dde2452db3deexxxxxxxxxx\"    }} Create the Index The Index is what defines our searchable content. For this exercise, I’ve made a very simple one. This is what my Create Index request looks like: Request:PUT https:// bgformrecognizer-search.search.windows.net/indexes/formindex?api-version=2019-05-06 Headers:Content-Type: application/jsonApi-key: 32-bit hexadecimal value    // From Search Service keys Body:{    \"fields\": [        {            \"name\": \"id\",            \"type\": \"Edm.String\",            \"key\": true,            \"searchable\": true,            \"filterable\": false,            \"facetable\": false,            \"sortable\": true        },        {            \"name\": \"content\",            \"type\": \"Edm.String\",            \"sortable\": false,            \"searchable\": true,            \"filterable\": false,            \"facetable\": false        },        {            \"name\": \"address\",            \"type\": \"Edm.String\",            \"searchable\": true,            \"filterable\": false,            \"retrievable\": true,            \"sortable\": false,            \"facetable\": false,            \"key\": false,            \"indexAnalyzer\": null,            \"searchAnalyzer\": null,            \"analyzer\": null,            \"synonymMaps\": []        },        {            \"name\": \"recipient\",            \"type\": \"Edm.String\",            \"searchable\": true,            \"filterable\": false,            \"retrievable\": true,            \"sortable\": false,            \"facetable\": false,            \"key\": false,            \"indexAnalyzer\": null,            \"searchAnalyzer\": null,            \"analyzer\": null,            \"synonymMaps\": []        }    ]} Create the Indexer  The Indexer does the work of cracking the documents, applying our skillsets, and extracting the document data into our index. This is what my Indexer request looks like. Note that you will get a created response back from your call, but the Indexer will take a little longer to complete. Request:PUT https:// bgformrecognizer-search.search.windows.net/indexes/formindex?api-version=2019-05-06 Headers:Content-Type:    application/jsonApi-key: 32-bit hexadecimal value    // From Search Service keys Body:{    \"fields\": [        {            \"name\": \"id\",            \"type\": \"Edm.String\",            \"key\": true,            \"searchable\": true,            \"filterable\": false,            \"facetable\": false,            \"sortable\": true        },        {            \"name\": \"content\",            \"type\": \"Edm.String\",            \"sortable\": false,            \"searchable\": true,            \"filterable\": false,            \"facetable\": false        },        {            \"name\": \"address\",            \"type\": \"Edm.String\",            \"searchable\": true,            \"filterable\": false,            \"retrievable\": true,            \"sortable\": false,            \"facetable\": false,            \"key\": false,            \"indexAnalyzer\": null,            \"searchAnalyzer\": null,            \"analyzer\": null,            \"synonymMaps\": []        },        {            \"name\": \"recipient\",            \"type\": \"Edm.String\",            \"searchable\": true,            \"filterable\": false,            \"retrievable\": true,            \"sortable\": false,            \"facetable\": false,            \"key\": false,            \"indexAnalyzer\": null,            \"searchAnalyzer\": null,            \"analyzer\": null,            \"synonymMaps\": []        }    ]}Searching on Form Key-Value Pairs Now to test the cognitive search we’ve just created. We can do that by opening our Index on the Azure Portal or by creating an API call to return results. Since our whole dataset consists of 5 documents, I did a quick query to return all documents so we could see the results. Here’s what my query looked like: Request:PUT https:// bgformrecognizer-search.search.windows.net/datasources?api-version=2019-05-06 Headers:Content-Type:    application/json Api-key: 32-bit hexadecimal value    // From Search Service keys Body:{    \"dataSourceName\": \"formdatasource\",    \"targetIndexName\": \"formindex\",    \"skillsetName\": \"formskillset\",    \"fieldMappings\": [        {            \"sourceFieldName\": \"metadata_storage_path\",            \"targetFieldName\": \"metadata_storage_path\"        },        {            \"sourceFieldName\": \"metadata_storage_path\",            \"targetFieldName\": \"id\",            \"mappingFunction\": {                \"name\": \"base64Encode\"            }        }    ],     \"outputFieldMappings\": [        {            \"sourceFieldName\": \"/document/address\",            \"targetFieldName\": \"address\"        },        {            \"sourceFieldName\": \"/document/recipient\",            \"targetFieldName\": \"recipient\"        }    ],    \"parameters\": {        \"maxFailedItems\": -1,        \"maxFailedItemsPerBatch\": -1,        \"configuration\": {            \"dataToExtract\": \"contentAndMetadata\",            \"imageAction\": \"generateNormalizedImages\"        }    }} We can now search our form fields directly with a search like this: Request:GET https:// bgformrecognizer-search.search.windows.net/indexes/formindex/docs? search=Contoso&$select=address,recipient&searchFields=recipient&api-version=2019-05-06 Headers:Content-Type:    application/jsonApi-key: 32-bit hexadecimal value    // From Search Service keys   Which results in the following response: {    \"@odata.context\": \"https://bgformrecognition-search.search.windows.net/indexes('formindex')/$metadata#docs(*)\",    \"value\": [        {            \"@search.score\": 0.095891505,            \"address\": \"22 1st way Suite 4000 Redmond, WA 99243\",            \"recipient\": \"Contoso 456 49th st New York, NY 87643\"        }    ]} Congratulations! If you’ve followed along, you’ve created an Azure Cognitive Search solution using the new Form Recognizer. Stay Tuned We’ll continue to explore ways to use Azure Cognitive Search to uncover knowledge from previously challenging data sources. BlueGranite can also help make the most of your data. Contact us today to learn how."
"50" "In a world where donations drive charities, maximizing impact and minimizing overhead is not just key to a nonprofit’s credibility, but also critical to its donor retention. For nonprofits, digital transformation – an overhaul of the way they leverage information to better serve stakeholders and amplify value – is increasingly critical to survival.   A global Microsoft survey shows nonprofit donors want to see the impact of their dollars; 89% of decision-makers and 86% of donors and volunteers said it’s important to know how a charity uses its contributions. The most successful charities understand this. Leaders in the nonprofit arena have a shared success strategy; they’ve tactically planned for and invested in modern tech platforms, leveraging critical data to inform decisions, streamline organization, and promote transparency. BlueGranite has had the honor of empowering several nonprofits with technology, including one of the nation’s largest, devoted to aiding U.S. families of children fighting life-threatening illnesses, and another, which lifts children across the globe out of poverty. These nonprofits have turned to data to support key decisions. From expanding reach and increasing donations to uncovering impact and analyzing success, the nonprofits that flourish are driven by data. What Does it Mean to Become Data-Driven? A data-driven charity is one that uses data to inform and direct critical decisions. Gathering and analyzing quantifiable and pertinent information is key to the process.  Rather than the old ways of doing things –  turning to intuition, gut feelings, or past experiences – information analysis guides the actions and choices of the most successful nonprofit decision-makers. For many organizations, the first step in a technology transformation begins by gathering and centralizing data that might be scattered across multiple systems. The children’s healthcare charity we partnered with once struggled with an expensive, unwieldy financial reporting system built on multiple vendors and services; others struggle with disparate budgeting or payment systems, or distributed donor lists. Other charities grapple with disparate external information, like the group alleviating childhood poverty once did; those external data challenges may include census data, education assessments, or even weather statistics. Once a data-driven organization has a strategic method for collecting its data and assuring the quality, it can begin to surface and analyze the information, gleaning and sharing insight through reports and dashboards. These tools can quantitatively convey a broad spectrum of information – from operations transparency to programming impact. The Benefits of Investing in Data-Driven Technology In the nonprofit realm, every dollar saved is a dollar that amplifies impact. As uncovered in the Microsoft survey referenced earlier, decision-makers, donors, and volunteers believe strategic technology investment is also ultimately key to a charity’s impact. A well-planned technology transformation, with leadership embracing good IT governance – a strategic scaffold aimed at meeting a nonprofit’s needs in alliance with its goals – and staff and volunteers who are prepared to adopt the new solution, are critical to overall success. Once in place, a successful modern data platform can drive transparency, efficiency, and cost-savings.  The increased transparency of a data-driven charity can boost satisfaction, provide clarity, and build trust across a nonprofit’s network – from board members and corporate and major donors, to staff and volunteers. Board members can easily ensure governance policies are in compliance. Staff can accurately and quickly track budgets. Volunteers and donors can visualize the extent of their impact. The ability to visualize and share the effect of a charity’s efforts can build momentum and boost involvement. In addition, adopting a data-driven business strategy offers great opportunity for increasing efficiency.  Data can highlight potential areas for improvement that are often otherwise overlooked.  This can look like identifying and removing delays from operations to amplify impact or enhancing targeted outreach to prevent wasted marketing dollars. Start Here How does an organization successfully embrace data-driven strategies? Think big, but start small. The amount of data that can potentially be collected and analyzed can initially seem overwhelming, but with a strategic road-map the steps forward become clear. Start small; identify important factors that drive operations but aren’t currently supported by data, particularly data you already have but that may be challenging or difficult to analyze. Utilize your existing resources. Microsoft provides help to get you started. Check out the Power BI starter kit here, to learn more about getting started with the dynamic, visually rich suite of business analytics tools; its simple interface lets you share data across your organization.  Keep an eye out for upcoming BlueGranite webinars and events https://www.blue-granite.com/events, or contact us when you’re ready to create a road-map to deliver value across your charity – from those you help to those who help you."
"51" "I recently worked on a project where I had to calculate a maintenance backlog. A maintenance backlog is a count of the number of tasks that are active (or in a specific status) at a specific point in time. These tasks have a duration of one or more days. This type of indicator falls under the category of semi-additive indicators. They are additive on all but the time dimension.         Opening Balance   Mon.   Tues.   Wed.   Thu.   Fri.   Sat.   Sun.   Total Week   Task Backlog   50   52   54   51   46   48   50   51   51   New Tasks   N\A   10   5   7   0   11   2   1   36   Tasks Closed   N\A   8   3   10   5   9   0   0   35    In the example above, the Backlog at the end of the week is not 352 (the sum of each day’s Backlog) but the Backlog amount for the last day. The aggregation chosen for this type of indicator on the time dimension will depend on your requirements. It could be the last value (as in our example), an average, or the maximum or minimum for the given period.   The “New Task” and “Tasks Closed”, on the other hand, are purely additive; the amount for the week is the sum of each day’s value.  While less common than fully additive indicators, stock-type indicators are still very common and depending on your specific situation, there are many ways to implement them. The method you choose will hinge on various factors. I will touch on some of the pro and cons of different methods.  There are several common use cases for stock-type indicators:  a company’s headcount, hospital or hotel occupancy, active tickets for a helpdesk service, maintenance backlog, and, probably the most common, a balance sheet. Proper implementation will improve performance, functionality, and potentially unlock new analysis for your end users.  While I was working on this project, my first instinct was to implement a technique with SQL Server Analysis Services (SSAS) Multidimensional Expressions (MDX) I’ve used many times before. However, after a few tests, I wasn’t satisfied with the result and decided to investigate other techniques to tackle our client’s maintenance backlog.   Build a Trial Model  For demonstration purposes, let’s work with a fictional healthcare company. This healthcare system receives patients in one of its hospitals at a specific date and releases them a few days later. We will only take inpatients (those who stay more than one day) into account. Our goal is to determine how many beds are occupied, and indirectly, remaining capacity.  There are three dimensions in this model:  Calendar  Patient  Hospital  And one fact table:  Occupancy  The data we’ll use was randomly generated in Excel; each patient is assigned a hospital (Site), an Arrival Date, and a Departure Date.    We want to know how many beds are occupied, at any point in time. Decide on a Data Storage Option At the very beginning of your project, you’ll have to decide how you’ll store the data in your data warehouse. It is one of the most important decisions you’ll make because it drives everything you later do in your semantic model (Power BI, SSAS Tabular or Azure Analysis Services). The format in which the transactional system captures data will also be a factor.  There are three data storage options:  Option 1:  Each of your events will have a start date and an end date. If no other measures are added to your fact, this is a factless fact table. The fact table looks like this:   Column   Data Type   Arrival Date   Date   Release Date   Date   PatientId   Integer   LocationId   Integer    Each time a patient is admitted to the hospital, a new line is added to the table, and the Release Date is set to NULL. When the patient is released, the Release Date is populated (assuming you use a relational database for your data warehouse you would do a merge, update or truncate, and load). This is called a factless fact table because there are no indicators or measurements, only foreign keys in the fact table (both date columns are foreign keys to the date dimension).  Depending on your situation, you might also need an opening balance. Having an opening balance equal to zero (no opening balance) is ideal, as it will greatly simplify your model; however, it is not always possible.   If the hospital was opened 50 years ago, chances are that last time there weren’t any patients was 50 years ago. It’s also extremely unlikely to have 50 years of daily transactions in a source system. An opening balance requires a snapshot-type fact table, as described below.   It can often be easier to have more history than required and no opening balance, rather than a shorter history and an opening balance. It’s a trade-off between volume and model complexity.   Option 2:  This  is  the most common approach; a Start Date – End Date-type snapshot fact table. It’s easy to implement in your ETL and it’s easy to understand. A snapshot-type fact table is one in which a new line is added every day for each patient who is still hospitalized. It looks like this:   Column   Data Type   OccupancyDate   Date   PatientId   Integer   LocationId   Integer    The opening-balance table mentioned in Option 1 is a snapshot-type fact table for one date only, the first date considered in your model.  Option 3: This option combines both a snapshot at a specific point in time (yearly, for instance) and an event-based fact table.   This solution adds complexity and is only useful in specific use cases. For example, this option is the traditional way to model a balance sheet.   Pros and Cons of Each Option  As mentioned earlier, the most commonly encountered approach is Option 2, the snapshot fact table. The main drawback of this approach is that the fact table’s size will grow extremely fast. For example, if you want to calculate the headcount in a company with 10,000 employees on average, and you want 5 years of historical data, you will add 10,000 rows per day to your fact table – that gives you (10,000 * 365 * 5 =) 18,250,000 rows after 5 years.   If you used the first approach, Option 1, the fact table would be (10,000 * 5 =) 50,000 rows after 5 years, assuming your employees change position or quit the company once a year, on average.  The snapshot fact table (Option 2) is (18,250,000 / 50,000 =) 365 times bigger. On the bright side, as the data is very repetitive, you might get a very good compression ratio on these tables.   However, the difference in size between Options 1 and 2  is major; as a result, the snapshot approach should only be used for smaller datasets, or if your budget allows you to have a massively parallel processing (MPP) type database, such as the modern Azure Data Warehouse.   In some cases, you can’t use Option 1, as the data is captured as a snapshot-type fact.  Data Analysis Expressions (DAX) and Our Models  After you decide on how to store your data, you will need to prepare your indicators for reporting layer consumption or for expert analysis. You will typically have three main indicators:  Number of arrivals  Number of departures  Occupancy  The focus of this post is Occupancy.  With a snapshot-type fact table:     For the purpose of this post I generated a model based on Excel data. I randomly assigned a start date, end date and hospital to random people to generate the fact table. In order to have a snapshot-type fact, I generated one row in Power Query for each day a patient is hospitalized, as follows:  let     Source = Occupancy,     #\"Added Custom\" = Table.AddColumn(Source, \"Custom\", each {Number.From([Arrival Date])..Number.From([Departure Date]) - 1}),     #\"Expanded Custom\" = Table.ExpandListColumn(#\"Added Custom\", \"Custom\"),     #\"Changed Type\" = Table.TransformColumnTypes(#\"Expanded Custom\",),     #\"Renamed Columns\" = Table.RenameColumns(#\"Changed Type\",) in     #\"Renamed Columns\" This M code will add a custom column that contains a list of dates between Arrival Date and Departure Date. As you can only generate an interval in M with integers, you should convert the dates to an integer and generate the interval.   {Number.From([Arrival Date])..Number.From([Departure Date])} Then expand the list column, convert back to a date, and you have the snapshot-type fact table. There are other ways (a calculated table in DAX) to generate this kind of fact from a Start Date – End Date-type of table, but this is fast, efficient, and quite simple. Generating the snapshot in a calculated table is also a good option.  From here we can create the Occupancy indicator:  Occupancy (Snapshot) =VAR CurDate =    MAX ( 'Date'[Date] )RETURN    CALCULATE (        COUNTROWS ( 'Occupancy Snapshot' ),        FILTER (            ALL ( 'Occupancy Snapshot'[PresenceDate] ),            'Occupancy Snapshot'[PresenceDate] = CurDate        )    ) This formula will count the number of rows in the current filter context, the date filter context being overridden by the LASTDATE(Date). This gives you the number of rows for the last day of the period considered.  Creation is simple and easy, the performance is adequate, and the table can hold massive volume. s refreshing it will take 365 times longer than if you use the Option 1 design – assuming the snapshot is stored in your data warehouse and not generated in Power BI.  If the data comes as a snapshot from the source system, calculating the number of arrivals and number of departures can be less straightforward than with other options. Another drawback is volume, and I’ve often seen a snapshot at a month-level rather than at a day-level, only to accommodate performance.   With a date-type (Start Date – End Date) fact table:  The main differences in this model are the fact table and the two relationships with the date table; the dimensions are identical. The second relationship is inactive.  Occupancy (Filter) =VAR CurDate =    LASTDATE ( 'Date'[Date] )RETURN    CALCULATE (        COUNTROWS ( Occupancy ),        FILTER ( ALL ( 'Date' ), 'Date'[Date] <= CurDate ),        FILTER (            ALL ( Occupancy[Departure Date] ),            Occupancy[Departure Date] >= CurDate        )    ) This formula will first store the current date (in the filter context) in a variable and count all the rows in the fact table for which the current date is between the Arrival Date and the Departure Date.  This calculation can be tuned in many ways, depending on your specific use case. It is worth mentioning that the performance depends on the size of your fact table and also on the duration of the events (Duration = Departure Date – Arrival Date). Also, this specific formula only provides the end of period value. Slightly different approaches are required to get an average, a minimum, or a maximum, over the period considered.  With a Start – End Date and inception-to-date indicators:  This was my initial approach, as it works well with SSAS Multidimensional and Multidimensional Expressions (MDX) (thanks to the aggregation mechanism).   The DAX is as follows: Occupancy (ITD) =VAR CurDate =    LASTDATE ( 'Date'[Date] )VAR AdmissionITD =    CALCULATE (        COUNTROWS ( 'Occupancy' ),        FILTER ( ALL ( 'Date' ), 'Date'[Date] <= CurDate )    )VAR ReleaseITD =    CALCULATE (        COUNTROWS ( 'Occupancy' ),        FILTER ( ALL ( 'Date' ), 'Date'[Date] < CurDate ),        USERELATIONSHIP ( 'Date'[Date], 'Occupancy'[Departure Date] )    )RETURN    AdmissionITD – ReleaseITD This formula will calculate all the Admissions since the beginning of the time (in the variable AdmissionITD), the number of Release since the beginning of the time (in the variable ReleaseITD, finally, the Occupancy will be the difference of both.   This last formula has two drawbacks:  1 – As it scans the Fact from the beginning of time twice for each filter context, it is not always the most performant. That being said, the recent availability of aggregate tables with Power BI might solve this performance issue, but I haven’t yet tried it. Maybe in a future blog post.  2 – If the data quality is not on point, the errors will amplify with time. For instance, when a system is first rolled out, there is a period during which end users will make mistakes, forget to close a case, forget to open it, close it too late, etc. Assuming they are not closed, these mistakes will carry over to the most recent period.  In the Power BI report below, you can see that the three approaches provide the same result.   While working on our client’s project, as well as this blog post, I intensively used the two following resources – worth considering if you need to work on these types of metrics.  SQLBI (Alberto Ferrari) : https://www.sqlbi.com/articles/analyzing-events-with-a-duration-in-dax/  Chris Webb: https://blog.crossjoin.co.uk/2013/06/13/a-new-events-in-progress-dax-pattern/  Alberto Ferrari’s solution also presents an alternative to simulate a snapshot fact table, using DAX rather than Power Query.  Mixed Approaches  You can mix some of these solutions; for instance, using a snapshot fact table as an opening balance every year and a Start Date – End Date fact during the year. In other words, you reset your “Stock   Indicator” calculation yearly. However, this will add some complexity and should be used sparingly.  The best solution is the simplest solution that solves your problem with adequate performance.   Conclusion  Stock-type indicators are slightly more complex to calculate than purely additive-type indicators. The most important point to consider is that each calculation has its pro and cons, and choosing the “best” approach will depend on these parameters:  How is the data captured?   What is the volume (in your data warehouse and in Power BI)?  How long are the events you want to count?  What is the data quality?  These factors will also strongly influence performance. Your final solution should take each of them into consideration.   In my experience, Option 2 is often the best approach to calculating stock-type indicators, but becoming comfortable with each approach makes sense, as real-life needs will vary. If you find performance is not adequate with Option 2 – bringing the data in Power BI with a Start and End date and doing the calculation in DAX –, you can create the snapshot in M or DAX, rather than having the snapshot transiting over the network.  Looking to maximize your data? Our experts can help. Contact BlueGranite today to learn more. "
"52" "This post contains highlights of our recent \"Intro to Personalized Marketing\" webinar. The continued growth of e-commerce, social media, and streaming services, combined with growing data volumes related to digital activity, has made it possible – and even an expectation – for brands to provide personal experiences to their customers. Global research giant Gartner estimates that organizations using online personalization will outsell companies that aren’t by 30%, and, that by 2020, 90% of brands will practice at least one form of personalized marketing[1]. Effective digital marketing provides richer insights from customer interactions, allowing organizations to create better content, develop deeper prospect relationships, and ultimately achieve greater return on investment (ROI) from advertising. Personalized marketing utilizes modern tools, like machine learning (ML) and AI, then operationalizes insights from those tools. Common barriers to success when using data science technology include difficulty collaborating among team members, managing experiments and other modeling artifacts, scalability, and using predictions in applications. This post will discuss some introductory concepts for digital marketing professionals and data scientists to overcome these challenges. What does a consumer want to see next and how likely are they to purchase? What information about the product should be presented? Personalized marketing refers to deciding what content should be shown to different groups of customers based on your knowledge of those groups. That knowledge can come from what the customer has told you about themselves, how they’ve behaved in the past (their purchases and online activity), and details about the products or services you’re offering. Above all, this content should be more interesting and relevant to your customer than if you were treating them “like everyone else.” Benefits of this type of modern marketing approach include increased open rates, conversions, and page views, shorter sales cycle times and lower overall marketing costs. HubSpot, a leading customer relationship management (CRM) provider, finds 68% of marketers say that personalization based on behavioral data has a high impact on ROI, while 74% say it has a high impact on engagement. But only 19% do it[2]. So, what are the challenges? Personalized marketing depends on the quality and volume of data available – about your customers and their preferences – and the analytic tools and talent to make sense of it. Defining different groups, or segments, can be performed in various levels of sophistication and detail. For example, basic customer list segmentation can be performed manually using information your users have volunteered about themselves, usually from registration forms. However, moving beyond the basics is best accomplished with advanced tools like machine learning. The essence of machine learning is using prior examples to predict outcomes of future occurrences, without being explicitly programmed. It’s pattern recognition from the data, but this can involve complex patterns across multiple data sets and variables. A common machine learning technique used for personalized marketing is a recommendation system. Most of us have been on the receiving end of a recommendation system if we’ve been advised of the “next best” product or movie we might like. What makes recommendation systems so powerful is the breadth and volume of data they can incorporate. In addition to the registered customer preferences discussed in the basic example, we can also include transaction history, click-stream data (i.e., Google Analytics), demographics from a third party, and ratings or sentiment from social media. Recommendation systems can use data sets and algorithms that, like personalization in general, can vary in complexity. You can read more about recommender systems here on this Microsoft data science blog[3]. Let’s discuss a basic recommendation scenario. You are preparing an email campaign for a new product. Your data set includes a customer list, purchase history, and product information about your existing products, as well as your new product. Patterns within this data set can reveal clues about the likelihood of each customer making a purchase. How recently has the customer made any purchase from you, and how much have they spent? Is this new product related to the customer’s interests – is it attractive for their age, geography, income level, and activities? How similar is the new product to products previously purchased? From this data, we can use a machine learning technique called classification – a type of recommendation system – that will predict the probability of purchase for each customer. With that, we can prioritize the mailing list, and maybe even avoid sending to large groups of customers unlikely to buy. It’s like lining up only our best customers (for that product) to come through the door first, and not bothering the rest. This preserves and improves relationships with customers over time, developing trust that we know what they do, and do not, like. In data science for marketing, capitalizing on predictions like this is called “lift.” If we have relatively accurate predictions, we can capture a large percentage of the relevant audience through contact with a comparatively smaller portion of the total customer base. In the illustration below, we achieve more than 30% lift by using machine learning compared to random guessing.  We still haven’t directly addressed all the challenges that might be inferred by only 19% of marketers using personalization. Data availability is one. Creating predictions using machine learning is another. But perhaps the biggest challenge is using the predictions in an operational system for real-time recommendations or decision making. Assuming you have the proper data, creating a classification model requires just a few lines of code in tools like Python or R. But in most organizations, these lines of code and the predictions they create remain in a data scientist’s local environment. Problems arise when trying to use big data sets like Google Analytics, collaborating with distributed teams, and keeping track of myriad development artifacts. For most organizations, getting real use out of a machine learning approach is an issue of scale and using best practices from software development like DevOps[4]. This is where a service such as Azure Machine Learning[5] can help. Azure Machine Learning service (Azure ML) is Microsoft’s cloud service for managing and deploying machine learning models, which can lead to major gains in terms of ML value and data scientist productivity. For model development, it allows data scientists to use their current code via the Python SDK and preferred frameworks (like PyTorch, TensorFlow, and scikit-learn) with scalable and on-demand compute environments like Spark and CPU/GPU clusters. Data scientists can stick with familiar development tools like local integrated development environments (IDEs) and notebooks, or use cloud-based notebook virtual machines (VMs)[6] that integrate directly with the service and are great for collaboration. Training experiments and results are registered in a workspace, and artifacts can be retrieved through the SDK, command-line interface (CLI), or web portal. The image below illustrates how a user can track experiment results across different model settings, such as hyperparameter values. This view can be accessed with a notebook widget or through the Azure portal.  Once a model has been trained and selected, a model object and associated metadata can be registered on the service. I like to think of this stage as the bridge between traditional data science development and operational deployment. With the registered model, Azure ML provides great tools to deploy it as a web service. The model, along with information about the compute environment, are used to create a Docker image. These images can then be deployed as containers for testing or production as web services in applications[7] – serving your model and its predictions where they’re needed most. Even better, Azure ML provides tools to automate this production process into ML pipelines[8], DevOps-style. An overview of the service and its benefits are below:  As I was preparing for this post and the associated webinar, I was inspired by the wealth of resources Microsoft provides for getting started with personalized marketing. A great place to begin is this post here about building recommender systems with Azure Machine Learning service[9].  It discusses different types of recommenders, multiple machine learning algorithms, and considerations for deployment. It also has a companion GitHub repository, here,[10] with a rich set of recommender code examples, best practices, and utilities along the spectrum of development environments and ML frameworks. It has great setup instructions, whether you’re using a local machine or a more complex environment like Azure Databricks. I hope you found this post helpful. The companion webinar also includes a live demonstration of using Azure ML from model training to deployment and accessing the web service in Power BI[11]. For more information on getting started with Azure Machine Learning, including a free Azure account, click here to visit Microsoft’s Azure account sign-up page. Feel free to Contact Us with any questions regarding this blog post or the webinar.  [1] https://blogs.gartner.com/noah-elkin/the-long-and-winding-road-to-real-time-marketing/ [2] https://blog.hubspot.com/blog/tabid/6307/bid/32848/Why-List-Segmentation-Matters-in-Email-Marketing.aspx [3] https://azure.microsoft.com/en-us/blog/building-recommender-systems-with-azure-machine-learning-service/  [4] https://en.wikipedia.org/wiki/DevOps [5] https://azure.microsoft.com/en-us/services/machine-learning-service/ [6] https://azure.microsoft.com/en-us/blog/three-things-to-know-about-azure-machine-learning-notebook-vm/ [7] https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where [8] https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-ml-pipelines [9] https://azure.microsoft.com/en-us/blog/building-recommender-systems-with-azure-machine-learning-service/  [10] https://github.com/microsoft/recommenders/blob/master/README.md [11] https://docs.microsoft.com/en-us/power-bi/service-machine-learning-integration "
"53" "In what’s become an annual tradition, BlueGranite team members across the nation recently spent a summer workday volunteering with a nonprofit organization of choice.  It’s our company’s third year sponsoring a \"Day of Service\", where we encourage our team to give back to their respective communities. The excitement surrounding this year’s series of community collaborations has us all already looking forward to 2020! We are truly grateful for these opportunities for impact. Read on to learn about some of the organizations we worked with in 2019, and how we were able to help out.  (Back row: Left to right) Austin Goodrich, Justin Bahr, Zach Conroe, Matthew Mace, Carrie Renfrow, Rosana VanVleet,  Eric Wozniak,  Mike Depoian,  Erik Roll (Front row: Left to right) Sawyer Nyquist, Kevin Jackson, Robert Hutchinson, Jon Trapane, and Dan Myers in Michigan Big Brothers Big Sisters Most of BlueGranite's employees work remotely, but we gathered in teams in our major U.S. hubs to volunteer. This year near our Michigan headquarters, area team members chose to spend the day volunteering at Kalamazoo County-based Big Brothers Big Sisters .  Founded in 1958, the national organization is based on the belief that inherent in every child is the ability to succeed and thrive in life. Big Brothers Big Sisters makes meaningful, monitored matches between adult volunteers and children, from 6 to 18, to promote positive friendships that have a direct and lasting effect on the lives of young people. Fourteen BlueGranite team members joined forces to re-landscape the BBBS headquarters –mowing, weeding, trimming trees, and removing brush and poison ivy, making the grounds more inviting for the organization’s activities. The team also freshened up the main conference room with a new coat of paint. We were so glad to be a part of this project and hope to partner with Big Brothers Big Sisters again in the future.   Left to right: Robert Hutchinson, Eric Wozniak, and Matthew Mace, in Kalamazoo, MIchigan. Keep Indianapolis Beautiful Just across Michigan’s border, our Indiana colleagues focused on community cleanup. BlueGranite’s Indianapolis-based group participated in a Keep Indianapolis Beautiful event to spruce up city public spaces. The team spent the day on the city’s east side, in Grace Tuxedo, manually picking up manageable litter, while staging large, bulk items for city refuse removal. The mayor even made an appearance to thank all the volunteers for their efforts, reiterating how much the clean-up impacts area residents.   Left to right: Larry Baker, Katy Ghormley , and Emily Depasse, in Indianapolis. Food Bank of the Rockies BlueGranite's Colorado team member, Andy Lathrop, spent his Day of Service packaging meals to be distributed at the local Boys and Girls Clubs in Denver, as part of the Kids Cafe program. He worked at the Food Bank of the Rockies distribution center, which provides, on average, 1,000,000 hot meals per week between after school and summer programs, community kitchens, and Kids Cafe sites. The distribution center also offers nutritionally balanced shelf-stable meals to areas where it's not possible to distribute hot meals. Food bank volunteers and Andy Lathrop in Colorado. Griffin Middle School BlueGranite’s Georgia team spent its Day of Service in Atlanta,  sprucing up Griffin Middle School.  One of our team members even brought his son along to help repaint the sixth-grade hallway, in preparation for the start of a new school year! Our team was given a warm welcome and was so appreciated by the school. Go Wildcats! Left to right: Jeff Seibert, Leo Furlong, Alex Toshev, Hunter Seibert in Georgia. Second Harvest Food Bank In North Carolina, Melissa Coates, Angela Henry, Jerry Lukomskiy, Terry Crist, and Colby Ford returned for BlueGranite's second year volunteering at the Second Harvest Food Bank of Metrolina in Charlotte. The team spent the day sorting and organizing donations from Walmart. The local stores donate all returns or nonselling items, and Second Harvest Food Bank distributes these items across their multiple pick up locations.  For more than 35 years, Feeding America, Second Harvest’s parent organization, has responded to the nation’s hunger crisis by providing food to people in need through a nationwide food bank network. John van Hengel started the nation’s first food bank in Phoenix, Ariz., in the late 1960s,  distributing 275,000 pounds of food to people in need within the first year. Second Harvest Food Banks were then developed in 1979 to continue alleviating hunger. There are now 200 Second Harvest Food Banks across the country, feeding 46 million people.   First photo (Left to right): Terry Crist, Colby Ford, Jerry Lukomskiy. Second photo (Left to right): Melissa Coates, Angela Henry, Jerry Lukomskiy, Terry Crist, and Colby Ford, in North Carolina Camp Barakel Our Charlotte team was not the only group that chose to revisit its charity from last year. Jim Bennett spent his Day of Service working with Camp Barakel  – a summer camp located in Northern Michigan. Jim has been volunteering for the camp for many years. This year, he spent his time painting the home of a staff member, doing electrical work, installing and configuring new uninterrupted power suppliers for the camp's server room, troubleshooting technical issues with the camp store's POS system (leave it to a BlueGranite employee!) and helping with the accounting process for the camp store!  Jim Bennet working at Camp Barakel in Fairview, Michigan Racine County Food Bank BlueGranite Account Lead Gary Lock, and his family, also spent a day volunteering at a Wisconsin food pantry. Racine County Food Bank in Racine has been around for over 30 years and partners with the United Way to supply the local emergency food network with more than 90,000 pounds of food every month. Gary and his family had a great time volunteering and were happy to give to this cause!  Gary Lock and his family sorting food at Racine County Food Bank, in Wisconsin. Habitat For Humanity    Habitat for Humanity, a global nonprofit housing organization working in local communities across all 50 U.S. states and approximately 70 countries, is also close to the hearts of many BlueGranite team members. Our Ohio-based colleagues spent the day hard at work painting, siding, and renovating for the organization; as did team members based in Massachusetts and Minnesota. Habitat works towards its vision of \"A world where everyone has a decent place to live\" by building strength, stability, and self-reliance in partnership with families in need of affordable housing. Habitat homeowners actually help to build their homes, alongside volunteers, and are given the opportunity to have affordable mortgage payments. This work is always rewarding, and our team was grateful to have a role in furthering this mission.    Top left photo: Jason Brugger, and Merrill Aldrich, in Minnesota.Top middle photo: Paul Poco, Chris Vo, in Massachusetts. Top right photo: Paul Poco  Bottom left photo: Chris Umbaugh and, Lindsay Pinchot. Bottom right photo: Josh Crittenden, Lindsay Pinchot, and Chris Umbaugh, in Ohio. Orland Grassland Volunteers Group In Illinois, David Barnhart, Eric Kieft, and Josh Roll spent their Day of Service removing invasive plant species, and seeding indigenous plants, at Orland Grassland. The 1,000-acre site, in the middle of Chicago’s southwest suburbs, is home to a number of protected bird and plant species. While the land is part of the Forest Preserves of Cook County, volunteer effort is key to maintaining its Land and Water Preserve status. Our Chicagoland team encountered a wide variety of plant, bird, and insect life that day. Team members were happy to do their part in preserving this beautiful space!  Top left photo: David Barnhart, Eric Kieft, Josh Roll, in Illinois. Loaves and Fishes Food Pantry As I was not able to make it to our Kalamazoo Day of Service, I supplemented with my own day volunteering at Kalamazoo Loaves and Fishes, a local food bank. In the early 1980s, a group of residents here recognized their neighbors were struggling to put food on the table. They founded Loaves and Fishes, never imaging how profoundly it would someday impact the community. Their efforts have grown into an organization that today distributes food to nearly 80 sites countywide and provides an average of 700 people daily with groceries.  BlueGranite has worked with Loaves and Fishes in the past; I was excited to continue the tradition, working with the nonprofit’s long-time volunteer group, The Pasta Pals. We sorted produce, cleaned the warehouse, and boxed food for distribution across KLF's network food pantries.  Left photo: The Pasta Pals volunteers of Kalamazoo Loaves and Fishes. Right photo: Jessica Diekema with The Pasta Pals, in Michigan. Getting inspired? We are too! That’s why we set aside time every summer to work with our local communities to give back. If you’re looking for ways to give back with your own Day of Service, check out the government’s National Day of Service and Remembrance website for upcoming opportunities across the nation."
"54" "(This topic was the Use Case example shown during our August 2019 session of Power BI Office Hours.) As this is a long blog article there are the relevant sections: Overview/Requirements Time Intelligence Measures Creating the Disconnected Date table Creating the Calculated Measures Refining the KPI Card Measures  Overview/Requirements  I was out on the interwebs the other day and there was a request on the PowerBI User Group: “So I'm working on a report and I need to be able to see the MTD and YTD of sales for a selected date. I also need a graph showing the sales of each day in the month of the selected day up to that day. These will be on separate pages so I do have the date slicer already synced but I threw the visual one page for simplicity's sake. Any advice? Right now I can only get visuals to show for the selected day.” For this blog post we’ll set the requirements as such: One (1) slicer that has a list of dates Three (3) KPI cards that reflect the daily, MTD, and YTD sales for the value selected in the slicer. One (1) clustered column chart that lists the MTD values by date from the beginning of the month of the date selected up-to-and-including the date selected.  Something that looks like this:  And for extra credit we’ll make the titles above the cards dynamic as well.  <U+0001F60A> How did I accomplish that?  The answer – disconnected tables.  Why? Well, think about what’s happening behind the scenes if I have the following dataset:   Time Intelligence Measures Imagine that I throw a slicer on the page with Date and then create the calculated measures for Daily Sales, MTD Sales, and YTD Sales.  These will all function just fine.  Here’s the code for the 3 calculated measures: Daily Sales = SUM(Sales[Sales Amount]) MTD Sales = TOTALMTD([Daily Sales], Sales[Date]) YTD Sales = TOTALYTD([Daily Sales], Sales[Date]) However, the moment I try to add the clustered column chart and use Daily Sales as my measure what happens?  It only displays the data for the one date selected in the slicer.  How can I get it to display all the dates up thru the date I’ve selected in the slicer?  Sadly, as long as they both come from the same Date field they will always both have the same Filter Context.  Enter the disconnected table. I use disconnected tables when I want to drive a slicer by one set of values and use the value(s) selected to then control what is displayed. Here’s the technique I’m going to use: Create a date table to drive the slicer – I’ll leave it disconnected from the fact table. Create a calculated measure that looks at the date selected in the slicer and determines if a value should be calculated. Use that calculated measure in my chart.  Creating the Disconnected Date table   There are a few different ways I could create my date table – I could reimport the source file, select just the Date column, and remove duplicate rows.  But that’s too much work.  Enter one of my favorite DAX function: CALENDARAUTO(). The CALENDARAUTO() function scans all Date/DateTime/Time columns in your entire model, goes to Jan  1 of the earliest date it finds and Dec 31 of the latest date it finds and returns a table of dates with all dates in-between.  All with very little typing.  <U+0001F60A> So, I’ll click the “New Table” button in the Modeling menu in the toolbar and replace the “Table = “ with “Disconnected Date = CALENDARAUTO()”.  Presto – magic date table. **Note: I want to reiterate what the CALENDARAUTO() function returns – it looks at all dates/datetimes in your model.  So if you have fields like date-of-birth or defaults that set null dates to, say, 1/1/1900 you may want to use a different technique so you don’t have a Date table that goes from 1/1/1900 – 12/31/2031.  But for this example it will suffice.** **Important: do not create a relationship between the date table and the fact table – in fact double-check and make sure the engine didn’t try to create one for you.**  Creating the Calculated Measures    Ok – next step – the calculated measures.  There are a number of functions that determine if something has been selected in a table: HASONEVALUE(), ISFILTERED(), SELECTEDVALUE(), etc.  I chose to use SELECTEDVALUE() in this scenario. We’ll start easy and then refine it. First we’ll start with the code that says “sum up Sales Amount for any day prior to the date I’ve selected in the slicer”. How do I know what date I’ve selected in the slicer?  SELECTEDVALUE().  Let’s see it in action. I’ll create a calculated measure in my Disconnected Date table called “Selected Date”.  Here’s the DAX: Selected Date = SELECTEDVALUE('Disconnected Date'[Date]) I’ll add that calculated measure to a card on my report canvas and create a new slicer with the Date column from the Disconnected Date table (leave the other one on the page if you’re working along with me). As I select different values in the new slicer note that the “Selected Date” measure changes to reflect the date selected.  Perfect. **Note: The SELECTEDVALUE() function has an optional second argument – we will use MAX(‘Disconnected Date’[Date]) as the second argument in the event the user multi-selects two or more dates in the slicer.**  Ok – now the hard part.  Creating the calculated measure that looks at that “Selected Date” value and figures out if the engine should calculate a value. Here’s the logic: “if the user selects a date in the slicer then calculate the MTD sales for any date on-or-before the date selected and in the same month as the date selected”. Here’s the DAX that does that work: (formatting provided by DaxFormatter.com – thanks SQLBI.com guys!) Rolling MTD Sales = CALCULATE (     [Daily Sales],     FILTER (         ALL ( Sales[Date] ),         Sales[Date] <= [Selected Date]             && MONTH ( Sales[Date] ) = MONTH ( [Selected Date] )             && YEAR ( Sales[Date] ) = YEAR ( [Selected Date] )             && Sales[Date] <= MAX ( Sales[Date] )     ) ) So, what does that do?  Well, we know [Daily Sales] is simply SUM(Sales[Sales Amount]).  The FILTER() function then filters the Sales table, clearing any context on the Date column using the ALL() function, and then applies a new filter saying “where the Sales[Date] column is less-than-or-equal-to the Date selected in the Disconnected Date slicer and has the same Month and Year as the Date selected”.  Why do we need that last “&& Sales[Date] <= MAX(Sales[Date])”?  Because the left-hand side of that argument is the value of the Date column in the filtered table (essentially “inside” the FILTER function) and the MAX(Sales[Date]) references the outer context in my visualization (essentially “outside” the FILTER function).  (Note: if you want to learn more about evaluation context I encourage you to check out this great video on SQLBI.com: https://www.sqlbi.com/tv/deep-dive-into-dax-evaluation-context/?nu=19529) If I add my new [Rolling MTD Sales] measure to a clustered column chart let’s see what happens.  The magic here: I’m using the Date column from my Sales table. So what’s going on there? Can you see that slight “lean” at the front of that chart?  We’re accurately calculating the MTD (if I hover over one of the points in the far left of the chart the tooltip returns the correct MTD value) but I’m then showing that value for the rest of time.  Let’s clean that up: Rolling MTD Sales = IF (     OR (         ISFILTERED ( 'Disconnected Date'[Date] ) = FALSE (),         MAX ( Sales[Date] ) > [Selected Date]     ),     BLANK (),     CALCULATE (         [Daily Sales],         FILTER (             ALL ( Sales[Date] ),             Sales[Date] <= [Selected Date]                 && MONTH ( Sales[Date] ) = MONTH ( [Selected Date] )                 && YEAR ( Sales[Date] ) = YEAR ( [Selected Date] )                 && Sales[Date] <= MAX ( Sales[Date] )         )     ) ) I’ve added a new IF() function at the beginning saying “if the user hasn’t selected a value in the Disconnected Date table OR the Date that we’re looking at in the visualization is greater than the date selected in the Disconnected Date table return a BLANK() otherwise do the math”.  That looks a LOT better.  If I select another date it does exactly what I want.   Refining the KPI Card Measures   Lastly I need to get those KPI cards working.  They’re currently not “hooked up” to the Disconnected Date slicer. The Daily Sales one is pretty easy to solve. Rolling Daily Sales = CALCULATE([Daily Sales], FILTER(Sales, Sales[Date] = [Selected Date]))   The MTD values is a little trickier.  If I try to use [Rolling MTD Sales] in a KPI card it’ll return a Blank.  Why?  Because in the KPI Card the Sales[Date] column has no context so the MTD DAX doesn’t know what date to use. We can use the same technique as above with one little tweak: Rolling MTD Sales Card = CALCULATE([Rolling MTD Sales], FILTER(Sales, Sales[Date] <= [Selected Date]))   I have to use “<=” so that I don’t focus the context on the Sales[Date] to just one date (try it – watch what happens if you use just “=”). And then for the YTD card I’ll create a [YTD Rolling Sales] calculated measure and then another [YTD Rolling Sales Card] measure to use in the KPI card. Rolling YTD Sales = IF (     OR (         ISFILTERED ( 'Disconnected Date'[Date] ) = FALSE (),         MAX ( Sales[Date] ) > [Selected Date]     ),     BLANK (),     CALCULATE (         [Daily Sales],         FILTER (             ALL ( Sales[Date] ),             Sales[Date] <= [Selected Date]                 && YEAR ( Sales[Date] ) = YEAR ( [Selected Date] )                 && Sales[Date] <= MAX ( Sales[Date] )         )     ) )   Voila!  If you’re interested, here’s the code to generate a few Dynamic Titles. (You can find a great blog article on dynamic titles by Lindsay Pinchot here). Rolling Daily Sales Title = \"Daily sales for \" & FORMAT([Selected Date], \"M/d/yy\") Rolling MTD Sales Title = \"MTD Sales for \" & FORMAT([Selected Date], \"MMM yy\") Rolling YTD Sales Title = FORMAT([Selected Date], \"yyyy\") & \" YTD Sales\"  If you have further questions please don’t hesitate to Contact Us and we’ll be happy to help! Be sure to join us next month for another round of Power BI Office Hours!"
"55" "BlueGranite is built on shared commitment to clients, innovation, and teamwork, but it’s the skills of our dedicated team that really shine. Our Meet our Team blog series highlights our many talented staffers across the country, all of whom drive our success. Today we are featuring Sawyer Nyquist, a recent addition to our BlueGranite Staff Consultant team. Sawyer is passionate about providing business-first data intelligence through human-centric reporting and visualizations, and \"making business users at home in the world of data.\"    Sawyer grew up across the Midwest – Nebraska, Iowa, and Missouri – and attended undergraduate college in Chicago before pursuing his master’s degree in Dallas.  \"My formal education is in the liberal arts, and I wouldn’t trade those experiences for anything. My bachelor’s and master’s degrees taught me how to learn, which is a skill I have been able to apply daily as I grow in technical domains.\" Sawyer has worked in many different industries, from marketing and retail to financial services and travel, as well as in assorted roles across various companies — working with everyone from Fortune 100 executives to independent small business owners. His diverse portfolio and background give Sawyer an adaptable and well-informed approach to business analytics. \"To merge business thinking and acumen with technological expertise often feels like the unicorn of modern enterprise. With departments increasingly siloed and niche specialization driving employees toward depth of individual skills, the need for integration of complementary and hybrid skill sets is even more valuable. Taking intentional steps to uncover what makes a business tick, and leveraging technical competencies, it’s possible to deliver tremendous value to teams, departments and companies. The need for a new paradigm is increasingly clear across organizations, but especially in the realm of data warehousing and business intelligence.\" Sawyer notes that he relies on discipline and focus to create lasting skills and valuable results.  \"In an era where everything calls for my time and attention, it takes regular pruning in order to apply myself to the most important work. I believe in the virtue of creating. The people in businesses are often the leading creators in our culture. I see my work at BlueGranite fulfilling the role of a creator and empowering other creators. BlueGranite is filled with elite performers and I am thrilled to be surrounded by such a skilled team. Moving from project to project as a consultant consistently keeps me on my toes, allowing me to engage interesting problems and sharpen my skills.\" Outside of work, Sawyer can be found tumbling around on his living room floor amid a wrestling match with his three sons. Sawyer credits his wife of 8 years with the harder job between them, as a stay-at-home mom.  Contact BlueGranite today to learn more about how our team of experts can tackle your data needs to build value across your organization. "
"56" "Pre-built AI now appears in many areas of Microsoft Power BI and helps to make advanced analysis accessible to anyone. Various AI features that have made their way into the product are targeted toward data modelers and report creators, not simply data scientists. BlueGranite recently explored one such “self-service AI” capability with the Key Influencers visual in Power BI Desktop. Next, let’s look at another recent AI enhancement to the Power BI Service called AI Insights.  What is AI Insights in Power BI?   AI Insights is a Power BI feature that blends machine learning into Power BI’s data transformation process. Closely tied to Power Query Online, commonly known as Dataflows, AI Insights adds additional capabilities to enrich your dataflow entities. With a subset of Microsoft Cognitive Services, Power BI authors can enhance dataflows with pre-built AI to obtain sentiment scores, describe images, and more. You should see AI Insights in the top menu bar alongside other options when adding or editing a dataflow entity.  The Cognitive Services functions inside AI Insights come ready to use with a few clicks. Power BI also has more advanced capabilities for using your own trained machine learning models from Azure Machine Learning. Besides the pre-built models, the AI Insights window will display any Azure Machine Learning models that have been shared for the user. Using your own models is outside the scope of this post, but BlueGranite will hold a webinar on August 27th that showcases these advanced features. Register for the free webinar here:https://www.blue-granite.com/personalized-marketing-azure-machine-learning Currently, the cognitive functions in Power BI do not represent the full spectrum of the Cognitive Services APIs in Azure. Rather, Cognitive Services in Power BI focuses on common data workloads such as Text Analytics and Image Tagging. If you are interested in the full capabilities of Cognitive Services, BlueGranite previously outlined everything available through the Cognitive Services APIs in a series of posts last summer.   The pre-built AI currently includes these four functions: Tag Images Extract Key Phrases Detect Language Score Sentiment   Image Tagging   To add tags for image files, it works best to host them online and pass URLs to Power Query’s Web.Contents() function to load them as binary files. While Power BI has some capacity to handle images stored as text in Base64 format, this method only works for smaller files at low resolution. After loading the column containing images, select the CognitiveServices.TagImages option and choose the appropriate image column from the source data.  The pre-trained AI will automatically detect the contents of the image and assign appropriate tags to each image in a new column along with the confidence score for each tag.    Text Analytics   Unlike image tagging, which relies on a more specialized type of data, the three Cognitive Services functions for Text Analytics will work with any text field. After loading source data, select the appropriate text column and choose between the three functions CognitiveServices.ScoreSentiment, CognitiveServices.DetectLanguage, and CognitiveServices.ExtractKeyPhrases.  As with images, results from scoring sentiment, detecting the language, and extracting key phrases appear as new columns alongside the source data.    Licensing   Licensing AI Insights relies on the purchase of dedicated capacity through either Power BI Premium (or Power BI Embedded A2 or greater). Simply creating a dataflow using a Power BI Pro license in a shared capacity workspace will not be enough to take advantage of AI Insights. While the button may appear, clicking it in a shared capacity workspace will notify the user that the workspace needs to be assigned to a dedicated capacity.  Any Power BI Premium node P1 or greater could be used for AI Insight's dedicated capacity. After a workspace is moved from shared to dedicated capacity, AI Insights becomes available. There is also no need to purchase additional keys as you would with Cognitive Services APIs in Azure. Everything is built into the cost of Power BI Premium. Unless you are processing  tens of millions of records using the Azure Cognitive Services APIs, its inclusion in Power BI would not by itself justify the cost of a Premium node. When it comes to considering Power BI Premium, however, it’s one among many significant features.   Get Started!   If you already have access to a dedicated capacity or are simply considering the purchase of Power BI Premium, it is definitely worth looking into AI Insights along with the other features that Premium offers. By embedding AI into so many areas in Power BI, content creators have many tools at their fingertips. If you want to know more about cognitive capabilities in Power BI or Azure, BlueGranite provides services across Microsoft's Modern BI, AI, and Data Platform tools. Contact us today!"
"57" "Over the past few years, Microsoft’s Power Platform has evolved rapidly. Analyze, Act and Automate are the simple words used to describe the broad capabilities provided by the tools within the Power Platform. Power BI (Analyze), PowerApps (Act) and Microsoft Flow (Automate) work together seamlessly to provide even users with very limited technical expertise the ability to build custom apps, automate workflows and analyze data for insights.  While the Power Platform can connect to many different sources and systems for data, Microsoft has been hard at work developing a standardized set of schemas for the Power Platform they call the Common Data Model. The Common Data Model is an open-sourced definition of standard (“core”) entities that represent commonly used concepts and activities. The Common Data Model simplifies data interactions by providing a shared data language for business and analytical applications to use. The Common Data Model metadata system enables consistency of data and its meaning across applications and business processes (such as PowerApps, Power BI, Dynamics<U+202F> 365, and Azure), which store data in conformance with the Common Data Model. Per the below Common Data Model schema, the entities have grown tremendously since the CDM was first announced back in July of 2016. Recently, Microsoft also announced the release of new Industry Accelerators that extend the Common Data model to support a data schema specific to industries such as Automotive, Banking, Education, Healthcare, Media and Nonprofit.  A specific Industry Accelerator I would like to showcase is the Banking Accelerator which is now generally available via Microsoft’s AppSource. This new accelerator extends the Common Data Model and contains a set of new entities to support a Financial Services Data Model designed for use in Retail and Commercial banking. The Banking Accelerator can be used with Dynamics 365 for Sales or it can also be used with a PowerApps instance. Microsoft has partnered with industry leaders such as Fiserv, VeriPark and Wealth Dynamix to ensure industry standards were followed during the creation of the new Banking Accelerator. Some of the entities included in the model include Bank, Branch, Collateral, Financial Product, Syndicate (Loan) and Opportunity. Workflows that have been released in the accelerator include Customer On-Boarding, Know Your Customer, Certificate of Deposit and Mortgage Application. Available Dashboards include a Retail Relationship Manager Dashboard, Daily Dashboard and a Prospects and Referrals Dashboard. To get started using the Banking Accelerator, it can be installed within an existing Dynamics 365 for Sales instance or within a PowerApps environment if you are not using Dynamics 365. The base solution has no dependency for Dynamics 365; however, there are additional extensions that require Dynamics 365 for Sales including the Commercial Banking, Retail Banking and the extension with enhanced scenarios for the customer experience.  Documentation and sample code for the Banking Accelerator is available in GitHub. This includes instructions for installing the accelerator, a walkthrough of the personas and workflows within the accelerator, sample code for extending the accelerator through the Common Data Service Intelligence Platform, sample code for interacting with the Dynamics 365 API, sample code for building Power Apps Canvas and model-driven UI experiences and sample code for extending the accelerator through Azure ML, Cognitive Services and other Azure related services. Also available is a sample Power BI Desktop file with a model that consumes the Banking Accelerator entities. The file contains reports focused on commercial and retail loan applications, application statuses, an overview of prospects and referrals and an overview of banking products. Here are a few screenshots of the sample reports:   Contact us for assistance working with the new Dynamics 365 Banking Accelerator or for help with your next Power Platform project.  "
"58" "The heart of any retail analytics solution is product data.  Retail and CPG organizations need the ability to model and load complex product granularities, attributes, and relationships in order to fulfill valuable analytics, visualizations, and Artificial Intelligence (AI) capabilities.  These requirements often materialize into the Product dimension in the organizational Data Warehouse.  The Azure Data Services provide a rich and robust set of tools for modeling, loading, and maintaining product data from various data sources.  With such a wide variety of options and a lack of official documentation on best practices, it can be confusing to know which tools and patterns to use to load your Data Warehouse in Azure.  The purpose of this blog post, and the subsequent ones to follow, is to help provide clarity on this topic.  In each post, we’ll review the tools and patterns to load your product dimension. 3 Simplified Patterns Heavily simplified, there are three Data Warehouse load patterns and tool combinations in Azure to perform ETL/ELT.  Each option has its own pros/cons that should be evaluated by organizations to understand the best fit for their use cases, requirements, cost, and staff skillsets.       In today’s blog post, we’ll review the pattern and tools associated with what I am calling In-Memory Transformations with Spark.  The focus on this pattern is to use Azure Databricks to transform the data in the Data Lake into meaningful data and land it into the Data Warehouse and our Product dimension.  The architecture diagram below outlines the tools used in this pattern and the order/flow of the data.  Ingest Data using the Azure Data Factory Copy Activity (ADF) – ADF Copy Activities are used to copy data from source business applications and other sources of data into the RAW area of the Data Lake. Store Data in Azure Data Lake Store Gen2 (ADLS) – ADLS Gen2 is used to store the data for our Data Lake. It combines the best of Azure Storage and ADLS Gen1 to enable the Hadoop Distribute File System (HDFS) as a service.  It is a cheap and robust storage system built for analytics. Transform Data using Azure Databricks – Databricks is used to source data from the Data Lake and enhance/transform the data in-memory before landing it into the Data Warehouse star schema. Model Data in Azure SQL Database (DB) or Azure SQL Data Warehouse (DW) – Azure SQL DB or Azure SQL DW are used to store the data for the Data Warehouse star schema. Both solutions are SQL Server based and provide an easy consumption layer for business/data analysts and dashboard/report writers.  Azure SQL DB is a robust SMP (symmetric multiprocessing) based relational database solution enabling scalability up to single-digit terabyte (TB) sized Data Warehouses.  For larger Data Warehouse solutions (single-digit TBs to triple-digit TBs) Azure SQL DW provides an MPP (massively parallel processing) capability based upon SQL Server.  Serve Data using Azure Analysis Services or Power BI Premium – Azure Analysis Service or Power BI Premium enable a rich semantic layer capability in the architecture. Tables are pre-joined, columns use business vernacular and are organized for ease of consumption, security is embedded into the model, and business calculations are added for analytics.  Consumption of Data using Power BI – Power BI provides a rich experience to Azure Analysis Services or Power BI Premium via Live Connection. Users can create ad-hoc reports, consume reports and dashboards, and even create paginated reports.  Sophisticated authors can even create feature rich reporting solutions that look and feel like a reporting application.   Ingesting Data to the Data Lake Ingesting data into the Data Lake occurs in steps 1 and 2 in our architecture.  Azure Data Factory (ADF) provides an excellent mechanism for loading data from source applications into a Data Lake stored in Azure Data Lake Store Gen2.  In fact, Microsoft offers a template in the ADF Template gallery which provides a metadata driven approach for doing so.  The template comes with a control table example in a SQL Server Database, a data source dataset and a data destination dataset.  More on this template can be found here in the official documentation.  The ADF pipeline starts with a Lookup activity which retrieves the record set from the control table and makes it available to the pipeline.  The control table holds records which have the table name, the query to use for filtering the data in the source system, and the destination folder path for the data in the Data Lake.  The ForEach activity iterates over the GetPartitionList data set.   Within the ForEachPartition activity, two Copy Activities execute for each record of the control table.  The CopySourceToStage Copy Activity runs the SQL query in the control table against the source system.  The results are then loaded to the Staging destination folder location in ADLS which is also provided by the control table.  The CopyStageToRaw Copy Activity copies the data from the Staging folder location and moves the data copy to a RAW location in ADLS that is partitioned by a timestamp. An example of an output directory structure of the Data Lake is below.  We can see that after our ADF pipeline completes, our Data Lake has a directory structure that mimics the following topology: storage container -> subject area -> data lake zone -> source system -> table -> table data.   In this example, the data for our tables are stored in Apache Parquet format (Product.Parquet).  Parquet file format is useful because it provides columnar compression by default and it stores the metadata of the file in the file itself, which can be used by downstream systems.   In the second Copy Activity, we can see that the data is copied from the Staging zone into a Raw zone of the Data Lake, which includes a timestamp of when the ETL was executed.  Subsequent runs will create new folders which will allow the Data Lake to show history in the data.     Loading the Production Dimension with Azure Databricks Pulling data from the Data Lake, transforming that data, and loading it into the Data Warehouse occurs in steps 3 and 4 in our architecture.  Azure Databricks does the bulk of the work for these steps.  Before we dive into our ETL code, it’s important to know why we’re using Databricks in this pattern.  The following list outlines why organizations should use Databricks for their ETL/ELT workloads: Efficient In-Memory Pipelines – Databricks, based upon Apache Spark, is a highly scalable Data Engineering platform that is able to source, transform, and load batch and streaming data using efficient in-memory pipelines that can be scaled out to multiple nodes (MPP).  This post focuses on batch loaded data; read this blog to learn about implementing a streaming pipeline using Databricks.   Connect to Almost Everything – Databricks has connectors for all of the Azure Data Services and can handle structured and unstructured data sources. It can also be used to make connections to relational database management systems (RDBMS) using java database connectivity (JDBC).  Many RDBMS systems are supported natively, like SQL Server, MySQL, and MariaDB.  You can also add your own JDBC driver for RDBMS systems like PostgreSQL, Oracle, or DB2. In-Flight Scalability – Spark clusters can be created in minutes and can be configured to automatically scale up or scale down in-flight and turn off when idle. Popular Language Support – Developers can use a collection of languages to complete their pipelines including SparkSQL, Python, Scala, and R. Multi Use Case Support – Databricks Unified Analytics Platform can not only handle Data Engineering, but also enables AI, Streaming, and Graph processing using the same solution. Pay for What you Need/Use – You only pay for Databricks when a cluster is up and running. Jobs can be created that spin up a cluster, perform ETL, and spin down afterwards to save cost.  The following Databricks Notebook provides a walkthrough/example of how to load a Product dimension table in Azure SQL DW using an Azure Databricks Notebook with code written in Python, SparkSQL, and Scala.  The notebook would be executed from a master Azure Data Factory pipeline using ADF’s native connectivity with Databricks.  At a high level, the notebook does the following: Establishes connections to the Data Lake and the DW. Loads the Data Lake product tables into DataFrames. Uses SparkSQL to form a source query. Loads the DW dimension table to a DataFrame and compares it with the staged data using SparkSQL. Updates existing dimension records if needed. Writes new records directly to the DW dimension. Stay tuned for the next blog post reviewing how Azure enables scalable, code-free ETL using Azure Data Factory Mapping Data Flows.  And as always, if you need help designing and building your Data Warehouse and ETL solution in Azure, contact BlueGranite."
"59" "This post discusses new features released in May 2019: shared datasets, as well as dataset endorsements which include certified and promoted datasets. These features are interrelated and can play an important role in a self-service BI implementation of Power BI. When you have self-service users responsible for report creation activities, allowing those report authors to easily locate trustworthy data is imperative. These new features can make that process easier.     What are Shared Datasets? A shared dataset is a data model which is reused, or shared, across many different reports. The related reports can reside in various workspaces within the Power BI Service. This new ability to span workspaces provides additional flexibility we have not previously had. As of this writing, shared datasets are a preview feature. A shared dataset must reside in an app workspace, thus requiring a Power BI Pro license (or use of Premium capacity for report viewers). The existence of a shared dataset can become more apparent to report authors via the use of endorsements, which are discussed next. What are Dataset Endorsements? Certified and Promoted datasets are considered dataset endorsements. Endorsements are a property of a dataset:  At the time of this writing, there are two badges (other than default): Promoted and Certified. The meaning of a promoted dataset or a certified dataset is not strictly defined, so it is up to you at your organization to determine what the specific definitions are. We recommend that the certification process be formally structured with specific subject matter experts who may certify a dataset. The promoted dataset badge may be less rigid, and is likely to be more user driven. In both cases, alignment with governance processes is very important to avoid the endorsements becoming meaningless, inconsistent, or confusing. Educating dataset authors from overusing the badges is also important. What is the Dataset Discovery Experience? When a report creator is working in Power BI Desktop and searches for a Power BI dataset that exists in the Power BI Service, they are presented with a dialog which represents the dataset discovery experience:  Once a dataset has been assigned a Promoted or Certified endorsement, reuse of this dataset is encouraged because its existence is more prominently displayed at the top of the dataset discovery dialog. Why is it Important to Reuse Existing Datasets? As discussed above, dataset endorsements influence the increased usage of shared datasets. In turn, the use of shared datasets impacts data reusability via the dataset discovery process. The frequent reuse of data by self-service report authors is generally seen as a very positive characteristic of successful self-service BI deployments. Following are some additional considerations:   Utilizing Shared Datasets in Power BI A shared dataset may represent multiple subject areas comprising a larger consolidated data model. A single semantic layer is what's commonly thought of when we start talking about data reusability (which certainly has many benefits – data is inherently more valuable once it's related to other data). However, we don't have to focus only on a consolidated data model in order to gain massive benefits from shared datasets – especially if self-service BI is decentralized throughout the organization. Shared datasets can be utilized very effectively for a consolidated data model, or within a single subject area like accounting. For instance, the following diagram illustrates use of a shared dataset called Accounting Transactions:  In the above diagram, a few noteworthy things are happening: The Accounting Data workspace (top left) is for the purpose of data only. Segregation of the data into its own workspace allows permissions to be defined for data management separate from report management. In this illustration, only a small number of people who are responsible for the accounting data may edit the contents of the Accounting Data workspace. The Corporate Data workspace (top right) is also for the purpose of data only. It will contain data which can be used across the organization, such as Date, Employees, Geography, Organization, and so forth. In this type of workspace, usually a centralized IT or the BI team have permission to edit the data. The two workspaces across the bottom, Accounting Monthly Close and Accounting Analytics, are report-driven workspaces owned by the functional area. Permissions for editing and viewing reports may be set with a focus on report authoring needs. Typically, only reports and dashboards reside in these workspaces, but it would be possible for a dataset to exist in one of these workspaces (not depicted above) if the dataset is highly specific with no potential for reuse. A linked Accounting Transactions dataset, which connects back to the original dataset, is displayed in each of the reporting workspaces. The workspace where the original dataset is located is where data processing will occur (which is particularly relevant when using Premium capacity). Report authors utilize a live connection to the Accounting Transactions dataset in order to take advantage of data reusability. As the name implies, a live connection causes the queries to go back to the original dataset and avoids re-importing the data into yet another dataset when it's not necessary to do so. Dataflows are depicted in the above diagram, but are not a requirement for the use of shared datasets. Since dataflows are another aspect of reusability, they are shown to illustrate what could be done. Dataflows become particularly useful if certain data will be utilized across many datasets. Technical Steps for Taking Advantage of Shared Datasets (A) New Workspace Experience. Shared datasets across workspaces (as depicted in the diagram above) are only supported with the new workspace experience. To be clear, in classic workspaces users can still use the live connection functionality in Power BI Desktop for referring to an existing dataset in Power BI; this technique still decouples reports from the dataset even if the objects all exist in the same workspace. Migration to the new workspace experience permits shared datasets to work across workspaces. (B) Workspace Permissions. The workspace role assigned to each user (admin, member, contributor, viewer) is crucial for ensuring that the desired user experience across workspaces is achieved. Also, the build permission is required for any user who will be connecting to an existing dataset within Power BI Desktop via the live connection functionality. (C) Tenant Setting: Use Datasets Across Workspaces.   Typically, we recommend the \"Use datasets across workspaces\" tenant setting be enabled for the entire organization. However, it can be enabled for selective groups if your implementation is more restrictive by design. See this post for the impact of restricting this functionality. (D) Tenant Setting: Dataset Certification.   We recommend the ability to certify datasets be highly restricted to ensure that certified datasets are indeed trustworthy. Subject matter experts (SMEs) who are permitted to certify content should be handled in conjunction with other data governance guidelines. Those who have the data knowledge, and thus should be permitted to certify datasets, should be a different group of people than the Power BI administrators. (E) Dataset Property for Endorsement.   The endorsement settings, including Promoted and Certified, are associated with each individual dataset. This setting impacts how the dataset is displayed in the dataset discovery process discussed earlier in this post. Final Recommendations for Using Shared Datasets Ensure that the Power BI authors in your organization are aware of the benefits of dataset reuse and how to utilize the live connection to a Power BI dataset when creating a report in Power BI Desktop. Define what a promoted dataset means within your organization, and how it differs from a certified dataset. Ensure that your report creators are very clear on the meaning of each. Integrate these definitions with your governance plan and data catalog when possible. Create a meaningful process for dataset certification which involves both a subject matter expert that knows how to evaluate data accuracy, as well as a Power BI expert who can attest to the quality of the data model. Ensure that when the certified badge is utilized it truly conveys trustworthiness and data quality. Audit your permissions assignments on a regular basis to ensure that security objectives are being met, and that workspaces are being used as intended in order to achieve the best (yet secure) experience for authors and consumers in Power BI. In addition to the considerations mentioned in this post, there are many more aspects of a successful Power BI implementation. Learn more about BlueGranite's Power BI deployment and adoption framework, or Power BI training with our team of experts. "
"60" "In this installment of BlueGranite’s Knowledge Mining Showcase, we are going to learn how to create an Azure Search Service and perform searches using application programming interfaces (APIs). Even though the Azure Portal gives us a reasonable user interface to create and manage our searches, APIs give us more flexibility and can be saved and rerun many times. Moving forward in the Knowledge Mining Showcase, we will primarily be using APIs, rather than the Azure Portal user interface.   Before we begin, you will need a few things, An API development environment. I use Postman, which you can get here. A familiarity with RESTful APIs and how to use them. If you are new to APIs, you can get a good start  and learn about the different API methods here. A familiarity with Azure Search and an understanding of how to create an Azure storage account, and an Azure Search Service. Both of these are covered extensively in the Azure Search post of the Knowledge Mining Showcase. A storage account with data to index and search. Getting Started Details on creating an Azure storage account with blob containers are provided in the Azure Search post of the Knowledge Mining Showcase. We will be working with a common dataset used in Microsoft’s Knowledge Mining Bootcamp, which is also an excellent introduction to Azure Search. You can go to the Bootcamp here. To get the dataset, go to the GitHub repository at https://github.com/Azure/LearnAI-KnowledgeMiningBootcamp.git and clone the repository. You’ll find the sample data in the resources/dataset/ folder.  Upload this data into your blob container if you wish to follow along and get the same results that I get. It’s fine to use your own data if you prefer. Try to use a combination of PDF, Word, Excel, HTML, and graphic files. Once you have your data loaded, you will need to create a Search Service. Again, details on how to do this can be found in the Azure Search post of the Knowledge Mining Showcase. Have your blob container loaded with data and your Search Service created? Let’s move forward. Creating a Dataset We need to give a link to our data to the Search Service. We do this by creating a data source in the Search Service. The format for the API call to create a data source is: POST https://[Search_Service_Name].search.windows.net/datasources?api-version=[api-version] Where [Search_Service_Name] is the name of your Search Service and [api-version] is the version of the API you are using. “2019-05-06” is the most current production version as the writing of this post. I’ve created a Search Service named ‘bgblogsearchservice’, so my call would be: POST https://bgblogsearchservice.search.windows.net/datasources?api-version=2019-05-06 But we’ve got to add information about the blob container we are using for our data source. We do that in the body of the POST call. The body for my call looks like this:  Where:name – The name of your new data sourcedescription – A description of your data sourcetype – Can be one of  the following: azureblob, azuretable, azuresql, or documentdbcredentials – Contains the connection string to your storage accountcontainer – The name of the container where your blobs are stored You can get the connection string to your storage account by opening up your storage account, selecting Access Keys under Settings and then copying the key1 Connection string.  One last thing – to complete our Create Data source API call, we’ve got to add the headers to our call. We need to add the Content-Type and the api-key.  You can get the API Key from your Search Service. Select your Search Service and then select Keys under Settings and copy the Primary admin key.  Once you copy the Search Service key into the header of your API call, you should be all set. Here’s what my call looks like in Postman:  Just hit Send and you should see a result something like this:  If you don’t trust the API response, you can go to your Search Service in the Azure Portal and verify that that data source has been created.  In case you ever need it, you can delete your data source via an API call using the following format: DELETE https://[Search_Service_Name].search.windows.net/datasources/[Data_Source_Name]?api-version=[api-version] Creating an Index Think of the Index as the definition of the data in our data source; how it will be searched, and how results will be returned. The format for the API call to create an Index is: POST https://[Search_Service_Name].search.windows.net/indexes?api-version=[api-version] Where [Search_Service_Name] is the name of your Search Service, [Index_Name] is the name of your Index, and [api-version] is the version of the API you are using. “2019-05-06” is the most current production version as the writing of this post. Using my Search Service named ‘bgblogsearchservice’ and an Index named ‘bgblogindex’, my call would be: POST https://bgblogsearchservice.search.windows.net/indexes?api-version=2019-05-06 Note that you can update an existing Index by using the following PUT format. PUT will create the given Index if it does not already exist: PUT https://[Search_Service_Name].search.windows.net/indexes/[Index_Name]?api-version=[api-version] The body for our Index POST is a bit longer than that of the Data source POST, but it’s not too bad. We need to name our Index and define each field that will be in our Index. The format of each field definition will be:  This is an example subset of commonly used field definitions; there are many more. For a full reference of available Index field definitions, see here. A pretty simple and standard definition of an Index may look like this:  We can use the above JSON as our Create Index POST. Don’t forget to add the Headers with the Content-Type and the api-key values. Here’s what mine looks like in Postman:  Hit the Send button and you should get a result like this:  We know that with this response, the Index was created, but we can verify that by going to our Search Service on the Azure Portal and finding the Index:  In case you ever need it, you can delete your Index via an API call using the following format: DELETE https://[Search_Service_Name].search.windows.net/indexes/[Index_Name]?api-version=[api-version] Creating an Indexer The Indexer is utility that moves the raw data from your data source into your Index. The format for the API call to create an Indexer is: POST https://[Search_Service_Name].search.windows.net/indexers?api-version=[api-version] Where [Search_Service_Name] is the name of your Search Service and [api-version] is the version of the API you are using. “2019-05-06” is the most current production version as the writing of this post. Using my Search Service named ‘bgblogsearchservice’ and an Indexer named ‘bgblogindexer’, my call would be: POST https://bgblogsearchservice.search.windows.net/indexers?api-version=2019-05-06 Note that you can update an existing Indexer by using the following PUT format: PUT https://[Search_Service_Name].search.windows.net/indexers/[Indexer_Name]?api-version=[api-version] The body for our Indexer POST is basically a mapping from our raw data to the Index. We only need to do this if we want the name of the field in the Index to be different than the raw data. We’ll also need to use the Indexer to perform mapping functions. Here is a basic Indexer body for our example:  We need to name our Indexer (“name”) and then identify our Data source (“dataSourceName”) and our Index (“targetIndexName”). The rest is field mapping. Remember in our Index definition we had a field named “metadata_storage_path”? This is a metadata field that Search gives us automatically. Common practice is to make this the unique identifier in our index, as it is guaranteed to be unique. Since “metadata_storage_path” is an odd name for an ID field, let’s rename it to “id”. We do this with the field mapping above. We also want to encode it into a number rather than a text field. We do that with the “mappingFunction” section and the “base64code” encoding function provided to us by Search. Let’s go back to our Index definition:  Since we’re mapping the “metadata_storage_path” field to “id” with our Indexer, we need to update our Index, or we will get an error that the “id” field does not exist on the target Index when we run our Indexer. We must DELETE our existing Index and then recreate it with a POST call. You cannot change field names on an existing Index with a PUT call. Using the Index created earlier, we make the following API call to delete it: DELETE https://bgblogsearchservice.search.windows.net/indexes/bgblogindex?api-version=2019-05-06 We can then make the same POST call we made earlier to create the Index, but replace the name “metadata_storage_path” with “id” in the body of the call. This is what my call looks like in Postman:  Now we are set to run our Indexer using the mapping we defined earlier. Don’t forget to add the api-key and the Content-Type to the header. This is what my call looked like in Postman:  Not only does the create Indexer call create the Indexer, it also runs it. This can take some time. You can check the status of the Indexer from the Azure Portal Search Service Overview page, or you can make the following API Call: GET https://[Search_Service_Name].search.windows.net/indexers/[Indexer_Name]/status?api-version=[api-version]  Once the “status” comes back as “success”, your Indexer has finished. Performing A Search We now have our Index populated with the data from our data source using the rules we defined in our Indexer. The Index is now ready to Search! The format of our API call to perform a search is: GET https://[Search_Service_Name].search.windows.net/indexes/[Index_Name]/docs?[query paramaters]?api-version=[api-version] There are a lot of potential [query parameters] we can use for our search. You can find a full list here. For this exercise we’re only going to use a few: search=[string] $count=[true|false] “search” is the term that we will be searching for and “$count=true” will give us back the count of the records found with our search term. If we use this API call: https://[Search_Service_Name].search.windows.net/indexes/bgblogindex/docs?search=microsoft&$count=true&api-version=2019-05-06 If you used the files from the Knowledge Mining Bootcamp, you should return 10 records back from your Search and it should look something like this:  Congratulations! You’ve now created a data source, index, and indexer, as well as performed an Azure Search, all using APIs. More to Come In the coming weeks I’ll be exploring the many ways to use Azure Cognitive Search to more easily unearth knowledge from once-difficult-to-mine data sources. Be sure to subscribe to our blog so that you don't miss a tutorial, or contact us today to discover the many ways BlueGranite can make the most of your data!   Reference and Resources Indexer Overview API Call Reference Create a Data Source Delete a Data Source Create an Index Delete an Index Create an Indexer Delete an Indexer Run an Indexer Reset an Indexer Perform a Search"
"61" "A typical pattern in retail scenarios involving multiple sales locations is that each periodically uploads its respective transaction summary information to a central entity (e.g., corporate headquarters) where total sales can be aggregated and analyzed. For our purposes, let’s assume that these sales locations each submit daily summary files detailing every sales transaction that occurred throughout the day. Let’s further assume that these locations exist in multiple time zones, with varying hours of operation, and that these summary files, possibly also requiring some human action to be performed before being finalized, will be sent at different times.   For data processing purposes, the process above can be imagined as a streaming scenario, which is easily implemented in Spark on Azure Databricks. But practically speaking, it is common for uploads of this nature to fail, arrive late, or even to be resubmitted with additions or corrections, thereby introducing redundancy that requires reconciliation. We therefore require a streaming destination that is robust enough to accommodate the dynamic nature of the business process. Fortunately, Delta Lake is perfectly suited for this situation. And, in a scenario that routinely comingles both inserts and updates, what we really want is a “merge” pattern, also called an “upsert.” In our example, files are being uploaded via an external process to a date-specific folder in Azure Data Lake and are distinguished from one another by a naming convention which consists of the Date and Time and StoreId (e.g., 201906210916_1000.json). But for our purposes, we will never look at nor query individual files. (Note that discussion of a robust data lake partitioning strategy is beyond the scope of this article. For further reading about data lake best practices, see BlueGranite’s eBook, “Data Lakes in a Modern Data Architecture”.) Given sample file content such as the following, in JSON format, we see that it includes a store identifier, date, and an array of transactions. (Note that multi-line JSON is discouraged for performance reasons, but is intentionally used here as a more likely reflection of a real-word scenario.)  We start by defining the schema of our sales document.  Then we create a streaming dataframe over the folder(s) into which our sales summary files are being uploaded. In this step we also explode sales transactions onto rows, which will help later with sales aggregation. We also derive a date key, which will help both with aggregation, and in construction of our merge statement to handle resubmissions.  Next, we need to create a Delta table as a destination.  Ideally, we want to stream directly to the Delta table, as opposed to performing a batch upsert. At this time, such an operation requires Scala (more about this here).  Meanwhile, the MERGE statement itself should be familiar to anyone who has prior experience working with merges in SQL.  Finally, we query our Delta Lake table and review the aggregated sales totals for three stores.  Later, we find that our first store has resubmitted its daily sales summary, although the file that was previously submitted remains unmodified and is neither overwritten nor removed. In addition, a fourth store has now submitted its summary file.  The revised sales summary for store 1000 contains an additional transaction, as well as a change to an existing transaction.  But observe that no further effort is required to process or retrieve the results of these updates. We simply execute the exact same query as before, but now we receive the updated results. Our streaming merge has seamlessly and transparently reconciled the data changes!  The key takeaway here is that once our process is in place, thanks to Delta Lake, no further action is required to update or reconcile the incoming data. If you have questions or are interested in more Delta Lake information, contact BlueGranite. We’d love to help steer your project in the right direction!"
"62" "The popularity of Data Visualization – translating complex data into graphics to ease understanding – has exploded over the past few years. The tools now at our disposal offer more advanced visualization capabilities than traditional reporting tools ever did; pie charts, bar charts, line charts, etc., are often quite static. With organizations discovering the powerful benefits of using advanced analytics to harness the recent boom in data, today’s business intelligence (BI) practitioners are expected to have a thorough grasp of data visualization theoretical foundations and best practices.  Data scientists, the pioneers of advanced analytics, brought clear Data Visualization theories and methodologies to business. They opened the doors of discovery to Data Visualization trailblazers Edward Tufte, Stephen Few, and “The Grammar of Graphics” –  the essential primer on the fundamentals of statistics and visuals –   to the business intelligence crowd. Today data scientists and business intelligence practitioners are speaking the same language, but their roles are often quite different. Both advanced analytics and business intelligence are part of the larger field of visual communication. Understanding the subtle differences between the two can make good dashboards and reports great. The Communication Model Let’s use a simplified version of the influential 1949 Shannon – Weaver communication model to try to understand the nuances between advanced analytics and business intelligence: A sender communicates a message to a group of people or an individual (the receiver) through a specific channel. The message might be altered by filters, either on the sender’s or the receiver’s side.  Figure 1 - Simplified version of the Shannon – Weaver model[1] Before we proceed any further, I would like to address the main critique of this model; it lacks a feedback mechanism from the receiver to the sender (a conversational mode). While true, this is not critical in our case; the amount of feedback is limited for business intelligence projects. However, it is worth mentioning that gathering usage statistics and feedback is usually part of the maintenance of a project. Why Create Data Visualization? “Which story do you want to tell?” It’s a common question when building Data Visualizations. Advanced analytics projects and business intelligence dashboards tell stories at different points in time. In an advanced analytics project, the data scientist analyzes the past to try to predict the future, or to highlight complex relationships not obvious to an untrained eye. These discoveries hopefully lead to business strategy or process improvements. The data scientist guides the audience though the reasoning – the “story” being told – using graphs and tables in a logical manner. This type of analysis is fixed in time. It is valid when the results are presented. On the other hand, a business intelligence dashboard is designed to be used in the future; the data that will be presented doesn’t exist at the moment the visualizations are created. The dashboard can tell different stories depending on when it is used. Time plays a much different role in this process. If you have any doubt, just imagine a major bank’s January 2007 financial reports and those exact same reports in January 2009. If, in 2007, you already knew the story that would rock the financial markets in the fall of 2009, you would be a billionaire by now. Corporate Strategy Key to Great Dashboards Because a dashboard tells many stories, and we cannot know those stories when we create the dashboard, we should understand a company’s strategy before getting started; then we must figure out how the dashboard can help execute that strategy. This company strategy – the first crucial piece of information – should be known before creating any visualization. Think of a dashboard like the CEO or CFO directing the organization: “This is what you should focus on. These are your objectives.” The choice of key performance indicators (KPIs) and metrics displayed should be inferred from the company’s strategy, indirectly or directly. For example, if the strategy is to increase margins, then one KPI should highlight cost reduction, or price increase, to improve upselling. If the strategy is market penetration, the KPI should feature quantity sold, customer loyalty, or marketing budgets. The choice of KPIs or metrics, their layout, and the time window is as important, maybe more important, than the actual numbers. You don’t even need data to pass this message; it will be identical regardless of the underlying data, and for the whole life of your visualization. A dashboard is a management tool used to communicate strategic or tactical decisions to the appropriate people in the organization. It should be adjusted as the strategy evolves. The correct visualization will highlight whether corporate strategy is being applied and if it is working. And the right mix of information – lag versus lead indicators, a proper time horizon, and relevant contextual information – will make the difference between a dashboard that is attractive but useless, or one that provides a strategic advantage over the competition. The table below shows the different goals usually associated with communication; why people choose to communicate in an advanced analytics context versus a business intelligence context.  Table 2 - Goals of communication for Advanced Analytics and Business Intelligence These goals are quite different, and this will have a direct influence on how the message is crafted. On one hand, we have to motivate, direct, and control, on the other hand, to persuade, educate and inform. This is the first crucial difference; the message will always be fundamentally different as its goals are different. The Sender Data scientists usually present their own work; they have a thorough knowledge of their data and of the insights they want to communicate. Their main challenge is to communicate these insights thoughtfully and clearly. In the case of a business intelligence dashboard or scorecard, the sender of the message and Data Visualization creator are often two different people: The creator: a report developer who creates the dashboard. The sender: the business owner of the report who provides the message. A report developer (usually a BI professional or a data analyst) doesn’t have anything to communicate. To an extent, they are part of the channel (as defined in our communication model). They are the business owner’s spokesperson, a translator of verbal communication to visual communication. Identifying the real sender while working on a dashboarding/reporting project is not always straightforward, but it is pre-requisite; his or her position in the organization will give precious information about the intended message. When you know who the real sender of the message is, you are more likely to understand their intentions. In large corporations, this can be a real challenge; however this is integral to the success of the dashboard. As a side note, this is one key advantage of self-service BI. The real sender of a message creates their own visualization (message) and doesn’t have to rely on an intermediary. Even if the Data Visualization is imperfect, it provides a BI professional or data analyst important insights regarding the message the sender is trying to communicate. On the other hand, the data scientist and the sender of the message are the same person. This makes crafting the message much easier, there is no translation involved. Also, as the data scientist already spent some time analyzing the data and tuning their models, they have already have a good idea of the best visuals to communicate their message. This is another point where there are some important disparities. While the data scientist has the latitude to decide the message, tone, and overall feel of his Data Visualization, the BI professional is merely interpreting the dashboard owner’s wishes. The Receiver In an advanced analytics project, the receiver is usually a group of managers, executives or engineers. Some, if not all, will be project stakeholders. You can expect them to have a decent level of understanding of visual communication.  Also, they are often aware of the goal of the project. On the other hand, the target audience for a BI visualization can be anyone in the organization, very often managers and executives, but also salespeople, factory supervisors, and others. You can expect a wider range of personalities and backgrounds. Finally, the receivers are not always included in the project, and they might even disagree with it. When creating a sales dashboard, there are often disagreements on the best way to count sales. Based on quantity? Based on revenue? When is the sale really counted? Does it count for the Q1 objectives? For the Q2 objectives? Sales are often paired with objectives, and, consequently, factor into salespeople’s’ salaries. The design can impact the receivers’ attitude, not only toward the project but toward their daily job. Also, as most employees are not part of the project, project designers can’t rely on providing context or explanations, as a data scientist would, when presenting results. The message should be crafted to maximize its impact on the audience, and as such we should use the proper vocabulary (think about the type of graph you will use; e.g., line chart versus Sankey) with the proper phrasing (level of granularity, use of corporate lingo, density and complexity of the information displayed). For example, in building production dashboards for both engineers and mechanics, regarding the same business process, we must adjust our message so it is easily understood by everyone. Filters Whether on the sender side or on the receiver side, filters can be numerous. At a minimum, consider the following points: Corporate culture, openness to innovation Maturity of their BI and advanced analytics initiatives Education level of the target audience, ability to understand complex visualization, statistical concepts Knowledge of visual communication Think of a maintenance dashboard for a manufacturing company versus a finance dashboard for the pharmaceutical industry. Their Data Visualization literacy might be very different. As a dashboard creator, you should incorporate these considerations when creating the visualization. A good dashboard is one that conveys its message quickly and efficiently, and communicates in the language of its audience. It is worth noting that BI professionals come to each project with their own set of preconceived notions, expertise, and knowledge gaps. Those are also filters; it is worthwhile to take some time to contemplate our understanding, or lack thereof, when approaching communication with a client or department.  Business intelligence and advanced analytics both make intensive use of Data Visualization. However, the use of visual communication is different in key aspects. Though the vocabulary (e.g., type of graphs: histogram, pie chart, violin plot, line chart, etc.) is identical and follows the same rules, and the grammar is identical (e.g., which graph to use and when, proper layout, colors, data-to-ink ratio), the similarity stops there. We should not forget that we need a message to convey, and we need to convey it well; otherwise it’s only pretty graphics. Good communication, in this context, means that we should not forget fundamental requirements gathering: why, from who, to whom and when. Data Visualization is the proper language of visual communication; it is expected that both the BI and advanced analytics professional be fluent in it. While the two professions share words and a common language; the best visuals acknowledge the subtle differences. [1] https://en.wikipedia.org/wiki/Shannon–Weaver_model"
"63" "BlueGranite’s tailored analytics solutions drive major business impact for clients. The key to our success is our deep bench of trusted technology experts – each with their own arena of expertise. As part of our Meet our Team blog series, today we’re introducing Angela Henry: a data solutions architect with over 20 years of experience in software development, database administration, and programming.    Angela is a Solutions Architect at BlueGranite. Her role relies heavily on listening to clients about what can make their job easier, followed by suggesting solutions that will meet their needs, and helping companies make better business decisions.  Angela received her bachelors degree in Computational Mathematics from the University of Colorado. She has achieved Microsoft certifications in Data Management and Analytics, SQL Server, and Business Intelligence, and is a Microsoft Certified Professional. She began her career as a Software Developer, before moving into a database administration (DBA) role. She worked as a database administrator for over 10 years before transitioning into the business intelligence space.  \"I have worked in small shops where I had to wear all the hats, but I've also worked in very large shops where work duties were extremely segregated. That experience, combined with the different vertical markets I've worked in, gives me a very rounded background and allows me to pull from a very large toolbox to architect solutions.\" When asked what she enjoys most about working at BlueGranite, Angela emphasized the teamwork and partnership within the company.   “The collaboration among employees at BlueGranite is truly amazing.  This is one of the biggest reasons that working for BlueGranite appealed to me.  In my last few roles, I was the technical lead and didn’t really have anyone to bounce ideas off of.  Even though we are a geographically diverse team, I know that I am just a chat away from getting feedback when I’m stumped.” Overall, Angela's favorite kind of project work involves helping companies figure out what they have in terms of amassed data, and how they can make better business decisions by using that data in productive ways.  Angela grew up continuously moving around the United States, as her dad completed school. She went to five different elementary schools in four different states, and two high schools in two different states. The frequency of moving and starting fresh, taught Angela to easily adapt to new situations, she said. She believes in hard work and honesty, and was brought up with a strong work ethic by parents that both worked out of the home.  \"I believe that there is nothing you can’t achieve with hard work. It may take years to achieve goals, which means you have to break them down into smaller chunks so you can experience smaller successes that lead to your ultimate goal.  This translates very well to IT projects; you have to have an overall goal with smaller goals along the way to show your progress.\" When Angela isn't working she enjoys sewing and athletic pursuits. Swimming is a major passion for Angela. She is a part-time swim instructor for both children and adults, outside of coaching the swim team at her local swim club. Want to know more about BlueGranite and the rest of our team? Contact us! "
"64" "For organizations developing Power BI reports, there is typically a strong desire to design them in a way that provides the best user experience. This includes not only providing excellent data visualization to meet reporting needs, but also designing reports for optimal performance. Too often the burden of performance optimization is placed solely on IT or Power BI administrators when it should be a function of anyone using the tool. Performance optimization may seem daunting, especially to a self-service user or someone new to report development, but there are plenty of actions to improve report performance without needing to be a seasoned professional.  In this blog post I’m going to walk through some practices that your organization can implement now, right in Power BI Desktop, to optimize report performance no matter what level of development experience your team has. Specifically, this post will discuss tactics in two areas where performance can easily be improved: report and visual design, and data transformation in the Power Query Editor.  Report and Visual Design First, let’s focus on report design and visualizations. What many Power BI users don’t realize is that every visualization displaying data issues a query to populate that data before rendering. Visuals like text boxes and images don’t issue a query, but they do take time to render on the page. All of this can add up quickly as more gets added to a page. The best approach to design is to only include smart, thoughtful visuals on a report page, striking a balance between giving users satisfactory insight and overcrowding the page. If a page is performing slowly and is loaded with visuals, find places where you can eliminate or combine visuals, or move them to a different page or report. Below, I’m going to show how I reduced visuals in a report without sacrificing insight, thus improving performance. As always, I love wordplay, so my “Hospital Performance and Patient Satisfaction” report is the one I chose to demonstrate Power BI performance optimization. The original pages have “Original” in the title, and the optimized pages have “Optimized” in the title.    The difference between the first two pages is subtle. The metrics displayed in \"Overview Original\" along the top are all separate cards sitting inside a rectangle shape. As a result, I have three visuals issuing a query and four that take time to render.  On my optimized page, I’ve taken these separate cards and combined them into a multi-row card. I formatted that card to closely match the visual effect of the single cards, and I turned on the border to eliminate the need for using a shape as a container. Thus, I have taken four visuals and made them one. Combining single cards into a multi-row card and eliminating shapes by making use of borders or custom page backgrounds are quick ways to reduce rendering time. This trick may not always produce the exact same visual effect, but in an instance where performance is concerned, it’s close enough.  The Patient Satisfaction Analysis page has more going on. As you can see in the original, this page contains a lot of slicers, because users need to be able to analyze patient satisfaction in a variety of ways.  Slicers are the biggest offenders for negatively affecting report page performance. With the new filter experience available in preview, consider using filters instead of slicers to give users the same functionality.  Filters do not issue a query until the users interact with them, whereas slicers issue a query before rendering. In my optimized page, I moved some of the slicers to the filter pane and I formatted the filter pane to be a very light blue, so it stands out just enough to make users aware of it.  The filter pane is not available in a report published publicly to the web, so you cannot interact with the filter pane in the embedded version in this post. However, as you can see in the screenshot, users accessing the report from my organization in Power BI Service can use it. Using filters not only reduces load time, but also frees up precious canvas space on the report page. You’ll notice, too, that I kept three of my slicers. I chose this design because my users like the experience of using the slider bar for these metrics and these are the slicers that users always interact with when viewing the page. The report is performing optimally, so unless it becomes an issue, I will keep these slicers as they are. This is how I struck a balance between performance and visual design, giving my users the best experience possible. Test Performance in Power BI Desktop I used the Performance Analyzer within Desktop to test how the simple visual changes I made on my optimized pages reduced load time. The Performance Analyzer records and displays how long it takes each visual to render on the page and breaks that time out into categories. You can learn more about how to use the Performance Analyzer and interpret its results from Microsoft's documentation.  In the instance of the \"Overview\" pages, the difference in performance was only about 100 milliseconds. On the ‘Patient Satisfaction Analysis’ pages, where I moved most of my slicers to the filter pane, the difference was anywhere from 300-500 milliseconds throughout multiple test scenarios. Keep in mind that in my example, none of the pages take terribly long to render because I’m not working with a large amount of data and my model behind the scenes is optimized for Power BI. The changes I made did save time, and in a report that is suffering, potentially with a larger amount of data, this makes a world of difference.   Power Query Editor and Data Transformation Next, let’s talk about Power Query Editor and data transformation. Power Query Editor is one of the most valuable aspects of Power BI. It lets users perform complex data transformation and preparation in a simple interface, without needing to know a query language or use an additional tool. While Power Query makes data transformation easy, it makes tanking performance easy, too. It’s easy to begin clicking around, transforming data, and end up with an Applied Steps trail a mile long accompanied by a report that renders at a glacial pace. Fear not — I’m going to explain a few aspects of Power Query Editor and present two simple practices that will help you avoid getting into a situation like that. First and foremost, do transformation steps in the right order. Yes, there is a right order, even if any order may produce the desired data transformation. It’s best to go in with a clear plan of what you need to do to the data before you start transforming it.   For example, if you know you only need a portion of the data from your source, filter it down at the first step. Working with only the necessary set of data during the rest of your transformations will not only speed things up, but there are also certain scenarios where Power Query Editor can push data back to the source, using  the power of a relational source like SQL Server database to do transforms. This is a process called Query Folding, and while it may sound complex, all you really need to understand is that sometimes certain data transforms can be done more quickly in an environment like SQL Server, and when it can, Power Query will send those transforms to be performed at the data source. If you do steps like filtering data up front, Power Query will recognize when this can be pushed back to the source, improving report performance.  To find out if a step has been included in Query Folding, you can right-click on it. If the option to ‘View Native Query’ is available (not grayed out) that step has been folded. You can see that the ‘Filtered Rows’ step was selected for Query Folding to improve performance. Some examples of transforms that can sometimes be included in Query Folding are filtering rows, changing data types, and concatenating fields. It’s best to get in the habit of doing these transforms first. Regardless of whether Query Folding is available, it is always best to filter data your data up front, thus working with a smaller set of data for the remainder of transformations. In keeping with doing transformations in the correct order, do similar steps together when you can. For example, if you need to change data types and rename columns, do these transforms all together. Change all the data types you need to, then move on to renaming. If you change a data type, perform some other transform function, then go back to changing data types, there will be two steps for data type change, rather than one.     The more steps present in Power Query Editor, the longer data will take to return on the report page, so pay attention to (and avoid) scenarios like the above. If you simply forgot to change a data type while you were doing it initially, remember that within the \"Applied Steps\" pane you can select a previous step to edit or add to it.   Be cognizant that adding or editing a step this will affect the steps below it, and proceed with caution. To sum up how to begin optimizing performance in Power Query Editor: do similar data transformation steps together and always filter your data first.   Performance in Power BI can be complex – there are a multitude of factors that go into it and a multitude of areas to investigate when a report is performing slowly. This post is not meant to be a comprehensive outline of how to navigate these challenges. Its purpose is to present simple design techniques that any Power BI report developer can employ to prevent some performance issues. Even with these design tips, you may find yourself with a report that is slow to render. If that’s the case, it’s time to investigate other areas and it may be time to pull in additional help to do so, such as this helpful post from our BlueGranite team on data modeling and the importance of structure in analytics. Adopting the basic design principles outlined in this post as habits will go a long way in not only optimizing performance, but also helping your organization build reliable reports that users enjoy. The ultimate goal is broad user adoption of Power BI across your enterprise, which can help you reap the rewards of fast and productive access to your organization's critical asset - data! If you have additional concerns about Power BI performance or managing your Power BI deployment in general, or need additional training for your team, we can help. Learn more about our Power BI Performance Assessment or Power BI Training with our expert team.  "
"65" "Pandas is an open-source Python package that provides users with high-performing and flexible data structures. These structures are designed to make analyzing relational or labeled data both easy and intuitive. Pandas is one of the most popular and quintessential tools leveraged by data scientists when developing a machine learning model. The most crucial step in the machine learning process is not simply fitting a model to a given data set. Most of the model development process takes place in the pre-processing and data exploration phase. An accurate model requires good predictors and, in order to acquire them, the user must understand the raw data. Through Pandas’ numerous data wrangling and analysis tools, this important step can easily be achieved. Databricks provides users with the capability of running the Pandas packages within a notebook. Once a spark data frame is converted to a Pandas data frame all Pandas’ functions can easily be applied using the same code as written in a standard Python IDE such as Spyder or a Jupyter Notebook. While converting a spark data frame to a Pandas data frame does result in the loss of performing on a distributed environment, there are several advantages for data scientists in being able to use Pandas over PySpark. The ability to use the Pandas package in Databricks allows data scientists the ability to leverage Databricks specific features without having to learn the new code associated with PySpark. Also, once all necessary Pandas functions are applied to a data frame, it can easily be converted back into a spark data frame to allow for more distributed storage or spark functions. This ability to seamlessly transition between Pandas and Spark data frames aids in making big data analysis accessible to a wider range of data scientists. The goal of this blog is to highlight some of the central and most commonly used tools in Pandas while illustrating their significance in model development. The analysis will be executed in a Databricks notebook operating on a Python 3 cluster to illustrate the high level of compatibility with standard Python code a data scientist would expect to see in other IDEs. The data set used for this demo consists of a supermarket chain’s sales across multiple stores in a variety of cities. The sales data is broken down by items within the stores. The goal is to predict a certain item’s sale.  Reading the Data When starting a new Python script, modules required for the analysis, including Pandas, must be imported into the environment:    For this example, the Store Sales data has already been loaded into a spark table. It can be accessed by running the following command: Since the data in the notebook is now loaded in a spark data frame it must be converted into a Pandas data frame to perform code specific Pandas functions. Once this conversion takes place all Pandas functions can be performed on the data frame as in any other IDE:    To start, there are a variety of viewing and inspecting tools available in order to achieve a better understanding of the raw data. The df.head(n) and df.tail(n) functions allow users to examine the first and last rows respectively:    The df.shape and df.info() provide information about the number of rows and columns in a data frame, the data types, and missing data:  One key difference in using Pandas within Databricks is ensuring the data types are appropriate after conversion. In this instance, all variables are returned as objects which will result in calculation errors. To correct this issue, the numerical variable must be cast as floats:  Basic statistical values, such as the minimum and maximum of a column, can be calculated by adding the appropriate function to the end of the df.column_name in parentheses:  Data Cleaning and Feature Engineering After getting a general overview and understanding of the data, the next step toward successful model development is cleaning the data and creating new, possibly more influential variables from the existing raw data. The variable Item_Identifier follows a labeling pattern of letters per each product (i.e., ‘FD’ for food, ‘DR’ for drinks and ‘NC’ for n) followed by a three-digit code. It may be more useful to have a group type variable with just these two letters rather than the entire code. To achieve this, the .map() function applies a selection of only the first two values in the item identifier and returns as a new column labeled Item_Group_Type.    In order to analyze the outlet establishment year as a numerical variable, a new column entitled Outlet_Age can be calculated by subtracting the outlet’s year by the the max year value of the dataset plus one (assuming this data’s collection ended the prior year). The Outlet_Establishment_Year variable can then be dropped using the Pandas df.drop() function.     Since most machine learning models in Python will return errors if null values exist within the data, identifying their existence and rectifying the issue is a crucial step. The code below counts the number of null values for each column in the store sales data frame:  There are a variety of methods to address null values within a data set. Some common approaches include simply removing the rows containing the null values, forward or backward filling of values for timeseries data, or replacing the null value with a calculated value. This data set contains a significant number of null values. Removing the entire subject could lead to a lack of complete data, therefore filling in the missing values is a more appropriate method. Since the Item_Weight variable is numeric, replacing a null value with the item’s average weight is a logical approach. The first line of the code uses Pandas df.groupby() function combined with the .agg function to find the mean weight of each unique item and store the results in another Pandas data frame. In lines two and three, setting the index of the original data frame to match the index of the new Item_Identifier_Mean data frame allows the null values to be easily imputed with their matching mean values. It is then necessary to check to see if this method resolved all the null values. Line 5 of the code reveals that four rows still contain null values.      To investigate the cause of this persisting issue, the new mean imputed data frame is merged on Item_Idenitifier with the original data frame using Pandas df.merge(). The merge reveals that the items with null values in the new data frame only appeared once in the original data and had no weight information, therefore a mean could not be calculated.    This same method is then applied to the Outlet_Size variable, except the mode of the outlet type is used as the imputed value since Outlet_Size is categorical. To calculate the mode, the outlet types are converted into a Pandas structure called a series, then the mode is applied to each of these series using the .apply function. From there, a similar merge as before can take place.  However, since the merge is on a column name instead of the index, both Outlet_Size columns are retained in the new dataset with “_x” and “_y” appended to the names. Only one of these columns is needed, therefore renaming to remove the “_x” and dropping the Outlet_Size_y column is conducted in line 4.    Examining the new cleaned data shows that Outlet_Type no longer contains null values.    Returning to the null Item_Weight values, since there were four items with only one record each, these values can simply be dropped. Checking the final cleaned data set reveals that all null values have been corrected:    Dividing the features into categorical and numerical sets allows for further examination of any other possible incongruities in the data. Using the df.select_dtypes() function, two data frames are created containing only the specified data types:    Through the use of a for loop and the .unique() function, the number of unique values for each categorical feature and the label can be displayed. As is fairly common with categorical variables recorded from different sources, the labeling technique of Item_Fat_Content is inconsistent. “Low Fat” is represented as both “low fat” and “LF” while “Regular” is also recorded as “reg”. This type of discrepancy will cause issues when creating dummy variables.    To fix the fat content labeling issue, the .replace() function applies a new uniform naming schema to the Item_Fat_Content variable:    Data Exploration With the data cleaning and feature engineering completed, an even closer examination of the data and its relations can be conducted using Pandas in conjunction with the Matplotlib  library. The histogram below shows that the target variable, Item_Outlet_Sales, is right skewed:    Applying the .corr() function to the numerical features data frame created earlier, provides a symmetrical data frame of the variables’ correlations to each other. Plotting a heatmap of this correlation data frame highlights that Item_MRP has the strongest, positive correlation with the target variable while Item_Visibility has a negative correlation.   Using the .pairplot() function allows for visualization of relationships among the all the numeric variables at once:    The categorical variables’ relationships with the target variable can also be examined through the use of df.pivot_table(). This function operates like pivot tables in Excel by creating an index and applying an aggregation function over a specified value. Plotting these pivot tables then allows for easy trend visualization. From the Impact of Item Type chart, it appears that there are too many unique groups for a significant difference in sales to exist.  The Impact of Outlet Size chart reveals that medium-sized stores have, on average, more sales.       This data analysis and exploration will aid in guiding feature, model, and parameter selection during the model development phase. Finally, the creation of dummy variables is required in order to use the categorical variables in the modeling process. Pandas has an easy to use function, pd.get_dummies(), that converts each of the specified columns into binary variables based on their unique values. For instance, the Outlet_Size variable is now decomposed into three separate variables: Outlet_Size_High, Outlet_Size_Medium, Outlet_Size_Small.    Model Development With pre-processing, cleaning, and data exploration complete, the final phase of modeling can now take place. Sklearn is a commonly used machine learning library in Python that contains multiple modeling and evaluation tools. The first step is to enable the train_test_split() function of this package to divide the cleaned data frame into two separate data frames. The larger data frame, which will represent 85% of the entire data, will be used to train the model, while the remaining 15% will be used to evaluate and determine whether the model is appropriate.  Next, the train and test data frames are each divided into two separate data frames, one containing the desired predictors and the other containing the target variable:    Since the target variable is continuous, a simple, yet standard approach is to test a linear regression model. Once imported from the sklearn package, the function is applied to the train data using the model.fit() function. The predictions are then stored in an array using model.predict(). Model evaluation is conducted by using a variety of the metric functions from sklearn, along with plotting the actual vs. predicted values. From these results, it appears linear regression may not be the best model for this data.    The next step is to try a different model to see if it produces better results. XGBoost is an implementation of gradient boosted decision trees. It is a commonly applied model that is designed for speed and high performance. In order to use the XGBoost library in Databricks, it must be imported on the cluster running the notebook. This can be achieved by clicking on the desired cluster then under the libraries tab install new. The library source is PyPI and will appear as follows once installed:    As in the linear regression modeling, once the xgboost tool is installed and imported, it is applied to the train data and predictions are stored to evaluate the model. The metrics and plot both reveal higher performance than the linear regression model:    This model then needs to be evaluated against the test data to determine if it is, in fact, a good model. The test data reveals that there is still some relevance to the model, but further parameter tuning and possibly other model selections may lead to better results.  Overall, as evident by this demo, the actual fitting and tuning of a model is a small step compared to the entire machine learning process. In order to even obtain a data set for modeling, pre-preprocessing, data cleaning, feature engineering and exploration are all required. These stages are simplified and easily executable using the Pandas package. Pandas enables data scientists to not only understand and analyze the data themselves but also display the results of their analysis and modeling to their audience, making this library imperative for successful machine learning in Python. Databricks ability to seamlessly run Pandas code without having to learn PySpark provides data scientists with an even greater array of  tool sets and options to analyze data."
"66" "In a former role, I was often called upon to assess the merit of health analytics proposals. The well-intentioned pitches often promised all the right things; to leverage information assets to give analysts the right data – anywhere, any time, and to target known opportunities to reduce health care costs and improve health outcomes. Frequently, an initial proposed step involved gathering data assets into a consistent format. Unfortunately, more often than not, when funded, even the seemingly best opportunities failed to deliver on their initial promise.  How can we best gauge which offers should be pursued?  Obviously, there is no perfect answer, but here are four factors to consider that reduce the risk of getting less than we bargain for: 1) What business problem are we trying to solve? To achieve success in our endeavors, we need to be clear on what we are trying to deliver.  Vague notions often result in vague solutions and vague success.  For example, “Build a data warehouse” may result in a pile of code and a pile of data that might be used if analysts can find what they want in it.  But if we have a specific business problem we are trying to solve, like “Reduce patient risk for complications following outpatient surgeries,” then we have a guiding focus on a tangible target that tells us what data to go after, how to process it, and what to do with the insights we glean. 2) How do we know it is a problem I once reviewed a proposal to identify new targets for treatment pre-approvals that promised to reduce the costs associated with unnecessary treatments by a wide margin. But when digging into the details, I found that the initiative had forecast savings for treatment denials, but had not considered the cost of alternate treatments; after all, simply denying a particular treatment will not make a patient better.  After adjusting for this oversight, the net result was that overall, the proposal would not result in significantly lower costs (compared to the cost of implementation), nor did it project significantly better health outcomes.  The takeaway was that the existing process balanced patient needs, doctor recommendations, and care delivery as well as, or better than, the proposal. We must take care to make sure we are pursuing a solution to something that is a solvable problem. Quite often we gravitate toward issues in our own sphere and work to solve them, only to find that sometimes, in optimizing locally, we have made the end-to-end outcome no better, or actually worse.  Often what we see as a problem is actually a balancing of opposing concerns that, over time, have reached equilibrium. That isn’t to say that the current balance is optimal for all time – sometimes the greatest opportunities arise from new forces coming into play (like technology innovation) and driving new equilibriums. So how can we recognize solvable problems? One key indicator is that variation exists in outcomes across locations, regions, providers, treatments, and other differentiators. Where these variations exist, our analyses can identify the drivers for the differences, and we can work to uniformly optimize all outcomes. Another indicator is when others’ outcomes are better than ours – it signals that potential improvements are possible; we have the opportunity to leverage our own data and analytics to follow (and potentially improve upon) their lead.  3) What does our success look like? It isn’t enough to just deliver data and information. Gathering data across our systems, analyzing that data, and then trumpeting our insights doesn’t do much toward affecting real-world outcomes. Our solution should envision pushing those insights back into our processes (human and technical) and monitoring outcomes for the impact of our efforts. With this end-to-end view of our problem, we will more often deliver end-to-end solutions that have real-world impact. We must also accept that our data and tools limit what we can achieve. Health analytics insights are limited by three factors:  (1) the amount of data we have; (2) the quality of that data; and (3) available analytics tools. The more data we have, the more subtle the effects we can detect. The better the quality of our data, the less that the more subtle effects will be lost in noise. The more sensitive our analytics tools (including technical platform and effect-specific analysis methods), the more subtle the effects we can uncover. Regarding the amount of data, we only have as much as we have, which is a function of how many patients we have had across the span of time under consideration. This is often a function of how long we have been operating, but also a function of the lifespan of the particular situation we ultimately will affect. If we haven’t enough data of our own to support the analyses, we might consider acquiring additional relevant data from other external sources. However, given that the data would have been produced under a process different than our own, there may be some impediment in pushing insights gleaned from that data back into our own processes effectively.  4)How much should we be willing to invest? It may seem obvious that if the proposal would result in a net improvement in costs or health outcomes relative to its implementation cost, then we should support the proposal. However, oftentimes the bottleneck for health analytics efforts is less about funding and more about organizational bandwidth. Knowledgeable experts spanning operations, care delivery, and data analytics are typically a scarce resource and should be applied to efforts that give the greatest return. Given this, the value of a proposal should be weighed against the opportunity cost of being able to pursue other proposals based on organizational bandwidth – or in other words, an investors’ view. Organizations that adopt this investor’s view of weighing health analytics proposal values against one another will generate a history of return-on-investment for funded proposals. They often will find that they initially realize a factor of four or more times return on their investments. Then, as time goes on and the backlog of high-value proposals is depleted, the realized value will settle to an average of two times return on the investment. With this understanding, data analysts putting together a proposal only need a good estimate of the value of gross improvements (cost savings and health outcomes), but do not need a detailed estimate of implementation costs. Instead, decision-makers can make funding decisions based initially on delivery team estimates, whether the proposal can be fulfilled for one-quarter to one-half (or whatever current fraction is indicated by their funding history) of the value of gross improvements. This can speed along promising health analytics initiatives while avoiding development of detailed estimates to deliver less promising ones. Time is also an important consideration when deciding which health analytics efforts to pursue. It is important not only for the obvious concern for timeliness of realized benefits, but also from a risk perspective. Often the longer that a health analytics initiative takes, the less likely it is to produce promised outcomes. This is because modern data analytics technology largely removes the uncertainty of technical performance, leaving issues associated with health data and analytics protocols to drive the timeline. To that end, any health analytics initiative should be able to produce demonstrable results within a short time-frame of 4-6 weeks; otherwise it should be re-evaluated.  Finally, despite considerations of focus, value, outcomes, organizational bandwidth, return, and timeliness, another important factor is strategy. Looking back, sometimes we see that the proposals we funded were not exactly aligned with our stated strategy – it can be said that strategy is how you spent your time and money! In some cases, proposals that according to all measures are less attractive can have longer-term benefits when executed as part of an overall strategy, since such efforts are often reinforcing. Special consideration should be given to proposals that align with current organizational strategy. In summary, health analytics efforts require scarce organizational resources that must be allocated to initiatives based on both cost and health outcomes. When we have a specific problem that is known to be solvable, the right approach to affecting real-world outcomes, streamlined decision processes, and robust, competitive funding practices, then our health analytics investments will more often yield the positive results for our patients and our communities that they promise. Questions? Feel free to Contact Us with any feedback, questions, or data and analytics needs.   "
"67" "BlueGranite’s tailored analytics solutions drive major business impact for clients. Key to our success is our deep bench of trusted technology experts – each with their own arena of expertise. As part of our Meet our Team blog series, today we’re introducing Leo Furlong: a longtime BlueGranite Solution Architect with an extensive background implementing on-premise and Azure cloud-based Digital Transformation strategies.   Leo holds a Master of Business Administration, Business Analysis degree from Georgia State University’s J. Mack Robinson College of Business. Prior to that, he earned a Bachelor of Science degree in Industrial Systems and Engineering from the elite Georgia Institute of Technology, consistently ranked among the top U.S. engineering schools. Though his former efforts in the classroom now inform his work, so do the efforts he once put in on the field; Leo was a college football defensive end for Georgia Tech.  “The biggest thing I learned was time management – the ability to execute and plan around a lot of different topics,” the former Yellow Jackets player said, regarding trying to balance intense engineering classes with sports. “When schools starts, you’re putting in 40 hours a week into football; then you have to sprinkle in your classes and get your studying and your homework done.” He not only managed to do both and graduate, he then went back to school to earn his master’s degree at night over the course of four years, while working full time and parenting the first two of his three sons (he has a fourth on the way in just a few weeks!) Besides time-management mastery, Leo’s time on and off the field also gave him a passion for self-improvement, with him often asking himself “How can I be better?” Part of the answer includes his commitment to continued education – earning the latest Microsoft, Databricks, and Hadoop developer certifications – as well as constantly learning from his BlueGranite peers. “One of the things I find very valuable about BlueGranite is we have a whole collection of people that are just completely dedicated to modern BI, data platforms, and AI,” Leo said. “It’s fantastic to have a pool of my peers to be able to bounce ideas off of and to share our collected knowledge, really to the benefit of our clients.” Leo began his career implementing logistics and warehouse management software; from there, he jumped into business intelligence, working as a Microsoft BI consultant since 2006. With more than a decade of experience spanning multiple industries and business intelligence projects, he finds he gravitates most toward Modern Data Platform builds – innovative software designs that form the foundation for secure, scalable advanced business analysis. He most enjoys consolidating, shaping, and modeling data for consumption, and he’s truly excited about the potential the cloud offers for data warehousing and analytics. He notes the superiority of cloud “scalability, elasticity, and engineering for cost and velocity.” “From executing over a decade worth of projects on-premises, versus executing projects in the cloud, we’re able to build and innovate so much faster,” Leo said. This allows for faster delivery, faster client feedback, greater value, and often major cost savings for clients – both in technology and delivery costs. Cloud-scale analytics allow unparalleled speed and storage, he added, and clients only pay for resources as they need them. As for his limited free time, Leo spends most of it with his children. He’s still committed to fitness, spending time in his basement gym lifting weights and fitting in the occasional mixed martial arts class. And he’s also still a Georgia Tech football fan (albeit one who married into a University of Alabama football family.) Contact BlueGranite today to learn more about how our team of experts can tackle your data needs to build value across your organization."
"68" "Welcome to the first installment of an in-depth look at Knowledge Mining –  the ability to use Microsoft Azure’s advanced AI Search capabilities to comb through all of your data (PDFs, emails, scanned documents, images, etc.) to glean insight. In this series, I’m going to take you through Microsoft’s Azure Cognitive Search (Azure Search with human-like reasoning capabilities) and show you the ins and outs of effectively using this awesome tool to uncover insight from enterprise data, whether structured or raw.  Let’s begin by digging in to Azure Search.  Azure Search Managed by Microsoft, the intelligent Azure Search cloud solution-as-a-service has built-in cognitive abilities. Recognizing and extracting text and identity from images; highlighting key talking points from text; and the power to recognize and classify people, places, and things from text and images, are among its innovative capabilities. This expansive offering can help dig deep into your organization’s data, often uncovering rich insight. BlueGranite is currently working on one such project for a private global energy exploration and engineering company. Leaders here needed a way look back at decades of data, recorded in paper files, without a team of archivists. We’re working with the worldwide energy giant, using cloud-scale technology, to digitize, store, and offer deep search capabilities on more than six million documents. By combining Azure Search with Knowledge Mining, we will ultimately provide nearly instantaneous access to information that once would have taken a team of people potentially months (if ever) to uncover. So, what is Knowledge Mining? Knowledge Mining Knowledge Mining is a cognitive search-based technique of extracting facts from unstructured data. It’s like having a crew of experts comb through your most important documents to discover and leverage data to drive your enterprise. This content comprehension capability can be used to create in-depth search resources that inform an organization’s employees and enrich its clients and customers.  In today’s tutorial, I’ll offer step-by-step instructions on creating a search service and an index. Creating an Azure Search Service and Index The first step in creating an effective search is providing the data that we are going to search. Azure Search can be used against several data sources, both structured (Azure SQL Database and Cosmos DB) and unstructured, in the form of Azure Blob Storage. Blob indexers can extract text from major file formats such as Microsoft Office, PDF, and HTML documents. In this how-to series on Knowledge Mining, we’ll be focusing on unstructured data, so let’s create some blob storage. First, we need to create a storage account to hold our blobs. From the Azure Portal, select the Create a resource option and then type storage account into the search box. Select Storage account from the drop-down under the search box.   Click on the Create button.   And now add the details: Select your Subscription and Resource group that you are using for your search. Enter a Storage account name. Note that this name must be unique across all storage account names. Select a Location that you will use for creating all of your resources throughout this exercise. Make the following selections: Performance: Standard Account kind: Storage (general purpose v1) Replication: Locally-redundant storage (LRS)  Click on the Next: Advanced> button. On the next page, make the selections shown below and then click on the Review + create button.   Now press the Create button and wait for the deployment of your new storage account to complete.   When your deployment is complete, you will see a link to your new storage account at the bottom of the page. Click on it to move to the next step.   You’ll now see the Storage Account Overview page. We will create our searchable blob storage in this storage account by clicking on the Blobs link in the middle of the page.  Click on the big plus sign + Container in the upper left of your screen and enter a name for your container. Think of a container as a folder that will hold all of our documents. Select Container (anonymous read access for containers and blobs) for your Public access level and click on the OK button.   Click on the name of your new container.  You should now see the Overview of your blob container. Now we need to add the files that we are going to be searching. We do this by clicking on the Upload option in the upper left of the screen.   You’ll see this on the right side of your screen. For this series, we’ll be working with a common data set used in Microsoft’s Knowledge Mining Bootcamp, which is an excellent introduction to Azure Search as well. You can go to the Bootcamp here. To get the dataset, go to the GitHub repository at https://github.com/Azure/LearnAI-KnowledgeMiningBootcamp.git and clone the repository. You’ll find the sample data in the resources/dataset/ folder.   From the Azure Portal, click on the selection button and select all of the files in the dataset folder.   Now we are ready to create a search service and index all of the documents we just uploaded. Create a new Azure Search Service by clicking on the Create a resource menu item in Azure Portal and then type Azure Search into the search box. Select Azure Search from the drop-down under the search box.   Click on the Create button.   Enter a name for your search service, the subscription and resource group you want your searching to be done in, and the location. You will want these to all match the setting on the storage account that you created earlier.  You can use the Free pricing tier for these exercises but understand that it is very limited and should only be used for dev/test. Click the Create button and wait for your new Search Service to be created.  Now that the search service is created, you should see something similar to this:  Next, we’ll create the index that will hold all of the information for our search. Click on the Import data link on the top of the page. On the next screen, select Azure Blob Storage from the Data Source drop-down list.  Name your data source and then click on Choose an existing connection and select the storage account that you created earlier. You will then select the container that you created and press the Select button. Leave all other fields at their default values and press the Next: Add cognitive search (Optional) button on the bottom of the screen. Azure Search will try to infer index fields from the files in your storage account. Since we are working with unstructured data, it will only come back with standard search fields.  We’re going to skip the cognitive search settings for now (much more on this in future posts). Press the Skip to: Customize target index button.  We’ll leave all of the settings at their default value for now. Press on the Next: Create an indexer button.  Change the Schedule value to Once. You can leave all of the other fields at their default value and press on the Submit button at the bottom of the screen.  When the index creation is complete, it will take you back to the Search Service Overview page. Notice that the Index, Indexers, and Data sources menu items in the middle of the page all have a (1) next to them showing the number of elements in each area. Congratulations! You’ve just built your first searchable index. Click on the Indexers(1) link in the middle of the page. You’ll see that the status is set to Warning. Let’s look into that by clicking on the line for your indexer.  Click on the line in the Execution details section for the indexer we just ran. In the new blade, you’ll see several entries in the Warnings section. There are two kinds of warnings: 1) Document has unsupported content type, and 2) Truncated extracted text to 32768 characters. The first warning is because several of our files contain images. The standard Indexer does index images. We have to add a cognitive service for image cracking. We’ll do this in our next session. The second warning is because we selected the Free pricing tier when we created our search service. The Free pricing tier only allows a maximum of 32,768 characters to be extracted out of a document.  Go back to the Search Service Overview page and click on the Indexes(1) link and then again on the line with the index that you just created. From the Index Screen we can query a selected index and test it out. Try it out. Type “Microsoft” in the Query String field and click Search. You’ll see the results returned in JSON. Using the dataset from the Knowledge Mining Bootcamp, you’ll see 10 documents returned.  More to Come In the coming weeks I’ll be exploring the many ways to use Azure Cognitive Search to more easily unearth knowledge from once-difficult-to-mine data sources. Be sure to subscribe to our blog so that you don't miss a tutorial, or contact us today to discover the many ways BlueGranite can make the most of your data! Check out the next post in the Knowledge Mining series, on Form Recognizer, HERE. "
"69" "It’s been a bit more than a week since BlueGranite’s Bryan DeZeeuw and I joined thousands of other developers and tech experts in Seattle May 6-14 for the ambitious Microsoft Build 2019. It was a year of firsts for the annual conference, with a heavy focus on Azure’s exciting future. As Microsoft’s CEO Satya Nadella announced during his keynote:  It’s the first year Build opened its doors to children of attendees, with events for aspiring programmers. Azure, now being used by 95 percent of Fortune 500 companies, recently became the first public cloud to launch data center regions on the continent of Africa, specifically the country of South Africa. The company is building out Azure as an open platform, creating limitless possibility and furthering Microsoft’s goal of empowering “every person and every organization on the planet to achieve more.” And, as everything in our homes and workplaces – from “smart coffee machines” to “connected cars” – becomes software-driven, Nadella said growth opportunities for the world’s most valuable company abound.  Microsoft Cloud Evolution Scott Guthrie, Microsoft Executive Vice President, Cloud and AI, gave the Azure technical keynote, introducing lots of cool new tools and features for developers, including the integrated Visual Studio 2019 development environment, and a bevy of AI-based tools and enhancements. “Microsoft has everything you need for creating the next generation of software applications,” Guthrie said. “Azure supports your tools, your languages and your apps; and every developer is welcome.” Guthrie’s highlights included: An overview of the new Visual Studio 2019 features, with Microsoft’s Scott Hanselman, that looked at: editor.config live unit testing IntelliCode and enhanced IntelliSense Enhanced GitHub integration Group/shared programming sessions that can be used with both Visual Studio Code and Visual Studio  Azure DevOps and GitHub integration Review of Azure IOT Edge An overview of the open-source Virtual Assistant, used in partnership with BMW, showcasing a vehicle equipped with the voice interaction functionality. AI - MLOps, ML.NET New features in Cosmos DB, including the integration of Apache Spark with Cosmos DB Check out Guthrie’s talk here. Charting the Future of .NET Microsoft’s Scott Hunter and Scott Hanselman looked at .NET and the roadmap for future releases, discussing some cool new features in the pipeline, with .NET Core 3 scheduled for September of this year. Other highlights included: .NET support for Apache Spark  run .NET apps on Spark clusters DataFrames that you can use LINQ on  ML.NET – create models in C# made easy with AutoML .NET Core 3.0 and C# 8.0 features, coming in September this year Combining .NET Framework, .NET Core, and Mono into a unified platform as .NET 5.0, expected in November 2020 Check out their talk here. Azure and the Future of AI Eric Boyd, Microsoft’s Corporate Vice President, Azure AI, talked about the explosion of AI – how it’s here and transforming business in amazing ways. Microsoft’s goal is making AI real for every developer and organization. Among the ways it’s doing so is with major advancements to its pretrained AI Cognitive Services offerings, which tie in well with the simple-to-use Bot Framework. Boyd kicked off his presentation by announcing a whole new Cognitive Services arena: the Decision category. New AI services in that group include: Personalizer, “a reinforcement-based learning system” available in preview (and which Microsoft has already used with Xbox to increase user engagement by 40 percent) that offers curated content, based on what it knows and learns about each user. Anomaly Detector, to help users spot problems before they happen. The company is also expanding existing Cognitive Services areas, including new APIs in: Vision, with Ink Recognizer, which can easily recognize handwritten content; and Computer Vision, which analyzes images. Speech: Conversation Transcription, which converts audio to text, even amid multiple speakers; Neural Text-to-Speech, in preview, allows adding voice to apps indistinguishable from the human voice; and the pre-tuned Speech Service Device SDK development library. And the Bot Service Framework, which has already been downloaded 2 million times, with thousands of new Bot Service creations weekly, is growing, too: The Bot Framework SDK 4.5, will feature adaptive dialogs, meaning developers can build conversations that can be changed as the conversation progresses. Multi-turn conversations, a new feature for QnA Maker, integrates with the Bot Service Framework, to save time coding custom dialogues. Likewise, the new Virtual Assistant solution accelerator, currently in preview, can save a great deal of time in developing new virtual assistants. Boyd also talked about taking Cognitive Services from the cloud to the edge, meaning the many tools can now be deployed anywhere, whether on-premises or on an edge device, using containers. New additions to that offering include the earlier-mentioned Anomaly Detector, Speech to Text and Text to Speech. C#’s Long History We also took a “Look Back” on C#, with Anders Hejlsberg, a Microsoft Technical Fellow. Hejlsberg, a core designer of the C# programming language, reflected back on the struggles and triumphs of different C# releases. Check out his talk here.  Sessions Explored New Features and Capabilities In addition to some great presentations, there were hundreds of other Build sessions and hands-on labs offered throughout the week. A few other highlights for us included: Introducing AI-driven content understanding with Cognitive Search and Cognitive Services, which delved into applying AI to data; introduced Knowledge Store, an optional in-preview Azure Search function; and explored new features and abilities. Some of the new capabilities are: 30 times faster indexing pipeline then previously Supports complex data types Example of multiple locations for a client Use new Shaper skill to create complex types  Improved image extraction through OCR Conditional skill sets based on document type New service tiers for large storage jobs (L1, L2) Use custom skill set to make use of the new Form Recognizer service, a new AI-powered extraction services can pick key details from mass documents Knowledge store - JSON or table index results in storage accounts New Knowledge Mining Accelerator  Check out the presentation here. Announcing Form Recognizer, which explains how the new Form Recognizer cognitive service can extract text, key value pairs and tables from forms. The service: Finds and maps key value pairs in a document. Can train the recognizer model with only five sample forms In the future will allow manual model adjustments Ingests - Clusters - Discovers - Extracts Will move to public preview in June. Features a pre-built recognizer for receipts, which has the capability to greatly ease auto expense reporting/auditing.  Check out the presentation here. Building data pipelines for Modern Data Warehouse with Spark and .NET in Azure. This session explored building data pipelines with Spark and your choice of .NET programming language – C#, F# – using both Azure HDInsight and Azure Databricks, and connecting them to SQL Data Warehouse for reporting and consumption  The highlights included: C# language extensions using .NET Writing .NET apps using DataFrames with NuGet package - can use LINQ or Spark SQL Running a cluster using Databricks or HDI Using ML.NET for machine learning The goal is to be as fast as PySpark Working on Spark interop layer where you can code in Jupyter using .NET (no LINQ/very Scala-like) All open-source Check out the presentation here. The Extras I was also lucky enough to be among about 100 people invited to talk with some of Microsoft’s program managers on the heels of the conference. We sat in groups at different tables and various program managers talked to us about products in the works. Those included: Git integration in Visual Studio Azure Cosmos DB Documentation The .Net Core Command Line Interface I talked to their Python programmer and others, and got to dive into the prototype Git interface Microsoft is building out in Visual Studio 2019, a tool we frequently use writing programs. They talked to me about how BlueGranite uses Git, and even let me work with the one they had in development, watching how I used it, and gauging what things came easy and which posed challenges. Sessions On-Demand, Help at Your Fingertips If you didn’t get a chance to attend the conference, check out some of the fantastic Build 2019 on-demand sessions here. Keep an eye out for future Business Insights posts digging deeper into some of our key Build 2019 takeaways. And, if you’re ready to explore the many ways Microsoft and Azure can boost your enterprise, contact BlueGranite. Let us put our digital transformation expertise to work for you."
"70" "At BlueGranite, our fundamental business strategy is simple: to build a world class consulting organization that delivers meaningful business impact for our clients through data & analytics solutions. Each of the three primary components of that strategy has gone through limited changes over the years, but they describe a consistent and fundamental guiding light for how we operate.  Discussions about strategy happen at all levels, and strategy factors into many tactical discussions and lower level decisions as a crucial reference point.  We refine and remain true to our strategy by constantly asking the question “Why does this matter?”.  We believe it’s easy for companies these days to have artfully written, inspirational vision statements, strategies, ideas, and values.  However, we ask ourselves – “Why does this matter?” What does it mean in translation to day to day activities and decisions?    We’ll start with world class consulting organization – it sounds good, right? But why does it matter? What steps do we take to achieve it?  For us, it is reflected in our talent and in our culture.  We value highly talented and diverse team members who are interested in contributing to the organization in a variety of ways, and who feel at home in a culture that is built on treating each other like responsible, professional adults. This means we reduce management overhead, encourage flexible working arrangements and hours (with a results-oriented work environment), and enable every employee to learn about all elements of the business, and to add their voices and efforts into the activities that they feel passionate about.  As we recruit new team members, we make a commitment to our employees that we will bring on only the highest caliber of people and provide them the types of opportunities that are the best mix of challenging and exciting.  We will encourage their growth and development in a variety of ways, but ultimately in a dynamic, self-directed manner that does not attempt to set and enforce narrow job descriptions or limitations on exploring strategic functions and roles across the business.  We encourage clarity and accountability from the team and to the team, from all levels of the organization.   We combine our individual talents with our team spirit and culture, and we employ it to deliver meaningful business impact for our clients.  We take our cultural belief of asking “Why?” and use that philosophy when approaching client engagements.  Although our team brings to bear considerable technological expertise, we decline to pursue work for the sake of work, or for the sake of technology.  We feel obliged to ask our clients “Why does this matter? If we accomplish this task – capture this data, generate this analysis - what does it change for you and your business?  Why would you pursue this effort?”  This philosophy, in turn, forces us to focus on the solutions we deliver in a way that has the best chance of realizing the potential of technology, regardless of the domain our client operates within.  While some technology service providers may be happy to take an order and check a box (and bill the hour and earn the dollar), our team is more likely to think like an MBA and thought leader than a heads-down technician, and that means we want to understand, track, and measure the successful delivery of business value resulting from the solutions that we build. And finally, we build our team and deliver our services to customers within the technology domain of data and analytics solutions.  This degree of focus allows us to be more singular in purpose across the internal functions of our business – every single team member, regardless of role or background, understands the domain within which we operate and brings their own specialized talents to bear.  This means that our marketing, human resources, and business development roles are not disconnected from the core concepts and value that our delivery team helps realize for our clients.  We execute on our clients' strategy across geographies, industries, and business departments without having to reduce the level at which we communicate – and all decisions have a better shot at being understood on their individual merits rather than the more common business tactic of comparing in terms of margin or dollars. We are increasingly focused on the primary arena of innovation for data and analytics – the cloud – where the expansion of possibilities has exploded into machine learning and artificial intelligence.  We think you’d be hard-pressed to pick a more dynamic and exciting domain!  Taken as a whole, our strategy has refined over time; the details of our culture, and the backdrop of technology in particular, have matured with us and made us a Great Place To Work.  The primary elements have remained constant.  Through it all, we seek to maintain transparency and accountability.  During all-hands staff meetings and individual one-on-one check-ins, we share performance numbers, we talk about our successes, and we explore our failures so we can all learn.  We encourage all team members to ask “Why does this matter?” when they hear something that sounds good, but doesn’t feel totally thought out, or doesn’t have a clear path or action. We measure and share across many forums – weekly all-hands meetings, annual staff retreats, team-wide dashboards, and individual conversations. And lastly, we perpetually reserve the right to get smarter and to learn new things.  We’re excited to be certified as a Great Place To Work for 2019 - our third year in a row - and we believe that by adhering to our strategy, we will continue on this path into the future.  "
"71" "As features like Drillthrough open the door for Power BI reports to become more interactive – and by extension, potentially more complex - it is important that they remain intuitive to use.  For example, I have been in several meetings where a Power BI report with drillthrough and bookmarking capabilities is used to drive conversation.  As the presenter navigates through the report, she is often asked to go back to the original page to double check her selection for drillthrough or check which bookmark option she selected.  This happens for various reasons, whether it’s because the data doesn’t match what people were expecting (eek!), or because the meeting veered off topic a little and now no one can remember with certainty what was selected. All this clicking back and forth eats up valuable time and reduces productivity. The worst part? It’s easily avoidable – simply add dynamic titles to your drillthrough and bookmarked pages. The suggestion of dynamic titling in Power BI was first introduced to me by a few of my colleagues at BlueGranite, and I was struck not only by its usefulness, but by how easy it is to implement. It’s simple, powerful, and immensely valuable from a user-experience stand point. It’s something every report developer should be able to do, and I’ll show you how in this post.   On the main page, users have the option to drillthrough on Breed or Group from the table, or to click the button to access a report page bookmarked for popular running breeds.  If you drillthrough on Group for “Herding”, you’re taken to a page displaying popular herding breeds (some of my personal favorites), with the title “Herding Group”.  After admiring the adorable herding breeds, you can navigate back to the main page and click the button, “View Popular Breeds for Runners”. This takes users to the Group drillthrough page, bookmarked to display running breeds rather than breeds for a single Group. The title reflects this:  The functionality of the title on this page was accomplished with a simple DAX measure: SELECTEDVALUE  SELECTEDVALUE checks to see if a field has only one value selected, and if so, it displays the value. If not, it displays an alternate result that you set. For this page, I created the following measure: Group Title = SELECTEDVALUE('AKC Top 10 Breeds'[Group Name], \"Popular Dog Breeds for Runners\") In this measure, SELECTEDVALUE checks to see if only one Group Name is selected, and if so, displays it. If not, it displays the default text string, “Popular Dog Breeds for Runners”.  I put that measure in a card visual to create my title.  When the page is filtered to display a single group via drillthrough, it displays the Group Name:  When a user clicks the button to view popular breeds for runners, the page displays breeds spanning multiple groups, so the default title, “Popular Dog Breeds for Runners”, is displayed.  This title improves user-experience by displaying the user’s selection prominently on the page, so she can be confident of her selection and subsequent analysis of data on the page. If you navigate back to the main page again, you can also drillthrough on Breed. This takes you to a page that can only be accessed by drilling on a single breed.   Since only one value from Breed will ever be shown on this page, rather than use a DAX measure to create a dynamic title, I display Breed in a card visual for the title. Because of the functionality of the report, there’s no need for a default value here, and thus no need to create a title measure. This avoids over-engineering the report with extraneous measures, as too many measures can affect performance.  Displaying Breed in a card visual works just fine as a title for this page:  As you can see from the examples, the second part of creating dynamic titles is controlling how the user can interact with the report. For the flow of the report to work correctly, I never want users to access either the Breed or Group drillthrough pages via the tabs. By hiding the pages, I ensure that users can only access these pages by either drillthrough or the button from the main page, and thus ensure that my dynamic titles display accurately and in the intended context.  A dynamic title takes only a few minutes to implement and provides significant value to users during data analysis. Even little details can go a long way, especially when you’re trying to stay on track in a meeting, right? For more help with DAX, or report design in general, please reach out to us here at BlueGranite!"
"72" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/azure-data-lake-analytics-holds-a-unique-spot-in-the-modern-data-architecture\"   },   \"headline\": \"AZURE DATA LAKE ANALYTICS HOLDS A UNIQUE SPOT IN THE MODERN DATA ARCHITECTURE\",   \"description\": \"Azure Data Lake Analytics is a distributed, cloud-based data processing architecture offered by Microsoft in the Azure cloud. It is used to help quantify Azure Data Lake which is an ever-evolving set of technologies.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/DataLakeStorage.png?width=548&name=DataLakeStorage.png\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Josh Fennessy\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 60     }   },   \"datePublished\": \"2019-05-03\",   \"dateModified\": \"2019-06-14\" } This technology is no longer the recommended approach. For a flexible computing platform, consider Azure Databricks.   Azure Data Lake Analytics is a distributed, cloud-based data processing architecture offered by Microsoft in the Azure cloud. It is used to help quantify Azure Data Lake which is an ever-evolving set of technologies that currently looks somewhat like this:  With Azure Data Lake Store (ADLS) serving as the hyper-scale storage layer and HDInsight serving as the Hadoop-based compute engine services, I've been confused as to where Data Lake Analytics fits. On the surface, it appears to provide the same services as HDInsight — Big Data batch processing — and it does. But it takes a unique approach.  In late April 2017, BlueGranite was invited to a partner-only Microsoft training event where I learned an awful lot about Azure Data Lake Analytics (ADLA), and got to spend time with the product team that's developing it. We learned about the ADLA product roadmap, most of which can't be shared publicly yet, but I'll give you a teaser; AWESOME STUFF IS COMING OUT IN THE FUTURE! STAY TUNED! More importantly, however, I learned how ADLA fits into a larger data management strategy and why you might want to use it in your next project.  In this article, I'd like to explain this approach to you in hopes that it might demystify where ADLA fits in with your Big Data solution. So, what is Azure Data Lake Analytics anyway? Azure Data Lake Analytics is a distributed, cloud-based data processing architecture offered by Microsoft in the Azure cloud. It is based on YARN, the same as the open-source Hadoop platform. It pairs with Azure Data Lake Store, a cloud-based storage platform designed for Big Data analytics. Aside from the technical specs, ADLA is a platform that enables processing of extremely large data sets, integration with existing Data Warehousing, and true parallel processing of structured and unstructured data. It's relatively easy to learn, builds on technical skills found in almost every enterprise, and has a very low entry cost and an easy-to-manage (and understand!) pricing structure. ADLA is a direct descendent of Microsoft's homegrown Big Data solution, Cosmos. While details for Cosmos are scarce due to its internal-only nature within Microsoft, we do know that Cosmos is built to handle \"exabyte-scale\" and \"hundreds of thousands of jobs daily\" from this article announcing Azure Data Lake as a new service. Cosmos, and its corresponding data language SCOPE (an ancestor of U-SQL), has been heralded within the Microsoft employee community as being easy to use and extremely scalable — two great features enterprise is looking for in a Big Data solution. What can I do with Azure Data Lake Analytics? Right now, ADLA is focused on batch processing, which is great for many Big Data workloads. Some example uses for Azure Data Lake Analytics include, but are not limited to: Prepping large amounts of data for insertion into a Data Warehouse Processing scraped web data for science and analysis Churning through text, and quickly tokenizing to enable context and sentiment analysis Using image processing intelligence to quickly process unstructured image data Replacing long-running monthly batch processing with shorter running distributed processes ADLA is well equipped to handle many of the types of processing we do in the T portion of ETL; that is, transforming data. If you've found that your data volumes have increased, changed shape, or you are generally not happy with your ETL performance, Azure Data Lake Analytics might serve as a good replacement for your traditional approach to prepping data for analysis. What makes it different? It's hard to not compare ADLA to Hadoop, and rightfully so. ADLA provides much of the same functionality that Hadoop provides with Hive and Spark. However, Azure Data Lake Analytics does take a different approach in a number of areas: Only one language to learn OK, this is kind of tricky, because, although there is only one language to learn, U-SQL, it's really an amalgam of SQL and C# – so if you're familiar with either or both of those languages then you'll be ready to tackle U-SQL. If you're not familiar with either, then it's OK, you don't have to be a SQL or C# master to understand U-SQL. With Hadoop, you'll have a few more languages to learn – I'd say at least six (Hive, Pig, Java, Scala, Python, Bash; yup, six). Only offered as a platform service Hadoop comes in many different flavors, some running on-premises, others running in the cloud. Some are managed BY you, others are managed FOR you. ADLA, however, is offered ONLY as a platform service in the Microsoft Azure Cloud. It's managed by Microsoft — you'll never have to troubleshoot a cluster problem with ADLA (only your own code). It's also integrated with Azure Active Directory, so you don't have to manage security separately. Really, all you have to worry about when you set up an Azure Data Lake Analytics account is building your application. Pricing per job; not per hour Most Big Data cloud offerings that are available are priced per hour based on how long you keep your cluster up and running. ADLA takes a different approach to pricing. With ADLA, you pay for each individual job that is run. Each job run through ADLA is assigned a number of Analytic Units (AUs). Each job is billed based on how many AUs were used and how many hours the job ran. So a simple job that used 10 AUs for one hour would be billed for 10 processing hours. As a matter of fact, just owning an Azure Data Lake Analytics account doesn't cost anything. You aren't even billed for the account until you run a job. That's pretty unique in the Big Data space. Any killer features to look out for? While Azure Data Lake Analytics are relatively new (to the public) technology, it does already have some great uses that cause it to stand out from the traditional Big Data offerings. When the U-SQL team released their first product, they had the foresight to include a rich set of Intelligent libraries. These libraries allow for deep machine intelligence to be embedded in day-to-day processing, and make it very simple to implement. This feature, above all others, stands out to me. As these libraries mature, and the U-SQL language gains more functionality, the ability to wring even more valuable data out of the myriad information we are creating daily will be possible. U-SQL, when paired with Visual Studio as a development environment, also makes the development and optimization process a pleasant one. Trolling through MapReduce or Spark logs is not high on the fun-scale for anyone that I know. The ADLA team recognized this and built a great set of debugging and analysis tools into Visual Studio that make it possible to squeeze every last bit of performance out of your U-SQL application. How do I get started? You might be thinking \"Oh great, this is where the catch is,\" but you're wrong! Getting started with Azure Data Lake Analytics is really easy! No, really, I mean it! Here's what you'll need: An Azure subscription — grab a free trial if you don't have one An Azure Data Lake Analytics account — create one in your Azure subscription You'll also create an Azure Data Lake Store account during this step  Some data to play with — start with some text, or maybe some images You don't even need to install anything on your personal computer to use it. You can write and submit jobs in your favorite browser. If you'd like a hands-on tour of Azure Data Lake Analytics, why not check out the free BlueGranite labs? We wrote these labs in partnership with Microsoft, and they are a great way to get to know ADLA and U-SQL."
"73" "You’ve heard about streaming, even seen a few live demos of people doing Twitter sentiment analysis, then gone back to the office to work on meeting the needs of your userbase with your existing batch system that the company has had for years. I get it. Been there, done it, sent a postcard.  I’d like to shift the perception for those of you that might not really be aware of either the value proposition or the low barrier to entry that streaming data offers.  Value Proposition Opportunities Enabling New Business Capabilities Putting Twitter sentiment analysis aside—there are core business processes that can be enhanced right now, today, using streaming data methodologies that can provide real competitive advantages. I don’t have to enumerate them here, because these are already available from well-respected organizations that you should be paying attention to: Forbes - The Competitive Advantage of Streaming Analytics Gartner - 5 Trends Emerge in the Gartner Hype Cycle for Emerging Technologies, 2018 (for purposes of this article, IoT Platform is synonymous with streaming data, albeit a more specific use case.) The competitive advantages that streaming enables in the enterprise are well known to technology companies and will soon be at the doorstep of almost every industry. Is your organization ready for this challenge/opportunity? Operational Efficiencies A True Common Data Service for Every Application Streaming data implements governance, integration, and messaging at scale within a common, extensible platform. It enables orthogonal applications to alert, act, and analyze against the same source definition. In the form of Kappa Architecture, streaming data means true implement once, consume multiple semantics, allowing for domain expertise to concentrate within the applications themselves (including self-service analytics), while governance and security exist within the data tier common to all applications. Governance and data access only needs to be implemented a single time—all applications reading from the stream source are equal in the eyes of the platform, whether messaging, alerting, analyzing, or deep learning. This extends to security and authorization rules as well.  Figure 1- from Confluent What About Data Lakes? Some of the use cases for streaming at first appear redundant with the capabilities of a data lake—ability to scale, enrich, and serve orthogonal applications from the same source. The differences in capabilities, especially in the form of Kappa, become significant when we step through the limitations of the data lake: Security and governance are managed separately from producing and consuming applications—roles must be applied based on file and folder hierarchies, which may or may not have anything to do with the actual data contained in each file. Streaming platforms maintain the same security definitions for all access requests to data. Latency and throughput are limited due to requiring disk writes—even if a cache system is implemented, file operations are still a limitation as opposed to single, additive events which can be read immediately in a streaming data source. Resiliency in writing output is dependent on either the source application or a separately triggered component—the source application or component must be built to scale with throughput, ensure checkpointing and failover on system error, as well as guarantee write once semantics, otherwise additional data cleansing will be required by downstream applications. Modern streaming platforms, like HDInsight Kafka and Azure Event Hubs, are resilient by default. Data lakes are best suited to serve analytical and predictive applications, whereas a streaming platform can be the source system for every application because it solves the security, latency, and resiliency requirements, as well as any database system, but simultaneously allows for the scalable read throughput needed by any analytic application. Data lakes then become useful as a data munging environment, but don’t require the organizational gravitas associated with being a system of record. If you reflect on why data warehouses and lakes were conceived in the first place—because the application’s data source couldn’t performantly serve both transactions and analytics simultaneously—then the removal of this limitation helps reset the expectation. Yes, you can have a single source of truth, and it can serve all your applications concurrently. This exists today and you can have it. Barriers to Entry Now that I have your interest as far as streaming capabilities, let’s step through some of the common blockers and misconceptions around what it takes to implement a streaming platform. Streaming Data <U+2260> IoT You don’t need IoT devices for a use case to implement streaming; there are plenty of requirements from your existing data where subject matter experts (SMEs) in your organization will make significant contributions with access to current state. The same no-code alerting and dashboards standard in self-service analytics for reporting become zero latency, domain enhanced, core business efficiency enabling tools when pointed at streaming data. See the Forbes article listed earlier for more specific examples. I Need to Create a Streaming Application Not that long ago, the only way to get streaming data was to engineer an entire end-to-end pipeline where sources and targets were required to be known entities in advance. This effort necessitated IT owning the entire process, as even deployments of reports reading from these definitions required significant technical expertise. This is no longer the state of the industry. It is possible to refactor your existing enterprise data architecture into streaming with zero code changes to existing applications. Relational database management system (RDBMS) table deltas can be polled, NoSQL object stores can have log listeners, directories and application logs can be read—even Excel file updates can be mirrored as an event. Sound like a lot of custom development? Actually, no. There are mature platforms known as data flow management tools that enable event streaming using visual layouts and components similar to batch based orchestration engines like Azure Data Factory and SQL Server Integration Services (SSIS). Data Flow Management Data flow management is an entire topic on its own—we’ll only touch on it briefly here. Two of the most popular frameworks are Apache NiFi and StreamSets. In Azure, StreamSets is supported as a published app in HDInsight, where Apache NiFi would require a custom deployment on HDI or a roll your own IaaS cluster. Apart from Azure considerations, this post gives a good overview for comparing the frameworks. There’s a lot of parity—evaluating each platform to support your specific use cases is key. For some use cases, a data flow framework might even eschew the need for an underlying streaming platform. Hands-on Experience Perhaps the most significant barrier to getting started is that many organizations aren’t familiar with these tools or what a streaming ecosystem even looks like. This applies equally to both business and IT. Having an experienced resource available, such as BlueGranite, when considering the impact and enablement that streaming can provide is a critical component that can help lead to a successful outcome, and a competitive advantage for your enterprise."
"74" "Databricks, the leader in unified analytics, honored the BlueGranite data and analytics consulting firm this week as its first-ever U.S. System Integrator Partner of the Year at the 2019 San Francisco Spark + AI Summit, the largest global data and machine learning (ML) conference. BlueGranite, a forward-thinking Azure Databricks pioneer, was recognized for its staunch platform advocacy, client success with data and artificial intelligence (AI) engagements on Databricks, and its deep bench of certified developers. From left to right: Jason Brugger, Ali Ghodsi, Matthew Mace, Josh Fennessy, Shannon St. Clair, and Leo Furlong. Since Microsoft announced Azure Databricks’ general availability in 2018, BlueGranite (Microsoft’s 2018 U.S. Big Data Analytics Partner of the Year) has championed the first-class Apache Spark-based service, speeding time to value and improving performance of modern data platform and AI solutions for clients. BlueGranite has also been pivotal in managing inaugural relationships between Microsoft, Databricks, and major retail, financial, government, healthcare, and educational sector clients. “We awarded BlueGranite the 2019 National U.S. System Integrator Partner of the Year award to recognize the company’s deep commitment to Databricks. BlueGranite’s rapid adoption and championship of the Azure Databricks unified analytics platform is removing big data and AI barriers for our mutual clients, accelerating innovation and insight.”----- Shannon St. Clair, Director of Consulting & SI Partners, Databricks Databricks, built specifically for Apache Spark workloads, provides a cloud-based environment for data scientists, data engineers, and business analysts to perform quick, interactive analysis, build models, and deploy workflows. The secure platform facilitates easy collaboration, making cloud-based data and AI simple by providing a single, notebook-oriented workspace environment that makes it easy for data scientists to create Spark clusters, ingest and explore data, build models, and share results with stakeholders. “We are thrilled to be recognized for our Azure Databricks’ advocacy and client success,” Matthew Mace, BlueGranite CEO, said. “We were fortunate to select Microsoft as our sole cloud partner, and just as fortunate that we chose to align with Databricks; these companies’ tools are helping our mutual clients innovate more effectively and efficiently execute their strategies for data platform modernization and AI adoption.” Nearly a quarter of BlueGranite’s staff has already stepped up this past year to earn their Databricks’ Apache Spark Certification. BlueGranite has also hosted a dozen nationwide Azure Databricks workshops and webinars – sharing detailed architecture overviews, hands-on labs, and use-case discovery with more than 600 participants across the Unites States – helping organizations master the transformative technology and supporting them when they need it. The high-value Azure Databricks enterprise software is in the midst of explosive growth and adoption. “Our team members and clients are really geared up about this platform,” Mace added. “Azure Databricks provides many of our clients the ability to quickly spin up and try out the technology, testing a specific use case to see for themselves if Azure Databricks will provide value. If not, they can turn off the service with no additional cost, which is a huge benefit of the cloud. There’s no software or hardware to buy. We’re there to help our clients with these proof-of-value engagements, assisting with architecture design, enterprise adoption strategy, and the build, deployment, and operationalization of these modern data and AI solutions.” About BlueGranite We use innovative Microsoft and Databricks’ technologies and other industry leading tools to help organizations quickly uncover business insights, building robust modern business intelligence and predictive analytics across all types of data. Contact BlueGranite today to explore how cloud-scale analytics with modern data platform, BI, and AI solutions can revolutionize your organization. About the Spark + AI Summit 2019 The annual Databricks Spark + AI Summit brings together developers, data engineers, data scientists, and decision makers from across the globe to dig into the latest Apache Spark and ML technologies."
"75" "Power BI real-time streaming is the perfect solution for IoT. Whether processing data through Azure Stream Analytics, the PubNub data stream network, or a custom Internet of Things solution, Power BI offers a fast, dynamic interface for displaying time-sensitive data. This is naturally what comes to mind when discussing Power BI real-time datasets.  However, I have found that Power BI real-time datasets have another great use. In situations where you have on-premise data you want to visualize and auto-refresh in Power BI but do not have a Power BI gateway setup, or you have a simple dataset that needs to be updated more frequently than the standard Power BI service refresh allows, Power BI real-time datasets can be the perfect solution! Of course, as with any technical solution, you must consider the positives and negatives of your approach before making a final decision. Below, I will go over some of the details of the different types of Power BI real-time datasets and demonstrate an example of using PowerShell to push data to a Power BI Push Dataset from a SQL Server data source. Types of Real-Time Datasets Power BI offers three types of real-time datasets: Push Dataset Streaming Dataset PubNub Streaming Dataset Microsoft has a great article describing the three types of real-time datasets. Instead of repeating what is in that article, I will provide a high-level summary of the key points to understand. With Push Datasets, your data is stored in the Power BI Service up to a maximum of 200k rows. If more than 200k rows are pushed to the dataset, rows are removed using a first-in first-out (FIFO) policy. Since the data is stored in a database within the service, Power BI Reports can be created with data from the Push Dataset. These Reports behave like any other Power BI report, making use of filters, custom visuals, etc.; however, there is one key difference. When visuals from a Push Dataset report are pinned to a Dashboard, the visual(s) will update in real time whenever the data is updated! Power BI Streaming Datasets are great when your data is only relevant in the moment. Power BI only stores the data in a temporary cache which is used to display your visual(s). Only tiles added directly to a dashboard can be used to visualize this data, as it is specifically designed for low-latency reporting. Dashboard tiles are limited to visualization types: Card, Line Chart, Clustered Bar Chart, Clustered Column Chart, and Gauge. PubNub Streaming Datasets are very similar to Power BI Streaming Datasets; however, PubNub Streaming Datasets are integrated with the PubNub data stream network. PubNub is a third-party company that offers real-time infrastructure-as-a-service for web, mobile, and IoT applications. If you are already using PubNub for your real-time streaming, connecting with Power BI is as simple as entering a dataset name, subscription key, channel name, and PAM authorization key. Below is a matrix summarizing the capabilities of the different real-time dataset types.  A few other important things to note about real-time datasets in Power BI is they cannot be joined to other datasets or combined in any way. Also, only three data types are available in the dataset: Text, Number, and DateTime. Finally, be sure to review the Power BI REST API limitations to understand the capabilities. For example, there is a maximum of 75 columns and 10,000 rows per single POST request to the Power BI REST API. Power BI Real-Time Push Dataset Demo In my demo, I will do a quick walkthrough of creating a new real-time Push Dataset in Power BI. Then, I will query a local copy of the AdventureWorks2017DW database and push some data to my new dataset using a PowerShell script. You will see how easy it is to get started with Power BI real-time datasets. I've been able to use them in quick POCs to produce results in a matter of minutes. To create a new real-time dataset in Power BI, click + Create from the Workspace you are working in, then click Streaming dataset. (Although the button is labeled Streaming dataset, you will be able to choose later whether you are creating a Streaming Dataset or a Push Dataset.)  Next, you will need to choose the source of your data for the real-time dataset: API, Azure Stream or PUBNUB. In the demo, I will be using the Power BI REST API (API). After choosing your source, you are then able to define your real-time dataset using a unique Name and the list of Values/Fields in your stream. Also, here is where you will choose whether the dataset is a Push Dataset or Streaming Dataset. When Historic Data Analysis is off, the dataset will be a Streaming Dataset. When Historic Data Analysis is on, your data will be stored in the Power BI service and the dataset will be a Push Dataset. The simple dataset I created from my AdventureWorks query contains six values/fields. When using a Push Dataset, it is often helpful to include a timestamp column to help with versioning your data. You will notice that as you enter new values/fields, a sample JSON document is created for you in the browser. After creating your real-time dataset, you will receive the Push URL for the dataset as well as sample scripts for pushing to your dataset (Raw, cURL, PowerShell). This Push URL is all you need to populate your dataset. Also, in the screenshot below, you will see the sample PowerShell script that was provided by the Power BI Service.  Below is a screenshot of the PowerShell script I will be using in my demo. The script makes use of the Invoke-Sqlcmd cmdlet, which is supported by the SQL Server SQLCMD utility. The cmdlet connects to a named instance of the SQL Database Engine on my machine and runs the T-SQL script I defined. The results are returned to a DataSet object that I then loop through and post to my Power BI Push Dataset using the Invoke-WebRequest cmdlet in PowerShell and the Push URL that was captured in a previous step. You can definitely get more sophisticated with your PowerShell scripting, if necessary (i.e. if you need to break your data into 10,000 row subsets or join multiple Dataset objects).  Your PowerShell script can be run manually or can be scheduled to run using Windows Task Scheduler or any other scheduling tool you use in your environment. Since the script makes use of the Power BI REST API, you are able to add up to 1,000,000 rows per hour, per dataset. Depending on your source query, this could allow you to update your dataset very rapidly since it does not use the same refresh intervals as the Scheduled Refresh in the Power BI Service. This can be very helpful when using a real-time Push Dataset in combination with a Power BI report containing visuals that update in real time. As previously mentioned, Push Datasets can be used to build full-featured Power BI reports, as shown in the screenshot below. Power BI real-time datasets are perfect for solutions involving device telemetry, social network feeds, geospatial services, log files and gaming activity. They can also be great for quick proof of concepts or for situations where you need to see your data in real time or near-real time. Contact us today for assistance with your next Power BI project."
"76" "I recently deployed an innovative production implementation using Azure Databricks. I worked with a large retail industry client to implement a Data Platform and BI solution that measures retail sales, market share, and product share for partnering firms. The company’s internal team and project sponsors were very familiar with the cutting-edge capabilities of Azure Data Services; this allowed us to collaboratively design and build a progressive solution to handle their Data Engineering and Artificial Intelligence use cases. For one specific solution, we designed an architecture and coding framework to deploy client-agnostic solutions using metadata. The solution template allows the company to very quickly ramp its new clients on the application by simply filling out some data source metadata and pushing some buttons. The organization can now get its new clients up and running in a couple days, plugging each one into amazing insights on their retail data.   This architecture includes the use of many of the data services in Azure. Azure Blob: used to land raw flat file-based data. Azure Data Lake Store: used to store data in Data Lake and Data Mart database. This is the primary storage facility for the implemented architecture. Azure Data Factory: event-based data orchestration pipeline execution. Azure Databricks: data engineering and artificial intelligence compute on top of the data using Apache Spark. This is the primary compute hub for the implemented architecture. Azure Analysis Services: rich semantic layer for enterprise-scale reporting. Azure Logic Apps: used for processing Azure Analysis Services. Power BI Premium: used to visualize and display retail data insights. Git Integration: all source code was stored in GitHub. Databricks and Azure Data Factory have native Git integration, which made it easy. Azure DevOps: Azure DevOps was used for Continuous Integration and Continuous Development (CICD) pipelines. NO relational database solution was used in this project  This Modern Data Platform architecture (or Data Lake architecture) provides many benefits to the client. Highly Scalable & Elastic: compute can be turned on or off on demand. While off, the solution incurs only storage charges. The solution also has built-in cost controls and auto-shutdown after periods of inactivity. Compute can also be scaled up/down as needed within minutes and has auto-scale up or down capabilities based upon resource utilization. Event-Driven Processing: data orchestration pipelines are event driven and only begin when initiated by upstream processes. This eliminates running jobs when data isn’t even ready. Flexibility of Multiple Languages: the application is programmed using a combination of SQL, Python, and Scala. SQL is primary used for data transformations. Python is being used for the flexibility of enumerating metadata and/or constructing dynamic code. Scala is being used to create data ingestion repeatability. The platform can also accept R and Java. Utilizing Fast and Cheap Storage: the platform is utilizing fast, cheap, and secure storage from Azure Data Lake Store, which is essentially the Hadoop Distributed File System (HDFS) as a platform service, built for analytics. Mixed-Use Case Support: the solution platform can handle Data Engineering, Artificial Intelligence (AI), and Streaming use cases in the same platform with zero additional tools or configuration. Our production solution is currently Data Engineering-focused, but the client is also prototyping AI use cases. No Relational Database Needed: while Azure SQL database is a powerful service, it currently stays on in a persistent state and has no Pause functionality (except for Azure SQL Data Warehouse, which wasn’t a good fit for this client). Azure DevOps: the client led the charge here, but Azure DevOps provided a crucial ability for the project. DevOps pipelines were created to read code directly from GitHub and deploy to the various environments automatically. It takes some setup, but Azure DevOps ended up saving a lot of time. Flexibility and Scalability were Key Benefits Overall, Databricks provides a solid platform for creating analytic solutions in Azure and even housing the analytic databases used for the solution when using Databricks Delta. The platform is highly flexible. We were able to create new Databricks services instantly and launch Spark clusters in less than 5 minutes to immediately begin coding. Being able to bounce back and forth between SQL and Python, depending on need, was immensely beneficial. This language flexibility also allowed for us to ramp new resources very quickly. Our project team consisted of client and BlueGranite developers, who were all familiar/comfortable with different languages as their primary go-to. The ability to scale up and down and turn the solution on or off automagically saved time and money. Finally, the ability to have the solution pivot between Data Engineering and AI with zero additional environment configuration made it a no-brainer to begin testing Machine Learning and Artificial Intelligence capabilities. Learning What Works Best Like any new technology solution (Databricks and Spark aren’t new, just new to Azure), there is going to be a learning curve and maturity to the product in general. While we found the Databricks integration with other data services in Azure to be good, each integration with another service is at varying levels of maturity. We had to experiment to find what worked best for our architecture. One example is the integration between Azure Analysis Services (AAS) and Databricks; Power BI has a native connector to Databricks, but this connector hasn’t yet made it to AAS. To compensate for this, we had to deploy a Virtual Machine with the Power BI Data Gateway and install Spark drivers in order to make the connection to Databricks from AAS. This wasn’t a show stopper, but we’ll be happy when AAS has a more native Databricks connection. One of our biggest project challenges was actually not related to Databricks or Azure at all. We suffered from the typical garbage in, garbage out scenario. At one point during development, we had a significant number of bugs reported, but they ended up being related to issues in the source files our platform was accepting. We had to add data validations upstream in order to remediate the issue. This was actually a wake-up call that sometimes process needs to be addressed and finalized prior to creating a technology solution. Overall, using Databricks as the backbone for our client’s solution was a huge success. If you’d like to learn more about how Databricks can work in your environment, contact BlueGranite today."
"77" "Power BI, PowerApps, and Flow Unlock World of Enterprise Capabilities As the world’s population continues to increase, and as internet usage reaches more and more people, the creation of data rises exponentially. It is estimated that by 2020, 1.7 MB of data will be created for every person, every second! It’s no surprise that businesses pursue new ways to harness this data effectively to drive outcomes. Leveraging Microsoft’s Power Platform – comprised of Power BI, PowerApps, and Flow – is one way that organizations can use tools to analyze, act, and automate. Each of these tools can be used on its own to solve a business problem, but it is in the integration of the three that the real value stands out.   Power BI – The Analyzer Power BI is a robust data analytics and visualization tool, with native capability to connect to dozens of sources, as well as the ability to create custom connectors. Power BI leverages the Power BI Desktop application to build data models and reports, and the online Power BI Service to deliver reports and dashboards to users. Power BI is also available on tablets and mobile devices, allowing users to make data-informed decisions from anywhere. PowerApps – The Actor PowerApps is an Application Platform as a Service (aPaaS) designed for users to build high-productivity business applications. PowerApps is completely cloud-based, meaning that all development and deployment is done through a web browser. Apps can be developed for both web and mobile devices, allowing users to take action in the field. PowerApps can connect to more than 200 different sources including Excel, SharePoint, SQL Server, Dynamics 365, Azure, and social media applications. PowerApps can leverage device features such as cameras, microphones, and GPS. Flow – The Automator  Flow connects different sources and applications to create automated workflows. Flow can collect data (e.g., adding a document to a SharePoint library), send notifications (e.g., alerting when you have a new follower on Twitter), and create, use, and share approval workflows. Flow can connect with more than 100 different sources and REST APIs. Flows can be triggered automatically by an event, triggered manually by pressing a button, or run on a schedule. Check out this post and this post for additional information on Flow. Putting It All Together To demonstrate the integration of these three products, I published a simple Power BI report with data from a fictional project management team. This report page has slicers for Project Leads and Teams, a matrix showing the number of projects by Lead, a line chart displaying the count of projects by due date through the end of the year, and a donut chart showing Project Status.  I also added an alert to the dashboard that sends an email when the number of “Delayed” projects changes to greater than 0.  Since the alert is sent only when the data changes, I would like to set a reminder to follow up with my team after three days. I can do that by clicking the “Use Microsoft Flow to trigger additional actions” link in the alert in order to create a custom Flow.  I selected the “Create event (V2)” action from the Flow builder screen and added simple functions to create a reminder on my calendar three days after the Power BI alert is triggered.  After receiving the Power BI alert, I navigate to the report and notice that four projects are in a delayed status. Through interacting with the report, I learn that all of these Delayed projects are owned by the same project lead:  Let’s embed a PowerApp so I can communicate directly with the project lead without having to cut and paste screenshots into an email or messaging app. Adding this functionality directly to the Power BI report reduces friction between having to “switch gears” and makes data immediately actionable:  From here, we can choose to add an existing app or create a new one. I created a new app based on data available in this Power BI report. After selecting the fields I needed – Project Lead, Project, and Status – I created the PowerApp in my web browser in just a few minutes with no significant coding required. PowerApps is meant to be accessible to users at all skill levels to quickly spin up new applications with minimal to no assistance from IT, though an experienced developer can take app design and function to the next level.  Here, I selected the “Delayed” projects from the donut chart. Notice that the other tiles on the report reflect this selection, including the fully-integrated PowerApp:  I would like to email the Project Lead about the fourth delayed project at “TypicalCompanyC.” Selecting the arrow next to the project name in the PowerApp navigates me to an email screen, where data is prepopulated based on selection in the reports.  Looking Ahead The demonstration above barely scratches the surface of what is possible when you choose to combine the analytical, actionable, and automatable components of Power BI, PowerApps, and Flow. Other use cases could include viewing and approving employee time-off requests and sending follow up confirmations to both the employee and manager; managing inventory and ordering additional supplies; and much more. Additionally, Microsoft plans to release enhancements to the Power Platform and Dynamics 365 this year, starting in April. Keep your eyes out for additional information, and review the April 2019 release notes here. If you want to learn more about how BlueGranite can help with your Power Platform analysis needs,  contact us today and we will be happy to answer your questions."
"78" "Long before \"Big Data\" was a buzzword in the business realm, geneticists, bioinformaticians, and computational biologists had been dealing with large-scale -omics data for quite some time. This data includes DNA/RNA samples, annotated variants, genotype/phenotype analyses, and more. When it comes to the large amounts of data and numerous steps it takes to get to the insights for which you're looking, processing genomic data is no small feat. Plus, biological data comes in a variety of shapes and formats, each adding its own bit of complexity to your analysis process. Azure Databricks, as I'm sure you're familiar with, is the premier platform for performing massively parallel processing tasks in the cloud. This platform serves as an optimized Spark service for users looking to scale up their ETL and Machine Learning pipelines. However, recent efforts in the life science development space have made some common bioinformatics tools available on the Spark platform.  Today, we'll introduce a specialized runtime for Health and Life Sciences soon to be available on Databricks and highlight a few Spark-based libraries that you can begin using today. Databricks Runtime for Health and Life Sciences The Databricks Runtime for Health and Life Sciences is a specialized version of Databricks that has been optimized for working with genomic and biomedical data. It is a component of Databricks' Unified Analytics Platform for Genomics.  POWER YOUR PIPELINES <1.5 hours Run your alignment and variant calls in less than an hour and a half   RAPID RESULTS 60-100X faster Tertiary analytics 60-100x faster on Databricks compared to open source Apache SparkTM   MORE EFFECTIVE TEAMS 30% + productive Leading healthcare company improved productivity 30% with Databricks' unified analytics  Source: https://databricks.com/product/genomics To sign up for the HLS Runtime Preview, click here. Included in the HLS Runtime: A fast, scalable DNASeq pipeline Spark SQL optimizations for common query patterns Hail 0.2 integration Popular open-source libraries, optimized for performance and reliability ADAM GATK Hadoop-BAM  Reference data (GRCh37 or 38, known SNP sites) In addition to the support for a few Spark-based genomics libraries (which we'll discuss in a bit), this runtime also includes support for various file types seen in genomics data. For example, just as you would use spark.read.format(\"csv\").load(\"file.csv\") to easily read in CSV files, you can use a very similar approach to read and write VCF and BGEN files. VCF and BGEN ## Read in VCF datadf = spark.read.format(\"com.databricks.vcf\").load(\"file.vcf\")## Write out VCF datadf.write.format(\"com.databricks.vcf\").save(\"newfile.vcf\")## Read in BGEN datadf = spark.read.format(\"com.databricks.bgen\").load(\"file.bgen\")## Write out BGEN datadf.write.format(\"com.databricks.bgen\").save(\"newfile.bgen\") An example Databricks notebook for working with variant data can be found here. DNASeq Pipeline A common pipeline for genomic analysis is the Genome Analysis Toolkit (GATK) by the Broad Institute. GATK creates best practice workflows for various tasks from data pre-processing to variant discovery and beyond. Using these best practices allows for research labs to have a standardized operation pipeline for performing analyses.  Source In the HLS Runtime, Databricks now includes a GATK-compliant DNASeq pipeline for short read alignment, variant calling, and variant annotation and an RNASeq pipeline for handling short read alignments and quantification. These pipelines make it easy to get started analyzing genomics data using popular techniques such as SnpEff annotation, STAR alignments, and ADAM. Plus, this allows for the use of a variety of other common input formats such as SAM, BAM, CRAM, Parquet, and FASTQ. An example Databricks notebook for using the DNASeq pipeline can be found here. Hail 0.2 Hail is an open-source, scalable framework for genomic data analysis and exploration. This project is supported by the Neale Lab out of Harvard Medical School. In the most recent edition of Hail (0.2), support for Spark (and thus Databricks) has been enabled.  Hail allows for the many different types of analyses from Genome-Wide Association Studies (GWAS), annotation, expression analysis, and visualization. Hail is designed to scale from a single laptop to a cluster with little to no code changes and is also meant for use on datasets that do not fit in memory. Once you have the HLS runtime enabled in Databricks, getting started with Hail is quite simple.  ## Set the environment variable: ENABLE_HAIL=true import hail as hl hl.init(sc, idempotent=True)  An example Databricks notebook for using Hail can be found here. VariantSpark VariantSpark, by O'Brien et al. (2015), is an interesting library for Spark. While other genomics packages provide general bioinformatics analysis of genetic datasets, this library provides a machine learning analysis framework for analyzing genomic variants using the Spark engine.  Source VariantSpark prides itself in being an efficient (fast) and accurate contender against other machine learning implementations, such as Spark's own MLlib, randomForest in R, H2O, and more.    Runtime vs. accuracy of six available implementations showing that VariantSpark has the highest accuracy and is substantially faster than its competitors, enabling point-of-care diagnostics within 30 minutes instead of 24h.   Source VariantSpark is developed for data with many samples and many features. It includes machine learning methods for clustering (k-Means) and classification (Cursed Forest). Though VariantSpark was originally developed for genomic variant data, it can cater to any feature-based dataset, such as methylation, transcription, and even non-biological applications. An example Databricks notebook for using VariantSpark can be found here. Getting Started Enabling the Genomics Runtime is easy. Simply go into the Admin Console in your Databricks workspace, click the Advanced tab, then enable the Databricks Runtime for Genomics. See the Azure Databricks Documentation for genomics pipeline examples here. Whether your bioinformatics practice is completely on-premise today or is growing into the Azure cloud, BlueGranite can help you get started using Azure Databricks. In addition to scaling up your analysis pipelines, setting up additional services, such as a flexible storage and visualization solutions, is also important. Since Azure Databricks easily integrates with Azure Storage (such as blob storage or Data Lake Store) and Power BI, using the Azure cloud from end-to-end is the best way to scale your Health and Life Science practice for faster, deeper insight."
"79" "For many companies, the initial attraction to Azure Databricks is the platform’s ability to process big data in a fast, secure, and collaborative environment. However, another highly advantageous feature is the Databricks dashboard. Dashboards are created directly through an existing Databricks notebook via a single click. They are essentially a presentation-friendly view of a Databricks notebook.  One of the more challenging tasks for data scientists and engineers is explaining the function and results of their code in both an interesting and intelligible manner to key stakeholders. While clients with programming experience may enjoy delving into lines of code, clients who focus more on marketing may place greater emphasis on result presentations. A commonly attempted solution is to piece together a combination of text, code, and results in a PowerPoint presentation, a time-consuming process that still may fail to capture an accurate overview of the entire analysis. This post highlights some of the dashboard’s useful features that aid in resolving these issues through a use case of classifying salary brackets. For this example, the data derives from census information of individuals, along with their annual income. The goal is to predict if an individual earns less than or equal to $50,000 or more than $50,000. Data Exploration The first step in any type of analysis is to understand the dataset itself. A Databricks dashboard can provide a concise format in which to present relevant information about the data to clients, as well as a quick reference for analysts when returning to a project. To create this dashboard, a user can simply switch to Dashboard view instead of Code view under the View tab. The user can either click on an existing dashboard or create a new one. Creating a new dashboard will automatically display any of the visualizations present in the notebook. Customization of the dashboard is easily achieved by clicking on the chart icon in the top right corner of the desired command cells to add new elements.   In this example, the Exploring the Data Dashboard shown below provides a general description of the dataset, as well as highlighting interesting trends. From this dashboard we can observe that the age is right-skewed and that there are more individuals in the less than $50,000 bracket than the greater than $50,000 bracket. We can also see that a possible key relationship exists between income and gender. From the pie charts, males seem more likely to fall in the over $50,000 bracket than females. To prove this is a statistically significant difference, the dashboard displays a breakdown of the descriptive statistics associated with each gender, as well as the results of a 2 sample T-test. Creating this type of initial data exploration dashboard can serve as an excellent way for data scientists to organize their thoughts about potential influential factors to consider during analysis, as well as highlight to clients possibly undiscovered trends in their data.  Another valuable feature of the Databricks dashboard is the ability to easily show the code associated with a certain visualization. For instance, if a client is interested in the generation of the Age Distribution graph, clicking on the Go to command icon appearing in the top right corner will automatically switch to the notebook view in the exact location of the command. This tool provides a convenient method to demonstrate both results and code without aimlessly scrolling through several lines or copying and pasting only small, dispersed snippets. It also provides data scientists the convenience of easily locating specific code for editing purposes.   Data Modeling Dashboards can also be created to present the method and results of the data model. For clients, this aids in making complex analyses more tangible. Data scientists gain benefit from having all the key results organized in one place, an extremely useful feature especially if additional analysis will be performed at a later date. In the Data Analysis dashboard below, we can see that logistic regression was applied to the salary classification problem, as well as the breakdown of the training and testing datasets. The Feature Selection graph illustrates that only 13 of the original 14 features are considered in the model. These features are then listed on the right side for easy reference. The next step in creating a model is finding the optimal parameter values. For logistic regression, this is the C value, or a penalty parameter, that reduces overfitting. The Optimal C Parameter chart compares both the area under the curve and accuracy scores for various C values between 1 and 3. For both score types, C = 2.75 provided the highest value. Using this parameter, the model is then evaluated using 10-fold cross-validation. From the K-fold box in the dashboard, we can see the model has both a high accuracy and area under the curve score. The Confusion Matrix table also illustrates the model’s hits and misses. Finally, the resulting model is also easily shared on the dashboard through listing the variables’ coefficients and their associated p-values. Markup command cells can also be added to the dashboard to create an overall summary of the findings as in the Conclusion box below.  Once a dashboard is completed it can be shared via a URL by clicking on the Present Dashboard tab in the left side pane. From this URL, users can examine specific data points by hovering over the charts, reorder data in tables, and even update the dashboard to reflect the most recent code in the notebook. This capability of user interaction and live data connection creates sophisticated presentations that are both interesting and beneficial for clients, more so than static PowerPoints or Word documents.  Overall, Azure Databricks offers data scientists the potential to both easily analyze and present big data. The dashboard’s ability to interactively display results, text, and code in an easily tangible and succinct manner not only saves data scientists time and resources but also provides clients with an interesting view of the data and process. Viewing an analysis in this manner fosters greater client understanding and reception of all the steps required in machine learning, including data exploration, modeling, tuning, and model evaluation. Even the most accurate and thorough model can be declined by stakeholders, if its presentation fails to accurately reflect its value, which is why the dashboard is such a crucial, complementary feature to the overall processing ability of Databricks. Contact BlueGranite today to learn more about how your organization can capitalize upon the data-driven insights of the dashboards available with Azure Databricks."
"80" "Since the introduction of Azure Databricks in 2018, there has been a lot of excitement around the potential of this Unified Analytics Platform and how it can simplify the complexities of deploying cloud-based analytics solutions.  One of the primary benefits of Azure Databricks is its ability to integrate with many other data environments to pull data through an ETL or ELT process. Let’s examine each of the E, L, and T to learn how Azure Databricks can help ease us into a cloud solution. Connecting to Source Data – (E)xtract Azure Databricks, with Spark as its underlying processing engine, is a distributed-processing platform designed to connect to a distributed file system. When deployed on-premises, data is typically read from the Hadoop Distributed File System (HDFS). In the Azure cloud, there are several storage options that can be utilized. Azure Databricks is equipped right out of the box to connect to Azure Blob Storage, Azure Data Lake Store Gen1, and Azure Data Lake Store Gen2. As a matter of fact, an Azure Storage account is created for you automatically when you create an Azure Databricks workspace, and is mapped to the root of dfbs:// Additional Azure Storage and/or Data Lake accounts can be connected using mount points, or by directly accessing them using wasb://, adls://, or abfs:// syntax In addition to connecting to cloud storage provided in Azure, Azure Databricks can also extract data from competitive cloud storage environments, such as Amazon S3 buckets. If the solution is using an Azure-based database provider, Azure Databricks includes connectivity options for Azure SQL Database (DB) using the Java Database Connectivity (JDBC) driver, Azure SQL Data Warehouse (again using JDBC) and Azure Cosmos DB using a specially designed adapter. In the competitor landscape, connections to Snowflake, Cassandra, MongoDB, Neo4j, and more are possible using libraries provided by Databricks. If your data is stored on-premises in a database, you can also connect directly to it using Azure Databricks configured with an Azure Virtual Network (VNet). This is a great option, as there is no need to first copy your on-premises database data to a storage platform. Not all data is sitting still. Ingesting data in motion is also supported with direct connections to Azure Event Hubs and Azure Event Hubs for Apache Kafka using libraries provided by Microsoft and Databricks. External or on-premises Kafka connections are also supported, and may require additional Azure VNet configuration. One important consideration when connected to source systems of data is security of authentication credentials. Azure Databricks has built-in connectivity with Azure Key Vault when using a Premium workspace.  Data Engineering – (T)ransform Azure Databricks’ key feature is its ability to munge data at scale. Several application programming interfaces (APIs) are available with Spark to help data engineers and data scientists transform data from its raw format into a more meaningful shape for analysis. DataFrames is the primary API used in Azure Databricks. A DataFrame is an abstraction of source data resembling a table format. The DataFrames API is available in Scala, Python, R, or SQL – meaning a developer can choose the language that best suits their experience. Writing DataFrame transformations is similar to writing SQL statements. Common operations that most data engineers need to complete include: Column projection Data cleansing Joining/merging Aggregation/calculations Even without deep knowledge of Spark, the DataFrames API is easy to understand. Most operations are defined using a syntax that resembles declarative SQL statements.  For streaming data, the DataFrames API has been extended with structured streaming capabilities that allow nearly identical code to be used for data at rest and data in motion. This means that adding real-time capabilities to an existing Azure Databricks application can be as easy as adding a few lines of configuration code. Common functions, such as data cleansing and aggregation, are written with equivalent code in a streaming DataFrame, as in a static one. This is a huge benefit that means that Azure Databricks developers do not have to relearn a new API to convert an application from a batch-load system to a real-time solution.  When data doesn’t fit the mold of traditional structured format (e.g., rows and columns), such as parent-child hierarchies, social network relationships, or clickstream analysis, a graph processing API has also been bolted onto the DataFrames API. GraphFrames, as the graph API is known, includes several key graph processing algorithms that can be used in a very similar manner to working with a traditional DataFrame. To setup a new graph for processing, begin with two DataFrames: one defining the vertices (objects or points along a graph) and a second defining the edges (relationship between two vertices). The two DataFrames are combined into a GraphFrame which can then be processed using common graph algorithms to calculate results including page rank, mean distance, number of stops between two points, parent/child, and many more. Here’s an example of calculating a page rank in just four lines of code!  Using a common set of APIs offers many benefits, not the least of which is performance optimization. Since advanced operations are based on the DataFrames API, those operations can take advantage of all of the optimizations included with Spark 2.0’s Catalyst engine. There are benefits for developers and engineers, as well. Having a common set of APIs reduces the amount of training a developer needs to become proficient. It also reduces the amount of development time to build an enterprise-ready solution versus implementing a solution across multiple platforms in a traditional n-tier application. Providing Data to Wider Audience – (L)oad Once raw data has been transformed, the final step is to make it available to data scientists, business analysts, and other users for consumption. As above, there are many options available to provide data for later consumption. First and foremost, Azure Databricks supports analytic sandboxes by providing a database-like environment of tables and views that describe the Data Lake. Data scientists and analysts can query data stored in the Data Lake using common SQL code. A premium Azure Databricks workspace also provides JDBC/ODBC (Open Database Connectivity) access, so many enterprise BI tools, like Microsoft Power BI, can directly connect to retrieve data for ad-hoc dashboards and reports. Tables in Azure Databricks can be based on data in many formats; CSV, JSON, Parquet – but an attractive modern option is to use Databricks Delta. Delta in as extension of Parquet that is unique to Databricks. It provides a layer of data management and performance optimizations most often seen in dedicated Data Warehousing platforms like Azure SQL Data Warehouse (DW). When structured analytics for a broader corporate user base is the highest priority, Azure Databricks can efficiently load the common Azure database platforms Azure SQL DW and Azure SQL Database (DB). These platforms are used to build dimensional models for analytics that are used in BI solutions. Relational database management systems (RDBMS) provide a structured environment to store data, which is a common need for end users of BI systems. These platforms are also designed from the ground up for interactive queries, so the reports developed will be highly responsive. If you are hosting a data warehouse in a competitive environment, such as Snowflake on Amazon Web Services (AWS), Azure Databricks can also efficiently load transformed data in that environment. Many libraries are available in Databricks that allow data to be loaded to any cloud. Often, the results of AI are used in operational applications. Azure Databricks has a unique ability to use the Azure backbone to load model results into Cosmos DB – a polyglot database used by many web and mobile applications. Storing results of advanced models that predict product recommendations, form suggestions, or security check results couldn’t be easier with Azure Databricks. No matter who the data audience is, Azure Databricks provides a method to store, update, and manage data so that it’s always available at the right time and in the right format.  Conclusion Azure Databricks is a unique offering in that it combines the ability to connect to many different data sources, uses a variety of commonly designed APIs, and loads data from any data store, in an easy-to-manage platform. Since it’s based on a highly scalable distributed platform, Azure Databricks has the capability to be used in scenarios that involve petabytes of data – but is also designed to work well with smaller, more manageable terabyte-level solutions. The developer API is unified in that most of the common operations are based on DataFrames – so whether you’re working with data at rest, data in motion, structured data, unstructured data, graph processing, or traditional “rectangular” processing; there will only be one base API to learn – all operations are based on that API. Many different platforms do not need to be integrated to build an enterprise-level analytics and AI solution. Finally, Azure Databricks supports loading transformed and cleansed data to many different data sources so users can consume data in the most appropriate way possible. Be it analytic sandboxes for data scientists, corporate BI portals, or web and mobile applications – cloud scale analytics is now in reach for your entire user base. If your organization wants to explore a secure workspace where data scientists, engineers, and analysts collaborate, contact us today. BlueGranite’s experts offer an in-depth, 1- to 2-week Azure Databricks proof of concept, giving groups a chance to dive in to the unified analytics platform’s vast potential At BlueGranite, we know there are lots of options. If Azure Databricks doesn’t seem like the right fit for your data management solution, perhaps you might like to review Azure Data Factory. Data Factory is an easy-to-use, GUI-based, data engineering tool that allows complex data pipelines and transformations to be built to move data between on-premises and cloud locations. It even uses Azure Databricks behind the scenes to perform its most complex tasks. BlueGranite consultant Merrill Aldrich wrote up a great introduction to Azure Data Factory here."
"81" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/tour-the-cortana-intelligence-suite\"   },   \"headline\": \"CORTANA INTELLIGENCE SUITE TOUR\",   \"description\": \"The Cortana Intelligence Suite is a collection of independent, but fully integrated data and analytic platform tools offered by Microsoft Azure. Customers can create complete streaming or batch-loaded Business Intelligence and Advanced Analytics solutions in these platforms without having to deploy or manage a single server. Each tool provides unique capabilities (with some overlap) and integrates together seamlessly. Customers can even mix and match tools in the suite to meet use case requirements without having to pay for each one because they are all considered to be pay-as-you-go cloud services.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/CortanaIntelligenceSuite.png?width=725&name=CortanaIntelligenceSuite.png\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Leo Furlong\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 15     }   },   \"datePublished\": \"2019-03-14\",   \"dateModified\": \"2019-06-14\" } This is the second post in the series Modernize your Business Intelligence Architecture with the Cortana Intelligence Suite. To see the first post, follow this link. The Cortana Intelligence Suite is a collection of independent, but fully integrated data and analytic platform tools offered by Microsoft Azure. Customers can create complete streaming or batch-loaded Business Intelligence and Advanced Analytics solutions in these platforms without having to deploy or manage a single server. Each tool provides unique capabilities (with some overlap) and integrates together seamlessly. Customers can even mix and match tools in the suite to meet use case requirements without having to pay for each one because they are all considered to be pay-as-you-go cloud services.  Comprehensive Architecture The architecture diagram below provides an example of how the Cortana Intelligence Suite services can be used to create a comprehensive real-time and batch solution. Starting at the top left of the diagram, code can be added to apps and IoT devices to send events in real time to Event Hubs. Stream Analytics can then be used to consume and publish the data from Event Hubs and simultaneously write a copy to Data Lake Store, perform analysis on the data and send the results to Power BI, and even run the data through a machine learning model.  On the batch side, Data Factory can be used to orchestrate the load of on-premises and cloud-born data into Data Lake Store and SQL DW. Aggregates from SQL DW can be stored in Analysis Services for a rich semantic-layer model for consumption from BI tools. Finally, Power BI can be used to create real-time streaming dashboards, or blended data models that mash up stored streaming data with batch data from on-premises. If that wasn’t enough, Power BI can also be used to create alerts. A closer look at each tool is highlighted in the sections below. Event Hubs – Ingest It Azure Event Hubs is a highly scalable data streaming platform and event ingestion service. It can be scaled up or down and can accept millions of events per second. It also provides robust security for letting apps, devices, and software connect to Azure to send data, events, and telemetry. Once loaded, the data stays in the Hub for a specified retention period so it can be consumed using other applications. This is the initiation point of getting streaming data into Azure for analysis. Stream Analytics - Stream It Stream Analytics is an event processing service that permits powerful, real-time analysis of your data. It allows you to write SQL code against streaming data which is executed in real time. Using Stream Analytics, you can integrate the data from Event Hubs to multiple tools simultaneously. For example, one Stream Analytics job could be copying the data to Data Lake Store while another could be sending the results of a SQL query to Power BI in real time. Stream Analytics uses a familiar SQL language providing common analytic SQL techniques such as Joins, CTEs, Where, Group By, Having, and Aggregations. Another consideration is the concept of time-series functions, which let you look forward or backward on the data. One of the coolest aspects of Stream Analytics is the ability to add machine learning code as a Function and process machine learning models on the data in real time. Data Lake Store – Store It Data Lake Store is a fully managed Big Data store in the cloud. It is essentially the Hadoop Distributed File System (HDFS) as a cloud service. It provides all the benefits of cheap HDFS storage without the need to deploy and manage a persistent Hadoop cluster. There are no file size limits and you can scale your data storage into petabytes. It also integrates with Azure Active Directory for easy access and security, plus it’s accessible via all your Big Data compute tools such as HD Insight. SQL DW – Relate It SQL Data Warehouse is a SQL Server in the cloud for massive scale. It uses a database cluster-based Massively Parallel Processing Architecture (MPP) to allow customers to scale from a few terabytes to one petabyte of data. It’s highly elastic and scalable – companies can scale their cluster up and down in minutes. It can even be paused to eliminate compute charges. An important feature is its ability to communicate with Blob Storage or Data Lake Store using PolyBase. With this integration link, storage directories can be mounted to SQL DW as an eternal table and queried just like a regular SQL table. Analysis Services – Speed It Azure Analysis Services is a proven analysis engine that has existed on-premises with SQL Server since the 2012 version of the product, and is the same engine used by Power BI. Through Azure Analysis Services, customers can deploy blazing fast in-memory analytic models through a platform service without having to manage hardware, SQL Server configuration, and licensing. It is a scalable service that allows companies to shrink or grow with their needs. Users can create blended data models using familiar tools through Visual Studio and SQL Server Data Tools and deploy them to the cloud. Additionally, if a customer’s Power BI model grows in popularity or size beyond Power BI’s size limits, models can be migrated to Analysis Services which can handle the elevated requirements. Power BI – Visualize It Power BI is an industry leading BI platform that allows you to create stunning visualizations with all your data, whether it is on-premises or in the cloud. You can use it to deploy corporate and self-service capabilities to your organization, create your own open-source visualizations, and create alerts on your data to generate actionable dashboards. ML – Learn It Azure Machine Learning is a fully managed machine learning service. It gives companies the ability to create machine learning models using a drag and drop interface, which in turn reduces the data science learning curve significantly. Additionally, it allows you to deploy machine learning code as a web service so that it can be used by all your applications. If your organization has already invested in machine learning code within open-source tools such as R or Python, Azure ML allows you to deploy your native open-source code and leverage it in your Cortana solutions. Data Factory – Move It Azure Data Factory is a fully managed data orchestration tool that allows for the creation of workflows/pipelines that ingest, prepare, transform, analyze, and publish data from on-premises or cloud sources to any of the tools available in the Cortana Intelligence Suite. It also provides monitoring and management of these pipelines through a rich portal interface. To the data practitioner, Data Factory is not SSIS in the cloud, but closely resembles the control flow of SSIS. HDInsight – Hadoop It  Azure HDInsight is a platform distribution of Hadoop living in the Azure cloud that supports virtually any Big Data and Advanced Analytics use case. Customers can easily deploy different configurations of a fully managed Hadoop cluster in minutes without having to manage and plan a hardware deployment. Compute and storage are separate, so customers can deploy a cluster, do their work, and tear it down to eliminate compute charges – all while keeping their data intact. HDInsight supports many of the popular Big Data technologies like base Hadoop, Hadoop with Hive LLAP, Storm, Spark, HBase, Kafka, and R Server integration. If your Apache project isn’t listed, it is also possible to customize your deployment of HDInsight to add it. Data Lake Analytics – Distribute It Data Lake Analytics is a platform service that allows companies to process Big Data workloads on a per job basis, without having to physically plan and deploy distributed infrastructure. When a job is submitted to the service, Data Lake Analytics automatically fires up distributed compute resources, processes the workload, and then spins down the resources. It is powered by U-SQL, a powerful new language focused around SQL, that brings in capabilities from popular languages like C#, R, Python, and Microsoft Cognitive Services. Since most BI developers already know T-SQL, this drastically reduces the learning curve to process Big Data workloads. Data Catalog – Document It Azure Data Catalog is an enterprise-wide metadata catalog that allows users to search, discover, and understand their data in one place. It’s a fully managed service where users are able to register, define ownership, enrich, tag, document, discover, understand, and consume data sources. Cosmos DB – NoSQL It Cosmos DB (formerly Document DB) is a blazing fast and geo-scalable, multi-model DB provided through Azure. Cosmos DB gives developers the capability to deploy document, key/value, column family, or graph databases through the same service. Using Cosmos DB, developers can deploy their intelligence applications at scale with the models they prefer. Cognitive Services – AI It Microsoft not only provides organizations with the tools and platforms to make their own intelligent applications, but they also provide prebuilt intelligence as well. Through Cognitive Services, Microsoft provides additional services, APIs, and SDKs that automagically provide Vision, Speech, Language, Knowledge, and Search-based intelligence. Customers can easily enhance their applications to call a Microsoft Cognitive Service API and perform sentiment analysis on text, perform facial recognition, or search the web. It is easy to leverage the Cognitive Services in your own app or one of the apps previously listed such as Power BI. Bot Framework – Automate It The Bot Framework is a platform for building powerful and integrated Bots that allow users to interact with custom-built apps in a meaningful and conversational way. It integrates with a custom app or enterprise apps deployed in your organization like Skype for Business, Teams, Slack, email, SMS Text, and many more. You can also enable your Bot with sophisticated intellect through Cognitive Services for Artificial Intelligence. Cortana – Speak It The Cortana Digital Assistant allows companies to build integrated apps that can be deployed/embedded directly to end-user devices. Users speak directly to the service, and Cortana integrates with various tools like the Bot Framework or Cognitive Services to provide rich capabilities and solutions. The Cortana Intelligence Suite is a very powerful set of tools that can be leveraged to transform your business. Organizations can pick and choose the tools they need to satisfy the requirements of their use case. At BlueGranite, we work with clients daily on Cortana to deploy meaningful business solutions across industry verticals and horizontals. Contact BlueGranite today to learn more about the entire Cortana Suite or a single service in its toolbelt. "
"82" "Microsoft Azure Resources Advance Digital Transformation Within Healthcare Healthcare organizations struggle with several daunting issues; many affect healthcare quality. Low unemployment, high demand for healthcare providers, and ever-changing healthcare epidemics are a few of the challenges facing healthcare industry leaders. Finding ways to optimize their workforce and workplace is imperative. For many organizations, Microsoft’s data analytics resources are becoming essential to the process. When healthcare organizations are unable to accurately predict patient volume or demand, it impacts care quality. Healthcare providers find it difficult to spend adequate time with patients; this undercuts the quality of patient care, increases job burnout among providers, and results in job turnover.  Healthcare administrators who can predict patient demand and provide necessary staffing levels can positively impact overall productivity. Accurately anticipating healthcare demands bolsters the most vital objectives: patient satisfaction, quality of care, satisfaction and retention of providers, and improvement in healthcare costs and revenues. Data-driven methods, such as predictive models, are replacing traditional practices of determining providers’ work schedules. The ongoing development, management, and renewal of models that predict patients’ length of hospitalization and the necessary staffing needs for a given patient load can improve the efficiency, cost, and quality of healthcare. Microsoft’s rich ecosystem of advanced data analytic tools and technologies can provide healthcare organizations with digital transformation opportunities to help control healthcare costs and improve patient care outcomes. Specifically, by providing resources to develop value-based care models faster and more efficiently, these analytic tools and technologies can be key to reducing costs. Two Microsoft Azure resources – Azure Machine Learning service and Azure Databricks – can advance the development and utility of predictive healthcare models. When using them in combination, a healthcare organization can see faster returns on data-driven initiatives and gain deeper insights. Azure Machine Learning Service & Azure Databricks Predictive models provide the data-driven and technology-enabled forecasting that can successfully address many of the predictive needs of healthcare organizations. However, a key bottleneck in predictive model development is the time and organization required to work through the iterative model development process. Several important steps in the model development pipeline can become time and effort-intensive processes; they include feature engineering and selection, algorithm selection, tuning of model parameters, model performance evaluation, and the cyclical process of working through all the combinations of these features, algorithms, parameters, and evaluation metrics. Building predictive models for patient demands (e.g., length of stay in the hospital) begins with gathering historical health records and running the data through as many of the permutations of the machine learning pipeline as necessary to achieve optimum results. This is where the Workspace feature in Azure Machine Learning (AML) service comes into play. “Azure Machine Learning service provides a cloud-based environment to develop, train, test, deploy, manage, and track machine learning models.” Microsoft Documentation Specifically, the Workspace feature available in the AML service provides a well-designed organizational structure for the building of predictive models. A key capability of the AML Workspace is its ability to reduce the time to develop predictive models by providing a space that organizes model development (model experiments) and the evaluation of model performance (model metrics). Data scientists can work from many Python-supported environments (e.g., their local machines, data science virtual machines, Azure Notebooks, or cloud-based compute resources) and use their favorite open-source machine learning libraries. Azure Databricks is an Apache-powered, unified analytics, cloud-based computing resource. A custom version of the AML service SDK has been created specifically for Azure Databricks. The combination of these Azure resources provides data scientists with added functionality to their machine learning experimentation, testing, and model evaluation processes. A view of the Azure Machine Learning service Workspace. Using Azure Databricks and an anonymized demo data set of patient health records, a machine learning pipeline was developed to predict patient length of stay in the hospital. The pipeline included data cleaning, exploratory data analysis, feature engineering, algorithm selection, and tuning of algorithm parameters. Model training, testing, and evaluation were organized and managed using the AML Workspace feature within the Azure portal. Within the modeling code, several of the azureml.core modules are imported, and code is written to log parent-child runs of the different model permutations (or experiments). These data points and metrics are then sent to the identified AML Workspace.  Code to run the model experiments within an Azure Databricks notebook:  In one command cell, several models are run and evaluated. These models and the associated metrics are then logged and tagged within the AML Workspace that is accessed via the Azure portal. This organization allows for faster, more organized, and less error-prone machine learning pipeline development.  Within a given experiment, model evaluation run times, algorithm choices, evaluation metrics, graphs, and more can be saved. Here are the model evaluation metrics for one child run (permutation) using a linear regression algorithm. The Workspace feature within the AML service provides a well-structured DevOps utility that reduces the data scientist’s workload of having to track each permutation of selected features, chosen algorithm, parameter selections, and model evaluation. The support features within the AML service extend beyond model development support. By ingesting clean training data and building the modeling pipeline automatically, the automated machine learning feature within the service takes control of the development process, further reducing the time and effort involved in model development (see Microsoft’s documentation for more information). Predictive Analytics in Healthcare  Healthcare organizations can use targeted predictive models to achieve tangible outcomes, such as higher staff morale, improved patient care, and lower healthcare costs. Models that provide accurate predictions (e.g., patient demand) and clear actionable insights (e.g., appropriate staffing schedules, purchase of sufficient medical supplies) position healthcare organizations for increased efficiency and the delivery of high-quality care in today’s cost-competitive markets. Predictive models such as these can be developed by capitalizing on the speed and organization of model development with Azure Machine Learning service’s Workspace and the analytic power of Azure Databricks. Contact BlueGranite to learn more about how your healthcare organization can capitalize upon the data-driven insights of predictive analytics."
"83" "Artificial Intelligence (AI), one of the hottest tech industry topics in recent years, is rapidly becoming intertwined in every aspect of daily life. Consumers are growing accustomed to “AI-assisted” offerings – ranging from the voice-to-text capabilities of messaging apps to the face “tagging” suggestions by social media platforms. Meanwhile, business investments in and approaches to AI receive heavy media coverage – such as Bank of America’s use of virtual assistant Erica, who provides 24/7 intelligent customer service, or the employment of predictive maintenance across automotive and manufacturing industries to reduce machine breakdowns. Because of rapid artificial intelligence advances and its seeming complexity, businesses that are new to the AI realm may be intimidated to take even the first step. However, as increasingly affirmed by academia and major corporations, these same businesses must begin leveraging AI to remain competitive. AI helps reduce operational costs and increase product and service revenues at a scale rarely offered by other technologies. To help business leaders who are early in their AI explorations jumpstart their strategies, BlueGranite published the whitepaper Building an AI Strategy for Business. This whitepaper introduces the basic ideas of AI, discusses the fundamental components of an AI strategy, and highlights the wisdom of starting small. It begins by identifying and discussing the intangible components of an AI strategy, before diving into the practical components needed to build an AI solution. A successful AI strategy conveys an organization’s long-term AI vision; it may continue to evolve, incorporating more AI solutions as it grows. Building an AI Strategy for Business identifies three intangible AI strategy components and answers the questions outlined below: Finding a Business Case: It’s important to get stakeholders on board before investing in AI by establishing at least one business case. But how can one discover a business case? What are some characteristics of an appropriate first case to tackle? Ensuring Leadership Support: Leadership support is needed in every company-wide strategy to ensure sufficient resources are provided. What does overall company leadership need to demonstrate to steer a productive AI strategy? Who should oversee the strategy? Aligning Culture and Priorities: A motivated workforce with aligned priorities is critical to the success of an AI strategy. What are the qualities of an AI team that will “make it work”? Regardless of how many solutions there may be in an AI strategy, and whether those change over time, four practical elements must be considered. The paper discusses these elements, and the various considerations and options each element may have: Finding the Talent: While the skill of a data scientist may be the first talent that comes to mind, implementing a successful AI solution depends on experts in multiple fields – spanning technology and business. What are these fields? How can outsourcing to outside consultants help with jumpstarting an AI solution? Choosing the Tools: The technical tools needed to create an AI solution are often perceived as complex and expensive. However, there are, in fact, pre-built AI algorithms that make leveraging AI much easier than one might imagine. What are some of these tools? And how does one determine when to use pre-built AI and when to use custom? Ensuring the Data and Infrastructure: Many people understand that AI requires a large amount of data. But what are other characteristics of the “right data”? As mentioned again and again, most recently by IBM’s CEO Ginni Rometty at the World Economic Forum in Davos, “You can’t have AI without IA (information architecture).” Setting up the right infrastructure is essential. So what does the right infrastructure need to do? Privacy Concerns and Regulations: Lastly, the whitepaper briefly touches upon the need for consideration of privacy concerns and regulations surrounding the data and algorithms related to any AI project. Because the use of data and AI models can profoundly impact people, consumer concerns and regulations must not be taken lightly. It is important to involve someone with experience and expertise in this arena.  hbspt.cta._relativeUrls=true;hbspt.cta.load(257922, '27bad92e-1722-4f20-bfb9-3150a971c9bf', {});  The AI space evolves very quickly; therefore, BlueGranite will strive to update the whitepaper periodically for new materials and insights. An AI strategy requires numerous considerations. In developing such a strategy, as well as those that follow, a business needs to consider many things, both tangible and intangible. Whether you’re strategizing an AI implementation, looking to explore an initial business case, or you want to quickly see how an AI solution can benefit your business, BlueGranite can guide and help you along the way. Contact us today."
"84" "Bringing more AI to Power BI The February 2019 release of Power BI Desktop unveiled a new, out-of-the-box visual to Power BI called Key Influencers. Key Influencers is Power BI’s first Artificial Intelligence (AI) powered visualization. Microsoft has long incorporated AI capabilities in Power BI through features such as Natural Language (Q&A) and Quick Insights. However, with Key Influencers report developers now have explicit control in leveraging AI to discover insights in their data. Even more, Key Influencers is essentially multiple visuals baked into one! Let’s take a deeper look at this new visualization in action. Key Influencers Example When learning new Power BI features, I find it helpful to start the learning process by playing with an example. So, feel free to download the report and play around to test drive the new Key Influencers visual yourself! After experimenting with the report, read on to learn more about specific features. NOTE: The downloadable report and screenshots in this post leverage data from the AdventureWorks demo database. All data is fictitious and is merely used to illustrate the new Key Influencers visual. The sample report uses customer demographical attributes to explain a calculated column called Customer Type. Customer Type counts the number of purchases made by each customer and categorizes the results as either a repeat customer (i.e., multiple purchases found) or a one-time customer.  To follow along with this post, download the Power BI Report pictured above with this link.  Two for the Price of One! The Key Influencers visual is comprised of two tabs: Key Influencers and Top Segments.  NOTE: The new visual itself is called Key Influencers. Also, within the visual, there is a tab with the same name.  The Key Influencers tab will display a ranked list of the individual contributing factors that drive the selected condition. The Top Segments tab will take this a step further and display groupings of key influencers and their impact on the selected condition. Key Influencers Tab The Key Influencers tab runs a logistic regression to analyze your data and identify the main factors that influence a specific metric or condition. For example, in the screenshot below, we are using the Key Influencers visual to identify the drivers behind both repeat customers and customers who have only made a single purchase, thus may have churned. Rather than spending minutes, hours, or even days building various visuals and reports to analyze our customer base, we can use Key Influencers to tell us what factors contribute most to each outcome in a matter of seconds. 1 - Analyzing the key influencers that drive repeat customers. Changing the value in the drop-down box will allow us to run a separate analysis against our subset of customers who only made a one-time purchase. 2 - Use the drop-down box to analyze a different condition or outcome. As previously stated, one of the things that makes Key Influencers a fun visualization is that it is essentially multiple visuals baked into one. Even the Key Influencers tab itself gives us two panes of visuals. For example, on the left-hand pane you have an infographic of the key influencers themselves, ranked based on their relative impact on the condition being analyzed (in our case, repeat customers). 3 - Customer Country of Australia is the most important driver found in the dataset towards determining repeat customers. When a key influencer is selected in the left-hand pane, the right-hand pane will display a chart containing the values of that attribute, along with the average percentage that value matches the condition. For example, the screenshot below tells us that, on average, roughly 72% of Australian customers have made a repeat purchase. This can be contrasted against the other individual customer countries, including the average of all other countries, for which only 29% of customers have made a repeat purchase. 4 - Australian customers are repeat customers 72% of the time. Clicking on a different Key Influencer will change the chart in the right-hand pane. Additionally, if your key influencer is a continuous attribute, then the chart will be displayed as a scatterplot, as shown below. 5 - As a customer's annual income increases by $33k, they are 1.5x more likely to be a repeat customer. Top Segments Tab On the Top Segments tab, you will see how groupings of key influencers affect the selected condition.  The segments are ranked based on the percentage of records where the condition is met. The size of each segment bubble represents how many records (population count) are in the segment. 6 - The Top Segments tab found five segments, or groupings, of key influencers that drive the outcome of repeat customer. Like the Key Influencers tab, the Top Segments tab also gives us multiple visuals to play with. For example, you can click on a segment bubble to see which individual key influencers combine to make that segment. Looking at the screenshot below, we can see that Segment 1 is comprised of customers from Australia, that have no children at home, and have an income greater than $55k. When a customer meets those conditions, they are a repeat customer 77.2% of the time, compared to the average of 37.1%. The visual also tells us that this segment accounts for 8% of the dataset. In other words, 8% of our customers are Australian, with no children at home, who earn more than $55k annually. Lastly, the Top Segments tab allows you to dive in even further by clicking on Learn more about this segment. When you click on that button you can play around with different scenarios to split your segment down even further by other key influencer fields. 7 - Click on Learn more about this segment to play around with other scenarios to identify additional patterns. Interactivity Key Influencers is fully interactive, meaning you can use filters, slicers, and selections on other visuals to affect the results. For example, we can select the United States in our country slicer to re-run the Key Influencer analysis for only U.S. customers. 8 - United States is selected and now the Key Influencers results have been recalculated. Next, we can select the Graduate Degree bar in our Sales Amt by Education bar chart to cross-filter as well. Notice how the key influencers have changed and now having a Professional occupation is the influencer with the largest impact. 9 - Cross-filtering also affects Key Influencers. The results are now for Repeat Customers from the United States who have a Graduate Degree. Configuring the Visual Setting up the Key Influencers visual is straightforward. There are two sections to drop fields in: Analyze and Explain By. Analyze is where you will drop your field that contains a condition or result you would like to explain. In our example, this is where the Customer Type field goes. The Explain By section is where you will drop your fields that you feel could influence the results of Customer Type. For example, I thought Yearly Income and Number of Children at Home both could influence whether a customer made repeat purchases. From a formatting standpoint, there’s not a whole lot to the visual, but you can disable the Key Influencers or Top Segments tabs.   10 - Configuring the Key Influencers visual. 11 - You can enable or disable the Key Influencers or Top Segments tab from the formatting pane.   Limitations Since the Key Influencers visual is in public preview, there are several limitations to consider currently. The list below is from Microsoft’s own documentation, which can be found here. The features below are currently unavailable: Analyzing metrics that are aggregates/measures Consuming the visual in Power BI Embedded Consuming the visual on Power BI mobile apps RLS support Direct Query support Live Connection support Publish to Web (thus the download link for the report) The documentation highlights other issues you may encounter and how to troubleshoot them. Additionally, the documentation also explains the machine learning algorithms behind the visual, so check it out! Final Thoughts The limitations listed above will most likely keep the Key Influencers visual on the sidelines for many organizations for now. However, the initial preview of the visual is extremely promising. As evidenced by the 2019 Gartner Magic Quadrant for Analytics and BI platforms, Microsoft is a leader in both BI and AI technologies. Like Natural Language and Quick Insights before it, Key Influencers is the latest example of Power BI converging AI and BI capabilities."
"85" "Microsoft’s Power BI is the go-to tool for organizations looking to quickly gain deep insight into their data. Accessible from pretty much anywhere – desktop, mobile or tablet – the business analytics solution brings data to life with brilliant visuals. During BlueGranite’s interactive monthly Power BI Office Hours, open to all, we explore simple ways to pilot new features with this dynamic platform.  This time around from Power BI Office Hours, there wasn’t a January release for Power BI Desktop (although there was a release for the Power BI Report Server version), and we took December off for the holidays, so today, we’re looking back at the December 2018 release. It offers a lot of great new highlights (a complete list can be found here), including improvements for accessibility features, Live Connect support for Q&A, and support for Top N matches in the Fuzzy Merge. BlueGranite’s January Office Hours focused on three features – Smart Guides, Tab Order, and DAX Formula Bar updates. You can follow along with our recorded presentation here. Smart Guides “snap” your visualizations on the page with other visualizations in a PowerPoint-like experience.  Tab Order allows you to specify a custom tab order for items on the report page. This is especially useful in accessibility scenarios, such as where a user may be unable to employ a mouse.  DAX Formula Bar updates include zooming in and a larger view. Prior releases included tab stops and line numbers. In our Use Case section this month we talked about Gateways. If you are publishing your reports to PowerBI.com and are using Live Connection/Direct Query data sources, or want to schedule your data refreshes, you’re going to need a Gateway. When we talk about data refresh for published Imported datasets there are two options: Manual Refresh refers to manually opening the PBIX file in Power BI Desktop and clicking the Refresh button. Once all the data sources are refreshed you then re-publish the PBIX to PowerBI.com manually. Scheduled Refresh is configured in PowerBI.com and allows you to automate the refresh of imported datasets by setting a refresh schedule (8 times a day for standard users and 48 times per day for Premium users). For the Scheduled Refresh using on-premises data (and/or Live Connection/Direct Query on-premises data sources) a Gateway is required. A Gateway is not required to schedule a refresh for cloud-based data sources, such as Azure SQL Database or Dynamics 365. A gateway is essentially a program running on a machine on your network (usually inside the firewall) that PowerBI.com acts as the data access proxy for PowerBI.com. When PowerBI.com kicks off a scheduled refresh, it contacts the Gateway and requests the data required. The Gateway has stored credentials and manages the connection from the data source to PowerBI.com. There are two versions of the On-premises data gateway: Personal Mode is generally installed on a user’s individual machine (ideally a machine that is always on) and can only be used by PowerBI.com. Enterprise Mode is generally installed on a server and managed by IT; it can be leveraged by not only PowerBI.com but PowerApps, Flow, and Azure Analysis Services for data access.  For a detailed deep-dive into On-premises Data Gateways see the “Planning a Power BI Enterprise Deployment” whitepaper by Melissa Coates and Chris Webb. Once your Gateway is installed (while Multi-Geo support is still in preview, be sure to install your Gateway in the same region as your PowerBI.com tenant) you add and configure your data sources in PowerBI.com under the Manage Gateways section.    After you have configured your data sources you can configure a scheduled refresh.   When using Gateways there are a few considerations to keep in mind: The account used for the Gateway should be a service account, ideally one where the password never expires. The Gateway cannot access mapped drives (i.e. H:\, S:\) for flat file data sources, so the UNC path (\\servername\share\folder\etc\file.ext) must be used. Consider using separate gateways for scheduled refreshes and Direct Query/Live Connections. That way, your scheduled refreshes don’t slow down the queries for Direct Query/Live Connection reports. Be sure to mark your calendars for our upcoming Feb. 28 Office Hours session! In the meantime, check out our archived Office Hours here to discover many more Power BI tips and tricks."
"86" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/exploring-the-lambda-architecture-in-azure\"   },   \"headline\": \"LAMBDA ARCHITECTURE IN AZURE\",   \"description\": \"Lambda architecture is the state-of-the-industry, Big Data workload pattern for handling batch and streaming workloads in a single system. If you’re researching how to modernize your data program, the lambda architecture is the place to start.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/Lambda%20architechture.png?width=805&height=509&name=Lambda%20architechture.png\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Jared Zagelbaum\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 60     }   },   \"datePublished\": \"2019-02-23\",   \"dateModified\": \"2019-06-14\" } Lambda architecture is the state-of-the-industry, Big Data workload pattern for handling batch and streaming workloads in a single system. If you’re researching how to modernize your data program, the lambda architecture is the place to start. Let’s review the key concepts, parse through the tooling options in Microsoft Azure, examine some sample reference architectures, and discuss common criticisms of lambda. In a follow-up post, we’ll introduce the emerging kappa architecture and compare the benefits and limitations against lambda.   Lambda Architecture Overview The key components of the lambda architecture are the hot and cold data processing paths, and a common serving layer that combines outputs for both paths. The hot path refers to streaming data workloads and the cold path applies to batch-processed data. The goal of the architecture is to present a holistic view of an organization’s data, both from history and in near real-time, within a combined serving layer, as the following Microsoft visual illustrates.  Lee Stott, Microsoft UK, Big Data on Azure with No Limits Data, Analytics and Managed Clusters. Retrieved from  https://blogs.msdn.microsoft.com/uk_faculty_connection/2017/02/24/big-data-on-azure-with-no-limits-data-analytics-and-managed-clusters Tooling for Lambda Architecture in Azure You can run any tooling you want in Azure as Infrastructure as a Service (IaaS), but the value-add in cloud platforms for Big Data is in the Platform as a Service and Software as a Service tiers. The non-IaaS options in Azure are to either use the fully managed, native Azure services, HDInsight, or the new Azure Databricks offering (currently in preview). Reference Architecture using Native Azure-Managed Services Implementing native, Azure-managed services for lambda simplifies your list of services to choose from. This may or may not be a good thing. The exciting and frustrating pace of change in distributed systems means there’s always something new on the horizon, so if you go all in with native Azure-managed services, you lock in your capabilities to the Azure product team’s release cycle. This isn’t a concern most of the time, however, there might be edge cases where managed services just can’t meet your organization’s needs. For example, when considering Stream Analytics: is your reference data for streaming greater than 100MB? Do you need throughput of more than 1GB / sec? Is a maximum time to live (TTL) of 7 days sufficient? Unless you’re outside of the current capabilities of a service (or will be soon), then the managed service architecture is the best place to start.  There are lots of options for augmenting, substituting, and extending this architecture. The goal here is to give a baseline of what you would probably need in your ecosystem to support lambda if preferring native Azure services. Reference Architecture for HDInsight Below is an implementation with preference for HDInsight services:  You’ll notice that we’ve listed Azure Data Factory (ADF) as the ingest engine for batch. The data movement, lineage, monitoring, and orchestration capabilities of ADF are extremely difficult to substitute for in the Azure cloud. Even running HDInsight jobs as Data Factory linked services automatically handles the spin up and tear down of clusters for you. So, while you have lots of options as far as analytics, machine learning, querying, and compute services, if you’re considering any type of Big Data workload in Azure, then planning on ADF as part of your architecture will simplify and accelerate your development cycle. Limitations of Lambda Extending for concurrency and frequency An assumption that you often see with lambda, as modeled above, is low concurrency and/or frequency, specifically in the cold path. This does not fit many large organizations’ internal needs, or even those of small organizations offering reporting and analytics to their end customers. In practice, the serving layer is usually extended to include a hub-and-spoke architecture that incorporates a structured data mart to support the most commonly queried data (either by partition or entity). I personally don’t know of a common name for this, but I like to think of it as the warm path. We are purposely prioritizing some batch-processed data into services that support higher concurrency at a lower cost. Over time, the partitions are aged out of the warm path, but persist within the cold query path. This augmentation to lambda also helps simplify multi-tenancy, self-service BI, and embedded analytics use cases.  Common Criticisms Lambda is an organic result of the limitations of existing tools. Distributed systems architects and developers commonly criticize its complexity – and rightly so. Those of us that have worked extensively in Extract-Transform-Load and symmetric multiprocessing systems see red flags when code is replicated in multiple services. Ensuring data quality and code conformity across multiple systems, whether massively parallel processing (MPP) or symmetrically parallel system (SMP), has the same best practice: the least amount of times you reproduce code is always the correct number of times. We reproduce code in lambda because different services in MPP systems are better at different tasks. The maturity of tools historically hasn’t allowed us to process streams and batch in a single tool. This is starting to change, with Apache Spark emerging as a single preferred compute service for stream and batch querying, hence the timing of Azure Databricks. However, on the storage side, what was meant to be an immutable store that is the data lake in practice, can become the dreaded swamp when governance or testing fails; which is not uncommon. A fundamentally different assumption to how we process data is required to combat this degradation. Enter: the kappa architecture, which we’ll examine in the next post of this series. For those looking to delve further in the lambda architecture, there are several highly detailed resources available. Check out this blog post by BlueGranite’s Josh Fennessy for starters. Want to discover the best way to handle your organization’s data? We design our custom cloud analytics solutions around your business. Contact BlueGranite today to learn more."
"87" "Over the past 5 years, the world of dimensional modeling and Business Intelligence has experienced major and disruptive technological change. Big data, machine learning, data science, deep learning – these terms and technologies are not all new, but they are now at the forefront of discussion for data practitioners. We’ve seen the emergence of various Business Intelligence tools claiming to have either automated or replaced the need for data modeling. There has also been an explosion, both in terms of general interest and implementation, of new data storage technologies, such as data lakes. In today’s climate, data modeling can be portrayed negatively, as a relic of the past.  For Business Intelligence projects, data modeling usually means dimensional modeling – the approach was developed by Ralph Kimball; this will be the heart of our discussion. Rather than rehash the merits of this approach over any other in the database structure context, let’s examine its relevancy after recent upheavals in technology. To understand current best practices, we will start with a review of dimensional modeling’s evolution. Dimensional modeling can apply to any data practitioner – from a financial analyst who needs to create an executive dashboard in Power Pivot, to a data architect in a multinational company, or even anyone who spends more than 20% of their time manipulating data. If your job involves creating reports, dashboards, or basic forecasts for the year, month, and weekday, then dimensional modeling is relevant to you. Laying an Intuitive & Effective Groundwork Let’s go back in time and review why dimensional modeling was created in the first place, to discover if these arguments still hold true today. The initial goals were: Performance: By denormalizing and simplifying the schema (fewer joins), we were able to obtain better performance, and we were able to better predict the performance of our data warehouse. It was also easier to create aggregate tables on a star or snowflake schema than on a normalized schema. Integration: The enterprise bus matrix was designed to integrate various business processes, agnostic to the application that implements them. For instance, a conformed customer dimension allowed finance, marketing, and sales teams to have one common customer reference regardless of the source application. Extensibility: Dimensional modeling is modular by nature; many components can and should be re-used. While there was no agile project management 20 years ago, this modular nature, in theory, helped build the data warehouse incrementally and avoid a big bang approach. Ease of understanding: The simple structure of the database allowed a non-technical end user, (e.g. an accountant or marketing analyst) to easily query the model without wondering if a relationship was 1-n, n-n, or if there was a loop in the model. Stable Groundwork Still Key Fast forward 20 years and the original tenets underlying a dimensional model’s use still hold true today. Performance Hardware and software have improved dramatically: a multi-month project, requiring a top-of-the-line server 20 years ago, can now be prototyped, with better performance, on a decent laptop with Power BI in less than a week. The emergence of cloud computing also allows greater access to massively parallel databases for a fraction of the price. And today’s big data technologies allow BI practitioners to manipulate a quantity of data that was unfathomable when dimensional modeling was created. While performance problems have improved, end users now expect less data latency. Most users won’t accept having their finance data refreshed just once a month; this was the norm 15 years ago. Daily refresh, or even multiple daily refresh, is now typical, but is only achievable if the whole reporting infrastructure is optimized, proving dimensional modeling still plays a role in performance. Integration The integration capability of the bus matrix was probably one of the most important features of dimensional modeling. However, the theory often failed to deliver in practice. Rotating stakeholders with changing needs often muddied an enterprise’s efforts to make use of its data. Additionally, the wait to add a new business process to the data warehouse often seemed too long – businesses often opted for quicker departmental solutions. IT departments talked about data integration, while business units wanted to break departmental silos; though everyone agreed that solving these organizational issues was one of the most important data warehouse roles, success here was a challenge for most BI teams. Effective data modeling requires bringing the IT, marketing, and sales departments to the same table – to decide on common definitions and the skills required to create the databases, tables, and ETL processes – a capability rarely found together in BI teams of the past. Team members were typically either too technical or too functional. The creation of hybrid teams, combining data engineers, business experts, and, with an increasing frequency, data scientists, is part of today’s answer to the integration challenge. These hybrid teams will take various names in an organization: BI Center of Excellence, BI Competency Center. These cross departmental teams are often derived from the more generic Community of Practice. Master Data Management (MDM) projects, which use a systematic data integration approach, tackle the other challenges; MDM emphasizes governance and business processes. Integration is still a valid argument in favor of dimensional modeling, but perhaps more so it is an argument for MDM. Extensibility The main concepts comprising dimensional modeling are facts and dimension. Dimensions are (or should be) designed independently from their source system. The surrogate key, which identifies each member of a dimension, is independent from the source system. As a result, it can and should be re-used for different business areas. By designing each dimension (and as a result, the facts also) independently, the data warehouse is modular by design. This is still true. The advent of agile project management is only the confirmation that developing a data warehouse by manageable chunks is key to the success of a solution. The extensibility of dimensional modeling is still a relevant, key feature in favor of its use. Ease of Understanding The recent dramatic evolution of technology has given rise to an exponentially complex data landscape. It includes semi-structured data, text analytics, sensor analytics, and web-related statistics. But if all of this information can’t benefit its end users, it’s useless to enterprise. And dimensional modeling still offers the best route to making sense of mounds of data. Data is typically consumed through reports or dashboards built from tables or graphs. A well-built dimensional model can field nearly any related end-user query, in the form of an easily digestible flat table or graph. Even seasoned report designers benefit from well-designed data models. Calculations are easier to develop, report creation is faster, and reports are more consistent from one developer to another. Dimensional models aren’t just key to dashboards, reports, and simple data analysis – they also benefit data scientists. Most data scientists spend around 80% of their time wrangling, cleaning, and organizing data to obtain a tidy dataset (Wickham, 2014): one observation per row and one variable per column. This type of data structure is extremely easy to obtain from dimensional modeling. A simple join between the relevant dimensions, aggregate the indicators, and you have a tidy tabular dataset. Cleaned, organized data ensures that data scientists – who are rare and expensive – can focus on actual data science, rather than on engineering tasks that your BI team has already completed. The real strength of dimensional modeling is its ability to be easily understood and used for a wide range of business problems, regardless of an end user’s technical knowledge. A carefully designed model saves your analysts, report designers, and data scientists countless hours. The time saved from cleaning and organizing data allows them to focus on gaining valuable insight. Dimensional modeling is not dead; far from it. As the data landscape evolves toward more complexity, dimensional modeling continues to allow more people to access and use the information buried in the mountains of data generated every day. In other words, the question is not whether you should build a dimensional model, but who will create it and when. If you want to learn more about how BlueGranite can help with your data modeling needs, contact us today and we will be happy to answer your questions."
"88" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/10-things-to-know-about-azure-data-lake-storage-gen2\"   },   \"headline\": \"AZURE DATA LAKE STORAGE GEN2: 10 THINGS YOU NEED TO KNOW\",   \"description\": \"Azure Data Lake Storage (ADLS) Gen2 reached general availability on February 7, 2019. This post will help you understand its advantages and what you need to know to get started.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/10%20Things%20to%20Know%20about%20Azure%20Data%20Lake%20Storage%20Gen2.jpg?width=805&name=10%20Things%20to%20Know%20about%20Azure%20Data%20Lake%20Storage%20Gen2.jpg\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Melissa Coates\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 15     }   },   \"datePublished\": \"2019-02-11\",   \"dateModified\": \"2019-06-14\" } Updated on October 1, 2019 Azure Data Lake Storage (ADLS) Gen2 reached general availability on February 7, 2019, and has continued to evolve and mature since then. This post will help you understand its advantages and what you need to know to get started. If you would like to become more familiar with the concepts of a data lake, please also check out our eBook: Data Lakes in a Modern Data Architecture.  1. The data lake story in Azure is unified with the introduction of ADLS Gen2 Prior to the introduction of ADLS Gen2, when we wanted cloud storage in Azure for a data lake implementation, we needed to decide between Azure Data Lake Storage Gen1 (formerly known as Azure Data Lake Store) and Azure Storage (specifically blob storage). This involved weighing the business and technical requirements versus features available in order to make the decision on which service to use. While ADLS Gen1 offers important optimizations important for analytic workloads and more granular security (see section 3 for details), Azure Storage has built-in features like geo-redundancy, hot/cold/archive tiers, additional metadata, and broader regional availability which are very compelling. In the past, we either accepted some trade-offs or stored the data twice in certain situations. The new ADLS Gen2 service is built upon Azure Storage as its foundation. When the hierarchical namespace (HNS) property is enabled (see section 2 for details), an otherwise standard, general purpose V2, storage account becomes ADLS Gen2. For this reason, you will not see ADLS Gen2 listed in Azure as its own service – since ADLS Gen1 is its own service, this shift has been confusing for many people. There are a couple of ways to verify if ADLS Gen2 is enabled for a storage account: When viewing the Azure Storage account, if the file system service is displayed this indicates that ADLS Gen2 is supported:  Or, when viewing the Azure Storage account configuration properties, if the hierarchical namespace (HNS) is enabled, this indicates that ADLS Gen2 is supported:  Key takeaway: When we need a data lake in Azure for an analytics project, we will no longer need to make a choice between multiple independent services. Azure Storage, with the hierarchical namespace enabled, is now the service of choice for building a data lake use Azure cloud storage. --------------------------------------------------------------------------------------- 2. ADLS Gen2 converges the worlds of object storage and hierarchical file storage Fundamentally, ADLS Gen2 is seeking to take advantage of file system benefits without giving up the type of scalability and cost-effectiveness available with an object store: Note that full feature support for ADLS Gen2 is still evolving, as discussed in Section 4. The following diagram represents the longer-term vision: Azure Data Lake Storage:  The dark blue shading represents new features introduced with ADLS Gen2. The three new areas depicted above include: (1) File System. There is a terminology difference with ADLS Gen2. The concept of a container (from blob storage) is referred to as a file system in ADLS Gen2. (2) Hierarchical Namespace. The hierarchical namespace (HNS), coupled with the DFS endpoint, is what enables the performance and security improvements, which are discussed in Section 3. (3) DFS Endpoint and File System Driver. ADLS Gen2 utilizes the ABFS driver, which is part of Apache Hadoop. For connectivity to ADLS Gen2, the ABFS driver utilizes the DFS endpoint to invoke performance and security optimizations. ABFS = Azure Blob File System DFS = Distributed File System   Documentation for each: ADLS Gen2 REST API Blob Service REST API Key takeaway: The longer-term vision (depicted in the image above), which includes full interoperability between the object store model and the file system model, will allow us to store the data once and access it multiple ways depending on the use case. This is referred to as multi-protocol access. ---------------------------------------------------------------------------------------  3. ADLS Gen2 has significant performance and security advantages for analytical workloads Both the object store model (such as Azure blob storage) and the hierarchical file system model (ADLS Gen1 and Gen2) are compatible with HDFS (Hadoop Distributed File System). This is achieved with drivers that implement server-side HDFS semantics to translate into remote storage APIs, allowing ADLS Gen2 to behave very similarly to native HDFS. However, there are important distinctions between object storage and hierarchical file system storage in terms of performance and security. With object storage, folders are virtual only. Although it appears like we can create folders in object storage, they are just mimicked within the URI string (or sometimes metadata is used as an alternative). Although that might initially seem trivial, it has the following implications: (1) Query Performance. When sending a query that is only retrieving a subset of data, with a hierarchical file system like ADLS Gen2 it is possible to leverage partition scans for data pruning (predicate pushdown). This can improve query performance dramatically for compute engines that understand how to take advantage of partition scans.  (2) Data Load Performance. Sometimes it is necessary to rename files or relocate files from one directory to another. With the object store driver, directory operations are not handled as efficiently. If the Temp directory shown in the below image held 10,000 files, relocating them to their permanent directory would involve 10,000 rename operations and 10,000 delete operations, resulting in 20,000 calls. Conversely, with a file system like ADLS Gen2, when connecting through the DFS endpoint this is a metadata-only operation. This results in significantly improved performance for the data load, particularly at higher data volumes.  In addition to improving query performance, metadata-only operations are ultimately more cost-effective because less compute engine resources are required. (3) Data Consistency via Atomic Operations. Continuing with the previous example of 10,000 files to be moved, the object store driver does not support atomic operations. If a failure occurred, the data could remain in an inconsistent state. Conversely, a file system like ADLS Gen2 does support atomic operations, via the DFS endpoint, which improves data consistency because the entire operation will succeed or fail as a unit. (4) Granular Security at the Directory and File Level. The hierarchical file system of ADLS Gen2 (and Gen1) is POSIX-compliant. Access control lists (ACLs) can be defined at the directory and file level to define granular security, which offers much-needed flexibility for controlling data-level security.   See section 6 for additional details about managing security in ADLS Gen2. Key takeaway: Enabling the hierarchical namespace for an Azure Storage account, along with usage of the ABFS driver for connectivity, is what facilitates file system optimizations which affect performance, data consistency, and security.  ---------------------------------------------------------------------------------------  4. Feature support in ADLS Gen2 is still evolving Although ADLS Gen2 is designated as generally available, there are still quite a few planned features which are being introduced over time. As is the norm with technology vendors, Microsoft introduces features to market as quickly as possible then iterates to the point of maturity. The initial focus for ADLS Gen2 is supporting the modern data warehouse and advanced analytics scenarios.  In July 2019, the preview for multi-protocol data access was introduced. This begins to open up quite a few of the features which were previously unsupported. As is customary, the initial preview is a 'whitelist' in which customers need to request access, following by an open public preview, followed by general availability. Also customary is to release new features to a limited set of Azure regions and expand over time.   Please verify current feature support utilizing these two resources: Upgrade your big data analytics solutions from ADLS Gen1 to ADLS Gen2 ADLS Gen2 Known Issues  Key takeaway: Multi-protocol data access (as depicted in the diagram in section 2) is a critical capability which is still evolving. When it arrives, that will provide significant flexibility to land the data using whichever endpoint is preferred (i.e., to support an unchanged or legacy application or service) and use the new endpoint for analytical processing to gain the performance advantages.  ---------------------------------------------------------------------------------------  5. ADLS Gen2 is the underlying storage for Power BI Dataflows Power BI dataflows are a new capability targeted towards reusable, self-service data preparation. The output from queries prepared in the web-based Power Query Online are output to ADLS Gen2. The objective is that the queries and data preparation are handled once and is then consumed by numerous Power BI datasets. Dataflows can be fully managed by Power BI, in which case the ADLS Gen2 account is present but only visible via the Power BI dataflows user interface. Alternatively, the ‘bring your own storage’ scenario (depicted below) is appropriate for organizations who wish to interact with the data in the data lake via additional tools and compute engines beyond Power BI:  Key takeaway: The storage service behind Power BI dataflows is ADLS Gen2 and can be an important part of the self-service business intelligence strategy.    --------------------------------------------------------------------------------------- 6. There are two levels of security in ADLS Gen2 The two levels of security applicable to ADLS Gen2 were also in effect for ADLS Gen1. Even though this is not new, it is worth calling out the two levels of security because it’s a very fundamental piece to getting started with the data lake and it is confusing for many people just getting started.  (1) Role-Based Access Control (RBAC). RBAC includes built-in Azure roles such as reader, contributor, owner or custom roles. Typically, RBAC is assigned for two reasons. One is to specify who can manage the service itself (i.e., update settings and properties for the storage account). Another reason is to permit use of the built-in data explorer tools, which require reader permissions. (2) Access Control Lists (ACLs). Access control lists specify exactly which data objects a user may read, write, or execute (execute is required to browse the directory structure). ACLs are POSIX-compliant, thus familiar to those with a Unix or Linux background. POSIX does not operate on a security inheritance model, which means that access ACLs are specified for every object. The concept of default ACLs is critical for new files within a directory to obtain the correct security settings, but it should not be thought of as inheritance. Because of the overhead assigning ACLs to every object, and because there is a limit of 32 ACLs for every object, it is extremely important to manage data-level security in ADLS Gen1 or Gen2 via Azure Active Directory groups. Fortunately, both the ACLs for both directories and files are enforced regardless of which multi-protocol access point is used to access the data.   Key takeaway: Via RBAC and ACLs, there is quite a bit of flexibility for defining security for ADLS Gen2. --------------------------------------------------------------------------------------- 7. Planning for ADLS Gen2 involves multiple levels There are quite a few considerations when planning for a data lake, particularly if you have numerous data ingestion patterns, different data usage patterns, various types of users, and several tools/languages. Some organizations seek to implement one global data lake, while others utilize a multi-lake approach. With the introduction of ADLS Gen2, there is one additional level to plan for that was not present previously in ADLS Gen1: the file system. A file system in ADLS Gen2 is the equivalent of a container in the blob service. The levels to be consider during planning include: Account File system(s) within an account Directory structure within a file system  A few considerations: Region and geo-replication are account-level properties. If there are multiple data residency requirements and/or different geo-replication requirements, that will need to be satisfied with multiple storage accounts. Alternatively, if you have specific compute engines (like HDInsight or Azure Databricks) which reside in a specific region, the best performance will be gained when the ADLS Gen2 account resides in the same region. The hierarchical namespace is enabled at the account level. Should there be use cases which have no need for the benefits of the hierarchical namespace, that data should reside in a different storage account. Immutable policies and shared access policies are set at the container level for blob storage (so we can expect them to apply at the file system level for an ADLS Gen2-enabled account). Should there be different policies required, that may justify separate file systems. For ACLs, the root in ADLS Gen1 was at the account level, whereas the root in ADLS Gen2 is at the file system level. Power BI dataflows, discussed in section 5, will require one or more file systems in its integration with the Common Data Model. Key takeaway: There may be use cases, permissions boundaries, or cost considerations (see section 8) that cause you to consider segregating data beyond one data lake. The file system is a new level which has its own set of properties and should be accounted for when planning. --------------------------------------------------------------------------------------- 8. Pricing for ADLS Gen2 is almost as economical as object storage Object storage, such as Azure blob storage, is known for being highly economical. With respect to the direct storage cost, Microsoft has released ADLS Gen2 at the same price as Azure blob storage (i.e., block blob pricing). You only pay for the storage that you use; there is not the concept of reserving a specific size. However, the transaction costs are somewhat higher for storage accounts which have the hierarchical namespace enabled. Transaction costs are usually measured in batches of 10,000. Please refer to the official documentation and the online pricing calculator for more complete pricing details. The FAQs section for ADLS Gen2 pricing has an excellent practical example which contrasts pricing for the flat namespace (i.e., block blob storage) and the hierarchical namespace (i.e., ADLS Gen2). Key takeaway: The transaction and metadata storage costs are higher when the hierarchical namespace is enabled for a storage account, while the storage costs are equivalent. Although the transaction costs are still exceedingly economical, workloads that will never take advantage of the hierarchical namespace (HNS) features should reside in a storage account that does not have the HNS enabled. --------------------------------------------------------------------------------------- 9. Azure Data Lake Analytics and U-SQL have an uncertain future The initial Azure services supported by ADLS Gen2 via the ABFS driver include: Azure Databricks Azure HDInsight Azure Data Factory Azure SQL Data Warehouse (PolyBase) Third party partner support is emerging as well. Considering that U-SQL within Azure Data Lake Analytics (ADLA) is not one of the initial services to be supported by the optimized ABFS driver, that says something about where we should be placing our bets. Microsoft has not announced the future roadmap for ADLA, but we are observing that open source technologies such as Spark appeal to a wider customer base vs. proprietary tools and languages. We would encourage any customers to be cautious in choosing to use ADLA on future projects. Key takeaway: Currently there is not a serverless (pay per use) way to execute queries against ADLS Gen2. Azure Databricks and HDInsight are currently the preferred methods for direct querying capabilities. --------------------------------------------------------------------------------------- 10. ADLS Gen1 will be supported for quite some time All signs indicate the ADLS Gen1 will not be deprecated anytime soon. If you have a large implementation on ADLS Gen1, there is no cause for immediate concern. If you do wish to migrate from ADLS Gen1 to ADLS Gen 2, there are several upgrade strategies. Following are a few key considerations: Migrating data via Azure Data Factory is currently the easiest way to do a one-time data migration, as there is not currently a migration tool available. If you have any files in ADLS Gen1 larger than 5TB, they will need to be separated into multiple files before migration. Any references which utilize the adl:// addressing scheme will need to be changed to utilize abfs[s]:// connectivity, the new REST APIs, and/or the new SDKs. Key takeaway: Migration from ADLS Gen1 is not urgent whatsoever, but you should migrate if it is practical to do so. Brand new implementations should utilize ADLS Gen2 if there are no feature gaps. --------------------------------------------------------------------------------------- If you're exploring the best Azure solutions for your firm's needs, BlueGranite would love to help. Contact us today for more information."
"89" "Azure Databricks & Azure SQL Data Warehouse make an increasingly compelling pair as SQL DW hits new peaks of price-performance. The foundation of any Cloud Scale Analytics platform must be based upon the ability to store and analyze data that may stretch traditional limits along any of the older “3 ‘V’s of Big Data: (Volume, Variety, Velocity), but realistically, must also excel with the fourth V: Value. Together, Azure Databricks and Azure SQL DW provide the most powerful 1-2 punch in the market across all 4 V’s. With recent announcements, the Azure SQL Data Warehouse (DW) value proposition knocks its competitors out of the ring. Taken individually, Databricks and SQL Data Warehouse provide compelling, best-in-class tools for data storage, analysis, orchestration, security, and AI; taken as a platform, and weighted for total cost of ownership and price-performance, it’s hard to imagine looking elsewhere!   Azure DW is now up to 14x faster and up to 94% cheaper than other major cloud DW solutions. Read the Feb 2019 GigaOM report here. Last year, when Microsoft and Databricks announced their ground-breaking partnership, they brought the de facto winner of the Big Data movement (Spark!) into Azure as a first-class PaaS offering, and at BlueGranite, we were practically shaking with excitement! Azure Databricks was a missing puzzle piece that fostered levels of integration, ease of use, and power (so much power!) and in combination with tools like Azure Data Factory and Azure SQL DW to create a singularly compelling platform for Cloud Scale Analytics. (New to that term? Learn why you should be getting on board here!) At the time of the partnership announcement, cloud migrations were becoming cloud modernizations; organizations were leveraging the promise of the cloud to add new capabilities and levels of performance for data and analytics that were previously practically unavailable. On the heels of Azure Databricks becoming generally available, Azure SQL DW upped its game with new features that enabled greater customization of storage and compute, then landed at the top of the price-performance (P-P) ranking compared to its primary competitor. That was then: Read the Jul 2018 GigaOM benchmark report. As organizations look to the cloud to enable Digital Transformation across their Data and AI efforts, both migrating and modernizing the existing DW environments and analytics becomes table stakes for any holistic strategy. And with another exponential improvement in P-P, Azure SQL DW is running away from the competition even before you consider all the complementary tools and features in Azure.  And on that last note, Microsoft is also announcing the general availability of Azure Data Lake Gen 2 and Azure Data Explorer. Along with Power BI for rich visualization, these support an enhanced set of capabilities that cements Microsoft’s leadership position in delivering Cloud Scale Analytics. Read the announcements here.  Now that we’ve covered Azure’s major advances and advantages, let’s take a moment to dig into what’s so exciting about employing it to modernize your organization’s Data and AI capabilities. Azure SQL Data Warehouse offers availability of skill set, ease of deployment, seamless transition and adoption, exciting visualization, and virtually unlimited new capabilities… Not to oversell it, but that’s what is available when you leverage Azure for your Modern Data Platform! Transition your core DW workloads onto the cloud with minimal friction Migrate ETL and take advantage of cloud flexibility Enterprise class environment for security & RBAC with Azure Active Directory Enjoy the full support of Microsoft as they help partners (like us) and customers alike on their journey to the cloud And when it comes to taking advantage of new capabilities: Modernize data analysis & visualization with Power BI Enable powerful analytics over virtually any data with Azure Databricks  Harness the transformative impact of AI With all that power behind Azure SQL Data Warehouse and Azure Databricks, the possibilities are seemingly endless. If you’re ready to learn more about the Modern Data Platform or taking advantage of AI on Azure, take advantage of BlueGranite’s expertise below! Next Steps: New to Azure? Try out Azure SQL DW for Free Ready to get a jump on the cloud? Let BlueGranite migrate your existing DW/BI environment to Azure, and help bring the Cloud Scale Analytics vision into focus Join us for an upcoming live webinar on April 4th: From Migration to Modernization: The Promise of Cloud Scale Analytics "
"90" "One of the great promises of AI (artificial intelligence) is to provide intuitive, useful interactions between humans and machines. Recent advances in AI technology have some proclaiming that intelligent bots can provide the most valuable form of these interactions. In this post, I’ll introduce some of the most important concepts of bots, particularly information chatbots built using the Microsoft Bot Framework. I’ll cover some of the most common use scenarios, and the business value those provide, as well as provide some examples of how bots work, and how to get started with your own bot.  First … what is a bot? A bot is a software application programmed to perform automated tasks, like answering questions in a chat user interface, based on programmed rules or artificial intelligence. In Microsoft’s words: “Bots provide an experience that feels less like using a computer and more like dealing with a person – or at least an intelligent robot. Users converse with a bot using text, interactive cards, and speech. A bot interaction can be a quick question and answer, or it can be a sophisticated conversation that intelligently provides access to services.” Example of a chatbot in Microsoft's Azure test UI Why you should consider a bot for your business I don’t mean to prey on your FOMO (fear of missing out), but according to global research giant Gartner’s 2019 CIO survey there’s been a 270% increase in the number of companies that have adopted AI in the last 4 years. And, as the tech analysts at ZDNet note, global AI-derived business value is projected to grow over 60% this year (in addition to last year’s estimated 70% growth).   Gartner’s survey also states, “If you are a CIO and your organization doesn’t use AI, chances are high that your competitors do, and this should be a concern.” If you are like most companies that have yet to develop an AI strategy, you may be wondering, “But what problems can I solve with AI, and how do I start?” From my practical experience working with BlueGranite clients, simply navigating the terms surrounding AI – like machine learning and deep learning – can be very confusing, let alone actually employing AI. For a great primer on AI concepts, check out this Wall Street Journal article.   In addition to understanding the concepts behind AI and how it works, other barriers to getting started include difficulty finding a business case, lack of leadership support, competing (non-AI) projects, and lack of data science talent/technology. These are all significant challenges. For the reasons I’ll outline below, I think bots can be a great way to break through these barriers and get started with AI. 1. Bots can solve important business problems Automation can improve quality of service and customer engagement: Many organizations have repetitive tasks that are of relatively low value, such as answering a high volume of frequently asked questions, reviewing and validating forms, reporting account information, and processing simple transactions, like making service appointments. Bots can automate these types of tasks, so they don’t require human intervention. It’s true this automation could reduce the need for some jobs, but in my experience with clients, it’s more likely to re-direct and focus existing human resources on more complex, high-value activities. The benefit to businesses is accurate, consistent answers to common questions, while tougher problems get the human touch! Another exciting feature of bots is the potential for automated outreach or intervention at important moments of customer interaction. Bots can be programmed to nudge – or provide “interventions that steer someone toward a better decision without taking away their choice.” In higher education, nudges from a bot might gently remind applicants they have just one more step to complete their application, then provide a link to a form. Or, when a student asks how to drop a class, the bot can reply with facts about how dropping classes can often lead to delayed graduation, but still offer the choice to drop the class. These industry examples really showcase how AI is improving the customer experience: In health care: The Cincinnati Children’s Hospital is using a digital concierge chatbot to help parents pilot their way through its large campus, look up wait times at its urgent care centers, get real-time updates on procedures, obtain parking passes, and discover ways to entertain their kids. All of this reduces strain on families and enables better patient care. In higher education: BlueGranite is building an information chatbot for the New York Institute of Technology to provide a unified starting place for searching diverse knowledge bases. Answers to frequently asked questions – such as how to get transcripts, obtain application and enrollment status, or access student services like counseling and financial aid – are currently located across many documents and web pages associated with their various departments. The bot will provide “one-stop shopping” for answers about the school, not only for current students, but faculty, alumni, prospects, and the general public. The bot is one part of an aggressive university AI strategy to extend and improve student outreach and boost enrollment. In financial services: Chatbots are also helping bank customers access account balances, alerts, credit report scores, and personalized product recommendations. Bots that do all of this can offer more speed and convenience than a traditional bank website or a help call center. The combination of utility and speed in an easy-to-use conversational interface is strengthening the connection between financial institutions and their customers. You can read more about how some of the largest banks in the world are using bots and AI in this article. 2. Bots offer AI without the expensive AI development time and costs One of the more exciting developments in AI are Artificial Intelligence as a Service tools. These pre-built, sophisticated models solve some of the most common and valuable AI use cases – like natural language processing (NLP), speech recognition, image recognition, and more. As recently as a year or two ago, if you wanted these tools you needed an expensive data scientist and/or programmer, and months, or years, to build a custom solution. Now cloud providers like Microsoft provide these in affordable, scalable services that take just minutes to deploy. What’s even cooler about these services is that they can easily be embedded in bots or other applications. I like the way Microsoft talks about this integration on its bot site: “Give your bot some super powers. Go beyond a great conversationalist to a bot that can recognize a user in photos, moderate content, make smart recommendations, translate language, and more. Cognitive Services enable your bot to see, hear, and interpret in more human ways.”   One of the key AI services that can be used in Microsoft bots is language understanding, or LUIS. LUIS takes regular language phrases – things that people would naturally say or type, (passed from the bot UI) – and extracts the most important elements, such as the user’s intent and key items. To use the LUIS application, you provide sample phrases to train the service what to look for. Different intents are essentially different named actions for a bot to take. An example phrase in the higher ed context might be, “I’d like my grade in math.” The intent is “get grade” and the entity could be “course” – in this case, the value for the “course” entity would be “math.” Once LUIS determines the important elements in a phrase, they are used in business logic. In this example, a student’s math grade is retrieved from a grade database and returned via the bot. The developers of the children’s hospital bot cited LUIS as a valuable component: “By leveraging the Microsoft Bot code and LUIS, we didn't have to worry about building out a language model, maintaining it, and training it.” 3. Bots use tools and technology already familiar to most IT organizations Building intelligent bots might sound difficult, but it’s probably very similar to other applications your organization already builds and maintains. In fact, it might be even easier. Bots are really web applications that send and receive messages through APIs (application programming interfaces). If you build a bot on the Microsoft Bot Framework, you can choose a .NET or JavaScript SDK (or let your data scientists loose with the Python SDK!) The hard-working people at Microsoft made sure these toolkits come with great templates, tools, and tutorials. A great place to start for developers is the BotBuilder home page on GitHub and the MS Virtual Assistant solution accelerator. Here’s a sample architecture: As mentioned in the previous section, you don’t have to code your own “intelligence” into an AI-based bot – you can just use an AI service. However, I think a good bot-development team should include an AI practitioner, in addition to a traditional developer, to facilitate and maximize the use of services. Someone who has worked with natural language models and/or regular expressions, especially in business applications, can be valuable in building a good bot. This doesn’t have to be a data scientist – it could be data engineer or architect who likes to dabble in data science on the side (we have lots of these at BlueGranite!) That’s a lot of talk about bots. To see one in action, check out this brief demo that illustrates some common uses and how easy it is to maintain a knowledge base.                                                       Thanks for reading! For more information on bots, please check out our upcoming webinar!"
"91" "IT projects are very complex. Their success depends on a wide range of factors including planning, requirements, communication, relationships, and more. When all is said and done, what is it that makes a customer happy with their provider? At a financial level, the customer will be happy if the project comes in under budget, right? They’ve got to be happy if you deliver more functionality than they expected. What about delivering before the deadline? If you can consistently deliver in all three of these areas, stop reading this article and go publish a book, because you are doing something that the vast majority of the IT industry has failed at over the past several decades. Of course, those end results make customers happy, but what we’re talking about is a day-by-day relationship with clients where they have confidence in us as technology service providers. Where they trust us when we tell them a certain task will take so many hours. Where they will believe us when we tell them option Y is the best solution. That level of relationship is not easily come by. Rather, it is earned over a period of solid performance and delivery. You may be thinking the key to success is hard work. You may be thinking the secret is having the smartest techs on staff, or you may be thinking the secret is … best project management, lowest costs, customer service, etc. These are all good things that a strong technology company should have, but are they really the key factors that build that solid, trusting relationship with clients? There’s a little more to it. So, what is the secret behind customer happiness? It’s not magic. In fact, you’ll be surprised by how simple it is. Two simple truths. They are: Get Things Done and Do What You Say.  Unfortunately, these simple truths can be so hard to put into practice. Get Things Done What does this mean? You work hard every day. You get things done. This is easy, right? What we mean here is completing tasks. Completing meaning done, delivered, finito. I have seen so many hard-working IT professionals in my career that put in the extra hours. They worked weekends and almost never left the keyboard. Unfortunately, they couldn’t cross the finish line. I find a significant percentage of technologist suffer from this malady. Is it a lack of focus? Laziness? Incompetence? In my experience, the number of people who really can bring complex tasks to closure is fewer than you might guess. In a previous job at a software consulting firm, we actually classified our development staff into two buckets: closers and non-closers. Closers were the ones who could complete a project to sign-off. We found that if we did not assign at least one closer to a project, the project was doomed to linger in the dreaded “almost done” phase. We had more non-closers on the staff than closers, and this was a real problem. Software developer and author Joel Spolsky wrote an excellent book on hiring technologists called ”Smart & Gets Things Done”. During his career as a technology business owner, he realized that hiring smart people wasn’t enough; he needed to hire smart people that could get the job done. You may be wondering, why aren’t more people closers? Or, how do I become a closer? Again, the answer is simple, but following through can be hard. I think that closers see projects a little differently than non-closers: They see the project with a clear finish line and are constantly considering the steps it will take to cross that finish line. They understand that effort does not equal progress. For example, they judge progress on what they have completed, not on the hours they have worked. They focus on completion. They don’t like loose ends, but are not perfectionists. They don’t let themselves get side-tracked on a multitude of tasks. They complete a task and then move on to the next task, rather than trying work too many tasks at one time. Follow these steps, and you just may find yourself becoming a closer too! Clients are far less concerned with the blood, sweat, and tears that we put into our work than they are about us delivering on time and budget commitments. Do What You Say It’s another maxim that sounds simple, but consistent follow-through can be much harder. How many times have you told a client you would get something done by a certain date and failed to hit that date? Reflect. Think hard. It may be more times than you’d care to admit. Every time we tell a client we will do something by a certain date, then miss that date, we lose trust. This includes minor administrative tasks,  such as making phone calls or sending emails, as well as major deliverables. Whenever you a give a date, or agree to one, you must consider that a commitment. You may be thinking, “I probably can do it by that date”, but the customer hears a date and is thinking, “That will absolutely be done by that date”.  If you get to the point where the customer is thinking, “They gave me this date, but they never get it done when they say they will”, you have lost the role of the expert on the engagement. We’ve got to treat our word as a contract. So often, I hear technologists give out dates, or agree to dates proposed by clients and/or management. Don’t do it! When we concede to a deadline and don’t deliver by that date, everyone sees it as our failure, not the people that pressured us to agree to it in the first place. Be careful when estimating a delivery date – even for the smallest tasks. Every time we don’t hit a verbal commitment to deliver, we erode client trust and confidence. Does this mean never missing a time commitment again? No. Unexpected things come up and clients will generally understand that. However, continually missed deadlines eventually put the client into a position where they can’t depend on a technologist’s estimated delivery dates. Here are some key factors you can use to better keep your commitments: Treat every commitment as a contract. Even very small ones like, “I’ll call you back this afternoon”. Don’t be pressured into agreeing to dates that you don’t feel comfortable with. Give yourself some buffer. Consider that everything may not go as you think it might. Issues/blockers will happen. You can’t make all of your commitments – extenuating circumstances arise – but don’t let that be an easy excuse to constantly underperform. Be careful what you commit to, and, when you do commit, make sure to deliver. It’s professional, and it’s what the customer expects. The world of technology is exciting, but engagements can be very complex. BlueGranite's success has been built on our clients knowing we deliver what we promise. Get Things Done and Do What You Say – consistency in these two areas, for every client interaction, is a surefire way to a lasting client relationship founded on trust. Want to know more about how BlueGranite can help with your data and analytics needs? Contact us today, and we will be happy to help you explore new opportunities."
"92" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/a-different-way-to-process-data-kappa-architecture\"   },   \"headline\": \"KAPPA ARCHITECTURE: A DIFFERENT WAY TO PROCESS DATA\",   \"description\": \"Kappa architecture proposes an immutable data stream as the primary source of record. Unlike lambda, kappa mitigates the need to replicate code in multiple services. In my last post, I introduced the lambda architecture tooling options available in Microsoft Azure, sample reference architectures, and some limitations. In this post, I’ll discuss an alternative Big Data workload pattern: kappa architecture.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/Kappa%20Architecture.png?width=805&height=509&name=Kappa%20Architecture.png\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Jared Zagelbaum\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 15     }   },   \"datePublished\": \"2019-01-25\",   \"dateModified\": \"2019-06-14\" } Kappa architecture proposes an immutable data stream as the primary source of record. Unlike lambda, kappa mitigates the need to replicate code in multiple services. In my last post, I introduced the lambda architecture tooling options available in Microsoft Azure, sample reference architectures, and some limitations. In this post, I’ll discuss an alternative Big Data workload pattern: kappa architecture.    Below, I’ll give an overview of what kappa is, discuss some of the benefits and tradeoffs of implementing kappa versus lambda in Azure, and review a sample reference architecture. Finally, I’ll offer some added considerations when implementing enterprise-scale Big Data architectures. Kappa Architecture: the Immutable, Persisted Log Kappa architecture, attributed to Jay Kreps, CEO of Confluent, Inc. and co-creator of Apache Kafka, proposes an immutable data stream as the primary source of record, rather than point-in-time representations of databases or files. In other words, if a data stream containing all organizational data can be persisted indefinitely (or for as long as use cases might require), then changes to code can be replayed for past events as needed. This allows for unit testing and revisions of streaming calculations that lambda does not support. Kappa architecture also eliminates the need for a batch-based ingress process, as all data are written as events to the persisted stream. Kappa architecture is a novel approach to distributed-systems architecture, and I personally enjoy the design philosophy behind it. Apache Kafka Kafka is a streaming platform purposefully designed for kappa, which supports time-to-live (TTL) of indefinite time periods. Utilizing log compaction on the cluster, the kafka event stream can grow as large as you can add storage. There are petabyte-sized (imagine the U.S. Library of Congress) kafka clusters in production today. This sets kafka uniquely apart from other streaming and messaging platforms because it can replace databases as the system of record. Here are a few fascinating write-ups on kafka’s capabilities: Questioning the Lambda Architecture, by Jay Kreps Kafka, Samza, and the Unix philosophy of distributed data, by Martin Kleppmann It’s Okay To Store Data In Apache Kafka, by Jay Kreps Publishing with Apache Kafka at The New York Times, by Boerge Svingen Lambda vs. Kappa Let’s go with kappa architecture. What are we waiting for, right? Well, there’s no free lunch. Kappa offers newer capabilities compared with lambda, but you do pay a price when implementing leading-edge technologies – specifically, as of today, you’re going to have to roll in some of your own infrastructure to make this work. No Managed-Service Options You can’t support kappa architecture using native cloud services. Cloud providers, including Azure, didn’t design streaming services with kappa in mind. The cost of running streams with TTL greater than 24 hours is more expensive, and generally, the max TTL tops out around 7 days. If you want to run kappa, you’re going to have to run Platform as a Service (PaaS) or Infrastructure as a Service (IaaS), which adds more administration to your architecture. So, what might this look like in Azure? Reference Architecture for Kappa with HDInsight  In this reference architecture, we are choosing to stream all organizational data into kafka. Applications can read and write directly to kafka as developed, and for existing event sources, listeners are used to stream writes directly from database logs (or datastore equivalents), eliminating the need for batch processing during ingress. In practice, a one-time historical load for existing batch data is required to initially populate the data lake. Apache Spark is the sole processing engine for transforming and querying during stream ingestion. Further processing against the data lake store can be performed for machine learning or other analytics requiring historical representations of data. As requirements change, we can change code and “replay” the stream, writing to a new version of the existing time slice in the data lake (v2, v3, and so on). Since our lake no longer acts as an immutable datastore of record, we can simply replay and rebuild our time slices as needed. With kappa in place, we can eliminate any potential swamp by repopulating our data lake as necessary. We also eliminate the requirement of lambda to reproduce code in both streaming and batch processing – all ingress events and transforms occur solely within stream processing. Additional Considerations Schemas and Governance You still need a solid data governance program regardless of which architecture you choose. For lambda, services like Azure Data Catalog can auto-discover and document file and database systems. Kafka doesn’t align to this tooling, so supporting scaling to enterprise-sized environments strongly infers implementing confluent enterprise (available in the Azure Marketplace). A key feature that confluent enterprise provides is schema registry. This allows for topics to be self-describing and provides compatibility warnings for applications publishing to specific topics, ensuring contracts with downstream applications are maintained. Running confluent enterprise brings in a third-party support relationship to your architecture and additional licensing cost, but is invaluable to successful enterprise-scale deployments. Which Architecture is Right for my Organization? There are a lot of considerations when developing Big Data solutions for enterprises, not the least of which is the experience and skills of your IT and development teams. Like most successful analytics projects, the key is to start small in scope with well-defined deliverables, then iterate. The primary goal is to minimize time to value – the reason for considering distributed systems architecture in the first place! Partnering with a trusted advisor, like BlueGranite, can help you avoid common pitfalls in implementing Big Data solutions and set your team and organization up for success. Want to learn more about how BlueGranite can help implement Big Data solutions at your organization? Contact us!"
"93" "Note: This blog post was originally published on 11/1/2017 and has been updated on 1/23/2019.--------------------------------------------------------------------------------------------------------------  Over time at BlueGranite, we have observed some customer confusion around when Azure SQL Data Warehouse is most appropriate to use. This blog post and the accompanying decision tree below are meant to help you answer the question: Is Azure SQL Data Warehouse the best technology choice for your implementation? Azure SQL Data Warehouse (SQL DW) is a cloud-based Platform-as-a-Service (PaaS) offering from Microsoft. It is a large-scale, distributed, MPP (massively parallel processing) relational database technology in the same class of competitors as Amazon Redshift or Snowflake. Azure SQL DW is an important component of the Modern Data Warehouse multi-platform architecture. Because Azure SQL DW is an MPP system with a shared-nothing architecture across distributions, it is meant for large-scale analytical workloads which can take advantage of parallelism. The distributed nature of Azure SQL DW allows for storage and compute to be decoupled, which in turn offers independent billing and scalability. Azure SQL DW is considered an elastic data warehouse because its level of compute power can be scaled up, down, or even paused, to reserve (and pay for) the amount of compute resources necessary to support the workload. Azure SQL DW is part of Microsoft’s ‘SQL Server family’ of products which also includes Azure SQL Database and SQL Server (both of which are SMP, symmetric multiprocessing, architecture). This commonality means that knowledge and experience will translate well to Azure SQL DW, with one notable exception: MPP architecture is very different from the SMP architecture of Azure SQL Database and SQL Server, thus requiring specific design techniques to take full advantage of the MPP architecture. The remainder of this post discusses some of the most important things to consider when making a decision to use Azure SQL DW.  The following comments highlight each of the items in the decision tree above: Q1: Have you justified that a relational data warehouse solution is supported by business requirements and needs? The most common justifications for a data warehouse implementation include: Consolidate and relate multiple disparate data sources. Data is inherently more valuable once it has been integrated together from multiple sources. A common example cited is the 360-degree view of a customer which could align customer master data, sales, open receivables, and support requests so they can be analyzed together. Centralize analytical data for user data access. A data warehouse is most often thought of as supporting corporate BI efforts (typically thought of as standardized corporate reports and dashboards). It can also play a big role in self-service BI efforts by providing consistent, cleansed, governed data. Realistically, a \"single version of the truth\" can never be 100% met, but effective governance and master data management can increase the odds that the data warehouse provides consistent and accurate data for all types of analytics throughout the organization. Historical analysis. The data warehouse supports historical reporting and analysis via techniques such as periodic snapshots and slowly changing dimensions. A common scenario is a customer's sales representative has changed this quarter, or a department rolls up to a different division now. The flexibility to report on either \"the way it was\" or \"the way it is\" can offer significant value - and is rarely available from standard source systems. User-friendly data structure. It is valuable to structure the data into a user-friendly dimensional model which really helps the largest portion of the user base. Other techniques such as friendly table and column names, derived attributes, and helpful measures (such as MTD, QTD, and YTD), contribute significantly to ease of use. Time investments here should encourage data analysts to utilize the data warehouse, leading to consistent results and, in turn, saving time and effort. Minimize silos. When a business-driven analytical solution (often referred to as shadow IT) becomes critical to running the business, that is a signal that it's time to promote the solution up to a centralized system so that it can be supported more fully, integrated with other data, and made available to a larger user base. The data warehouse can take advantage of business user efforts and continue gaining in maturity and value, as well as minimize silos and “one-off” solutions. Multi-platform architecture which takes advantage of existing investment. If your existing data warehouse does bring value for certain use cases, it is not economically feasible to retire it or migrate everything to another architecture (ex: Hadoop or data lake). Instead, we recommend a multi-platform architecture in which the data warehouse is one, albeit important, component. For instance, using a data lake for data ingestion, exploratory analysis, staging for a data warehouse, and/or archival from the data warehouse, are all complementary to the data warehouse which can handle serving much of the curated, cleansed data. Tip: A data warehouse is most advantageous when is deployed alongside other services, such as a data lake, so that each type of service can do what it does best. -------------------------------------------------------------------------------------- Q2: Are you comfortable with a cloud-based Platform-as-a-Service solution? Azure SQL DW is a service offering for the public cloud and the national (sovereign) clouds. It is a PaaS (Platform-as-a-Service) solution in which the customer has no responsibility for, or visibility to, the underlying server architecture. The storage and compute are decoupled, which is a very big advantage of Azure SQL DW. Costs for processing power (compute) are based on a consumption model, which is controlled by data warehouse units (DWUs for Gen1, and cDWUs for Gen2) that can be scaled to meet demanding data loads and peak user volumes. The persisted data is required to be stored on Azure premium storage, which performs better than standard storage and thus is more expensive.  Tip: As a PaaS service, Microsoft handles system updates. For Azure SQL DW, customers may specify a preferred primary and secondary day/time range for system maintenance to occur. -------------------------------------------------------------------------------------- Q3: What kind of workload do you have? Azure SQL DW is most appropriate for analytical workloads: batch-oriented, set-based read and write operations. Workloads which are transactional in nature (i.e., many small read and write operations), with many row-by-row operations are not suitable.  Tip: Although ‘data warehouse’ is part of the product name, it is possible to use Azure SQL Database for a smaller-scale data warehousing workload if Azure SQL DW is not justifiable. Keep in mind that Azure SQL DW is part of the SQL Server family; there are some limitations and feature differences between Azure SQL DW, Azure SQL DB, and SQL Server. -------------------------------------------------------------------------------------- Q4: How large is your database? It is difficult to pinpoint an exact number for the absolute minimum size recommended for Azure SQL DW. Many data professionals in the industry see the minimum “practical” data size for Azure SQL DW in the 1-4 TB range. Since Azure SQL DW is an MPP (massively parallel processing) system, you experience a significant performance penalty with small data sizes because of the overhead incurred to distribute and consolidate across the nodes (which are distributions in a “shared nothing” architecture). We recommend Azure SQL DW for a data warehouse which is starting to approach 1 TB and expected to continue growing.  Tip: It’s important to factor in realistic future growth when deciding whether to use Azure SQL DW. Since the data load patterns are different for Azure SQL DW (to utilize PolyBase and techniques such as CTAS which maximize MPP performance) versus Azure SQL DB or SQL Server, it may be a wise decision to begin using Azure SQL DW to avoid a future migration and future time redesigning data load processes. Do keep in mind though that it is a myth that you can provision the smallest size Azure SQL DW and expect it to perform just like Azure SQL DB. -------------------------------------------------------------------------------------- Q5: Do you have firm RPO, RTO, or backup requirements? Being a PaaS offering, Azure SQL DW handles snapshots and backups each day. The service automatically creates restore points throughout each day and supports an 8-hour recovery point objective (RPO) over the previous 7 days. Once a day the service also automatically generates a geo-redundant backup, with its recovery point objective being 24 hours. Customers also have the capability of creating a user-defined restore point as of a specific point in time. The retention period for a user-defined restore point is still 7 days, after which it is automatically deleted.  Tip: Backups are not taken when the compute resources for Azure SQL DW are in a paused state. -------------------------------------------------------------------------------------- Q6: Do you plan to deliver a multi-tenant data warehouse? A multi-tenancy database design pattern is typically discouraged with Azure SQL DW.  Tip: Although features which can be important to multi-tenancy (such as row-level security and column-level security) are available, you may instead want to evaluate using elastic pools in conjunction with Azure SQL Database for multi-tenant scenarios. -------------------------------------------------------------------------------------- Q7: What kind of data model represents your data warehouse? A highly normalized data warehouse structure does not completely preclude you from using Azure SQL DW. However, since Azure SQL DW takes significant advantage of clustered columnstore indexes (which utilize columnar compression techniques), Azure SQL DW performs substantially better with denormalized data structures. For that reason, following sound dimensional design principles is strongly advised.  Tip: Modern reporting tools are more forgiving of a substandard data model, which leads some data warehouse developers to be less strict with dimensional design. This can be a mistake, particularly if there are many users issuing self-service queries because a well-formed star schema aids significantly in usability. -------------------------------------------------------------------------------------- Q8: How is your data dispersed across tables in the database? Even if you have a large database (1-4 TB+), table distribution is another consideration. An MPP system such as Azure SQL DW performs better with fewer, larger tables (1 billion+ rows) versus many small to medium-size tables (less than 100 million rows).  Tip: As a rule of thumb, a table does not benefit from being defined as a clustered columnstore index until it has more than 60 million rows (60 distributions x 1 million rows each). In Azure SQL DW, we want to make the effort to use clustered columnstore indexes (CCIs) as effectively as possible. There are several reasons for this, but one key reason is because CCI data is cached on local SSDs and retrieving data from cache improves performance significantly (applicable to Azure SQL DW Gen2 only). -------------------------------------------------------------------------------------- Q9: Do you understand your data loading and data consumption patterns extremely well? Being a relational database, Azure SQL DW is considered “schema on write.” Since Azure SQL DW is also a distributed system, distribution keys inform the system how it should allocate data across the nodes. The selection of a good distribution key is critical for performance of large tables. In addition to distributing data, partitioning strategies are different in Azure SQL DW versus standard SQL Server. It is extremely important for Azure SQL DW developers to deeply understand data load patterns and query patterns in order to maximize parallelization, avoid data skew, and to minimize data movement operations and shuffling within the MPP platform.  Tip: PolyBase can be used one of two ways: (1) for loading data into Azure SQL DW (in fact, it’s recommended), or (2) for querying remote data stored outside of Azure SQL DW. PolyBase is the recommended method for loading data in Azure SQL DW because it can natively take advantage of the parallelization of the compute nodes, whereas other loading techniques do not perform as well because they go through the control node. Usage of PolyBase for querying remote data should be done very carefully (see Q15 below). -------------------------------------------------------------------------------------- Q10: Are you comfortable with ELT vs. ETL data load patterns, and with designing data load operations to specifically take advantage of distributed, parallel processing capabilities? The principles and patterns for loading a distributed MPP system are very different from a traditional SMP (Symmetric Multi-Processing) system. To utilize parallelization across the compute nodes, PolyBase and ELT (extract>load>transform) techniques are recommended for Azure SQL DW data load processes. This means that migration to Azure SQL DW often involves redesigning existing ETL operations to maximize performance, minimize logging, and/or utilize supported features (for example, merge statements are not currently supported in SQL DW; there are also limitations with respect to how insert and delete operations may be written; using CTAS techniques are recommended to minimize logging). New tables added to Azure SQL DW often involve an iterative effort to find the best distribution method.  Tip: Although PolyBase does significantly improve the performance for data loads because of parallelization, PolyBase can be very challenging to work with depending on the data source format and data contents (for instance, when commas and line breaks appear within the data itself). Be sure to include adequate time in your project plan for development and testing of the new data load design patterns. -------------------------------------------------------------------------------------- Q11: Do you have staff to manage, monitor, and tune the MPP environment?  Although Azure SQL DW is a PaaS platform, it should not be thought of as a hands-free environment. It requires monitoring of data loads and query demands to determine if distribution keys, partitions, indexes, and statistics are configured well.  Tip: Azure SQL DW does have some emerging features offering recommendations. Integration with Azure Advisor and Azure Monitor is continually evolving, which makes it easier for an administrator to identify issues. Specifically, Azure SQL DW Gen2 utilizes “automatic intelligent insights” within Azure Advisor to display if issues exist related to data skew, missing statistics, or outdated statistics. -------------------------------------------------------------------------------------- Q12: Do you have a low number of concurrent query users? Queries are queued up if the maximum concurrency threshold is exceeded, at which time queries are resolved on a first-in-first-out basis. Because of concurrent user considerations, Azure SQL DW frequently has complementary solutions in a multi-platform architecture for handling different types of query demands. We often see Azure Analysis Services and/or Azure SQL Database as the spokes in a hub-and-spoke design.  Tip: The number of concurrent queries executing at the same time can be as high as 128 depending on the service tier (i.e., the pricing level) and based on resource class usage (because assigning more resources to a specific user reduces the overall number of concurrency slots available). -------------------------------------------------------------------------------------- Q13: Do you have a lot of self-service BI users sending unpredictable queries? A data warehouse is primarily intended to serve data for large queries. Although a tool such as Power BI supports direct query with Azure SQL DW, this should be done with some measure of caution. Specifically, dashboards can be troublesome because a dashboard page refresh can issue many, many queries all at once to the data warehouse. As noted in Q12, for production use of Power BI we often recommend using a semantic layer, such as Azure Analysis Services, as part of a hub-and-spoke strategy. The objective of introducing a semantic layer is to (a) reduce some of the query demand on the MPP (reducing data movement when unpredictable queries come in), and (b) reduce concurrent queries executed on the MPP system, and (c) include user-friendly calculations and measures which can dynamically respond as a user interacts with a report.  Tip: It is certainly possible to use Azure SQL DW directly with an analytical tool like Power BI (i.e., with Power BI operating in DirectQuery mode rather than import mode). However, usage of DirectQuery mode should be tested thoroughly, especially if the user base expects sub-second speed when slicing and dicing. Conversely, if the expected usage is more data exploration where query response time is more flexible, then it’s possible direct querying of Azure SQL DW from a tool such as Power BI will work. The Gen2 tier of Azure SQL DW introduced adaptive caching for tables which are defined as a clustered columnstore index (CCI). Adaptive caching increases the possibility that self-service user queries can be satisfied from the cached data in Azure SQL DW, which improves performance significantly. Another option to potentially consider: Power BI Premium now also has aggregations which can cache data in Power BI’s in-memory model for the first level of queries, requiring a drillthrough to Azure SQL DW only when the user gets to a lower level or less commonly used data. -------------------------------------------------------------------------------------- Q14: Do you have near-real-time data ingestion and reporting requirements? Distributed systems like Azure SQL DW are most commonly associated with batch-oriented data load processes. However, capabilities continue to emerge which support near real-time streaming data ingestion into Azure SQL DW. This works in conjunction with Azure Databricks streaming dataframes, which opens some interesting new scenarios for analysis of lower latency data (such as data generated from IoT devices or the web).  Tip: When using Azure Databricks for streaming data, it is the front-end for the ingestion stream before being output to Azure SQL DW in mini-batches via PolyBase—meaning this can be classified as a near-real-time system, but you should expect there to be some latency. Also, keep in mind that Azure Databricks can also be effectively utilized as a data engineering tool for data processing and loading to Azure SQL DW in batch mode (the JDBC Azure SQL DW connector from Azure Databricks does take advantage of PolyBase). -------------------------------------------------------------------------------------- Q15: Do you have requirements for data virtualization in addition to or in lieu of data integration?  PolyBase in Azure SQL DW currently supports Azure Storage (blobs) and Azure Data Lake Storage (Gen1 or Gen2), which can be used for very selective data virtualization and data federation needs. Data virtualization refers to querying the data where it lives (thus saving work to do data integration to relocate the data elsewhere).  Tip: When using PolyBase for data virtualization (i.e., querying remote data stored in Azure Storage or Azure Data Lake Storage), there is no pushdown computation to improve query performance. This means that Azure SQL DW needs to read the entire file into TempDB to satisfy the query. For queries which are issued rarely (such as a quarter-end analysis, or data to supply to the auditors), it’s possible to use virtualized queries effectively in very specific situations where expectations for query response speed is not of utmost importance. -------------------------------------------------------------------------------------- Q16: Do you anticipate the need to integrate with multi-structured data sources? It is very common for a data warehouse to be complementary to a data lake which contains multi-structured data from sources such as web logs, social media, IoT devices, and so forth. Although Azure SQL DW does not support data types such as JSON, XML, spatial or image, it can work in conjunction with Azure Storage and/or Azure Data Lake Storage (Gen1 or Gen2) which might provide additional flexibility for data integration and/or data virtualization scenarios.  Tip: When connecting to external data, PolyBase currently supports reading Parquet, Hive ORC, Hive RCFile, or delimited text (such as a CSV) formats. Parquet has emerged as one of the leading candidates for a data lake storage format. -------------------------------------------------------------------------------------- Q17: Do you intend to scale processing power up, down, and/or pause to meet varying data load and/or query demands? One of the best features of a cloud offering like Azure SQL DW is its elasticity of compute power. For instance, you could scale up on a schedule to support a demanding data load, then scale back down to the normal level when the load is complete. The Azure SQL DW can even be paused during times when no queries are sent to the data warehouse at all, during which the storage of data is safe, yet there are no compute charges at all (because storage and compute are decoupled). Utilizing scale up/down/pause techniques can prevent over-provisioning of resources, which is an excellent cost optimization technique.  Tip: When an operation to scale up or down is initiated, all open sessions are terminated, and open insert/update/delete transactions are rolled back. This behavior is to ensure the Azure SQL DW is in a stable state when the change occurs. The short downtime may not be acceptable for a production system and/or may only be acceptable during specific business hours. Also, keep in mind that the adaptive cache (available with the Gen2 tier) is cleared when a scale or pause occurs, requiring the cache to be re-warmed to achieve optimal performance. -------------------------------------------------------------------------------------- If you're exploring the best data analytics architecture for your firm's needs, BlueGranite would love to help. Contact us today to discover how our team designs solutions that fit your company and your budget."
"94" "Cloud Scale Analytics – these three words carry a lot of individual weight these days, and taken together, they offer insight into capabilities that I project will have a major business impact in 2019. Let’s take a closer look.  First, the break-down: Cloud – Operating Expense (OpEx) model versus Capital Expense (CapEx); ease of provisioning; ease of deployment; modernization of available services; speed of iterations and pace of change Scale – Across all major analytics workloads: storage, processing and compute, and serving; scale of access/deployment; and availability Analytics – BI modeling; data visualization; machine learning/data mining; artificial intelligence and cognitive analytics Most of these topics could be the subject of their own article, (or perhaps their own book!) such is the impact of the technological forces collectively labeled “Cloud Scale Analytics”. In this post, I’ll break down one way of looking at these terms and explain why any business leader not actively developing or executing plans that heavily leverage Cloud-Scale Analytics in 2019 should head back to the drawing board, ASAP! Next, the build-up: Cloud – By this point, anyone who isn’t proactively technology averse has been subjected to the cloud pitch. Whether it’s Microsoft Azure, AWS, or some other provider, cloud has been to the technology landscape what sliced-bread apparently was to the baking industry (I wasn’t there, but I’ve heard). That is to say, a literal and figurative game-changer.  The physical world of Information Technology has changed. The purchasing cycle of IT assets, both hardware and software, has changed. The legal landscape of IT has changed. And the definition of the user base has changed. If it needed to be summarized quickly, I think the No. 1 outcome of the cloud paradigm shift is accessibility. The cloud has made access to IT capabilities easier and cheaper, and it has enabled a model that allows those leveraging these capabilities to more seamlessly stay on top of the innovation wave.  Gone are the days of the multiyear CapEx hardware and software cycles (well, maybe not quite gone; check out some of SQL Server 2019’s industry-leading offerings here), as well as the cadence of adoption that left many organizations standardized on a platform that was not just potentially several years old – but in many cases several versions old – in a time when even the most recent version available could be a year (or more) out of date. Looking at you, Office 2010! Say hello to the cloud era, where an OpEx model partners with a software release cycle and the cloud-hardware paradigm to enable a potentially near real-time cadence of new capabilities and opportunities. Cloud Scale – Piggy-backing upon that aforementioned model/cycle/paradigm, we can not only increase the speed at which new capabilities are delivered to our business, but we can, often with the click of a button, increase the scale at which we deploy those capabilities. Need to add more users? Click – done; the bill goes up next month. Need to increase storage capacity? Click – done; the bill goes up next month. Need to quintuple processing power for the next day (or three, but then scale it back down)? Click – (OK, maybe a couple clicks) done; the bill goes up next month (and then back down if this event doesn’t recur) and somewhere, a datacenter goes up a couple degrees of heat.  You get the picture; the cloud model allows not just the ease of access to new IT capabilities, but also the ability to scale the volume at which you consume those capabilities to match your usage needs. As someone who has provided ‘server sizing recommendations’ for projects in a past life, I cannot overstate the value here!  This means that the scale of the impact of a new technological capability is no longer limited to the scale of the expected user base and concurrency (which, in turn, had limited the scale of resource sizing); the scale of adoption is now easier than ever to adjust with the click of a mouse, and indeed, is increasingly automated to provide elasticity. The scale of the resources powering a solution can be reactive to the scale of the active/concurrent user base, and that means that everyone can be invited to this party. Cloud Scale Analytics – Finally, let’s zero in on what these words mean together. If we move past the core IT services that underpin all of this – to wit, storage, processing and compute (and serving, to the end users) – with either Data or Analytics as the focus, we arrive at a more refined set of capabilities that belong under this term: Data Lakes (itself comprised of scalable storage for Structured Data, Unstructured Data, Semi-structured Data, Streaming Data) Analytical Modeling capabilities (that can aggregate and query billions of rows of data with scalable compute) Interactive Visualizations, and the ability to train and process Machine Learning models over multithreaded, distributed compute clusters That last one has given rise to accessible Cognitive Analytics (think analytics that mimic human cognition, e.g., capabilities of the human brain like vision, hearing/speech, conceptual abstraction, etc.), which in turn lights up Artificial Intelligence in a practical and available manner. And then I think we finally have it: a working definition of the key term used in the title!  Glad that was easy. And finally, the pay-off: With this newly established context, let’s take a stab at answering the initial question:Will 2019 be the year of Cloud Scale Analytics?In a word, “Yes!”  But of course, it’s not so simple. For the purposes of this next section, we could modify the question a little bit:Will 2019 be the year that the impact of Cloud Scale Analytics matches the hype?Again, I think the answer is a strong “Yes!” The respective ages of the Cloud and Big Data as industry buzz words are getting up there, and 2018 was overwhelmingly the “Year of AI”, at least to this observer. And now we roll all three together, add in tremendous advances in traditional data analysis and visualization tools like Power BI (complete with its game-changing deployment models), and the potential impact should be clear. The backdrop: An increasingly digitized world capturing data about everything, via an explosion of connected devices and sensors and electronically recorded audio, video, and images. The scale of data generated (and captured) by our world. The Cloud Impact: The cloud model has rewritten the rules of technology acquisition, and the pace of updates. The last few years have seen the maturation of cloud technology in terms of management and accessibility, which, combined with the gradual breakdown in resistance to what this paradigm represents, means that in 2019, adoption is no longer reserved for early adopters or bleeding-edge technology firms. The Big Data impact: Changes to distributed storage and computing have dramatically increased our ability to capture, store, and process the tidal wave of data, complementing the continuously rising speed of traditional database functionality. Coupled with the cloud, Big Data technologies mean we can seamlessly scale each point of the equation to respond to the sheer volumes and varieties of data that are available. The AI Impact: It’s hard to overstate the overall significance of AI to the modern organization; it has almost limitless potential to change and improve nearly every task, job, or process in the world today. And it is no accident that Artificial Intelligence has become one of the hottest topics in technology – the preceding Big Data and Cloud impacts have served to make something that was once science fiction into reality, and at an acquisition cost that guarantees wide-spread adoption! But we’re not talking about sentient computers; AI augments human abilities and can automate complex, repetitive actions that previously required a lot of ‘rinse & repeat’ time from workers. Cognitive analytics allow basic computer automation to function over domains that deal in ‘analog’ inputs, regardless of complexity. That is, in the past, computers could add two numbers together effortlessly (whether that’s 1 + 1 or something too long to type out!), but completely fell flat when needing to identify whether an object presented was a living animal, or perhaps a baked good. With the rise of Cognitive Analytics, AI has improved the digital world’s ability to ingest, interpret, understand, and analyze the non-digital world, and by so doing, has fundamentally altered the ability of technology to improve almost any job or task. In parting, 2019 is the year of Cloud Scale Analytics. Rarely before have we seen the timely coalescing of multiple, massive technology trends so complementary as this; and I’ve declined to dig into additional supporting trends like IoT (Internet of Things) and the digitization of our personal lives as consumers, which is in turn giving rise to both more data, as well as a greater appetite for data analysis! There will be headwinds, surely. Two that we see impacting 2019 are the growth of regulation (from a variety of government and non-government bodies and agencies) and a degree of social pushback arising from amoral actors leveraging analytics and AI in less than ethical ways. Neither of these is likely strong enough to prevent the exponential growth of Cloud Scale Analytics in 2019. Neither of these should prevent it; however they do warrant the right level of attention be paid to the ethics of Big Data and AI, as well as proper diligence in observing compliance.  As a technologist and data aficionado, 2019 will be an exciting, potentially break-neck year. As a business leader and organizational culture (and efficiency) devotee, the possibilities are nearly breath-taking and certainly invigorating. Envision the possibilities. Validate the theories.Transform your business! Want to know more about Cloud Scale Analytics? Contact us!"
"95" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/ssis-inspired-visual-data-transformation-comes-to-azure-data-factory\"   },   \"headline\": \"AZURE DATA FACTORY ADDS SSIS-INSPIRED VISUAL DATA TRANSFORMATION\",   \"description\": \"Give Data Flows a try in Azure Data Factory – as Microsoft rolls these Control and Data Flow concepts into the service, it’s rapidly becoming a compelling, modern service for all kinds of ETL work.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/SSIS-Inspired%20Visual%20Data%20Transformation%20Comes%20to%20Azure%20Data%20Factory.jpg?width=805&name=SSIS-Inspired%20Visual%20Data%20Transformation%20Comes%20to%20Azure%20Data%20Factory.jpg\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Merrill Aldrich\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 60     }   },   \"datePublished\": \"2019-01-08\",   \"dateModified\": \"2019-06-14\" } You may have heard about upcoming enhancements to Data Factory. Back in September we talked here about some of the architectural differences between SQL Server Integration Services (SSIS) and Azure Data Factory Version 2 (ADF V2), and the questions to pose as you try to select one of these products for data processing work. Well, the latest enhancements are out and they are exciting.   Data Flow SSIS has always delivered two key features to ETL development: a graphical user interface for building data transformations, and the metaphors of a control flow and a data flow in creating packages, which act like small programs. A control flow is a series of separate tasks that you can chain together into a visually authored program, and a data flow is a similar, visually authored pipeline that processes rows of tabular data through transformations, filters, lookups, joins, and so on. Data flows are always called from control flows, generally in the context of some larger process. This metaphor is so commonplace to users of SSIS that it’s sort of like the water we swim in – and when initial versions of ADF arrived without this, it was puzzling for some. Never fear, though – because the team at Microsoft knows how successful the control flow and data flow concepts are, they have been actively phasing it into ADF. The first version of Data Factory was based on moving data using a tumbling time window approach, but V2 brought the introduction of a Control Flow (June 2018). Soon, the Data Flow* feature will also become generally available. Since September we’ve been able to trial the data flow features in a preview. It’s worth kicking the tires, and it looks really promising.  How is this Different? (Enter Databricks) While the visual metaphors and the ease of use look similar to SSIS, the technology under the hood here is radically different. SSIS works, at a high level, by the visual editor building your packages as XML files. There’s an executable dtexec.exe that can read, parse, and execute those package files. Over the years different flavors of this have come along, such as Project Deployment in the SSIS catalog in SQL Server, but the fundamental architecture is the same. It works well, but it’s not necessarily cloud-optimized, and it’s a bit limited in terms of taking advantage of new innovations that have been created in the big data world – especially with respect to parallelism/massively parallel processing (MPP)/multi-machine scale-out. The approach in ADF V2 is sort of 180 degrees from SSIS – it starts with the premise that data is going to be processed on a Spark cluster (specifically an Azure Databricks cluster, which is a specific flavor of Spark) in the cloud, and the engineering of the data flow ADF components is all about making it easier and more intuitive to harness that massive processing horsepower. We’ll return to how it does that in a second, but first, why is it important? Well, it means that we get two wonderful capabilities right in the product: Automatic scaling to fit workload requirements. We don’t have to buy machinery big enough to handle the peak, maybe infrequent workload, and the infrastructure can be tailored for best fit, all the time. On top of auto scaling, there’s the opportunity for this system to do execution optimization on your code. Where SSIS will reliably run the packages you feed it, it does little in the way of tuning or optimizing those, and they just run in the sequence that you’ve written. Think about SQL Server’s Query Optimizer – it’s not an exact match, but Spark has some similar features that enable it to do optimization of code that you provide. Because ADF is deliberately built over the top of these cluster technologies, we get them with no additional or special effort. That’s huge. ADF Data Flow Design There are several excellent, existing walkthroughs out there showing how to build your first data flow, so I won’t take you through that, but I do want to talk about how ADF harnesses the different technology in this new world. If you try ADF Data Flows, you’ll find the visual editor looks cosmetically different, but conceptually is much the same as SSIS, so the learning curve should be short. Many SSIS transformations map directly to equivalents in ADF, such as joins, filters, branching, derived column expressions, and so on. What’s happening behind the scenes is quite different, though. First, you’ll be working in a browser instead of a desktop tool, but it’s a fully featured authoring experience. Second, the Data Factory setup will compose and store your Data Flow as a JSON object (think: a modern version of the SSIS XML file). Third, and this is the new bit: Data Factory will automatically compile your work into ready-to-run code for Apache Spark, on a Databricks cluster – with no additional effort from developers. This is really the vital difference: ADF V2 Data Flow is, in some sense, a visual editor to enable you to “write” code for a Databricks/Spark cluster without code-writing. This means it can handle huge sets of data with a lot less time and energy invested in infrastructure concerns. Sure, there is some important knowledge a team will need to gain to configure the cluster correctly, but it’s dramatically faster and easier than the old days of building out physical Hadoop clusters with their administrative workload, and it’s much easier for an ETL developer to take advantage of the power of that underlying technology. New Decisions So how does this change the math for SSIS vs. ADF from our last installment? Until now, the fact that ADF didn’t have the equivalent of the SSIS data flow was a barrier for some teams and some types of workloads. Once this feature becomes generally available, that barrier will disappear and the ADF service will become viable for teams who: Want to work with a visual/GUI editor as opposed to writing traditional source code Need that “T” in the E “T” L – that is transforming the data during its trip to the destination system, as opposed to having to stage it first and then transform it (ELT) Want to take advantage of modern/better scaling and parallel processing that comes with access to a Spark cluster, with an easier learning curve than building out a cluster   ETL pattern(transform in flight) ETL pattern(load first, call functions in the destination to transform the data) ADF V2 Limited EXCELLENT Excellent SSIS Excellent Excellent That said, as you can see, this is a cloud service-centric idea. You will still need, as before, at least a hybrid on-premises/Azure environment to take advantage of this new feature. The processing will be performed in Azure. If you are truly constrained to on-premises systems, then SSIS may still be your best option. One other adjustment to consider is that this new architecture really favors the inclusion of a data lake in your overall plan. ADF is designed for, and works really well with, a cloud-hosted data lake. Where SSIS architectures often transport data straight from source database to destination database, consider using a lake to land your data, and keep copies of the raw data files for different potential use cases. Finally, a word about cost. If you choose a traditional, on-premises SSIS deployment, you will probably run that under some level of purchased SQL Server license, while for ADF you’d pay on a typical cloud-service model, with the usual differences between the two. One tends to be a one-time, sunk cost, while the other you pay for as you go, and can turn off if needed. Because Data Factory V2 is based on this pay-per-use model, estimating the total cost for this new tool requires a bit more detail about how your solution will work, which translates into a monthly operational cost estimate**. The details of pricing for ADF V2 existing (meaning already generally available) features are published, but for the yet-to-be-released data flow feature, are not formalized as of this writing. However, if we look at the ADF V2 pricing examples we can at least get some sense of how to think about this. Each operation in ADF has a small incremental cost, so the total bill will be the sum of executions of those over time. Some of these operations are timed, especially data movement, so there’s an impact derived from the volume of data (represented as Data Integration Units or DIUs). In addition, Data Flow uses a Databricks cluster in the background, which might be spun up on demand or left on, depending on the frequency of your loads. Finally, the cluster itself can have different scaling depending on the quantity of data you have to process, which probably will also have some cost implication. So give Data Flows a try in Azure Data Factory – as Microsoft rolls these Control and Data Flow concepts into the service, it’s rapidly becoming a compelling, modern service for all kinds of ETL work. * Not to be confused with Power BI Dataflow.**  If you go down the path of blending both these services by running SSIS packages inside ADF V2 using the Integration Runtime, then the SSIS components and required SQL database do have a cloud-pricing model, as well."
"96" "Needing to scale up your predictive power and data processing capabilities, but a bit apprehensive about moving awesome machine learning models to a new platform? No need to worry! In today's post, I'll show you the easy way to migrate and scale machine learning and deep learning models from Python over to Azure Databricks. Plus, I'll also talk about why reworking your existing models using MLlib in Spark might be a good idea.  Data scientists spend a lot of time training models and tuning them to optimize their performance for whatever use case is at hand. Traditionally, this is done on local workstations using machine learning libraries such as scikit-learn, H20, or XGBoost in Python. Many AI teams are making the shift to begin developing on Spark and Databricks, which allows for embarrassingly parallel model training, tuning, and cross-validation on a cluster. However, this doesn't necessarily mean that we have to throw away the Python models that we've already built and have put to use. Migrating them to Databricks is easy!  Migrating and Scaling from Python (scikit-learn) Anyone who has used Python for machine learning has heard of scikit-learn. It's one of the most popular libraries for machine learning, consisting of a plethora of clustering, classification, regression, and dimensionality reduction algorithms.  In 2016, the team at Databricks saw the need for users to be able to migrate their scikit-learn models to Spark. Thus, they released the spark-sklearn package. This package allows users to: Train and evaluate models in parallel. Spread the work across multiple machines with no changes required in the code between the single-machine case and the cluster case. Convert Spark DataFrames seamlessly into Numpy ndarrays or sparse matrices. Distribute SciPy's sparse matrices as a dataset of sparse vectors.  These features allow for better scalable processing of machine learning models without making users leave their scikit-learn comfort zone.  from sklearn import svm, datasets from spark_sklearn import GridSearchCV iris = datasets.load_iris() parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]} svr = svm.SVC(gamma = 'auto') clf = GridSearchCV(sc, svr, parameters) clf.fit(iris.data, iris.target)  In the example code above, you can see the use of both the normal scikit-learn package to bring in the desired algorithm, as well as the spark-sklearn package to perform a grid search across parameter and CV folds. Scikit-learn isn't the only library that can be used in Databricks. Other packages such as H2O and XGBoost have Spark counterparts as well. To learn how to use other third-party libraries in Databricks, click here. Importing Trained Neural Networks (ONNX) ONNX, or the Open Neural Network Exchange, is a format for representing deep learning models such as neural networks. This format allows for users to train models on popular frameworks such as Cognitive Toolkit, TensorFlow, PyTorch, and MXNet, and save them for distribution and use in other places.  For example, let's say we've created an awesome deep learning model on our local GPU-based workstation using Cognitive Toolkit. Saving the model in the ONNX format is easy.  import cntk as C  x = C.input_variable(<input shape>) z = create_model(x) #your create model function z.save(<path of where to save your ONNX model>, format=C.ModelFormat.ONNX)    Once we've saved our model in a location accessible by Databricks (Blob Storage or Data Lake Store), we can import the model just as easily.  import cntk as C z = C.Function.load(<path of your ONNX model>, format=C.ModelFormat.ONNX)    ONNX exporting and importing not only works with Cognitive Toolkit, but a variety of other frameworks as well. For a list of tutorials on how to get started, click here. In addition to using ONNX, you can also import from MLeap, which is a common serialization format for machine learning pipelines. To learn how, click here. Retraining using MLlib Don't forget that Spark includes a really powerful set of algorithms in MLlib, Apache Spark's scalable machine learning library. Personally, I've used MLlib for quite a few clients here at BlueGranite and am starting to love it. MLlib includes the following classes of algorithms and functions: Classification - logistic regression, naïve Bayes, decision trees, and random forests Regression - generalized linear regression and survival regression Recommendation - alternating least squares (ALS) Clustering - K-means and Gaussian mixtures (GMMs) Topic modeling - latent Dirichlet allocation (LDA) Frequent itemsets, association rules, and sequential pattern mining Distributed linear algebra - singular value decomposition (SVD), principal component analysis (PCA) Statistics - summary statistics, hypothesis testing, standardication, normalization, and much more. If you have a machine learning model that you've trained outside of Spark/Databricks, you can always retrain the model using MLlib to sweep through additional parameter combinations or perform a more robust cross-validation of the model. This can help in situations where you are getting lackluster performance from your previous model, but it takes too long on your local workstation to optimize the model any further. For some examples to get started using MLlib, click here. Putting your Model to Work As you can see, scaling up your AI practice is easier than ever thanks to Azure Databricks. Whether you're creating new machine learning solutions or wanting to operationalize your existing models, Azure Databricks is the premier platform for AI in the cloud.  One common issue I hear from clients is that they find it difficult to operationalize their models. In other words, despite having great data science teams creating robust machine learning models, organizations still struggle to use their models in an automated way.  Azure Databricks Job Scheduler makes operationalizing your models super easy. Within a couple clicks, you can have a notebook in Azure Databricks scheduled to run and score your new incoming data every day. So, even if you aren't having training or performance issues with your models, automating the use of the model may be reason enough to give Azure Databricks a try!"
"97" "As 2018 comes to an end and the winter weather takes hold of BlueGranite’s headquarters in southwest Michigan, we’d like to take a moment to reflect on our appreciation for our clients and partners, our organization’s growth, and to share some of the best of our business insights from the past 12 months.  This proved to be another record year for BlueGranite – we’re thankful to our staff, clients, and partners for that growth, and for our continued success. We added new team members, expanded our partnerships, and gave back to communities across the U.S. We also continued to foster strong client relationships through our innovative solutions.  Highlights of 2018 This year we welcomed 14 new faces to the BlueGranite team. Thanks to our newest members, we have expanded our nationwide presence with clients located in 32 states and team members located in 16 states. In addition to our team expansion, we are pleased to announce that we spurred business growth by 24 percent this year. This year, Microsoft awarded BlueGranite as its 2018 MSUS Data & AI Partner for Big Data Analytics, and as a global 2018 Big Data Analytics Partner of the Year finalist. We are excited that our successful delivery of Microsoft Data & AI solutions to our clients stood out among the 2,600 partner entries from 115 countries, meriting one of only 39 coveted Microsoft award opportunities. We are honored by Microsoft’s recognition of our commitment to innovation, and we can’t wait to see what is in store for BlueGranite in 2019. Our success allowed us to again give back to our local communities. BlueGranite sponsored its second annual Day of Service program, where we encourage team members to spend a work day volunteering for a local nonprofit organization of their choice. Across the country, employees came together to donate their time and service to 10 charitable U.S. organizations – volunteering for multiple food banks across the country; working to help provide housing for those that cannot afford it; implementing and upgrading tech at a kids’ summer camp; preparing fleece blankets for children in need; cleaning up Michigan’s rivers; and more. We would like to extend a huge thanks to our team members for giving their time and energy to these great causes! In addition to volunteering time in 2018, BlueGranite announced its second annual BlueGranite Give Back program: our team members nominate, then vote for, a nonprofit organization to support in December. This year, we chose the National Alliance on Mental Illness of Greater Toledo, a nonprofit organization that promote wellness – through education, support, and advocacy – for individuals and family members in Greater Toledo living with mental health issues and illness.  2018’s Most Popular Content Every year our writers create a wealth of valuable blog content, and 2018 proved to be no different. Check out our top technical tips and some of 2018’s greatest Data and Analytics industry advancements. Blog Posts Let Azure do the Heavy Lifting on Your AI Workload  If you're a data scientist or an AI aficionado, you already know the struggles of using your laptop or workstation to train complex models. In this post, explore how Azure can help offload work from your data and analytics team to a highly scalable cloud environment to complete the job faster than ever before. Cognitive Services Showcase In this six-part series from BlueGranite’s Data Science and Analytics team, we've highlighted some of the most useful and popular Microsoft Cognitive Services, such as Search, Vision, Speech, Language, and Knowledge. Learn more about each service and key use cases in this educational series. Beyond the Basics: Report Design Considerations for Self-Service Users Your company rolled out self-service Power BI – how exciting! But what can you do to take your reports and visualizations from “just adequate” to “truly great”? How can you better engage your audience and convey the most important information clearly? Learn more in this post about building up your Power BI skill sets. The Right Tool for the Job: Azure Data Factory V2 vs. Integration Services With the features of Azure Data Factory V2 becoming generally available in the past few months, especially the Integration Services Runtime, the question persists in our practice about which data integration tool is the best fit for a given team and project. Both Azure Data Factory and SQL Server Integration Services are built to move data between disparate sources, and they do have some overlapping capabilities. If you are new to either of these tools, this post summarizes some questions to help you begin understanding the differences. Solution Briefs From Code to Table - Using Technology for Smarter Food Production Conserving water and power while improving crop yield is vital to the commercial agriculture industry’s long-term success. Using natural resources as efficiently as possible translates to higher-quality food and good land stewardship. A global leader in freshwater and wastewater management, about to embark on a high-stakes environmental project, wanted to thrive in both arenas. It partnered with BlueGranite to ensure success. Together, we are applying data science and information technology to create a state-of-the-art precision irrigation system. Modern Data Platform Gives Operational Insights & Saves Millions Encumbered with poor data retrieval capabilities and slow turnaround time due to a third-party data storage vendor, one of the largest transportation providers in North America enlisted our help to migrate its on-premises data warehouse to an Azure environment. The client’s third-party data storage outlet ran on an archaic system, which often created more questions than answers, and was quite expensive in terms of storage cost. Our state-of-the-art Azure-based solution helped to streamline the company’s data collection and retrieval procedures, fostering actionable insights. Webinar Getting Started with AI: An introductory session for business and technology leadersCognitive-based organizations have adopted artificial intelligence (AI) technologies and integrated AI solutions into the core areas of their business. Transforming a business with AI begins with a clear understanding of AI, key technologies, and use cases. Integrating AI into your business is not only a viable and realistic option, but it can also be a key to the future success of your business. What a Year! BlueGranite had a record year in 2018 – and we couldn’t have done it without you! From the BlueGranite team, we thank you for your support and wish you a wonderful holiday season and happy New Year!"
"98" "If you are like many data professionals, you may have years of experience building and supporting on-premises data warehouse implementations, but have become overwhelmed by the blisteringly fast rate of change in the industry over the past few years. Business Intelligence tools and platforms have never required more agility and adaptability than they do today, and never have the choices and technologies been so plentiful. With this constant change can come much anxiety and confusion, but also great innovation and opportunity.  You Got This! My hope with this post is two-fold. Primarily, I’d like to set your mind at ease. The more things change, the more they stay the same. You may not know how to program and build an autonomous highway vehicle, but you are not as far away from BI relevance as you might fear. I also intend to introduce you to a modern Business Intelligence architecture stack that should feel familiar to you as you explore the vast array of Microsoft Cloud offerings. To that end, I would like to share with you one possible arrangement of Azure tools which might begin to help you build an understanding of possible roadmaps for moving your data warehouse into the cloud. Let's start with an overview of some key Azure services: Azure SQL DatabaseAzure SQL Database (DB) can simply be your traditional SQL Server database, except it’s in the cloud. That’s the simplest way to think about it. Developers and administrators with an on-premises SQL Server background will find a low learning curve when moving to Azure SQL DB. Your data will have a comfy new home in the cloud and you will reap the performance benefits of managed services. Along with great flexibility and scalability, you will pick up many of the features previously only available by ponying up for an Enterprise license. With two deployment options, Azure SQL DB can be a simple cloud replacement of on-premises databases, but it can also extend and expand upon your traditional relational data through support of many other data types like JSON, XML, spatial data, and much more. Azure SQL Data WarehouseAzure SQL Data Warehouse is the Azure service for processing and storing massive volumes of data, and for consolidating disparate data into a single location using simple SQL constructs. It offers flexibility and cost savings by separating compute activities from storage, so you only pay for what you are using. Spin down your compute capacity when you don't need it without losing access to your data. SQL DW scales easily by distributing compute throughput which is a major component to this service’s performance advantage through Massively Parallel Processing (MPP). For the sake of keeping this article digestible, we’ll forego the technical details under the hood of MPP. Just know that it is the secret sauce that brings to Azure SQL DW the ability to ingest and analyze petabytes of data with unbelievable performance. Azure SQL DW is not the service for processing small datasets. If you are comfortable with relational data warehouse structures, think bigger, more diverse and performant, and you’ll begin to see the power of Azure SQL DW. Azure DatabricksAzure Databricks is an interactive platform built to run and manage Apache Spark in the Azure cloud. Spark is a data processing engine designed to be open and versatile, and serve the needs of everyone from data engineers to data scientists. Use it for data movement activities, to collect and process live streams of data, even IoT sensor feeds. Additionally, Azure Databricks supports a variety of languages: R, Python, SQL, Scala. Azure Databricks provides a friendly workspace to manage Spark clusters, and manipulate and visualize data. The marketing material really gravitates to the phrase “unified analytics platform” which, once you really dig in and start to understand it, becomes less marketing noise and more “Yeah, that actually describes it quite nicely.” There is not much to offer here in the way of drawing a parallel between Azure Databricks and something you are already familiar with in a traditional data warehouse environment. There is a steeper learning curve with Azure Databricks, but this service offers excellent opportunity for innovation. Azure Data Factory V2You can use Azure Data Factory (ADF) V2 as the cloud incarnation of SQL Server Integration Services. As is par for this post, this statement massively understates the capacity of ADF. It provides a hybrid “pipeline model” so you can group logically related units of work and seamlessly span your cloud and on-premises data sources while leveraging advanced compute services (think HDInsight, Hadoop, Spark, etc.) ADF provides 75+ connectors and can be used to lift and shift your existing SSIS packages from on-premises into your Azure environment. You can also automate and schedule workflows with data-driven activities. If you are familiar with visual methods of data integration, you’ll pick up on ADF like a fish to water. Azure Analysis ServicesPerhaps the lowest learning curve in this transfer is to Azure Analysis Services (AAS). This Platform as a Service (PaaS) is your analytics engine and all-important semantic layer for your data warehouse. You can still access your on-premises data through the gateway while taking advantage of the scalability benefits of managed services, or you can move your data assets to the cloud using the previously mentioned services to provide an end-to-end cloud data solution. AAS is fully compatible with all your current known and trusted tools (SSMS, SSDT, Power BI, etc.) so you’ll be comfortable working with it from day one. Putting It All Together (batch marries stream) Now that we’ve covered some basic components, lets pull them all together into a possible use case to envision our “Modern BI Architecture.” Let’s suppose you have IoT sensors capturing measurements from a manufacturing machine. This could be in the form of temperature, rotation speed, tension, amperage, etc. Those sensors could produce a staggering amount of data, but how do you measure the efficiency of that machine, or even the machine’s task at hand? You may need real-time on-the-ground operator feedback. You may need to turn that data into efficiency metrics monitored not only by line operators and foremen, but by managers and corporate decision-makers who all have a different stake in and opinion of the timing of inbound data. Further suppose that you have finished good quality data which is generated and stored separately by independent systems, but that these systems are subject to less frequent batch processing. Your data scientist may need to pull data from both your batch and your streaming data sources in order to make meaningful prescriptive and predictive recommendations and to train machine learning models. Landing your sensor data and your batch data in a semi-structured blob store, you can use Azure Databricks and Azure Data Factory to stage and eventually land in an Azure SQL Data Warehouse. Once there, it will be ready to be hooked by Azure Analysis Services for presentation. Azure Reference Architecture Diagram Maybe you have never dealt with any of these services or technologies. I made a claim earlier in this post: The more things change, the more they stay the same. When you boil the above scenario down to its simplest form you have what you should find a familiar formula. Regardless of the source or the target, you must extract data and load data. Somewhere in between your source and target you may need to do some transformations – extract, transform, load (ETL). If you are familiar with data warehouse modeling, SQL and the concepts of ETL or ELT, you should be right at home with modern BI architecture. If you’re looking to advance your organization’s Business Intelligence skills, contact us today. We offer hands-on, instructor-led training in the latest analytics technologies, and design custom solutions that transform your enterprise through data."
"99" "Azure Data Factory (ADF) V2 is a powerful data movement service ready to tackle nearly any challenge. A common task includes movement of data based upon some characteristic of the data file. Maybe our CSV files need to be placed in a separate folder, we only want to move files starting with the prefix “prod”, or we want to append text to a filename. By combining Azure Data Factory V2 Dynamic Content and Activities, we can build in our own logical data movement solutions. Follow our walkthrough below to discover how.  Dynamic Content: What, Why, Where and How to Use What: Dynamic Content is an expression language that uses built-in functions to alter the behavior of activities in your pipeline. Many of the functions, like IF() and CONCAT(), are familiar to many users of Excel or SQL. Why: Dynamic Content decreases the need for hard-coded solutions and makes ADF V2 Pipelines flexible and reusable. Where: To access Dynamic Content, place your cursor into the file path or file name areas of Datasets. How to use: Combine expressions to easily create endless dynamic pathing options. From updating filenames using CONCAT() to complicated directory structures and file pathing based upon pipeline and file names, pipeline execution time, and more.  Tutorial Requirements We’ll use three additional Azure Data Factory V2 tools for our use case: To copy our data from one location to another we will use the Copy Activity  For file or directory information (like the contents of a directory), we need the Get Metadata Activity  To iterate through a list of files contained within a directory, we have the ForEach Activity  Tackle Complex Data Movement with Ease Now that we have some background, let’s get to our use case. The Census Bureau releases new American Community Survey data annually, which contains U.S. household education, housing, and demographic information. We’re going to split the individual files out by multiple criteria. Estimate files and margin files each need to be grouped together and segmented by sequence, plus some files need to be handled based upon their file extension. To get our data, we used the HTTP connection with the ZipDeflate option to download Alabama_All_Geographies_Not_Tracts_Block_Groups.zip, a file containing census data for the state of Alabama, to our Azure Blob Storage account. Alabama_All_Geographies_Not_Tracts_Block_Groups.zip has over 200 files that contain city and county level information along with corresponding statistics.  We want to segment the files to make it possible for another application to efficiently process entire folders of files that share the same schema. The rules for segmentation are below: If file starts with ‘e’ for estimate, place in a folder like ‘seq/<sequence number>/e/’. If file starts with ‘m’ for margin of error, place in a folder like ‘seq/<sequence number>/m/’. If file starts with ‘g’ for geo, place in a folder like ‘geo/csv’ or ‘geo/txt’ based upon the file extension. For example, e20165fl0001000.txt starts with ‘e’ indicating it is an estimate file (e20165fl0001000.txt). The sequence number for the file is 001 (e20165fl0001000.txt). The final location for the file should then be ‘seq/001/e/e20165fl0001000.txt’. We handle the margin file, m20165fl0001000.txt, in a similar way. The final location should be ‘seq/001/m/m20165fl0001000.txt’ The census bureau also has geo files like g20165fl.csv and g20165fl.txt, which are not specific to a particular sequence, so we will handle them differently. The final location for g20165fl.csv will be ‘geo/csv/g20165fl.csv’. For g20165fl.txt, the location we want is  geo/txt/g20165fl.txt To complete our goal, we will need to use Get Metadata, ForEach, and Copy activities in combination with the Dynamic Content functionality provided in ADF V2. We start by creating a dataset referencing our Alabama census zip file. We can put in any name we would like to use (I named mine AlabamaCensusZip) and then point to the blob storage location where we have saved our zip file. Note that when we point to the zip, it displays in the Directory portion of the file path.  Once we have the dataset created, we can start moving the activities into our pipeline. First is the Get Metadata activity. We point the Get Metadata activity at our newly created dataset and then add an Argument and choose the “Child Items” option. The Child Items option reads in the file names contained within the .zip and loads the names into an array we will iterate through.  We can see this by running Debug on the pipeline and then viewing the output of the Metadata activity.  Next, we add the “ForEach” activity to the pipeline.  We name our activity and connect the Get Metadata activity to the ForEach activity. Once named and connected, we access the Settings tab of the ForEach activity and then reference the Get Metadata activity child items in the “Items” section by entering “@activity(‘ZipMetadata’).output.childItems”. Now the ForEach activity is setup to iterate through the files contained within the zip file (the same items we saw as output from the Get Metadata activity).  After we update the settings, access the Activities menu inside the ForEach activity and drag in the Copy activity.  In the Copy activity, create a new Source that points at the zip file location in our blob store, then we use the “@item().name” value being generated by the ForEach activity to create the reference to the individual files within the zip.  Our next task is determining where our files will go by using Dynamic Content. Looking back at our requirements, we need to split the individual files out by multiple criteria. Estimate files and margin files each need to be grouped together and segmented by sequence, plus some files need to be handled based upon their file extension. Let’s return to our Copy activity; we select Sink and add in a new location. Then select the Connection tab and place your cursor in the Directory box. You should see “Add dynamic content” appear below.  Click the Add dynamic content link to bring up the editor.  Now we get to translate our requirements into code using the Dynamic Content expressions provided by ADF V2. If we look back to our requirements, we see the following: If file starts with ‘e’ for estimate, place in a folder like ‘seq/<sequence number>/e/’. If file starts with ‘m’ for margin of error, place in a folder like ‘seq/<sequence number>/m/’. If file starts with ‘g’ for geo, place in a folder like ‘geo/csv’ or ‘geo/txt’ based upon the file extension. Given those requirements, we first identify the starting value of the file names. In the Dynamic Content editor, we can see there are several function categories, including logical functions and string functions. To reference the names of the files within our zip, recall we are operating inside of the ForEach activity and the files can be referenced with item().<property>. For the purposes of this example, we will use item().name. Let’s combine the IF function with STARTSWITH, CONCAT, SUBSTRING, and item().name to tackle the first piece of our scenario, what to do when the file starts with ‘e’. @{IF(STARTSWITH(item().name,'e'),CONCAT('seq/',SUBSTRING(item().name,9,3),'/e/'),'false')} As you can see, Dynamic Content in ADF V2 looks like what we see in the Excel formula bar or SQL editor.  This should be comforting for new users. However, there are a few things new users should be made aware. With Dynamic Content, we wrap our entire expression within braces {} and place @ at the beginning. Also, SUBSTRING starting position is 0. After validating the pipeline and then selecting Debug, we let the pipeline run and check how our Dynamic Content expression handled our files.  It worked! From here, we can continue to expand our Dynamic Content expression to handle the remaining items from our use case. @{IF(STARTSWITH(item().name,'e'),CONCAT('seq/',SUBSTRING(item().name,9,3),'/e/'),          IF(STARTSWITH(item().name,'m'),CONCAT('seq/',SUBSTRING(item().name,9,3),'/m/'),                    IF(AND(STARTSWITH(item().name,'g'),ENDSWITH(item().name,'txt')),'geo/txt/',                              IF(AND(STARTSWITH(item().name,'g'),ENDSWITH(item().name,'csv')),'geo/csv/','fail')                              )                    )          )} Our use case is complete. The 200+ files contained in the Alabama census zip file will now automatically move to the proper location. Because our solution isn’t hard-coded, we can use the same code to ingest additional states from the Census website. Dynamic Content can even be used to create dynamic SQL queries! When Azure Data Factory V2 Dynamic Content and Activities are combined, even complicated data movement can be easily tackled.    BlueGranite helps organizations discover opportunities to evolve through technology and realize their full potential. Subscribe to our blog to gain insights into the latest tools for data management, the modern data platform, business intelligence, and AI.   "
"100" "Today, we’ll round out our Digital Transformation series with a detailed look at the fourth and final pillar, Transform Products. We’ve already learned how data and analytics drive transformation in the areas of Customers, Employees, and Operations, and now we’ll explore how innovative organizations are using information and insights to offer new and improved products and services.  Pioneer Value First, let’s clarify the scope or definition of Transform Products, in particular, the word ‘products,’ which more broadly defined, represents the goods and services an organization provides to its customers. Depending on the industry in which your company operates, you may provide products (software, consumer packaged goods, medical devices), services (consulting, education, legal), or both – whatever it is that reflects value in the eyes of your customers. In this way, the Transform Products pillar is like the Engage Customers pillar, in that both are outward looking, focused on customer wants and needs, as compared to Empower Employees and Optimize Operations, which are inward looking, focused on organizational needs. While Engage Customers is about delighting existing customers and finding and acquiring new customers, Transform Products is about creating new value (products and services) through innovation. Gain Competitive Advantage Here’s one example of how an enterprise can innovate to stay ahead in its field. Insurance companies operate in a dynamic and highly competitive industry. It’s also an industry that is awash in data, and has at its very core, a deep appreciation for data to develop risk models, pricing, and so forth. So how might an insurance company leverage technology to offer new and innovative products? Take a core process, such as claims processing. Assume you have a supplemental insurance policy for injury or illness and experience an incident which leads to filing a claim. As a claimant, you’re required to provide evidence of your incident. The variety of supporting information could include hospital bills, receipts for medication, photographs, and more. While this is pretty straightforward for the customer, it presents a real challenge to the claims processors, in that it requires a tremendous amount of manual review of materials in order to approve or deny a claim. Artificial intelligence (AI), which harnesses the power of machine learning, can help. One promising area of AI is in what Microsoft calls Cognitive Services, which includes capabilities in the area of Vision (interpreting documents and photos) and Text (extracting insight from unstructured text). These services can be used to automate, in part, the claims processing scenario we’ve described. Of the variety of supporting documents, some are standard, such as industry-accepted forms, or a bill from a specific hospital, and these standard documents can be processed by Cognitive Services to synthesize this information, allowing claims processors to spend their time on the unique elements of each claim. So, how does this translate into innovation and new products or services for the insurance company? Now, claims of a particular type, with some common document types, can be expedited and processed within a shorter timeframe, allowing the insurance company to offer a “quick claim” service which differentiates them in the marketplace and delights their customers. And, as the capabilities of the AI services continue to mature, additional, and more complex documents could be included in the solution. Boost Achievement The above example belies a common misconception about the role of AI, specifically how AI may displace workers. Rather, in this scenario, it provides a very real example of how AI can amplify the impact of what a knowledge worker can accomplish by blending human skills and experience with those of AI. As Digital Transformation advances, businesses are increasingly able to take advantage of the innovation, flexibility, and creativity it can offer; frequently reaping agility, growth, and cost savings for their efforts. Enterprises embracing the process often leap from backward-looking businesses to forward-thinking industry leaders. BlueGranite continues helping organizations discover opportunities to evolve through technology to realize their full potential. Keep an eye on our Business Insights blog to read about future enterprise advances, or explore our Solution Briefs to dig into our previous transformations."
"101" "This is the second part in my running series on the importance of data structure in analytics (you can catch Part 1 here), where I seek to demystify – and perhaps justify – some of the commonly accepted “best practices” around how we, as data engineers and business intelligence developers, seek to arrange data for analytical purposes. For this entry, I’ve decided to focus on a narrow (and thus defensible) slice of a broad and sometimes contentious topic: the dimensional model, or star schema, and its role in the Microsoft analytics world. Specifically, I want to take a look at why adherence to what can often seem like something of an esoteric concept really and truly can pay dividends when working with Microsoft analytics tools, and especially Power BI – one of the most capable and popular analytics tools out there today. Let’s first take a quick look at the star schema and talk a little bit about its history, and then from there look at how it has come to remain a best practice gold standard in Microsoft analytics, and how it can help guide your efforts when working in Power BI.  The Dimensional Model – A (Very) Brief History I realize there are some of you who are thinking, “Ugh, did he say ‘schema’?” or who might otherwise have no idea what I’m talking about when I refer to a dimensional model. For those of you who do – please, bear with me for a moment. In this case, both “star schema” and “dimensional model” can be used interchangeably, and refer to a set of related data tables, both in terms of how those tables are built and what data they contain, as well as how – quite specifically – they are related to one another. We can think of it as being representative of a particular type of data model, or a way of structuring and storing our data, and one which is designed specifically with analytical use-cases in mind. This is different than data models designed to favor speed of writing data rather than reading it, such as highly normalized table structures. Dimensional models, conversely, tend to be de-normalized – or built with as few tables as possible, and in such a way that makes analysis more natural and intuitive – more on that in just a minute.   Figure 1 - The Star Schema The dimensional model has been around for quite some time, at least relative to the fast-paced advance of information technology as a whole, and continues to confer benefits to its adherents today. Made popular by Ralph Kimball years ago, the dimensional model was central to Kimball’s prescribed approach to data warehouse architecture. Just as a simple level-set for the sake of this post, we can think of a data warehouse as a common repository where data is combined (originating from many different source systems) and conformed (made to adhere to common definitions and concepts) for the sake of helping facilitate analytics and reporting, usually at a large scale. While plenty of data warehouses are still built today using this design, some of the core benefits to its implementation were embraced by other analytics tools along the way, and those legacies live on in the here and now – particularly within the Microsoft Business Intelligence and analytics tool set. The Dimensional Model and Power BI Modern tools like Power BI might allow you to get away with building reports out of, and generating analytical value from, data in an unstructured, or improperly structured, data model. However, the fact that a traditional architecture was embraced early on in the evolution of analytics tools – especially by Microsoft – is key to its importance today, right alongside the features that transcend any particular technology altogether. For Microsoft in particular, the first incarnation of their dedicated data analytics engine, named SQL Server Analysis Services (part of their data technology stack), was absolutely dependent on the data source being structured as a dimensional model. These models were referred to as “cubes”, and generated aggregate values as “intersections” of data. The current version of Analysis Services, known as Tabular, while being somewhat less dependent on this type of data structure, still benefits immensely from it. Power BI’s analytics engine is based upon this same technology; so the same techniques that would benefit you in an enterprise setting – with something like Analysis Services or Azure Analysis Services – apply just as much to analytics and self-service reporting using Power BI. In addition to being able to consume data from existing structured data sources, Power BI allows users to build their own data model right there within the tool itself, offering a number of useful features, such as being able to draw from and analyze many different sources of data at the same time, adding custom repeatable logic using DAX (Microsoft’s Data Analytics Expression language) for translating business and analytical requirements into results, and to have control over the end-user experience. Reports and analyses should be performant and intuitive – and this all begins in the data model itself. What It Is And Why It Matters I’ll be perfectly honest with you: there’s a lot that can go into building a proper dimensional model, and sometimes familiarity – or knowing what to do – is contingent solely on good old exposure and time. But understanding how to get the most out of your Power BI data model, even if it’s just knowing that a particular type of model works best, can often be all the difference in the world between building something that “just works” and something that “maybe works OK some of the time”. There’s unfortunately, and perhaps obviously, more to it than I can fit into a blog post – or even a series of posts – but covering some of the basics is still warranted. In very general terms, a dimensional model seeks to divide your data into two big groups, and thus two different types of tables in your model: facts and dimensions. It’s helpful to think of a “fact” in this case as being some event we’re interested in analyzing. This can be a line item in a sales transaction, an encounter between a physician and a patient, or an entry in a general ledger. This event, and its frequency, forms what we would call the base “granularity” of a fact table – or that which would spark the creation of a new row in such a table. As for what data a fact table contains – we strive to keep it as close to being strictly numeric values as possible. Thus, it would contain the quantifiable data points that describe an event: How much was an item sold for? How many items were sold? What was the amount posted to the general ledger? Sometimes our event doesn’t have much, or even any, implicit numeric information – such as a patient making an appointment to see their doctor – and in these cases the fact table acts as a “bridge” between our related dimensions. If fact tables contain the quantifiable data we’re interested in, dimension tables contain the contextual information that we wish to analyze: Who was the client? Where did the event take place? What day did the event take place on? In a dimensional model, we would generally seek to take all of this contextual data and create a dimension table for each logical grouping – and then make each value there distinct. In other words, we would have a Date dimension with each date, month, and year on the calendar to supply the temporal context. We might also have a Client dimension with each unique client we do business with and each aspect, or attribute, of a client that is analytically useful. The dimensional values represent the context of our analyses. The end result for users is a logical and intuitive grouping of values: numeric values are stored in one place, while each set of related contextual values is grouped in as few thematic sets as needed. This is especially helpful for analytics tools which feature a “drag-and-drop” approach to data exploration and report building – a feature central in Power BI and Microsoft’s Excel. Humans inherently break down events this way logically. We take some metric and then ask, “Where was this metric impacted?” “Who or what impacted it?” And so on. This factor alone is no small part as to why tools, particularly those aimed at business users, were built around this approach – and that’s something that can’t be said enough when it comes to some of the less obvious benefits to embracing a dimensional model as the source for analytical initiatives. Power BI data models get loaded into memory which, for those not particularly interested in knowing the intricacies of modern computing hardware, generally means they can crunch numbers very quickly – no matter how your data is structured. However – and this may especially resonate with those of you who came here because you saw “Power BI” in the title – it’s still easy to end up with a data model that isn’t fast at all. Even a Ferrari can only go as fast as the cars in front of it when there’s a traffic jam – and having the right data model design can often be thought of as the best defense against traffic jams – in short, because the tool was designed with just such a model in mind. Ninety-nine times out of 100, Power BI is simply most performant when the data it’s using comes from a dimensional design. Lastly, there are analytical languages like the earlier-mentioned DAX. Just as the actual engine a tool uses to process data is designed with certain prerequisites in mind, analytical languages work best with certain formats. DAX, used in Power BI, is often described as “elegant” because it can do in a single line of code what would maybe require hundreds of lines of SQL (or a lot of head scratching and convoluted statement building with earlier languages like MDX, multidimensional expressions.) But this is not at all guaranteed, no matter what the sales pitch is! Trying to create a Power BI data model consisting of one immense table with nearly a hundred columns could require you to write incredibly complicated (and slow-performing) DAX queries, whereas the exact same results could be produced simply and elegantly were that data transformed into a dimensional model. What To Do Next There are lots of great resources out there on this subject – and certainly enough to help just about any level of experience when working on designing or modifying data models to better suit analytics and reporting needs. We at BlueGranite can lend our expertise, as well as offer training, to give you that extra leg up when working with Power BI, Dimensional Models, modern analytics and more!"
"102" "The BlueGranite team has a lot to be thankful for when working in the data and AI space. Whether we are working to support our clients, our team, or building relationships with our partners (Microsoft, Databricks, and others), we are constantly learning and growing as an organization. As 2018 draws to a close, we’d like to take a moment to reflect and express our thanks to the industry, the community, and the people in our organization.  We decided the best way to show our thanks would be to ask around and get feedback from our experts. Here are some fun and interesting insights from the BlueGranite team on what they are most thankful for: Matthew Mace, CEOIt’s a privilege to collaborate daily with our talented, flourishing team at BlueGranite. I’m grateful for our ability to leverage tremendous value for our clients, across industries, using our partners’ innovative and agile platforms and groundbreaking analytics tools.  Amy Ford, Director of People & CultureI am thankful for so many things. This past year my family moved from Michigan to Seattle and working at a company that allows remote work, and values work-life balance, is something that I am very grateful for. I feel honored, blessed, and humbled every day to work with such amazingly smart, talented individuals. As the Director of People & Culture I talk to a lot of candidates each week and my favorite question that I am asked is “What is your favorite part of your job?” and I am proud to answer “The people! Working with the team members we have is incredible!” David Eldersveld, Senior Solution ConsultantI'm thankful to work with some of the best people in the industry at BlueGranite, Microsoft's 2018 US Data and AI Partner of the Year. I wouldn't be in the position I am today without the guidance and expertise of so many great colleagues. I'm also grateful to work with Azure data tools and Power BI on a daily basis. Andy Lathrop, PrincipalI’m thankful to wake up each day and have an almost 100% chance of learning something new about AI, collaborating with a BlueGranite expert, or helping a client solve an important problem. The opportunities are so plentiful and interesting, it’s a challenge to pick the right one – and that’s a great “problem” to have. I guess you could say I’m thankful there’s never a dull day at work!   Jim Bennett, Principal, Enterprise Data ArchitectI am thankful for great team members who continue to challenge and inspire me.  I am also thankful that the BlueGranite leadership continues to promote learning and professional development across the team. I’m encouraged by BlueGranite’s support of both long- and short-term goals. Mike Depoian, VP Sales/Business DevelopmentI am grateful for the people I work with every day, including partners and clients. They push me to grow and improve more than they know. Technology and client needs change fast and furiously. I appreciate the opportunity to work with and learn from bright teammates, partners, and clients. Meagan Longoria, Solution ArchitectI’m thankful for the SQL/Power BI community. I learn so much from them through blog posts and tweets and presentations. It’s a welcoming community that freely shares knowledge and encouragement. Being a part of the community has grown my knowledge, my career, and my friendships. Josh Crittenden, Senior Data Analytics ConsultantI am thankful for technologies such as Power BI Report Server (PBIRS), which is a perfect example of how Microsoft is investing in hybrid BI architectures. PBIRS allows customers to modernize their on-premises analytics today, while also setting the stage to shift to the Power BI Service tomorrow. As we celebrate more than 20 years in business, we at BlueGranite are most thankful for you; the sustained opportunity and support from our collaborators, clients, and affiliates helps us continue to flourish. We’re excited to discover what lies around the corner in the progressive world of big data and analytics. We’re just as excited to master those potential new advancements, while staying at the top of trusted current solutions, to continue making the most of your data. Keep up on the latest in the data and analytics industry by subscribing to our blog. Have questions or want to chat with an expert?  Contact us today. We’re here to help!"
"103" "Many maps in Power BI require latitude and longitude coordinates to display geography, but many data sources only contain place names. Geocoding is the process of obtaining latitude and longitude for a given location. Geocoding with Power BI is nothing new, but it may never have been as simple. In addition to several existing solutions that you could use to get coordinates for your data, the new Python script in Power BI Desktop is now an option. As a benefit, you do not need to manually construct web requests or parse JSON responses. In fact, it only takes one line of Python code.  To see geocoding and more solutions using Python for Power BI, be sure to attend BlueGranite’s webinar Python in Power BI: Navigate the Possibilities.  Date: Dec. 6, 2018Time: 11 a.m. ESTPresenter: David Eldersveld, Senior Consultant and Microsoft MVPRegister here: Python in Power BI Webinar Sample: Geocode Location Data for Power BI using Python and Azure Maps Python makes translating location names to latitude and longitude easy by offering connections to numerous geocoding providers. One of these services is Microsoft's own Azure Maps. All you need is a simple Python script to specify a column, choose the provider, and supply the provider's access key. While the following is not a complete step-by-step process, please join the webinar on Dec. 6th to see this demo live. First, obtain a key for the Azure Maps service using the Azure Portal. Sign up for the Azure Maps service within the Portal if needed. In your Python environment, install both the GeoPandas and GeoPy libraries. For Python, I use the Anaconda distribution available from https://www.anaconda.com/download. The following conda commands were executed to install the libraries using the Anaconda Prompt. If you are not using Anaconda, you can install using pip instead of conda from your command line.   Import your location data into Power BI. If needed, use Power Query to prepare the data so that a single column contains the address or other type of location that you need to geocode. In the sample, I create a simple column named Location that combines separate City and State columns. While this type of data preparation can be done in Python too, I want to focus on a simplified Python script that only handles the geocoding.     Add the Python code. The Transform tab in Power Query has the option to Run Python script. This adds the corresponding Python.Execute function to the underlying code. While the original DataFrame (named dataset by default) contains a single Location field, Azure Maps provides two new fields in response: Address and Geometry. The geopandas.tools.geocode function only needs three arguments: the original \"Location\" field, the service provider (Azure Maps), and the key. The Python code combines the Azure Maps response with the original data in the same line by adding \"Address\" and \"Geometry\" as new columns within the dataset DataFrame.     Parse the Geometry field to split out separate Latitude and Longitude columns. This is another series of steps that could be done in the Python script itself, but which Power Query is perfectly capable to handle as well.       The final Power Query script includes a parameter named AzureMapsSubscriptionKey in place of the initially hardcoded Azure Maps key. Ignoring the imports, uniting Python with Azure Maps only takes one line of Python code embedded within your Power Query M script! #\"Run Python script\" = Python.Execute(\"import geopandas#(lf)import geopy#(lf)#(lf)dataset[['Address','Geometry']] = geopandas.tools.geocode(dataset.Location, provider='azure', subscription_key='\" &Text.From(AzureMapsSubscriptionKey)& \"')#(lf)\",[dataset=#\"Added Custom\"]) Happy coding! Once again, if you would like to learn more uses for Python in Power BI, attend BlueGranite’s free webinar on Dec. 6."
"104" "The high-growth Financial Services industry is vital to the modern economy. It allows consumers to save and borrow money, provides capital for companies to invest in their futures, offers clients advice on investment vehicles, and supports risk management of various life and corporate events through insurance. This multifaceted economic sector is core to keeping society functioning and progressing.   Because Financial Services touches so many aspects of most lives and business organization, it can collect and leverage data at an astonishing pace – and it should. Particularly, with an abundance of choice among industry providers, individual and enterprise clients are now unforgiving towards mistakes, be it inaccurate information resulting in higher loan rates, or tardy money movement causing lost investment opportunities. It is crucial for Financial Services firms to be at the forefront of the data and analytics game; ensuring data integrity, increasing processing speed and accuracy, and reaping the rewards of advanced analytics tools and techniques, including streamlined operations and superior products. With a strong history of serving Financial Services clients and their data analytics needs, BlueGranite has deep expertise in delivering technological solutions that help Financial Services firms realize their analytical visions, no matter what stage of the journey they are in. The summaries below highlight three real-world digital transformation strategies in the Financial Services industry provided by BlueGranite. Read on to discover how we helped these enterprises begin reimagining the way in which they serve their stakeholders and provide marketplace value. Seamless Cloud Migration Preparation A global insurer was ready to migrate its data to cloud, both to lower operating costs (e.g., from data storage and administration) and to increase analysis speed. However, without prior experience, it sought guidance as to where to begin the migration journey. Because harmonious data migration requires a solid foundation, BlueGranite proposed and conducted a thorough assessment of the enterprise’s existing analytics and reporting platform, architecture, and performance. The customized assessment identified whether users were appropriately leveraging the organization’s platform capabilities; if the platform’s performance efficiency and stability could be further enhanced; and when, where, and how to make data warehouse improvements. The proactive health check gave the insurer the opportunity to confirm that the original platform was well-designed, minimizing the errors that can surface from a cloud migration. Utilizing Cloud to Modernize Data Solutions As the client base of a national consumer bank increased, so did the data it needed to screen (e.g., for checks, loans, and wire transfers) and the performance metrics it had to monitor. The bank’s day-to-day functions and supervision became increasingly burdensome and ineffective as the volume of data exponentialized, while the monitoring of it remained manual. Additionally, with over 600 metrics to examine, leadership had a hard time identifying relevant numbers to base decisions upon. Upon understanding the existing technology and business situations, BlueGranite: utilized Microsoft Azure tools, such as Azure SQL Database, to streamline and automate the bank's data collection, storage, and governance process – allowing business users to easily and quickly access data and report metrics. worked with the business to identify a manageable and meaningful set of metrics to monitor and define a series of Power BI reports – allowing executive leadership and analysts to analyze at the appropriate and desired granularity. By focusing on the needs of the business users, BlueGranite built an end-to-end data solution that provided fast, accurate, and relevant information. For more context and technical details on this case, see this solution brief. Leveraging Databricks to Model Risks We are currently leading an international insurance provider through a multiyear data and analytics overhaul that will improve its operational efficiency, increase its profitability, and enable scalability of its business. To date, we have helped the insurer centralize and model necessary data for the underwriting process in the cloud. Traditionally, such data had been fragmented and stored on-prem across geographies, business groups, technologies, and use cases. Centralizing the data and revamping the model virtually using Databricks has allowed analysts, actuaries, and policy writers at the bank to quickly and holistically: Access clients’ risk factors Simulate natural disaster risk over a longer time horizon (100k years vs. the previous 10k years) Conduct sensitivity analysis through modifying different risk factors, and Determine the optimal rates to offer and policies to adjust Such ability leads to a more customized offer for each client, meaning more profitable pricing and better cashflow management (e.g., identifying timing for insurance payouts). Outside of the specific use case of underwriting, we are also helping to ensure the scalability of the client’s data operations by scaling the use of cloud storage and Databricks across the organization. Additionally, we are enabling further modeling and computing capabilities by mapping out the right Platform as a Service (PaaS) tools in Microsoft Azure to the appropriate use cases. Eventually, we will also assist with the creation of captivating data visualization and UI through Power BI, ensuring the results of any data modeling and analyses can be meaningfully conveyed. The figure below provides an overview of the three cases above and how the pillars of Digital Transformation are touched by each case:  BlueGranite is an expert in modern data and analytics solutions, with veteran Financial Services experience. We look forward to serving all those in the industry with a need or desire to further capitalize the potential of data. Contact us today to learn more."
"105" "Microsoft Azure reference architectures, such as the diagram displayed below, can be very helpful when planning an implementation:  Although we have best practices and common practices, there is rarely just one right answer. Nearly every technology present on an architecture diagram has a valid alternative. It is great to have a lot of “tools in the toolbox” though the wide number of choices can lead to some uncertainty. Following are a few high-level considerations for when you are evaluating which services to use in Azure. 1: Be aware of your goals for going to the cloud, and the tradeoffs involved No matter what your role is on the team, it is important for you to know what your company’s goals are for moving to a cloud provider such as Azure. When deciding on architecture components, there are constantly tradeoffs and decisions to be made around aspects such as cost, control, complexity, performance, and security. The choices you are faced with during the decision-making process and during implementation will become easier if you have a firm handle on your objectives for cloud solutions.  Considerations: Are you willing to incur additional cost for increased performance? Are you willing to take on more complexity in exchange for an increased level of control? Are you willing to accept reduced agility for greater security? 2: Realistically assess what your team can support When an architecture is composed of numerous technologies, it can be a challenge to support, especially if you have a small team. In the diagram above, there are eight distinct services (not including other aspects such as Azure Active Directory, virtual networks, and so forth). Each service has its own characteristics, languages, tools, and best practices. There are always preferences from team members as to which skillsets they can utilize (or things they want to learn), which can significantly influence the services that are selected. Considerations: Who will build and support the solution? Does your team have existing expertise, or will they be expected to learn new capabilities quickly? Do you have personnel available to handle activities such as capacity planning, performance tuning, and cost management? 3: Decide the extent to which your team follows best-fit engineering principles The term polyglot persistence refers to using multiple data storage technologies, each representing “best-fit engineering” for the type of data being stored or for a specific use case. If you have a strong belief in best-fit engineering you will have a more flexible, albeit more complex, architecture. Strong opposition to polyglot persistence leads to valuing simplicity in architecture above all else. In the above diagram, four services are present which store data: Azure Blob Storage, Azure SQL Data Warehouse, Azure Analysis Services, and Azure Cosmos DB. Each service absolutely brings value for fulfilling certain scenarios. However, with every additional service that is introduced, the complexity level increases because there are (usually) more data integration processes, additional security layers, more tools and languages to learn. The above diagram also shows Azure Databricks which is a compute service targeted towards data engineering and data science processes. Because Azure Databricks supports multiple languages (SQL, Python, R, Scala), it is seeking to satisfy multiple use cases and multiple user demands with one “unified” tool. The simplification aspect is certainly an appealing proposition. Considerations: Does your team value best-fit engineering over architectural simplicity? Is your team prepared to support numerous layers? Can you justify the use of every service selected in the architecture? 4: Decide your preferences for data integration vs. data virtualization techniques Data integration processes physically move data to another data storage service, whereas data virtualization refers to querying data in disparate sources without moving or replicating the data (i.e., querying data where it lives). Although we do not have a full-fledged data virtualization platform in the Microsoft ecosystem, we do have various data virtualization techniques available to us. For instance, PolyBase and Elastic Queries can both execute remote queries to another data storage solution. We can also use DirectQuery or Live Connections in Power BI to avoid redundant data involving duplicative data refresh operations. As the number of data storage services in an end-to-end architecture increases, your options for potentially using data virtualization techniques increase as well. Considerations: Do you have concerns about redundant copies of data? Do you need alternatives for closer to real-time data access? Are you required to meet data residency or compliance requirements? 5: Routinely conduct a technical proof of concept during the decision-making process  It is common for new features to be introduced as an MVP, known as a minimally viable product. This approach allows vendors like Microsoft to obtain feedback from early adopters and prioritize the incremental improvements. From the customer perspective, this is advantageous because we see new features and functionality quicker. However, it also means that it takes time for features to mature which means that key functionality may not be available initially. For this reason, it is wise to do a proof of concept to reduce risk to the project. This is particularly true if you are planning to utilize an unfamiliar service or a relatively new service. The fast pace at which services evolve in Azure makes it very challenging to keep up with the current state of capabilities. All services incrementally improve over time, such as Power BI which introduces new functionality every month. Some services also undergo larger, more fundamental changes which involve a migration, such as Azure Data Factory V2 or Azure Data Lake Storage Gen 2. Considerations: Do your processes and team structure support working in a manner which starts small and iterates? How receptive is your team to learning new things on a regular basis? Is your team prepared to stay current and adapt to the pace of change? At BlueGranite, we thoroughly enjoy helping customers make these types of decisions. Contact us today and we'll be happy to assist in getting you through the process."
"106" "As we continue our exploration into the topic of Digital Transformation, let’s take a moment to recap our first two pillars, Engage Customers and Empower Employees. With Engage Customers, we discussed the importance of leveraging data to better understand your customers to provide them with better service and more impactful experiences. With Empower Employees, we outlined ways in which forward-thinking organizations equip their team members with timely information and improved tools to better execute their responsibilities. Both pillars help drive improved business outcomes, which is the hallmark value proposition of the Digital Transformation concept.  In today’s post, we’ll discuss the third pillar, Optimize Operations. Since Henry Ford, the Model T, and the modern assembly line, organizations have realized that an important aspect of competitive advantage is in streamlining key business processes in order to improve efficiency and quality. Streamlined business processes ease friction in the organization, which leads to improved productivity and reduced risk – key factors which can drive desired business outcomes, such as improved financial results.  Data and AI Vital to Operations Optimization It’s common in today’s modern business world for organizations to employ teams of professionals who focus on optimizing operations. They work on teams with names like “Continuous Improvement” or “Operations Excellence” or are embedded within business functions, such as “Manufacturing Operations” or “Sales Excellence.” They employ programs and standards, such as Six Sigma, ISO 9000/9001, and others, to guide their efforts through industry best practices, and measure their progress among their peers through benchmarking exercises. These efforts have been very effective in many organizations, but there’s still a long way to go, especially when it comes to technology, and specifically Information Technology with Data and Artificial Intelligence (AI). Technology has always been a critical lever in improving operations, but the emergence and maturation of data and AI enables organizations to reach new heights when deployed effectively. And somewhat surprisingly, even “traditional” companies are leveraging data and AI in innovative ways. Take, for example, one of our clients in the industrial agriculture industry. A key challenge they face is in optimizing irrigation of fields and crops. Success requires orchestration of large equipment and personnel, with significant investments in infrastructure and resources alike. What was traditionally done through basic measurements and gut feel, based on experience, is now being driven by data and AI. This is a transformation which will yield massive benefits and solidify the corporation’s place as a leader in its field. Learn more about this case study here: From Code to Table - Using Technology for Smarter Food Production.   Modern Reporting Enhancing Efficiency Across Industries Data and AI not only drive operations optimization in the agricultural industry, but in the transportation industry, too. Imagine the challenge of coordinating thousands of busses and thousands of routes, and the costs associated with even slight improvements in efficiency, or the converse, declines in efficiency. Not only do efficiency declines increase cost, they impact customer satisfaction, too. Thus, the imperative to ensure top-notch performance in transportation with another BlueGranite client. This solution leveraged huge amounts of data, including sensor data, in a centralized platform which enabled deeper insights, which led to improved scheduling and cost savings. Check out the case study here: Modern Data Platform Gives Operational Insights & Saves Millions. Predictive Analytics, Demand Forecasting Driving Industrial Innovation The examples above show the power in leveraging data and AI to improve operations. There are many more BlueGranite use cases across industries that further illustrate this point. Take healthcare, for example. One of our clients is exploring how advanced analytics can improve wait times at the ER, leading to more efficient patient care and improved outcomes (including patient satisfaction). Manufacturing is another industry hyper focused on optimizing operations. Another of our clients is investing heavily in “Smart Manufacturing” which will outfit legacy equipment on the shop floor with IOT sensors, enabling improved asset utilization and improved quality. A common manufacturing example is around predictive maintenance – preventing equipment failure by tailoring repair times and types to ensure ongoing performance. There are certain patterns common across industries, as well. For example, demand forecasting, which may be used to drive improvements in key business processes, such as ordering, workforce planning, growth, and more. Solutions like demand forecasting rely on historical data and advanced machine learning algorithms to create models which provide predictions on future events and help guide decision makers. It’s one of the countless ways industries across the board are using technology to reimagine operations. As we’ve stated here and in past posts, Digital Transformation, and its four pillars of Engage Customers, Empower Employees, Optimize Operations, and Transform Products, provides us an excellent framework to identify opportunities to leverage data and AI to drive improved business outcomes. If you’re well on your way in this journey, our team of experts can help you implement those solutions. If you’re still in the planning, or even pre-planning stage, we can help there too. Contact us today to learn more."
"107" "In my BI Basics series of blog posts, I’ve already discussed visualization and data transformation. Those are crucial aspects of Business Intelligence. Without the ability to acquire data and present it to people within the business, BI becomes somewhat of an academic exercise. Today, though, we want to talk about the goals that BI attempts to achieve. What is BI all about anyway?  I’ve made sidelong comments about it here and there, but I really want to spell it out and dig into the mission. Business Intelligence harnesses technology to collect, transform, and use data to support business goals. We’ve got four fundamental aspects here to our definition: Collecting data Transforming data Using data Supporting business goals In this post, we really want to zero in on point number 4. For tech people who love technology for technology’s sake, I cannot stress it enough; everything is oriented to support the business. As a programmer, an IT geek and a techie, there’s the ever-present temptation to pursue technology for technology’s sake. If you mention a tech buzzword, it gets my heart racing. Cloud computing! Big Data! Data lakes! Machine learning! Artificial intelligence! We’re living in the future, folks! The problem is that technology is not the end goal. It always needs to serve the business. BI has always been a support to the aims of the business. It doesn’t make a product which can be sold, but it still needs to demonstrate return on investment. Business Intelligence might do this by identifying ways to better serve customers or by uncovering waste or inefficiencies. It might also pinpoint products or services that may increase profits. It could reveal ways to make employees happier or more productive. It could also be used to improve R&D or internal processes. But if technology doesn’t serve a business need, it’s a distraction and a liability that needs to be killed off. How do we do it? How do we see some of that application to business needs in the real world? Well, see our solution briefs to get some stories of how we’ve helped customers in real life. I don’t want to turn this into a wholesale ad for BlueGranite, but we’ve done some cool stuff. A few examples? Everything from helping the agriculture industry effectively and responsibly irrigate vast areas of land, to helping transportation firms save millions on maintenance. We’ve also helped improve the lives those in need – helping a nonprofit better connect with donors to continue critical medical care funding for children’. The important part is that it isn’t just “We built this cool process to ingest and process terabytes of data” but that the process improved lives, the environment, profits and ultimately, for each organization and business. When a BI project does not have business focus, it tends to become a sinkhole. If all you can say is “Isn’t it cool?” but can’t answer “What can it do for the business?” then it’s not business focused. Without a clear business application, the intended users will ignore the system or find ways around it. In fact, a project venture shouldn’t even be approved without any clearly defined business goals. Answering “What can it do for the business?” should be the first consideration. If there are no business users in meetings, that is a major red flag that the project does not have business focus. Without stakeholder input, features may be incomplete or even fundamentally broken. This can lead to unnecessary expenditures and may even disrupt existing processes with disastrous results. To maintain business focus, involve business users continuously – from the inception of a project all the way through to the end. Their involvement is critical to success. In the design phase, they reveal project needs that are often invisible to the technical side. They can help spot flaws that will doom the project from the beginning. By participating regularly, these stakeholders see the progress being made and can keep the technical team apprised of new or unforeseen challenges. They remind the technical team that things like accessibility, user interface and polish matter. By guiding development, they are more likely to champion the project’s long-term results. And the project will be far more likely to succeed. If you need help keeping a business focus on your BI projects, contact BlueGranite to help keep you on track!"
"108" "Microsoft’s Power BI SharePoint is the go-to tool for organizations looking to quickly gain deep insight. Accessible from pretty much anywhere – desktop, mobile or tablet – the business analytics solution brings data to life with brilliant visuals. During BlueGranite’s interactive monthly Power BI Office Hours, open to all, we explore simple ways to pilot new features with this dynamic platform.  Today we’re looking at how to navigate SharePoint files, lists, and folders. You can follow along with the recorded presentation here.  In our last Power BI Office Hours session, we previewed a few of the great features in the September 2018 release of Power BI.  We specifically looked at M IntelliSense, Copy and Paste in Tables/Matrices, and Aggregations (preview) (demos begin at 7:39). We had a lot of great questions at the end of the session, as well! Keep those questions coming, and maybe we’ll look at Project Online in the near future.  An important note briefly touched on regarding Aggregations, and more specifically, Composite Models: remember that switching from DirectQuery to Import is an irreversible operation.  It cannot be undone.  So please be careful before changing storage modes willy-nilly. Our Use Case this month was pulling from SharePoint.  For any of you that have tried (and, like me, initially failed) you know that this can be a painful process if you’re not careful.  In our demo (starts at roughly 21:05) we walk you through importing data from the three popular SharePoint sources: files, lists, and folders. If you have any questions, please don’t hesitate to contact us directly! Let’s get started. Using SharePoint Files To use a file from SharePoint you don’t use a SharePoint connector, you use the Web connector. Once Power BI has connected to the file you choose, it’s smart enough to extrapolate the file type and grab the right navigator dialog box. The trick is getting the correct URL to the SharePoint folder.  The easiest way to do this is to open the file in question from SharePoint, and grab the URL from there. In the example below I have a SharePoint folder with a collection of different files. However, I’m only interested in the Geography_Extended file.  First, I’ll open the file in SharePoint, and then open the file from there in Excel (Desktop).  Once I have the file open in Excel, I’ll get the folder path in SharePoint by selecting File -> Info from the menu.  I’ll left-click the breadcrumb under the file name to copy the path to the Clipboard.  Alternatively I could right-click where it says “Open File Location” and select “Copy path”.  (The result of “Copy path” is: “https://bluegranite1.sharepoint.com/training/powerbitraining/Materials - Day 2 - Intermediate Power BI/Source Files”) From the Get Data menu I’ll select “Other – Web”.  In the “From Web” dialog box I’ll paste in the URL that I copied from Excel. **Notice: it’s just the folder path. We need to manually type in the name of the file.**  Power BI is now “smart enough” to grab the appropriate connector for the file, and I can continue working with the Excel file just as I would one that is stored locally.  Using Lists from SharePoint Here I have a list of Regional Directors that I maintain in a SharePoint List. Let’s pull this into our Power BI dataset as well.  From the Get Data menu I’ll search for “sharepoint” and select the “SharePoint Online List” connector. (If your organization uses SharePoint on-premises you’d select the “SharePoint list” connector)  The URL I’m going to use is the site-level URL – i.e., no sub-sites.  The Navigator will display all Lists in that entire site independent of what page/sub-site they appear on. We can search and select the list(s) that we want. I want the “Regional Directors” list. The list has a lot of columns I really don’t care about – so I can eliminate them, just as I would any other data source, and continue about my business. I can also rename the columns to suit my needs, as I would with any other data source.  Using Folders from SharePoint Folders in SharePoint can get a little tricky because, like Lists, they exist at the site level as opposed to in the different Pages as we see them in the browser. So how to grab a few files from a SharePoint folder and consume additional files as they are placed in said folder? We need to get all the folders from the site in question and then start filtering them down. You’ll see how we can consume multiple files in a folder as we go. I have a folder in SharePoint at the following location (not the true URL here, for illustrative purposes): SharePoint/Training/Power BI Training/Materials Day 2/Source Files. I want to pull any Excel files that start with “Stores” in this folder. (I could also use this technique to pull only one file from a folder, but earlier we showed an easier way to pull just one file from SharePoint.)  Let’s use the SharePoint folder data source and the URL from the browser and give it a shot.   Nope, that didn’t do the trick. OK, let’s try the part just before the “/Forms/”. In this case, “https://bluegranite1.sharepoint.com/training/powerbitraining”.  Nope, that didn’t do it either. Let’s try one level higher: “https://bluegranite1.sharepoint.com/training”.  OK, that did the trick – although now I’ve got many more files than I hoped for. But I’ve got that handy Folder Path column that I can probably use to narrow things down.  Let’s use our typical “Combine & Edit” technique and see what happens.  Nope, that didn’t do it. If I look at the screenshot above, I see that the first file found in the Training site is a .vsdx file, which isn’t supported. Let’s just Edit for now and get this data into the Query Editor and we’ll see what we can do there.  OK – so we’ve got something in the Query Editor. Let’s narrow things down a bit by filtering down the Folder Path column to just the folder we want and additionally filter to .xlsx files that start with “Store”. Remember that these filter steps will be applied every time I refresh the file, so I only have to do this navigation once.  There we go – we now have Power BI looking at just the files I want. I can now manually kick off the “Combine” wizard by clicking the button in the right corner of the “Content” column header.   Now we can continue on as though pulling from a network/local folder.  Though I wish that pulling data from SharePoint was easier – especially for a SharePoint noob like myself – I hope this helps you on your Power BI journey. Discover many more Power BI tips and tricks when you check out our archived Office Hours here or join our next Office Hours session on October 25th at 11am EST to learn more about what's new in Power BI, plus get all of your questions answered!"
"109" "Recently, a member of our team ran into a client request that posed a bit of a challenge. The client wanted to display employee pictures in Power BI reports using images saved in a database. But the image data was stored using a varbinary (max) data type column. The problem? Power BI can only display images if they are categorized as URLs, because the URL is a pointer to a location where the image exists as a file.  Back in the day when Power BI was a set of tools in Excel, it was possible to load images straight into the data model (PowerPivot) using the binary column data type, and then categorize that data as an image native to the model itself. Times have changed. To get an image into a Power BI report now, you must use a URL, either from an external website or a file system. There are two main drawbacks to this method: The images will not load if you are working offline If the file is moved to a different location, or if the name is changed, the image will “break”, or be unable to render How to embed images The good news is that there are workarounds to this challenge. We are going to reconstruct the above use case and demonstrate how to pull in images from a local database, and then use custom columns in Power Query to reformat the source data in a way we can render graphically. Note: This same custom column technique can also be used to display images imported using a local folder as a data source. If you have Power BI Desktop installed, you can work through along with this post by downloading the .pbix file with this link.  In our sample database we have a couple of tables containing images stored in a binary format, as well as a few columns of metadata for the images. The images being used here are a JPEG file type, but this technique can also be used for PNG files. We imported the data into Power BI and loaded two tables of images: Examples 1 and 2. For the first example we used three small images in the table, as shown below. Example 1 First, we will break down the custom column creation into two steps. Later, in the Example 2 table, we will explore how to combine the technique into a single custom column. The first step is to convert the binary column ‘PictureData’ into text. Using the following code with a custom column in the Power Query Editor window will accomplish this: Binary.ToText([Column to covert], BinaryEncoding.Base64)  Using the Binary.ToText function the binary column has now been converted to a Base64 string. This is important, as a standard non-Base64 conversion will not work. However, we are still missing the prefix that makes it a URL. A second calculated column using the following formula will get us to a usable URL column: \"data:image/jpeg;base64, \" & [BinaryToText] If you are converting png files, the code you would use is: \"data:image/png;base64, \" & [BinaryToText]  The final step is to set the Data Category of the ‘ImageURL’ column to “Image URL” so that Power BI will recognize it as a URL rather than regular text and display the image.  Now you can use those images in a Power BI report without worrying whether online graphics will result in broken links.  Character limit poses challenge There is one major limitation to using this technique and it stems from the string length allotment in Power BI. Power BI has a maximum string length of 32,766 characters. This causes issues when trying to embed large images. To demonstrate this limitation, I took the same image and replicated it multiple times adjusting the size of the image. For testing purposes, I also added pixel width and height measurements as well as the length of the ‘ImageUrl’ string as columns. Example 2 The dataset used for Example 2 also combines the two formulas – used separately in Example 1 – into a single formula. The combined formula looks like this: \"data:image/jpeg;base64, \" & Binary.ToText([Column to Convert],BinaryEncoding.Base64) As you can see from the picture below, the image starts to truncate once we get to the 400x440 pixel image. As you move down the list and add larger images, more and more of the image gets truncated even though the image within the report stays the same size. Note the string length for the images, and how after the second image, all of the lengths say 32,766. The string for the image is being cut off at the 32,766-character limit.  As of this writing there is no way around this issue. If you have the option, you can scale down images until they are under the limit and can be fully displayed. Want to learn more? There are a few ways to work through these issues. Microsoft’s Jason Thomas also covered Power BI URL workarounds earlier this year using a different technique. His method demonstrates how to embed images by using an external website to convert to Base64 Code, where this post focuses only within Power BI. You can read more about Jason’s method here. For those looking to master Power BI’s many capabilities, consider checking out BlueGranite’s free eBook on implementing a self-service BI program. If you need further assistance, we help teams master the platform to quickly build stunning visualizations and share valuable data insights. Contact us today to learn more."
"110" "Your company rolled out self-service Power BI – how exciting! As a new user, you’re set up to start developing reports and sharing them with colleagues. You’ve probably gone through some basic training, so you know how to connect to data and create visualizations, but if you’ve seen or used reports before, you know that some are better than others. What can you do to take reports and visualizations from “just adequate” to “truly great”? How can you better engage your audience and convey the most important information clearly? In my experience training new users during self-service roll outs and delivering Dashboard in a Day (Microsoft’s one-day introductory course for new users), some of the biggest misses for new report developers involve ignoring visual interactions, not tailoring each visual to best fit its purpose, and not providing adequate context for users. Some of this comes from an initial hesitance to change Power BI’s default format settings, and some of it stems from the fact that these topics are not usually covered in detail during basic trainings. The subtle design considerations discussed here can help you develop a report that’s not only used but relied upon. This post walks through several examples to illustrate this. The examples refer to a report created for a fictional IT department. The IT department manager needs to see how the IT team is performing, which departments have the most open tickets, and what type of issues users are submitting tickets for. Then she must determine what action, if any, she needs to take.    Edit Interactions One of the features that makes Power BI such a popular and powerful tool is the interactions between visualizations. In their default state, these interactions work well, but there are steps you can take to make them work best for your report. By default, visualizations that interact with bar, clustered column, and other charts do so by “highlighting”. When a chart is highlighted, it can be difficult to discern the selected portions, and the sort order may not reflect the selection. For example, before editing interactions, when Sarah Schriebner is selected in the IT team member bar chart, the breakdown of tickets by department highlights rather than filters.  If this interaction is updated to filter instead of highlight, it takes no time to see that Sarah has handled more tickets for Sales than any other department. It is also important to point out that the funnel chart remains set to highlight because it shows amounts in relation to steps in a process. If it were filtered, all the steps would not be displayed, and the chart would be misleading. The interactions are set to best show how many tickets Sarah has in each step of the process and which departments she’s handling the most tickets for. The report works in both instances, but it works just a little better after updating the interactions. Anything you can do to deliver insights quickly and clearly, that saves users time and effort, is a reporting win – no matter how small.  Slicers vs. Interactions Often, a new developer hears the need for users to filter on a field and immediately thinks, “Slicer.” Yes, slicers work for this purpose, but they work for only this purpose. Beyond filtering data, slicers don’t add analytic value and they take up valuable canvas space. Consider the following scenario. The IT manager needs to able to filter the data on the page by department, IT team member, and ticket status, but it would also be useful to see these fields broken down by ticket number.  The ability to filter by IT team member and department is handled by bar charts and their interactions with the rest of the page, because we have meaningful data that we’d like to attach to those attributes visually. When the manager clicks Nick Aranez in the IT team member bar chart, the other visuals filter and highlight to reflect only his tickets. The manager quickly sees that while Nick has taken on the most tickets this week, he has yet to resolve any, and has one that’s older than 5 days. It’s worth following up with Nick to see what’s going on and if he needs any help resolving his tickets.  This report page still has two necessary slicers. These slicers contain attributes that we only wish to use for filtering what’s on the page, and they have both been changed from lists to dropdowns. Changing slicers from lists to dropdowns saves space, especially for fields that have more than two values. Context and Titles Using titles and text wisely can simultaneously add context and reduce clutter. The first page of the report has two carefully planned titles that provide context. The main title for the page, “IT Service Ticket Breakdown”, indicates to users that they’re looking at ticket numbers in the charts. This eliminates the need for individual titles on each chart, thus reducing clutter. However, there is one visual on the page that needs its own title.  The table in the upper right corner is filtered to display tickets older than 5 days. To make this clear to users, it’s explained in the title. The IT manager quickly understands she’s looking at a list of unresolved tickets older than 5 days and she can act accordingly.   Creating concise and meaningful titles is a great strategy to give users context and direction. If you share your report and are immediately asked “What am I looking at?”, take a minute to revisit it.  What can you make more obvious with a title? Optimal Chart Formatting With all the features and formatting options available in Power BI, you’ve probably been warned about not overloading and cluttering visualizations. I’m here to warn the opposite – when user needs dictate displaying labels and text, add them! Give users what they need for quick insight. Take the below chart for example. Its purpose on the second report page is to show average satisfaction ratings over time in relation to the amount of tickets submitted. The IT manager cares about the exact average satisfaction rating numbers, but only needs a general sense of how many tickets were submitted each week. This chart is tailored for this exact need. The data labels added to the line show the exact numbers. The y-axis is hidden, and the title eliminated to reduce clutter on the visual. The important information is suitably displayed by the legend, data labels, and bar size. The result is a clean chart that focuses the user’s attention immediately on the most significant data. It’s easy to see that the week with the lowest satisfaction rating had a relatively high amount of tickets submitted, so it is worth the manager’s time to take a closer look at what happened that week.  If she wants to take a closer look at that week, she can click on it. The visual interactions are set up for this type of analysis. From here, she can decide what action to take next.  Development Checklist The goal in report design is to strike a balance between context, features, and clutter, that fulfills requirements and caters to user needs. This is a skill that takes practice, but here are a few things to ask yourself to help find that balance as you develop reports: When I click around on the page, do the visual interactions make sense and add value? Do I have any filtering on a visualization, page, or entire report that I need to make users aware of? Can users understand what they need to know at a glance, or do they need to do some detective work? In the latter case, is there something simple I can do to lessen this work? Have I added any hidden features like bookmarks or drillthrough that I should make obvious to users? Are there redundancies between visualization displays, chart legends, and titles that I can eliminate or make more concise? Have I used canvas space wisely? Is anything (like a slicer) taking up more room than it deserves? Are there slicers that either already are or could be handled by a visualization? While this gives us a look at a few underused features and overlooked formatting considerations, there is much more to learn about report design, and BlueGranite is here to help. For a more in-depth look at reporting best practices, check out Meagan Longoria’s whitepaper and accompanying webinar. You can also browse our Power BI showcase examples, under Resources, for inspiration."
"111" "Power BI has long had the ability to apply conditional formatting to values in table columns. However, many felt that the functionality for managing the colors and parameters used in Power BI Desktop was too limited and didn’t provide the level of granular control desired by many users. In a recent update to Power BI Desktop, Microsoft has addressed this limitation and introduced the ability to use DAX measures to control the conditional formatting of table columns. As we’ll see in this blog post, by creating a simple measure using the SWITCH function, users can take granular control of conditional formatting and apply it to values in columns.   Conditional Formatting using Power BI formatting pane If you have a table of data and would like to apply conditional formatting to a numeric column, you can choose between three different formatting options. First, make sure the visualization is actively selected in Power BI Desktop. When you have a visualization selected that supports conditional formatting the option will be available to you in the formatting pane:  Expanding the Conditional formatting option will show the list of numeric columns that can have conditional formatting applied. After selecting one of the columns, the user can then choose from applying conditional formatting to the Background color, Font color, or adding Data bars:  Clicking on the slider for any one or more of the Conditional formatting options will apply it to the visualization. Let’s apply Font color Conditional formatting to the Revenue column.  Power BI will automatically apply color formatting to the value, and the range of colors will be automatically calculated based on the range of values in the column. If we click on the Advanced controls link for the Font color option, it shows the default (Color scale) which generates a color for each cell based on the color scale generated from the lowest and highest values in the column.  There are a lot of options in the Color scale formatting that can be modified by the report creator. The colors selected for minimum and maximum values can be changed to generate a different color scale, and the values used to generate the scale can be changed to eliminate outliers or to adjust for scale.   The second advanced option for conditional formatting is to use Rules instead of the Color scale to determine the formatting:  This option uses standard If/Then conditional rules to specify what color should be applied to a cell when the rule applies to the value. Users can add additional rules to create complicated conditional formatting scenarios and assign specific colors for values that meet the criteria for each rule. One of the drawbacks of using either the Color scale format or Rules format is that any changes made in the advanced editor are specific to that visualization. If the same data is used in a different visualization, none of the conditional formatting rules will apply. This can possibly lead to confusion if different rules are being used on the same data in different parts of the report or if other reports are being built from the same dataset. It can also take a lot of additional effort to ensure that the metrics and conditional colors remain consistent across all visualizations. The recent update to the advanced options for conditional formatting now allows users to format by field value. This option allows the report developer to use a calculated measure that defines the thresholds for a value and the color that should be displayed when those thresholds are reached. As an example, to define the thresholds and assign a color value for sales revenue, we create a calculated measure with a simple DAX expression: Revenue KPI Color = SWITCH(     TRUE()     ,SUM('Sales'[Revenue]) < 50000000, \"#f44242\"     ,SUM('Sales'[Revenue]) < 1000000000, \"#f4f142\"     ,SUM('Sales'[Revenue]) > 1000000000, \"#5ff442\",     \"#f44242\") This measure will apply a condition to each value in the column and then apply the appropriate color. To use this in a visualization, change the Format by option to Field value in the advanced controls, then select the new measure as the source for the formatting. Once applied, the visualization changes to show the color values as defined by the measure:  While this example is using summed values as thresholds, we could easily use percentage of total, YoY differential, or any other value as the field to generate the color. And since the colors and thresholds are defined as part of a DAX calculation, the measure can be reused in other areas of the report or in other reports based on the same data model without having to recreate the conditional formatting rules every time, saving effort and ensuring consistency. For more Power BI tips and tricks, subscribe to BlueGranite’s Business Insights blog. We also offer comprehensive on-site Power BI training. Our 3-day course teaches organizations to create and share spectacular business visuals on the web and across mobile platforms."
"112" "Over the last two months, BlueGranite has encouraged team members to spend a work day volunteering with a nonprofit organization of their choice.  This was the second year that BlueGranite has offered an opportunity to employees to volunteer, and by far, we thought it was another great success! The work was rewarding, plus allowed us all to give back to our communities. Read on to see a quick summary of some of the organizations we worked with this year and how we were able to help out. Left to right: Justin Bahr, Jon Trapane, Robert Hutchison, Erik Roll, Mike Depoian Kalamazoo Loaves & Fishes Most BlueGranite employees work remotely, but we did have plenty of opportunities to get together in our major hubs across the U.S. to volunteer. This year in Kalamazoo, Michigan, we chose to spend our day volunteering with a local food bank, Kalamazoo Loaves & Fishes (KLF). KLF started almost 40 years ago with a mission to end hunger in our community and help families put food on the table when they couldn’t afford to. Today, our local distribution center provides food to 73 food banks throughout Kalamazoo County via donations and what KLF deems its “pennies-on-the-pound purchasing power,” providing groceries to over 700 families per day. Fourteen of us (myself included) joined forces to sort through all kinds of food throughout the day. We started with sorting through and boxing up fresh corn, potatoes, and cabbage that had been donated by farmers so that it would be ready for distribution. Our group then boxed dozens upon dozens of eggs, as well as re-labeled and boxed canned goods to be distributed across the county. It felt great to give back and lend a hand to the large volunteer team at KLF! Left to right: Jacque Carlson, Amanda Mulholland, Angela DeYoung, Zach Conroe, Dan Meyers, Matthew Mace, Larry Baker, Eric Wozniak, Carrie Renfrow Austin Habitat for Humanity Moving farther south, BlueGranite’s Texas-based team spent its Day of Service working at Austin’s Habitat for Humanity (AHFH). AHFH is dedicated to trying to end the cycle of poverty housing and deeply believes that everyone deserves a decent, affordable place to live. Our three Texas team members spent the day moving and loading doors, windows, and pallets of tile to assigned areas to be utilized by AHFH’s construction team. Top left photo, from left to right: Jenny Tseng, Steve Krause, Mike Cornell Second Harvest Food Bank of Metrolina In North Carolina, BlueGranite’s Charlotte team of four returned to the Second Harvest Food Bank of Metrolina – the same organization the group gave its time to last year. This food bank has a wide reach, spanning 19 counties. Second Harvest also works with multiple charitable agencies and distributes more than 50 million pounds of food annually in its fight against hunger. In addition to food donations from supermarkets, large retailers in the area also donate all sorts of returned goods that cannot be sold. This year, the team spent its day sorting through a truck of those donated goods, including diapers, pillows, mops, and more. Left photo, from left to right: Jerry Lukomskiy, Colby Ford, Vincent Staropoli, Melissa Coates Camp Barakel Our Charlotte team was not the only group that chose to revisit its charity from last year. Jim Bennett and his family spent his Day of Service working with Camp Barakel again – a summer camp located in Northern Michigan. Jim has been volunteering for the camp for many years. This year, he spent his time serving food to over 200 campers, washing dishes after each meal, and assisted in cleaning the facility. Jim Bennett getting ready for clean up. My Very Own Blanket In Columbus, Ohio, Josh Crittenden and Lindsay Pinchot spent their day working together at a local nonprofit called My Very Own Blanket (MVOB). MVOB creates blankets for children in foster care, dedicating its work to help children feel comforted and valued. Josh and Lindsay helped to prepare fleece blankets, organized the nonprofit’s workshop, and boxed up finished blankets for delivery. Middle photo: Josh Crittenden; Right photo: Lindsay Pinchot Jordan River Clean Sweep Levi Syck spent his Day of Service volunteering with the Jordan River Clean Sweep. This is an annual river clean-up put on by the Antrim Conservation District, Michigan AmeriCorps, and Paddle Antrim. A group of around 30 people, including Levi, used its time to clean the river in canoes, kayaks and a drift boat. The Jordan River is a designated Natural River in the northwestern part of Michigan’s Lower Peninsula. Due to its natural beauty, Michigan's legislature has created a Natural Rivers program as a protection effort to preserve the quality of select river systems throughout the state. Like many rivers, it receives an ample amount of canoe, kayak and tube traffic throughout the year, and all this traffic leads to a lot of garbage left behind or lost when a paddler flips their boat. Cleanups like this are necessary to maintain the river for everyone, including the participants, to enjoy. Levi Syck helping with the river cleanup. CoderDojo CoderDojo is a global nonprofit aiming to provide volunteer-led, community-based computer programming clubs for young people between the ages of 7 to 17. David Barnhart used his Day of Service working with kids to teach them new tips and tricks as they start learning to code. David Barnhart teaching kids how to code. Feeding Tampa Bay Jason Brugger volunteered with Feeding Tampa Bay, part of the national Feeding America network, an organization devoted to ending hunger in the greater Tampa Bay area. The food bank aids more than 700,000 people in need across 10 counties in West Central Florida. Jason helped to sort and box up food to get it ready for distribution to his local community. Jason Brugger getting ready to sort produce.  Twin Cities Habitat for Humanity Twin Cities Habitat for Humanity (TCHFH) works hard to help improve lives across almost 10 counties spanning metropolitan Minneapolis. Since the organization started in 1985, it has helped over 1,200 local families purchase and restore homes for safe and healthy living. Merrill Aldrich and Jared Zagelbaum volunteered for TCHFH for their Day of Service. They spent their day mostly outdoors, working with the organization to fix up a house and get it ready for a family to move in. Left to right: Jared Zagelbaum, Merrill Aldrich Denver Rescue Mission Last but certainly not least, BlueGranite’s team in Colorado got together to volunteer for the Denver Rescue Mission. The Denver Rescue Mission is committed to helping people in its area who are experiencing homelessness and addiction. The organization’s goal is to transform the lives of people in the community through education and accountability, along with much-needed help from local volunteers. The organization helps teach residents how save money and develop the life skills and relationships needed to maintain self-sufficiency after graduating from the Rescue Mission’s program. Our team helped clean the dining room and serve lunch to residents, working through one of the organization’s residential programs for homeless and needy in the suburban Denver area. Left to right: Andy Lathrop, Nathaniel Scharer, Meagan Longoria Getting inspired? We are too! That’s why we set aside time every summer to work with our local communities to give back. We’re looking forward to next year already!"
"113" "In BlueGranite's initial Power BI showcase, Lindsay Pinchot stepped through features such as drillthrough and conditional formatting while analyzing employee flight risk. Meagan Longoria also looked at retail reviews. Continuing the series, which is designed to highlight key capabilities within the Power BI platform, let's turn our attention to world energy.  The “World Energy Production Capacity” report explores the energy capacity of various nations and highlights a few components of Power BI along the way. The Data The data is regularly compiled by the World Resources Institute and consists of over 28,600 power plants and production facilities from 164 countries. It is important to note that the data relates to production capacity rather than actual production or energy usage within a country. What is capacity? It is the maximum amount of energy that could be produced by a power plant under optimal conditions. Not all facilities operate at 100% capacity, however, and countries trade energy for consumption. As a result, the Power BI report shows the production potential by various countries if all facilities were operating at their maximum capacity. A comparison of estimated generation vs capacity could be a future report, but attempting to delve too deeply into multiple topics from this dataset felt like it was diluting the story. This decision speaks to the difficult choices content authors face. Rather than put everything available from a dataset into the report for the user to explore, this report attempts to guide the audience with a narrower focus. The Report The report attempts to drive viewers toward a better understanding of each country's capacity by energy source. It starts with raw numbers and then considers why raw numbers may not be the most appropriate measure to derive insights by country. It also moves from summary to detail by eventually allowing users to obtain plant-level data across the world.  First Page – Raw Production Capacity The first page of this report provides a measure of energy capacity in megawatts. It introduces raw measurements of capacity in a series of faceted bar charts, or \"small multiples\", by energy source. Small multiples aid in the ability to view each country's values by source, but there is a key problem with displaying raw megawatts. China and the United States lead the world in capacity, but this is largely a reflection of population and industrialization factors. Color helps highlight China (orange) and the US (blue), while the rest of the world appears grey. This page helps showcase \"how much of what\", but many potential insights are overshadowed by China and the United State's large potential.  While viewing the page, users can easily identify the diverse mix of energy sources for both the United States and China immediately by looking at color and getting a sense of the magnitude if they consider the data labels. It takes some digging to understand what might be going on with other countries though. To help with that, the Country slicer at the top right allows viewers to easily filter down to an individual country's capacity. Overall, however, this page does not easily show how certain energy sources may drive a country's contribution. Second Page - Percent Contribution All is not lost! The second page contrasts the prior focus on raw numbers with a measure of each country's share by energy source. By shifting the measure from raw megawatts to contribution percentage, this page helps showcase the imbalance of many country's production capacities between sources. While still retaining the orange and blue colors for China and the US respectively, viewers will immediately notice that neither country appears at the top of any particular multiple. Instead, users see how different countries may rely heavily on one particular source while other countries have a more balanced mix. If viewers want to easily see the production mix for either China or the United States though, they can click on the country in the bar chart at the top, and the small multiples will filter to show a single country. Additionally, users can still select from the Country slicer at the top right to get data for any country.  Another possible way to highlight what is going on in the rest of the world would have been to normalize the data by accounting for each country's population. The production capacity per capita for each energy source would also help offset the heavy focus on China and the US based on the raw numbers, but that also leads to a slightly different story. Third Page - Numbers vs Percentages Building upon the idea of contribution percentage from the prior page, the third page helps balance the views based on raw megawatts with production mix by energy source. The colors change focus so that instead of highlighting countries, it now changes to focus on what sources are renewable (green) versus non-renewable (grey). This distinction is not part of the original dataset, so the categories for Renewable Source and Non-renewable Source were added using Power BI's \"Groups\" functionality. The introduction of the scatterplot helps provide answers for questions surrounding both \"how much?\" and \"what type of mix?\". To quickly filter down to a single energy source, users can click on the bars in the charts at the top and to the right. Raw capacity appears on the Y Axis while contribution percent appears on the X Axis. This chart helps viewers see insights such as: While the United States has a high capacity for each non-renewable source, each source accounts for a small percentage of the US production mix. The United States leads the world in raw capacity for sources such as nuclear, wind, solar; but each source makes up a small percentage of the share of US energy. In contrast, France has both high capacity and a high share of nuclear energy. No country is exclusively reliant on non-renewable sources such as wind and solar, but many countries produce all energy with hydro power. Many of these sample insights would have required additional thought when seeing the same data presented on separate pages with small multiples. To account for having both measures presented on the same page with one visual instead of eight, different shapes help users more easily distinguish between energy sources when looking at the scatterplot. The bar chart on the right also helps show the overall production mix for all countries combined, something that was lost when the data was presented in small multiples. It is easy to see that the combination of coal and gas makes up more than half the world's production capacity, hydro makes up a larger percentage than may have been expected, and wind makes up almost 5% of the world's capacity at this point.  Final Page - Facilities The final page reintroduces the bar chart by energy source from the third page and allows viewers to explore how each source is distributed throughout the world. To view this distribution, the report uses a heatmap that plots each facility's latitude and longitude on Power BI's Mapbox custom visual. What started as an aggregate view by country ends with plant-level detail. Areas of high density appear orange while areas of low density (and eventually individual facilities when zooming in) appear blue. The heatmap also visualizes the issue when overall distribution largely mirrors population. Clicking on individual bars on the bar chart, however, shows how energy is spread by source without as noticeable of a population factor.  To interact with the report in more detail, view the live version here! Why Power BI? Power BI is the leader in modern business intelligence with the most balanced capabilities between data preparation, data modelling, and visualization. It also integrates well with a variety of other Microsoft services and technology investments both in the cloud and on-premises. While this showcase report demonstrates the several ways to visualize data, there is a lot more to a successful analytics initiative than what appears on the surface.  If Power BI intrigues you, BlueGranite offers engagements for both deployment and training. In addition, we also have deep expertise in the Microsoft data platform overall. Whether you are interested in launching Power BI within your organization or incorporating it as part of a larger solution, please contact BlueGranite today. "
"114" "With the features of Azure Data Factory V2 becoming generally available in the past few months, especially the Integration Services Runtime, the question persists in our practice about which data integration tool is the best fit for a given team and project. Both Azure Data Factory and SQL Server Integration Services are built to move data between disparate sources, and they do have some overlapping capabilities. If you are new to either of these tools, here are some questions to help you begin understanding the differences to better make a selection.   Does your organization use Azure today, or is Azure an option for your project? SQL Server Integration Services (SSIS) is a tool that has been around for many years. It started as an on-premises service, but that doesn’t mean it’s impossible to run in Azure in either a fully-cloud environment or a hybrid-cloud environment where your organization’s network is extended to Azure. On the other hand, Azure Data Factory is a cloud service only, and while it absolutely can integrate into on-premises services and data, it does require a cloud service to be running in Azure. So, the first question to address is whether Azure is in use today or could be deployed to help create your solution.   Hybrid On-Prem & Azure Solution Pure Azure Solution On-Prem Only Solution Azure Data Factory (ADF V2) Yes Yes No Integration Services (SSIS) Yes Yes Yes   Can either or both services connect to the data sources and destinations you need? Both ADF and SSIS have the capability to connect to a huge variety of data sources and write to different destinations, but in order to move forward it does make sense to verify the specific needs you have at the endpoints of your data integration. If your environment includes anything atypical besides common database engines, files and services, it pays to validate that you can consume and then write what you need to. Azure Data Factory uses the concept of a source and a sink to read and write data. Microsoft’s Data Factory Documentation covers all ADF’s possible sources and destinations; check out Copy Activity in Azure Data Factory for an overview. ADF supports a huge variety of both cloud and on-prem services and databases. If you are integrating it into data sources or destinations which reside within your corporate network, you will need a self-hosted Integration Runtime (Gateway) that handles connecting to your internal resources and transmitting the data up to Azure. SSIS uses the connection manager concept to pull the data from a source and put it into a common/consistent format to work on internally in a data flow. There are standard connections built into SSIS, but also third-party connection managers that you can purchase and plug in for a variety of services. Will you write ETL or ELT? The next consideration is a bit more involved if you are new to data integration. Both of these tools excel at transporting data from place to place, but they have important differences in terms of what you can do to modify the data in transit. As a matter of emphasis, ADF has more features geared toward moving the data than performing any complex transformation along the way. SSIS, on the other hand, was built with a large library of transformations that you can chain together to make elaborate data flows including lookups, matching, splitting data, and more. The tools also overlap quite a lot. In projects this seems to lead to the question of whether you’ll transform the data “in flight” using Extract Transform Load (ETL), or instead move the data to a destination where it’ll be transformed using Extract Load Transform (ELT). Imagine loading a data warehouse from two or three business application databases. The two patterns here, at a high level, would be to pull the data out of the source and stage it into the data warehouse, essentially as a copy, and then call some stored procedure code to transform it into the warehouse proper, or to pull the data from the sources and use the data flow transformation within the integration tool itself to transform the data. There are pros and cons to each pattern, but deciding criteria includes: Does the destination for the data have the capability, like a database, to transform the data after it lands there? If not – say you are loading into a cloud service and not a database – then the decision is sort of forced to transform the data before it lands. Does the integration tool have the capability to transform the data? If not, then it may not be practical to try to use it to transform the data in flight. Finally, under those constraints, where would you prefer to do the transformation? Put another way, the place you need to transform the data is key to this. Consider whether it’s possible to transform the data within your destination, as in a warehouse database, and/or whether it’s possible to transform it in transit. The table below represents current generally available capabilities as of September 2018:     ETL pattern (transform in flight) ELT pattern (load first, call functions in the destination to transform the data) ADF V2 Limited Excellent SSIS Excellent Excellent   What scalability challenges are you facing? The next factor to consider is how much data you are dealing with. SSIS can be made to scale and can offer very good performance, but being mostly a purchased/on-prem tool it also may require that you tangle with servers and configuration to make it fast. ADF has the typical advantages of cloud services: it’s something you subscribe to and you don’t have to carry so much of the burden of infrastructure. To get some idea of possible throughput for ADF see this matrix. If you have a hybrid on-premises/cloud design, remember that the link to the Azure data center and that Integration Runtime (Gateway) that transmits the data up and down can also be a bottleneck. There are gray areas here, but many organizations find it’s very helpful not to have to work with underlying hardware, virtual machines and servers. As we talked about above, SSIS, recently, can also run within ADF V2, which we’ll turn to in a minute, but if you are starting a new solution that may muddy the waters a bit. Should you combine these two technologies? ADF V2 now contains an Integration Services Runtime that enables the cloud service to execute SSIS packages. The idea here is that one can create a hybrid solution with both technologies, which is an exciting possibility but begs some questions. Here are some things to think through if considering combining these two technologies: Do you have a large SSIS deployment already, that would benefit from moving it to the cloud without rewriting? What’s the volume of data you will be moving? Is it thousands of rows, hundreds of thousands, millions, billions? ADF, again as a matter of emphasis, is mostly a big-data tool, and so it can move large quantities of rows quickly, but with some performance dedicated to overhead operations, start up, and so on. SSIS seems to be somewhat lighter weight, so it may be more appropriate if you have a smaller workload – but conversely SSIS could be slower for huge data volumes. One analogy I like is to think of ADF as a freight train compared to SSIS as a truck. Both capable, but in the case of the train it might take a bit of time to get set up and moving. Once moving, though, the train can move a lot of freight very quickly. Since the two can now be combined, you can do things like call SSIS packages from ADF, if SSIS is a better fit for your workload. What’s your larger orchestration strategy for scheduling work? ADF V2 has a good scheduler with a lot of options, including the ability to chain tasks together with dependencies; will it fit into the larger strategy in your solution for calling automated ETL tasks? SSIS solutions have traditionally used SQL Agent as the scheduling engine and something like a “Master Package” to orchestrate running the ETL process with the appropriate dependencies, and that certainly still works. In this new world it’s possible to mix the two; for example using ADF to schedule the work, but call SSIS packages in the cloud, using the Integration Services Runtime. If you are planning to use SSIS in the cloud, how will you host it? On a virtual machine without using ADF, or within the new ADF environment? Is your team ready to tackle the two technologies together? It’s not terrible, but there is some learning curve if you are taking on both at the same time, or moving from one to the other. I hope this will help you to understand some of the important differences between ADF and SSIS for your data integration work, and to make an informed decision so you’re happy with whichever platform you end on. If you are looking to address your data and AI challenges, contact us today! Our experts are ready to assist and determine how we can help you streamline your business. Check out part 2 of this blog post here!"
"115" "This time around in BlueGranite’s Meet our Team blog series, we are highlighting our Microsoft MVPs. Microsoft gives this honor to technology experts who show “deep commitment to innovation” and “make outstanding contributions to their communities.” Melissa Coates, Meagan Longoria, and David Eldersveld have been doing both for years. Their volunteer roles – from technical blogging and running and participating in conferences, to providing feedback on new technologies before they are generally available to the public – have had a major impact on the Microsoft community.   Melissa Coates Melissa believes strongly in the importance of contributing to the Microsoft community, in part because the community has been so helpful to her throughout her career. Microsoft has honored those efforts by recognizing her as a Microsoft MVP since 2013. Melissa’s SQL Chick blog focuses on providing practical information on data warehousing, data lakes, and business intelligence. She speaks frequently at events, user groups, and conferences. Melissa is a co-founder of the Charlotte BI Group, and currently handles its website management, social media, and marketing. Melissa is very appreciative of her 5-year MVP status. When asked about what it means to her, she said: “Earlier in my career when I transferred to a new role in IT, I was asked to build a data warehouse. My best resource for learning SSIS at the time was from another MVP’s blog. The level of sharing among our community is so impressive. Community resources help me do my job each and every day, and my personal contributions not only help others do their jobs well – but force me to continually get better myself.” Melissa has worked for BlueGranite for more than 4 years. She said she appreciates the company culture of shared knowledge and added that it’s reassuring to know if she needs to talk through issues on a project, she can turn to BlueGranite’s versatile expert team for assistance. When Melissa puts down her laptop and steps away from the ever-changing world of data, she likes to spend time paddle boarding with her husband and dog.  Meagan Longoria Meagan was first awarded Microsoft MVP status in 2016. Her contributions to the community include her Data Savvy blog – which thoughtfully demonstrates how to make data accessible and useful; speaking at conferences and events; volunteering to organize meetings for her local Denver SQL User Group; organizing Denver’s SQLSaturday events; and providing feedback on new technologies to Microsoft. One of Meagan’s favorite things about being an MVP is sharing her knowledge and receiving feedback that a blog post or presentation really helped a community member do their job. She also appreciates the opportunity it offers her to “meet up with some great people across the U.S. and the world” that she might not have otherwise met. Meagan’s favorite technologies to work with are Analysis Services and Power BI. She said: “I enjoy building out semantic layers where you turn data into something useful for people. All of the technologies in our industry are important and useful in the right circumstances, but the value becomes obvious through data visualization. It takes effort to do it right, but that is where people see the value because they are finally getting the information they need.” Outside of being an MVP, Meagan also enjoys her work at BlueGranite. She said she appreciates the flexibility that working from her home office brings, and especially enjoys the ability to work with her bulldog, Buster, by her side every day. When Meagan isn’t creating striking and informative Power BI visuals, or helping people better understand their data to make good decisions, she likes to spend her time hiking in Colorado and photographing the mountains and nature that surround her.  David Eldersveld David is a two-time Microsoft MVP awardee. The honors stem from his multiple Microsoft community contributions, which include his Microsoft-focused DataVeld blog, his work with the company to improve new technologies, and his social media engagement. David is grateful for the Microsoft MVP recognition. It’s an added bonus for what’s become a labor of love – sharing his Power BI excitement and experiences through his blog (which he describes as “his own stream of consciousness and a bit off the wall”). His style is one readers appreciate. DataVeld followers continue to grow, and Power BI learners continue to turn to the blog as a trusted educational resource. With a degree specializing in Business Intelligence and over 10 years of experience with data and now AI, it’s safe to say David has a knack for working with Microsoft technologies. When asked about what he likes about being an MVP, he said: “I enjoy getting the opportunity to pioneer thoughts and activities around Azure data services. Working through Microsoft’s documentation resources is a great way to learn, but there is so much more that isn’t covered. I like taking on the challenge to address what might be missing from our community and using it as a way to connect and help others learn.” When David isn’t busy “geeking out” with data and BI work, he enjoys spending time with his young daughters, who have also helped him to extend his expertise to Disney princesses. Want to learn more about the BlueGranite team and how we can help with your data and analytics projects? Contact us – we’re always ready to assist! "
"116" "In addition to the primary Cognitive Services tools that BlueGranite has previously explored – Search, Vision, Speech, Language, and Knowledge – Microsoft has experimental APIs under the name of Cognitive Services Labs. While you should not use these APIs in production, they offer a glimpse into pre-built AI that Microsoft may eventually promote as a supported API.  Get more detail about the different Cognitive Services Labs APIs here. Vision Project Gesture Vision Project Ink Analysis Search Project Local Insights Search Project Event Tracking Search Project Answer Search Search Project URL Preview Language Project Conversation Learner Language Project Personality Chat Knowledge Project Knowledge Exploration Knowledge Project Academic Knowledge Knowledge Project Entity Linking Knowledge Project Anomaly Finder Knowledge Project Custom Decision Anomaly Finder Example As a data professional, I frequently work with Power BI, SQL Server, and a variety of tools in the Azure Data Platform. Analysis over time is key to much of the data that I encounter. With time series data, one of the ways that I could benefit from Cognitive Services is to use pre-built AI for automated anomaly detection. The Anomaly Finder API from Cognitive Services Labs flags each data point as an anomaly or not. An example of an anomaly would be a value that falls outside the expected range. The service also returns the expected value and the upper and lower thresholds outside of which it flags anomalies. As with other Cognitive Services, you don’t necessarily need to be a developer heavily reliant on code. Even Power BI can call the API without using any external tool or language. With Power BI as an example, I’ll use transformations in Power Query to change the source table data into JSON. The API requires JSON keys named Timestamp and Value, but you can also include supporting fields. Why? The API response will not include the original Timestamps or Values, so additional fields help you tie the response back to your original data. In this case, I’ve included Order Date for each record. Having Order Date will later allow me to join the results to the original data on that field. Here’s a glimpse at the data in tabular form prior to converting to JSON:  Here’s a subset of the data that is in a form ready to send to the Anomaly Finder API: { \"Period\": 0.0, \"Points\": [{\"Order Date\":\"2010-01-01\",\"Value\":194,\"Timestamp\":\"2010-01-01\"},{\"Order Date\":\"2010-01-02\",\"Value\":11808,\"Timestamp\":\"2010-01-02\"},{\"Order Date\":\"2010-01-03\",\"Value\":7936,\"Timestamp\":\"2010-01-03\"},{\"Order Date\":\"2010-01-04\",\"Value\":402,\"Timestamp\":\"2010-01-04\"},{\"Order Date\":\"2010-01-05\",\"Value\":6258,\"Timestamp\":\"2010-01-05\"},{\"Order Date\":\"2010-01-06\",\"Value\":9385,\"Timestamp\":\"2010-01-06\"} [..] Once the time series data is in the appropriate format to send in the body of the request, use Power Query to call the API as well. The Anomaly Finder API requires a POST request containing the authorization token and content type in addition to the body. In Power Query, this type of web request requires some manual M coding with the Web.Contents() function. In addition, since the response is JSON, use the Json.Document() function and then other functions to clean up the results. You may end up with something similar to the following M code. = Json.Document(Web.Contents(_CogServUrl, [Headers = [#\"Ocp-Apim-Subscription-Key\" = _AuthKey, #\"Content-Type\" = \"application/json\"], Content = Text.ToBinary(Body)])) After some transformation of the JSON, the results are in table format.  I’ve also created a relationship between the original Orders table and the table containing the API results.  Finally, after adding the results to a visual, I can easily view any day with sales anomalies – all accomplished without leaving Power BI.   Remember that Cognitive Services Labs contains a set of experimental APIs. Anomaly Finder is one of the many promising Lab APIs that will hopefully make its way to production-level Cognitive Services at some point. While it and the other experimental services are not appropriate for production workloads, they showcase future possibilities and the immense potential of using AI to enrich your solutions."
"117" "Over the past couple months, we've highlighted some of the most useful and popular Microsoft Cognitive Services such as Search, Vision, Speech, and Language. Our final post in the series is about the Knowledge service and its capability to intelligently deploy resources like Question and Answer (QnA) and FAQ content where they are needed most. In this article, we’ll cover some of the basics for getting started, and some examples of how this service can be used by itself and in combination with other Azure components. Before we get into the details of the Knowledge service, let's review the business value of AI and Cognitive Services in general.   Microsoft’s Cognitive Services provide artificial intelligence (AI) capabilities – which is essentially the ability of a machine to learn from experience and perform a task normally requiring human intelligence. The most popular AI applications are around vision, speech, and text processing – such as identifying objects in a picture, translating speech to text, or extracting the most common or useful parts of a body of text. I like to think of this as converting unstructured data into structured data that is more easily mined for information and insight. These capabilities represent tremendous value, but have traditionally been difficult to deploy directly in support of a business function. They are complex and expensive to develop manually (usually requiring data scientists) and can be challenging to integrate into applications. Microsoft Cognitive Services relieves this pain by acting as the central intelligence components of an AI Oriented Architecture on Azure, where the complex data science elements come pre-built, you can couple many different services together, and easily deploy the architecture where your customers are most likely to use it. It’s a microservices pattern for AI. I’ll use a crawl-walk-run approach to show how your organization might use the Knowledge Cognitive Service in increasingly powerful ways. Crawl Writing this blog was my second opportunity to use MS Cognitive Services. I’ve used the Vision service with a client before and was very impressed with how easy it was to get started and demonstrate accurate object recognition in photos in just a couple hours. I had a similar experience with the Knowledge service, and as a bonus, got to learn about and create a Bot! The Knowledge service consists primarily of the QnA Maker, which “enables you to power a question and answer service from your semi-structured content like FAQ (Frequently Asked Questions) documents or URLs and product manuals. You can build a model of questions and answers that is flexible to user queries, providing responses that you'll train a bot to use in a natural, conversational way.” The value here is taking a variety existing knowledge resources – whether they are on a website, in product manuals, text files, or other documents – then putting them all together to form a unified and interactive question and answer service readily available to users. This is a great way to help employees or customers quickly find answers they need about products or services across distributed sources, without using valuable human resources to answer repetitive questions. Here’s a sample architecture: I developed my own QnA Maker service and here is my experience. I got started at QnA Maker. If you’d like to create your own service, make sure you have an Azure account (free trials are available). Once logged into the QnA Maker portal there are three primary tasks, all accomplished without coding and using nice user interfaces (UIs) in the QnA and Azure portals: Create a QnA service in Azure. This provides the engine to extract question and answer pairs, as well as publish your service endpoint. Populate a knowledge base. Use the QnA Maker interface to add existing URLs or files with question and answer pairs. There are lots of options for data sources – so no need to re-create existing FAQs! I spent 5 minutes typing some simple questions and answers into a plain text file and it worked perfectly. You can also manually add Q and A pairs in the interface. Connect your service to your knowledge base. This aggregates the sources you provide to create one knowledge base and enables an API (application programming interface) endpoint – usable in your own applications, third party chat channels like Skype, Slack, and Facebook Messenger, or with something pre-built like a Microsoft Bot. Once you’ve completed these steps, you’re guided to other activities like testing and training your knowledge base, or connecting to a bot. Let’s look at both. You’ll notice in my sample questions and answers in the screen shot above that I have a question “Who is the speaker?” Using the test feature in the QnA portal, I can get the correct answer. But modifying the question to “What is the presenter’s name?” provides no good match.  Using the UI, I can easily provide an answer to this new question and/or provide alternative phrasing for the question. Adding my name as a new answer worked as expected! You can also update your documents and refresh – so it’s easy to update your knowledge base with files, or through the UI.  Now, what about making the Knowledge service available to users interactively? This is a job for a bot. Once I finished making my Knowledge service, I was provided instructions to build a QnA Bot. I did this on the Azure portal, and from there I was able to connect my shiny new bot to my existing QnA Maker service and test my knowledge base. There’s also great documentation for connecting your bot to channels like Facebook, Skype, Slack, and others.   Whew! That’s quite a bit just to learn to crawl. But going through the QnA Maker creation process was relatively simple, and very rewarding. I can already see how I might use this on a current DevOps project to provide quick project answers across business, IT, development, and data science teams. The Walk and Run sections are shorter, but demonstrate how we can group multiple Azure components together with the Knowledge service. Walk Once you’ve seen a knowledge base in action, you might wonder, “What about a QnA service when there are many different domains of knowledge – such as separate business units or specialty areas?” Glad you asked! Within the service, you certainly could create one knowledge base with all the disparate domain content, but that could be difficult to maintain among a diverse set of curators. Azure has a better way, using the LUIS (Language Understanding) service.   In the diagram above, several areas of knowledge, such as Payroll, HR, and Finance, all have separate knowledge bases (KB) within a single QnA service. To provide a single point of access to all three KBs, LUIS is used to take an initial user question and determine where it should be routed. This is a great example of the power of AI; to take a natural language question and distill it into a smaller “package” – like a single word or phrase – so that it can be related more easily to other data. In this case, a LUIS service is trained to recognize these distinct areas, and then call the appropriate knowledge base endpoint. As you might expect, all of this can be handled seamlessly within one Azure bot. This is a simple but powerful example of an AI Oriented Architecture. You can read more about integrating LUIS and QnA here. Run Our final example is the most comprehensive use of the Knowledge Cognitive Service paired with other Azure components, and was recently featured in a MS Machine Learning blog about building a Conference Buddy intelligent app.  The scenario: users at a conference presentation wish to learn more about the speaker and/or the speaker’s content, but don’t have the opportunity to directly ask “live” questions. For example, an attendee might want to access the speaker’s bio by asking a question in a simple interface, rather than following a series of links on web pages. Or attendees may ask questions about the presentation topic that weren’t covered during the talk and want focused results, like a video that just plays the portion relevant to their question. The speaker also wants to know what questions the audience is asking. All of this can be handled by grouping the Knowledge service with LUIS and other services like Bing Search, Text Analytics, and Video Indexer, then presented to audience members in a chat (bot) interface. The speaker can have a dashboard to monitor all the questions in real time. The bot greets attendees like this:  Behind the scenes, there are multiple services configured to handle the different question types. You can review the full architecture on the blog, but it is analogous to our previous example of having LUIS route the question intent to the appropriate knowledge base within the Knowledge service. In this example, however, we use LUIS to route among multiple Cognitive Services and Azure functions.  Finally, a speaker’s dashboard can be built using an Azure web app to provide insights from attendees’ questions. This could be shown on stage, provided to users, or just used by the moderator(s) to gauge interest and participation.  That’s it! I hope this helped familiarize you with the power of Cognitive Services in general, and the Knowledge service specifically, in a variety of ways. Stay tuned to the BlueGranite blog for more on Microsoft AI!"
"118" "Combining Power BI with statistics yields some very powerful results. In this post we’ll show how easy it is to do Linear Regression with the Power BI tool. Linear Regression is a very useful statistical tool that helps us understand the relationship between variables and the effects they have on each other. It can be used across many industries in a variety of ways – from spurring value to gaining customer insight – to benefit business. The Simple Linear Regression model allows us to summarize and examine relationships between two variables. It uses a single independent variable and a single dependent variable and finds a linear function that predicts the dependent variable values as a function of the independent variables. We look at two statistical values to determine if there is a relationship between the two variables and how closely related they are.   The Coefficient of Correlation is a statistic we use to determine if there is a relationship between two variables. The output of this statistic equals somewhere between 1 and -1. The closer to 1 the number is, the more positively related the variables are. As in, if X increases, Y increases. The closer to -1 the number is, the more negatively related the variables are. If X increases, Y decreases.   The Coefficient of Determination is a related statistic that then tells us how well our model fits the data. This statistic is always between 0 and 1, and the closer to 1 the value is, the better our model fits the data set.   So how do we perform Linear Regression in Power BI? First, we make a scatter plot and visually examine the data to see if we think there is a relationship. Scatter Plot in Power BI In this example, I used my own financial data to see if I could understand the best ways to save money each month. This analysis shows the relationship between the number of times I went to restaurants and the money spent in this category of my monthly budget. Food is my second highest budget category each month. I don’t think the bank will cut my mortgage down to save me a bit each month, so my restaurant spending seems like the next best place to start. Here’s how to follow along using your own data: Create a Scatter Plot 1. Click the Scatter Plot visualization and add your columns. In my case, I used Year Month, Count Days, and Amount to determine how often I’m dining out and the associated cost.  2. From the Analytics pane add a Trend Line.   The results should look something like this:  There definitely seems to be a correlation between my dining out and my increased expenses, so I’ll make the calculations to see if I’m right. Creating the Coefficient of Correlation Right click on the table and click New quick measure.  Select Correlation coefficient from the Calculations under “Mathematical operations”.  Select the Category, Measure X, and Measure Y. These columns will match the dot plot we created earlier.  The Coefficient of Correlation will now be available in your table, and it’s ready for use.  Creating the Coefficient of Determination In this case, a quick measure would be overkill. The Coefficient of Correlation is notated as the letter R. The Coefficient of Determination is R2. Coefficient of Determination = [Coefficient of Correlation]2 I now have two statistics based on my data set that tell me how and to what degree my X (Count Days) and Y (Amount) variables are related.  As we can see, per our definitions above, both the Coefficient of Correlation and Determination are very close to 1. This means Count Days is certainly related to Sum of Amount and does a very good job of predicting how much I’ll spend given the number of days I eat out every month. Model’s Many Benefits Besides offering basic budget insight, Simple Linear Regression analysis is useful for a wide variety of verticals and business cases. Combining it with Power BI can create powerful analytical capabilities. We can use Linear Regression to analyze the effect of marketing on sales and profits. Or it can clue a company in to how raising prices may affect a consumer’s buying habits. Insurance companies can also use this technique to assess risk between customer demographics and insurance claims. Looking to Learn More? If you are interested harnessing statistics with Power BI to boost your business, contact us today. Our Power BI training covers everything from basic reporting to advanced querying and modeling techniques."
"119" "The big data and AI world can be a complex one for companies to navigate alone. This has led to the perception of a high entry barrier among many organizations. Some of the early comments and questions we hear from companies we partner with include:  Wow, there’s a lot of tools and services out there! Which one do I need? Do I need more than one? Do I currently have the necessary skills in my organization? Do I have the money to invest in a new solution, its infrastructure, and the new hires or training it will require? With all this configuration, administration, training and hiring, etc., how long will it take me to realize any value from such a platform? Often, individual big data tools and services satisfy very specific use cases. This means that if a solution requires streaming, batch processing, and machine learning, companies must invest in multiple tools and services for each of those individual functions. An example of this can be seen in the Azure ecosystem where clients might string together multiple services like Azure Data Lake Analytics for batch processing, Azure Stream Analytics for stream processing, and Azure Machine Learning for machine learning and data science. Each of those services includes, and caters to, its own interfaces, languages and syntaxes, and users. This means that companies will likely need to invest to hire or train employees in unfamiliar, highly specialized skillsets. With a flood of tools and methodologies, the machine learning and data science space carries similar complexities. We find many companies asking questions like: “Do I use R or Python?” “What framework do I invest in for deep learning?” and “Will it even still be relevant in 6 months?”  Azure Databricks simplifies the big data and AI conversation and removes many of the barriers to entry. The service provides a platform that unifies many of the disparate use cases in the big data space, supports many of the skillsets and languages that most clients already have in house, and is super easy and fast to administer – meaning a quicker time to value. Let’s look more closely at why many of our clients are realizing major value in Azure Databricks. Single, Unified Platform First, Azure Databricks brings together most of the common big data use cases under a single platform. This means that if a solution requires the use of scheduled batch processing, near-real-time stream processing, interactive querying and data analysis, or just about any other common big data use case, we don’t have to string together several disparate services. The same goes for machine learning and data science. Azure Databricks, built on Apache Spark, includes many of its own machine learning libraries but also supports easy interaction with many other popular machine learning frameworks like XGBoost, scikit-learn, TensorFlow, Keras, and Horovod.  Unifying all these different use cases also means uniting all data users under a single platform. As discussed in a recent blog, Azure Databricks brings together data engineers, data scientists, and business analysts into a single collaborative workspace. This again eliminates the need to try to make multiple disparate tools and services work together to satisfy different users. Familiar Skillset and Tools Our clients also appreciate the familiar skillsets and tooling the platform provides. Azure Databricks allows for querying, analyzing, and processing data in SQL, Python, R, and Scala.  Organizations can begin working and developing in Apache Spark on Azure Databricks with a very small learning curve. In many cases, that means existing teams can transfer their current skills and knowledge straight into Azure Databricks, and expensive new hires or extensive training is unnecessary. Among the users who will quickly acclimate to Databricks’ environment are: ETL developers and business analysts who are already using SQL in their day-to-day work data scientists that are already using R or Python for data science and machine learning and even application developers who are familiar with Python or Java Easy and Fast Administration Finally, clients find that administering Azure Databricks is easy. As soon as the service is turned on, it is connected to a client’s Azure Active Directory. From there, adding users, creating clusters, and managing the workspace is intuitive, and can be performed through a very simple UI. Almost everything an organization would typically need to configure can be done through this UI, but there is also a REST API and CLI that can be used for more advanced configuration and automation. These are all functions that a typical systems admin is already comfortable doing. This simple experience typically leads to much faster time to value, as companies can spend less time laying the pipes and configuring the system, and more time working with their data. So many of our clients are discovering Azure Databricks’ appeal – it’s proving to be a service that allows us at BlueGranite to simplify the message around big data and AI development, and it is removing many of the questions and concerns that clients previously perceived in those spaces. If you are interested in seeing for yourself how Azure Databricks can simplify big data and AI in your organization, please contact us. Whether you’re looking for more information, are interested in an Azure Databricks proof of concept, or you want to learn more about our 1-day workshop, our experts would love to help."
"120" "This post walks through a new report in BlueGranite’s Power BI showcase – a series designed to highlight key capabilities and tools within the Power BI platform using real and reproducible methods – and introduce the data and features used to create it. The fictitious ABC Apparel Company would like to see what buyers have been saying about their products recently. Users who make a purchase from ABC Apparel are asked: whether they recommend the product how they would rate the product on a scale of 1 to 5 to review the product in a free text form A Power BI report was made to analyze this information. It contains a summary of reviews, ratings, and recommendations sliced by the most important categories for ABC Apparel. In addition, we can also see ratings distributions for the most important categories, and we can drill down to see the individual reviews. ABC might be having a problem with repeat customers, so they are turning to reviews to see if it exposes any product issues. Or they might just be doing a brand health check and want to be sure that there are mostly positive reviews left on their website. The Data The source data came from Kaggle and is available under a CC0 1.0 Universal license, making it free for commercial and non-commercial use. The dataset contains real data that has been anonymized.    Power BI Features Showcased This report takes advantage of several great features and capabilities for building models and reports in Power BI Desktop: Layout background images Tooltip reports Drillthrough actions Custom visuals Bookmarks Buttons DAX calculations An explanation of how these features are used can be found in this post after the report description. The Report The native interactivity of Power BI reports facilitates quick and easy exploration of trends and relationships in the data. The summary page of the report presents totals and averages for all reviews and then goes one level deeper to look at reviews by age group of the reviewer and the department, division, and class of the product.  Note: As you explore the report on your own, I recommend clicking the double arrow in the bottom right corner to view the report in full screen mode:   As users click through the summary page, they notice that buyers age 35 - 44 are by far the most common reviewers on the site. They can also see that tops are the most commonly reviewed products. In general, ABC’s products are highly recommended. Just by clicking on the bars in the Reviewers by Age chart, we get some insights. Buyers age 18 - 24 are a smaller group but seem to be happier with the products they buy. They recommend 86.7% of the products they review and give the products an average rating of 4.33. This is higher than the overall 82.2% of products being recommended and 4.20 average rating. This could mean that ABC has a mismatch between the styles offered and the target audience, or it could mean that buyers age 18 - 24 simply don’t leave as many reviews, which could be remedied with some incentives such as discounts or loyalty points. Items in the trend department have a lower percentage of reviews that recommend the products. Hovering over the row for Trend in the Reviews By Department table displays the last review that was left on the website in the last 6 months (the time period shown in the report). The comment mentions sizing issues, so it’s worth investigating further if that is a common trend. Users can then drill through to a detail page to see whether the sizing issue is commonly mentioned in comments. From browsing the first several reviews, more than half of the reviews mentioned size or fit issues. By selecting a single card in the visual on the bottom half of the page, we can see more information about the selected review, including the product ID, class, department, division, and whether the reviewer recommended the product.  When the user is finished looking at individual reviews, they can return to the summary page using the navigation button at the top left of the page. The bottom visual on the summary page allows users to see whether there are any large variations in ratings when looking at the products by class. We again see that trend products are rated lower than products of other classes. We also see chemises are generally rated lower than other classes. By clicking on the Intimate department in the table above to cross-filter the Ratings by Product Class chart, we see that everything else in the department is doing well, and it is just the chemises suffering from lower ratings. People from a variety of backgrounds and abilities use Power BI to get important organizational information. For those that may be new to Power BI, a link to a separate page highlighting accessibility features is included at the bottom of the summary page. This can be helpful for users that don’t use Power BI much and aren’t aware of the accessibility features or for when new accessibility features have been added that might be important to the report audience. Clicking on the icon takes the user to a quick page that highlights three big accessibility features. Alt text has been populated for users that use a screen reader to consume the page. Keyboard navigation is available for users that can’t or prefer not to use a mouse. High contrast color settings are available for those with low vision. And the Spotlight feature on each visual helps users that struggle with processing visual information to send everything else to the background and focus on one chart at a time.  The second page of the report shows the distributions of ratings for the selected category with some summary statistics in the bottom left quadrant. Where users saw the average rating on the Summary page, they can now see the proportion of ratings that each rating value (1 - 5) represents. On the Ratings Distribution page, we can see that the low average rating for chemises is largely due to a small number of reviews which lean negative. When looking at the ratings distributions by department, we see the lower percent of recommended products goes along with the higher percentage of rating values of 1, 2, or 3 compared to other departments. And Trend is the only department that does not have a majority of ratings with a value of 5.   After exploring this report, users should have a better understanding of which products are highly rated and which are struggling, as well as which age groups review and recommend the company's products the most. Drilling into details provides specific feedback on the best and worst features of each product, which could be useful for making changes both to product sourcing/purchasing and to how the product is described on the website.  Power BI Features Explained The layout in the background of both the Summary and Ratings distribution pages is a background image created in PowerPoint, exported as an image, imported into the report, and applied to the page. Some third parties offer layout images for Power BI reports as well as report themes, so you don’t even need to make your own. Tooltip reports provide the functionality to hover over the table and see the last review. To make a tooltip report, it’s easiest to use the Tooltip page size. Then enable the report to be a tooltip in the Page Information settings and set the tooltip on the visual over which you want to hover to see the report. A drillthrough action is used to navigate from the Summary page to the Review Details page, and a button is provided for navigation back to the Summary page. Users can right-click on any age group, department, or division and choose Drillthrough and then Review Details to access the drillthrough page.  Custom visuals allow visualizations that aren’t available out of the box in Power BI. The dot plot on the Summary page is the Dot Plot by OKViz custom visual available for download in the marketplace (AppSource). The Accessibility Help icon is a button linked to a bookmark. The Accessibility Help page was created, and then a bookmark was created to point to that page. Then the button was added to the Summary page, and the button action was set to go to the bookmark. The Ratings Distribution page makes use of some DAX tricks to allow users to change the category shown on the y-axis of the chart. A calculated table was created that makes extensive use of the CROSSJOIN, VALUES, and UNION functions. And then calculated measures were added on top of it. Why Power BI? Power BI combines data visualization capabilities with powerful modeling capabilities to create interactive reports in a short amount of time. Power BI Desktop and PowerBI.com provide the features necessary to create reusable semantic models that support compelling visuals which effectively communicate insights to users.  You can explore the live version of the report here. Check out the previous installment in our Power BI showcase “Employee Retention – Organizational Flight Risk” here. Whether you are interested in deploying Power BI interactive analytics within your organization, or you want to get up to speed with training on its capabilities, please contact BlueGranite. We would be happy to share our knowledge and experience.   "
"121" "We recently explored how powerful visualization is in Business Intelligence (BI). In this next BI Basics post, we’re investigating the impact of the “T” in “ETL” – the Extract, Transform, Load data metamorphosis that is central to BI. Data transformation turns numbers into insight. In the process of reporting data, even the most routine operational reports do basic transformations – joining different data tables together, filtering rows and returning requested fields. The information returned to the user is almost always distinct from the information as stored. That transformed data is what gives an organization leverage to compete and succeed.  Data Transformation is Vital What’s really happening to an organization’s data between the at-rest stage data and that being served to a report? It’s being transformed from a handy storage and processing format to one that can change the fate of a business. Reporting systems make these transformations by applying rules that give a specific interpretation of the data. For instance, a typical sales report gathers the date of sale, sales price, item(s) purchased, discounts applied, store or channel, and other relevant information. While those bits of information might be stored on 27 different tables in the source system, the report can present all those pieces of data as a single block of information to the user. By combining these numbers and applying rules we get a logical or a representative look at the data, bringing what’s pertinent to the forefront and removing much of the behind-the scenes housekeeping miscellany. Stored Transformations Offer Stable View Transformation is essential to BI. It offers a logical interpretation of the data that drives business. But it’s just as important to keep a record of those alterations to maintain a consistent, logical data view. Why? Because different people might have different interpretations of the same data. Let that sink in. One sales manager might look at the numbers and see $15 million in sales, while another might see only $14 million. That’s a $1 million difference! One might have included discounts while the other didn’t. Or one might have deducted returns while the other didn’t. Stored transformations equip us with the necessary tools to enforce one version of the truth. These stored transformations can help define that sales totals include deductions for returns and discounts. They can also define a concept of raw sales that does not include those deductions. Stockpiling transformations helps ensure consistency of interpretation. You might notice that we refer to storing the transformation rather than the transformed data. It is not strictly necessary to store the transformed data. If the reporting data goes through the transformation, you can achieve the same results for current data, whether that transformation is done on the fly or as part of a traditional ETL process. That being said, there are some real benefits to also storing the transformed data. Remember, this data has been converted to a format that is aligned with the organization’s use and its logical understanding of the data. When you store this transformed data in a secondary data store, it becomes a data mart or a data warehouse. You’ve arrived at the classical Extract, Transform, Load process, and storing that transformed data in a new reporting database has some real benefits. Converted Data Eases Reporting The transformed data is more efficient for reporting. When the data is converted as part of an ETL process, it can drastically reduce the processing need when generating a report. Queries for a system designed for day-to-day transactions are called online transaction processing (OLTP) databases. They often require many joins to arrive at the final dataset for a report. OLTP systems separate the data into a multitude of tables to eliminate duplicate data. However, a data mart or data warehouse uses fewer tables, and those tables are aligned with business definitions. A report generated from a data warehouse structure, then, requires less processing to arrive at the final dataset. An independent reporting database improves availability. Reports often comb large swaths of data as they build their datasets. An OLTP system can be brought to its knees if a large report is run against its database during peak hours. It can cause that system to become unavailable for the normal day-to-day operations for minutes or even hours. For mission-critical systems, that kind of downtime is unacceptable. Any downtime might be unacceptable. If the reporting data is on a separate system, loaded periodically, each system can be optimized for its own needs. A data mart or data warehouse is not limited by the source system. As a source system is upgraded or replaced, the reporting database can continue to feed carefully crafted reports with less interruption. Because it reports the logical business view rather than the nuts-and-bolts of the source system, a new source system can be integrated seamlessly with the existing data sourced from the older system. Likewise, it can maintain historical data that the source system may throw away. By being stored separately, it can become a more powerful source of information than the source system alone. Flexibility in Hybrid Approach There is a downside to the traditional data warehouse. What if the business definition changes? What if the business wants to capture a new aspect of the data? At those junctures, a data warehouse can only begin capturing the revisions after they’ve been discovered, coded, tested and deployed. With the advent of big data and data lakes, that need not be the case. Big data technologies allow for nearly unlimited storage and processing. A data lake leverages those technologies to store versioned copies of the raw source system’s data. When the source system is new or updated, those structural changes are incorporated to that period’s revision. Don’t we lose all the benefits of a transformed data source with this data lake? Not necessarily. As mentioned previously, we keep all the revisions of the source data, so history is preserved. And, there’s nothing preventing a data lake from co-existing with a data warehouse or data mart. In fact, it’s entirely possible to build a data warehouse as a virtual layer on top of a data lake. Or, alternatively, have a data warehouse supplied by the data lake. What does that achieve? Well, at the expense of storage efficiency, it allows for the logical view, the business definitions, to change even retroactively. Since all the raw data is collected and stored with historical revisions, the data warehouse can be rebuilt from scratch without losing anything. Such an architecture offers the ultimate in flexibility and features. Transforming Architectures Wow. Let’s take a step back for a moment. We just described the evolution of BI data provisioning. We started with transformations and arrived at architecture. In the beginning, there was the source system report. It was good but flawed. That evolved into mirrored reporting copies and quickly transformed into the data warehouse. That enabled more intense, efficient and business-oriented analysis, but technology isn’t content to stand still. The data lake looms to make the data warehouse flexible and rebuildable. I hear one final whisper, “What of OLAP, then? Where does that fit?” Indeed, I would do well to address OLAP. On-line analytical processing databases are highly indexed databases designed to serve analytic queries involving lots of aggregate calculations. They’re often called cubes or tabular models. If a properly designed data warehouse is the source, then there is actually very little transformation in structure between the data warehouse and the OLAP database. What they do offer, however, besides query performance, are measures and key performance indicators (KPIs). In some sense, this is the capstone of business-oriented transformation. Measures are the final encoding of a business definition, and KPIs measure the goals of the business. With that, you have the business view of the data fully encapsulated in an analytic system. Business Transformation In the end, data transformation gives an organization the tools it needs to see what’s working and what isn’t. The ability to make decisions based on statistics, rather than educated guesses, is the first step in becoming a true data-driven business. Stay tuned for our next BI Basics post by subscribing to our blog. If you need help transforming your organization, contact BlueGranite. Our expert team is eager to help."
"122" "A new version of the popular whitepaper, Planning a Power BI Enterprise Deployment, is now available as a resource for the community. This whitepaper was a joint effort between BlueGranite and Chris Webb of Crossjoin Consulting. Two of our solution architects, Melissa Coates and Meagan Longoria, were co-author and tech editor, respectively.  This revision of the whitepaper incorporates changes which have occurred in the 13 months since V1 was published. The evolution of Power BI continues to move at a fast pace, so updating the whitepaper involved a large number of edits and additional content. Some of the noteworthy content changes include: Section 2: Power BI Usage Scenarios. This section has been expanded to provide ideas for how Power BI can be used in an organization. Section 3: Power BI Architectural Choices. This is a new section that helps you understand what the best circumstances are for using the Power BI Service, Power BI Report Server, Power BI Premium, and/or Power BI Embedded. Section 4: Power BI Licensing and User Management. This content has been expanded significantly to bring clarity to the user-based and capacity-based licensing options. We also share some ideas for how to manage user licenses and trials effectively. Section 9: Power BI Collaboration, Sharing, and Distribution. These concepts are some of the most complex when it comes to planning a deployment. Therefore, we put a lot of time into trying to clarify the choices and suggestions for what to use in what circumstance. Section 12: Power BI Limits and Feature Comparisons. This is a new section which serves as a helpful reference. Section 13: Power BI Deprecated Items. This is a new section to clarify which features are currently deprecated or on a deprecation path. A few specific things that we spent time clarifying in V2 include: What is the difference between Power BI Premium (organizational embedding) and Power BI Embedded (external embedding)? [pg. 23-26, 39, 44, 178-180] How does the Power BI product team define sharing, collaboration, and distribution? How do companies use each approach effectively? [pg. 111-115] What techniques are available currently for handling deployments between development, test, and production environments? [pg. 118-121] What features does Power BI Premium offer, and when is Premium capacity most useful? [pg. 36-42, 145-149] What are the motivating factors for using Power BI Report Server? [pg. 35] What are some techniques to monitor usage of Power BI? [pg. 134-139] There were less significant changes to the topics like storage modes, data sources, data refresh, and gateways. Those sections did get some updates, and are still chock-full of great info and suggestions. As we know, Power BI evolves at a very rapid pace. That’s great from the perspective of getting new features regularly. Realistically, however, it does also mean that it’s very difficult to keep documentation current and complete. Therefore, please remember to verify the information presented in the whitepaper as some things will become out of date. Enterprise deployments involve a lot of considerations. Some best practices are clear, while others can differ from customer to customer. BlueGranite can help you understand what it takes to succeed with a Power BI deployment. Contact us today."
"123" "At the recent Spark+AI Summit 2018, Databricks unveiled a few amazing platform enhancements. One of the most exciting ones (in my opinion) is the new Unified Analytics Platform for Genomics. As a computational biologist, much of my research time is spent waiting on something to run. This is due to the fact that many bioinformatics tools simply aren't built for the modern data platform, not to mention the cloud. Thanks to the amazing people at Databricks, the Azure cloud is ready for scalable genomics workloads!   Accelerating Discovery Since the first human genome was sequenced over a decade ago, the cost and time to sequence subsequent genomes has drastically reduced. This means that the amount of data being generated from genomics-related work has grown exponentially (and is expected to continue to grow).  This also means that the potential for scientific discovery is increasing. From specialized drug treatments to combating infectious diseases to making genetically modified foods (albeit controversial), the area of genomics is definitely gaining momentum. The rise in popularity of genomics calls for an overhaul of the traditional methodology in order to meet the research demands. Databricks has answered the need with its Unified Analytics Platform for Genomics. Traditional Challenges When it comes to the modern genomic workflow, there are plenty of challenges that come into play:  Complex Pipelines To effectively process terabytes or petabytes of data, there are many steps that go into the pipeline to transform the data, align sequences, call variants, annotate them, and then analyze the output. This inherently produces bottlenecks due to the complex nature of the workflow.   Antiquated Analytics Tools In the sample pipeline above, you'll notice that there are plenty of tools that are used along the way. For example,  SnpEff, BWA, GATK4, and others are used to perform very specialized tasks on the genetic data. Many of these tools are command line-based. Some are only single-threaded and therefore very slow. And, even if the software has some notion of parallelism, it's often only been implemented using Message Passing Interface (MPI). This is certainly the case at many universities. Furthermore, it's quite rare that any of these important packages have been fully implemented in Spark and therefore are not \"embarrassingly parallel\".  Links:   FastQC   cutadapt   BWA   SAMtools   Picard   GATK   SnpEff   Diverse Teams From the white coat-wearing geneticists/biologists/lab monkeys to the data science/bioinformatics team to the data engineering group, there are lots of moving parts to a research project. Many times, each of these groups use different technologies and therefore use different types of files or programming languages to complete their work. This makes it hard to work together and transport results back and forth.  How The Unified Analytics Platform Helps The Databricks Unified Analytics Platform addresses each of these aforementioned struggles in very innovative ways.   Simplified Genomics Pipelines Using pre-built pipelines created by industry standards, you can easily pull in data, analyze it, visualize it, and output the results.  Interactive, Scalable AI New enhancements to traditional libraries allow for scalable analysis using the power of Databricks. Currently, distributed versions of Joint Variant Calling, GWAS, PheWAS, eQTL, and machine learning frameworks are available in the unified platform. Databricks reports that these are optimized to run in parallel 60x-100x faster than the open-source (traditional) versions. Plus, because of this speed, this allows you to interactively work with your data rather than having to wait hours or days for something to finish running.  Team Collaboration Since the Databricks notebook environment can be shared with anyone you wish, this means that it's easy to collaborate among your teammates. Plus, you can have notebooks for each part of the pipeline (in varying languages) for each subset of your team. This will all work on a single storage location, which can help you keep your datasets tidy and all in one place.  Resources To view sample genomics notebooks from Databricks, click here. To view genomics pipeline examples from the Azure Databricks documentation, click here. Read the blog post from Dr. Ion Stoica from the Spark Summit here. View BlueGranite's collection of free Azure Databricks resources here.  Want to learn more about how BlueGranite can help with your data and AI needs? Contact us today!"
"124" " In BlueGranite's penultimate showcase of our Microsoft Cognitive Services APIs series, we're surveying various Language services. Working with these APIs will provide your solution with the ability to extract and understand natural language. All About Language The Language capabilities increase your application's ability to \"read\", comprehend, and enrich written text. The following Language services handle common tasks for intelligent applications.   Text Analytics Extract key phrases, measure sentiment, and recognize linked entities  Bing Spell Check Contextual spell check available for multiple languages  Translator Text Detect language and translate text to another language  Language Understanding Build custom natural language understanding models  Content Moderator Moderate language and detect profanity, detect words from custom lists, check for personally identifiable information, flag content for human review To try any of these Language Cognitive Services, get free trial API keys here.  Language APIs for Data Professionals For data professionals, Text Analytics likely represents the most approachable API in this category. At BlueGranite, we regularly take advantage of the text analytics capabilities in Cognitive Services to gauge sentiment and extract key phrases. We even have a webinar recording that showcases these capabilities alongside some other Microsoft technologies. In our last post in this series, we also discussed how to combine the Speech to Text API with the Text Analytics API in the same solution. The Language APIs represent much more than sentiment analysis though, even for data-centric and non-application development situations. You might employ APIs such as Bing Spell Check or Translator Text in storage, transformation, or reporting scenarios. For example, your business might regularly receive customer feedback via email. Bing Spell Check could help enhance the quality of your email text.  While most mispellings misspelings misspellings likely would not impact the writer's intent, it can be useful in some cases to run raw text through Spell Check if you intend to automatically parse and classify it as part of a data pipeline. There could be enough difference in the raw versus corrected text to impact sentiment or influence the extraction of key phrases. Similarly, if customer emails regularly arrive in different languages, the Translator Text API is useful to both identify the source language and obtain a translated copy of the customer email.  With the various Language APIs, developers have the power of natural language understanding and enrichment at their fingertips. By providing the convenience of pre-built AI, the Language APIs from Cognitive Services let you focus on your results rather than worry about training your own models. In many situations, that may be all you need, or it could serve as a pilot to determine if you truly require a data scientist to develop custom models. Text Analytics API The Text Analytics API is a very popular and easy-to-use service. Its endpoints can be built into a simple demo, as shown below, or can be integrated into larger applications or big data solutions using a wide variety of programming languages such as C#, Java, Node.js, PHP, Python and Ruby.  Click on the image to try the Text Analytics API demo  In order to get started using the Text Analytics API, the first step is to acquire an API Key. A free trial, offering a 7-day tryout and up to 5,000 transactions (calls), is available. To use it, you will need to agree to the Microsoft Cognitive Service Terms and sign up using your Microsoft, LinkedIn, Facebook or GitHub account. Otherwise, a Text Analytics API Key is also available via a Microsoft Azure subscription. Once you have signed up for your Text Analytics API Key, you can begin making calls to the service.  To demonstrate how easy it is to get started with the Text Analytics API, I built a demo using Power BI Desktop. In my demo, I am using the Power BI Ideas Title and Text to determine key phrases within the data. I performed key phrase extraction on the top 500 suggestions in the Power BI Ideas forum.  If you are not familiar with Power BI Ideas, it is an open forum where users can make suggestions for improving and adding new features to the Power BI product. Users can also add votes to others' ideas to express their interest in the suggested Power BI enhancements. Microsoft uses forum votes as a guide when adding features to Power BI. To access the Power BI Ideas data for my demo, I used Power Query to import data from the ideas.PowerBI.com web page. Here is the link I used returned the top 500 ideas in JSON format, sorted descending by vote count.  After doing some transformations in Power Query (Converted to Table, Changed Types, Renamed Columns, etc.), my data was ready for the Text Analytics API. Next, I created a custom function that was used to submit the Power BI Ideas text to the Text Analytics API and return the key phrases. I combined the Power BI Ideas title and text columns into a custom column called “title_text” for this purpose. Using this new custom column, I used the Invoke Custom Function feature in Power Query to call out to the Text Analytics API using my function 'fnKeyPhrases' and created a new column containing the key phrases that were returned from the API.  The resulting key phrases were returned to my dataset.  Using this key phrase data, I was able to do analysis in Power BI and quickly see a breakdown of Power BI Ideas by Created Date, Vote Count, Comment Count and Key Phrase.  The Word Cloud custom visual came in handy to show the Key Phrase frequency. This brief Text Analytics API demo highlights how quickly these easy-to-use APIs can be integrated into your existing applications or processes. Stay Tuned for Knowledge APIs In addition to this Language services overview, BlueGranite has also surveyed Microsoft’s Cognitive Services Search, Vision, and (as mentioned earlier) Speech APIs. That only leaves Knowledge APIs, which we'll cover in the near future. If you haven't already, please subscribe to our blog so that you won't miss future posts. Finally, contact us if you would like to learn more about incorporating Cognitive Services into your own solutions."
"125" "Microsoft recently recognized BlueGranite’s expertise among big data, AI, and analytics partners, awarding us its highly sought 2018 MSUS Data & AI Partner Award for Big Data Analytics as well as a Finalist for the global 2018 Big Data Analytics Partner of the Year Award. Microsoft made the announcement in preparation for the annual Microsoft Inspire conference, taking place in Las Vegas July 15-19. BlueGranite is excited that our success with Microsoft Data & AI solution delivery for our clients stood out among the 2,600 partner entries from 115 countries to merit one of only 39 coveted Microsoft award opportunities. We are honored by Microsoft’s recognition of our commitment to innovation and our pledge to provide our clients exceptional data and analytics solutions.  Members of the BlueGranite leadership team in Las Vegas at the 2018 Microsoft Inspire ConferenceFrom left to right: Mike Depoian, Erik Roll, Eric Wozniak, and Matthew Mace Why We Won Microsoft recognized the commitment to quality, innovation and digital transformation BlueGranite brings to every client engagement. We help companies across many industries – life sciences, healthcare, financial services, retail and manufacturing – leverage Microsoft’s revolutionary technologies, to re-imagine their products, services, and operations through solutions such as data lakes and machine learning.  Our team of experts, the backbone of BlueGranite, takes pride in its ability to help at every stage of our clients’ data development. Whether an organization needs to modernize its data warehouse or data science workloads with Azure, or wants to create a roadmap to define its data strategy and assess areas of business impact, our world-class consulting team is up to the challenge. David Willis, Corporate Vice President of Microsoft’s US Partner Group, said: “By winning this award (2018 MSUS Partner Award for Data & AI), BlueGranite has shown leadership in customer impact, solution innovation, deployment and exceptional use of advanced features in Microsoft technologies over the past year. Your work to enable our mutual customers to achieve more shows your dedication to their success and stands out as a model to other US Partners.” Nationwide Achievements We are honored to be recognized among the best of the best in Microsoft’s partner network. BlueGranite’s founder and CEO Matthew Mace credits our team with the tribute for its dedication to continuous learning, innovation and creating a great experience for every client: “It is an honor to be recognized and listed alongside a great number of strong Microsoft partners. We are thrilled by the recognition of our team's work to create value for our clients on Microsoft's Data and AI platform. Innovation, speed, cost savings, and flexibility that inherently come along with the use of the Azure Data Platform have helped our organization transform client enterprises by utilizing data as a strategic asset to drive insights and analytics.” -- Matthew Mace  Want to learn more? Head on over to Microsoft’s Partner of the Year Awards and the MSUS Partner Awards to read more about the great achievements recognized this year. If you are looking to address your data and AI challenges, contact us today! Our experts are ready to assist and determine how we can help you streamline your operations, gain valuable insight, and translate that information into a larger bottom line."
"126" "This post will walk through the first report in BlueGranite’s Power BI showcase – a series designed to highlight key capabilities and tools within the Power BI platform using real and reproducible methods – and introduce the data and features used to create it. The “Employee Retention – Organizational Flight Risk” report displays the results of a real-world, predictive analytics and machine learning scenario within a healthcare organization. Its intended use is for HR to explore flight risk data (high risk being most likely to leave the organization, and low risk being least likely) to uncover patterns at both a high level and a detailed, employee-population level. It tells the user who is leaving, from where, and why. This organization especially wanted to see how its leadership training program for supervisors affected employee retention. Keep in mind that this dataset is solely for demonstration purposes, relating only to this organization, and that all the data within it has been anonymized. The Data The model behind the data was built by taking several different internal and external datasets and running them through an Azure Machine Learning model created by one of our data scientists.  This model was then used to predict the probability of an employee leaving, using the flight risk score assigned to an employee by the machine learning algorithm. If you’d like to learn more about the process of developing a predictive model for employee flight risk, check out this blog from our data scientist, Jacque Carlson. The Report Acquiring and modeling data is only the first step on the path to data discovery and actionable results. For a data model to be a success, end users need a way to examine and draw conclusions from it. This is where Power BI comes in.  Power BI reports present data in a manner conducive to exploration and discovery. This report introduces users to the results of the model, leads them through a deeper analysis of it, and at the end of that process delivers actionable insights.   Note: As you explore the report on your own, I recommend clicking the double arrow in the bottom right corner to view the report in full screen mode:   First Page – Dataset Overview The first page of this report gives an overview of the data but doesn’t go too deep. This page is intended to answer the “who” and “where” questions surrounding flight risk.  As users click around on the first page, they start to notice which jobs have the highest and lowest flight risk scores and how many employees are at risk of leaving. Data is broken down by flight risk level and employee population, and sliced by attributes such as age group, facility, and job category. Not only do the visualizations on this page interact by highlighting and cross filtering each other, but users can click the “High”, “Moderate”, or “Low” buttons under the flight risk header to reveal a configured view of the page for that risk level.   Note: These detailed, risk-level views were created using bookmarks. To return to the original view, simply click the arrow that appears in the Flight Risk header bar. After exploring the first report page, users have some insight as to which employee populations are most at risk of leaving, thus giving them some direction as they delve further into the dataset on the next page. Second Page - Exploration The second page of the report takes analysis a step further. Here, the user begins to understand not only the “who” and “where” of flight risk, but the “why”, as well. Users can explore data either as a scatter chart or a table by utilizing the selections in the bottom right of the page. There are a multitude of slicers for users to pick from as they conduct analysis. At the top, users can make selections to filter down to different parts of the organization. From there, numeric slider bars along the top of the chart can be adjusted to see how factors like wage, patient to staff ratio, paid time off (PTO) used, and commute length affect flight risk.  The scatter chart is great at showing relationships and potential correlation between multiple numeric values. Users can also click the arrows in the upper left corner of the chart to drill up and down between job and job category, and they can select the arrow in the upper right to enable drilling in on a job category bubble, down to its jobs. As users make selections with slicers, they begin to see patterns in the data, helping to identify which factors seem to most affect the probability of flight risk. Among nurses, for example, high flight risk numbers increase when “RN Specialist”, “Registered Nurse II”, and “Registered Nurse III” employees have a higher patient to staff ratio, have used less than half of their PTO, and have recently had a change in supervisor where leadership training was NOT completed:  With so many slicers to make selections from, it can be easy to lose track of what has been selected. To start fresh, users can click the “’Clear All Filters” button in the lower right corner of the page to clear all of their selections. To view the data in a table rather than a scatter chart, users can click the “Explore as Table” button in the bottom right corner.  Having a table view is great for users who want to do “show me the numbers”-type analysis. This table has been conditionally formatted to show correlation, too. Values for factors that are assumed to be generally positive, meaning one would expect them to result in a lower flight risk score, are colored in a deep shade of green. For example, cells indicating higher wages, longer tenure, lower patient to staff ratios, more PTO used, and shorter commutes are the deepest green. The table is also sorted so that the highest flight risk populations are at the top, letting users easily identify who is likely to leave, why, and from where.  This type of examination leads users to actionable insight, but analysis doesn’t have to stop here. In many cases, it’s worth delving into data at the employee level. This report is conducive to that kind of analysis, too. Drillthrough Pages  There are two drillthrough pages built in, where users can explore either a certain job category or facility more closely. To get to either of these pages, users must right-click on a visualization where either the facility or job category fields are used, select “Drillthrough” and then “Dig Deeper into this Facility” or “Dig Deeper into this Job Category”:   Both drillthrough pages are set up similarly, only the data behind them is different (to reflect either a facility or job category):  These pages still help to answer “who”, “where”, and “why”, and they also give detail down to the employee level. At this level, users can see which factors are affecting a specific employee. Drillthrough is a great way to give users access to a finer grain of detail, without sacrificing the high-level overview and analytic functionality on previous pages.  On this page, DAX – or Data Analysis Expression language – was used to create a dynamic title that reflects the user’s drillthrough selection.  This gives context to the page, so users always know what they’re looking at. Additionally, the drillthrough pages are hidden to ensure that users use the back button in its upper left-hand corner when navigating back to the original page (this clears the drillthrough value and resets the page). The donut chart on this page is intended to convey some information – the distribution of flight risk levels within this specific facility or job category – but it also allows the user to filter the rest of the page by risk level. After exploring this report and all its pages and bookmarks, HR users within this organization will better understand flight risk and its causes. They should also have an idea of what actions they can take to retain employees and reduce high and moderate flight risk numbers at both the population and individual levels. Why Power BI? Data is best when shared. Even if a developer creates the most detailed, impressive predictive or analytical data model, if end users cannot view, interact with, and draw insight from that model, it fails to serve its purpose. Power BI facilitates sharing data with end users, with powerful visual representation and interaction, giving a data model the chance to accomplish what it was intended to do – share valuable insights with end users. Power BI has a multitude of features to create reports that lead end users through an immersive, app-like exploration of data, making it easy to delve into data, digest and understand it, and take data-driven action. Want to check out the report for yourself? You can explore the live version here! Whether you are interested in deploying Power BI interactive analytics within your organization, or you want to further explore its capabilities, please contact BlueGranite. We would be happy to share our knowledge and experience. "
"127" "BlueGranite is built on shared commitment to clients, innovation and teamwork, but it’s the skills of our dedicated team that really shine. Our Meet our Team blog series highlights some of our many talented staffers across the country, all of whom drive our success. Today we feature Florida-based Gary Lock, an advanced analytics veteran, whose proficiencies span the full Microsoft Business Intelligence (BI) stack.  From Finance to ERP Expertise The University of South Florida grad studied finance and accounting, earning a Bachelor of Science degree in Business Administration in 2001. Gary then continued his studies, diving into management information systems (MIS). He secured his Master of Business Administration degree in 2004, with MIS, advanced MIS and management concentrations. He’s since spent more than a decade immersing himself in enterprise resource planning (ERP) and customer relationship management (CRM) systems, mastering all aspects of Microsoft BI – from SQL Server and its add-on tools and components to Azure SQL, Power BI, and more. The Microsoft-certified Big Data professional said he discovered his passion for emerging tech in college, where he enjoyed working with data, uncovering insight and automating processes. Broad Experiences Share Commonality On the road to joining BlueGranite, Gary mastered many roles, including an early position as a one-man IS team while working for a Midwestern pizza manufacturer. He succeeded as the company’s sole information systems resource covering database administration, business intelligence (BI), report development, and ERP system management including financials, inventory, sales, supply chain and materials resource planning. Despite the challenge of those multiple roles, Gary says he really enjoyed digging in to such a big system (Microsoft Dynamics NAV), applying BI to it, developing analytics for it, and seeing the resulting end-to-end visibility and efficiencies. He’s also relished past roles with startups, the energy sector (nuclear power and utilities), and software manufacturers. For Gary, the common denominator across those wide-ranging industries is his relish for problem solving and the satisfaction he gets from positive end results – “gaining efficiencies, reducing costs and cutting waste.” Embracing Challenge Enhances Expertise Gary said one of the keys to his success in the ever-evolving analytics industry is surrounding himself with experts in the field. He said he likes to push himself by “working with the best of the best.” To that end, he said, he joined the BlueGranite team to “progress his career and take on new challenges.” But what really makes the job, Gary said, are the shared goals of BlueGranite’s team members and its management: to deliver a stellar client experience while prioritizing teamwork, integrity, innovation and, of course, fun. As for that last goal – fun – for Gary it means staying active and involved. He said that living in Tampa, Fla. gives him ample opportunity to do both. He enjoys outdoor activities year-round, including paddle boarding and fishing, and he also recently presented at SQLSaturday South Florida. He said he’s looking forward to future participation with the group. Like Gary, the BlueGranite team thrives on challenge. We help people like you get the answers they need from data, and we get it right the first time around. Contact us today to discover how we can help with every stage of your data culture."
"128" "In this next installment in BlueGranite’s series on Microsoft’s Cognitive Services, the Speech APIs will be considered. For app and website developers, these Speech APIs provide natural language processing capabilities that add functionality and value to the customer experience. In addition to an enhanced customer experience, the Speech APIs can be used by organizations to improve business practices and/or increase customer security. To improve business performance, an organization may use Speech and Language APIs to transcribe call-center recordings to develop deeper understanding of product performance and customers’ concerns. To provide increased customer security, an organization may use the Speaker Recognition API to add a second layer of security by verifying customers’ identities via voice recognition.  All About Speech First an overview of the Speech APIs – these pre-trained AI Speech models can hear and speak to your customers with personal and convenient voice-based interactions.  Speech to Text  Transcribes spoken audio to text with standard or custom models. A custom model can be trained for specific vocabulary or unique speaking styles.   Text to Speech  Bring voice to any app by converting text to audio in near real-time with the choice of over 75 default voices.    Speaker Recognition  Voice verification and speaker identification can identify who is speaking; providing increased security in authentication experiences for customers.    Speech Translation  Provides speech-to-speech or speech-to-text translation in 10 different languages.  For more information, see Microsoft’s Cognitive Services Speech Directory. Develop Business Insight with Text Analytics using Speech to Text and Text Analytics APIs Organizations with call centers can use the Custom Speech to Text API to transcribe call center recordings that then could be explored with the Language Text Analytics API. Text analysis of call center interactions could lead to answers for questions such as ‘what are our top three product-related issues’ or ‘what issues are of most concern to our customers’? While analysis of call center recordings is not a new data analytic practice, use of Microsoft’s Cognitive Services pre-trained AI models can make the development of the data analytic pipeline faster and more robust. Speech to Text API Microsoft’s Speech to Text API is the powerful speech recognition technology used by Cortana and several other Microsoft products. The Custom Speech to Text API allows an organization to build on this technology by training the model to accurately ingest terminology that is unique to the organization’s business practices; for example, distinct sounding terms for products or product functionality. The steps necessary to train the Custom Speech to Text API may be repeated until the desired level of accuracy is reached. Once this has been achieved, the organization’s call center recordings can be transcribed and readied for text analysis.  Click on the image to try the demo with your own recording. Text Analytics API With accurately transcribed call center recordings, Microsoft’s Language Cognitive Services Text Analytics API provides in-depth text analysis. Text analytics is an umbrella term that can encompass a wide range of practices for analyzing transcribed speech. Practices can include the identification of themes, entities, and sentiment. Themes represent the general ‘gist’ within the recording; frequently occurring patterns found within the communication can be identified. Identification of entities within the text is the primary reason for use of the Custom Speech to Text API – that is, an organization may now learn what specific products or services are being discussed in the call center recordings. Finally, there is sentiment analysis, sometimes referred to as Emotion AI; this analysis identifies the positive and negative language used within the interaction.  Click on the image to try the demo with your own transcription. In combination, these services provide businesses with several options for the development of a highly customized and flexible data analytic pipeline for the analysis of their call center recordings. Enhance Customer Authentication Processes with the Speaker Verification API Identity and data privacy threats are as rampant as ever and have revealed that one-factor authentication processes (e.g. basic username and password usage) are quite vulnerable to theft. Two-factor authentication (2FA) allows customers and organizations to increase the security of their data and identities. One form of a two-factor authentication process is the biometric authentication known as voice recognition. The Speaker Verification API can be used to create a voice recognition system that will recognize the customers’ voices as a means by which to identify them. Such a two-factor authentication can take place during a routine call center interaction. Voice recognition can be accomplished by modeling a customer’s unique voiceprint using the Speaker Verification API. Once a voiceprint of a customer has been created, it can be saved for use for whenever the customer calls again, wherein a call center system can compare the current voice to the voiceprint on file. There are several options for two-factor authentication processes, however a voiceprint authentication has the distinct advantage in that a customer does not need to know or provide additional information (e.g. confirmation codes sent via text or answers to secret questions.) The Speaker Recognition API can create the voiceprint files for authentication and security processes and is robust enough to operate in quite complex acoustic environments. Speaker Recognition API  Click on the image to try the demo with your own recordings. Currently, Microsoft’s Cognitive Services has more than 20 pre-built APIs; many of those allow for customization. Individually or in combination, these APIs enable software developers and data scientists to implement powerful AI solutions that can significantly transform and improve business processes. More to Come BlueGranite has surveyed several Microsoft Cognitive Service APIs, including search and vision. We will continue to explore the remaining Cognitive Services categories: knowledge and language. Subscribe to our blog so that you don't miss out and contact us if you would like to learn more about incorporating Cognitive Services into your organization’s business practices."
"129" "In the Business Intelligence community, it’s easy to get caught up in data. Numbers form the basis of everything we do. We want to count, add, estimate. We pull data from different sources and coalesce that into one version of the truth. We want to acquire and generate data. Really, that’s only the first step of BI. The acquiring, modeling, and calculation of data cannot stand on its own. The heart of BI is turning data into information. It’s not information if we can’t use it. The core of that transformation is turning that heap of data into something that humans can process. And humans can learn a lot by looking at a picture.  Take, for example, addresses. I had a problem recently where I needed to compare a handful of addresses to a much larger population of addresses. The test addresses had to be ranked based on their proximity to the control addresses. I extracted both the test addresses and the control addresses and mapped the results. Being ad hoc, it didn’t have any measures. It didn’t calculate any distances. It was just mapping a series of addresses and color coding them. Just looking at the map was all I needed. Compare test address A:  To test address B: Just by looking at test A, the former, you can tell that it had much greater proximity to a cluster of control addresses than the latter. Those pictures are far more insightful than the list of addresses I started off with. Just from plotting the addresses on the map, I can see that address A is in a subdivision near a highway, and the subdivision has lots of control addresses. In comparison, address B looks more rural and only has a few control addresses. It’s something you can pick up intuitively in a few moments. Compare that to looking at a table:  It would be more difficult to gain the same insight without visualizing it on a map. In fact, it would be quite a bit more difficult. Internally, the tool I used looked up geospatial coordinates and filled in bubbles against a map at the proper locations. So, in attempting to arrive at a similar conclusion, I’d have to grab those geospatial coordinates. Then, I’d have to calculate the number of control addresses that fall within a given distance from the test addresses. You could then calculate that test address A is a higher rank than test address B. That calculation only addresses the primary insight. To calculate other conclusions that you can make from the map, you need quite a bit more data. In fact, you might run out of resources before you could accurately calculate the other conclusions. Let’s look at another example. This time, I won’t give any context, just the picture.  What can you tell just by looking at the picture? Let’s break it down. First, we’re looking at how a percentage measure changes over a two-year timespan. The first 3 months of 2015 had a much higher percentage than any other quarter, but it was followed by numbers dropping like a rock with a giant dip in the summer months. It seems to have evened out a bit for a much more stable 2016. Again, this analysis becomes much more visceral when looking at a graph rather than a bunch of numbers. However, the graph also shows a little bit of the flip side of visualization. Look at the scale on the left. The scale is showing percentages, but it starts at 35% and only goes to 90%. Depending on what I’m intending to show, that could be misleading. By truncating the scale, it emphasizes the relative differences between month-to-month values, rather than their absolute values. A 0% to 100% scale would instead emphasize their absolute values. Which approach is “right” depends on what you’re intending to communicate to the audience about the data. In short, a chart like this isn’t useful in the real world without any context. Realistically, this graph in isolation is useless. It could be a failure rate (lower is better) or it could be an office occupancy rate (higher is better). Let’s look at a revised chart, then. Here, the chart itself gives the context. It calls out the measure in the title and clearly distinguishes our key performance indicators for Occupancy. We see that one month had an exceptionally low occupancy rate and that several months were in the acceptable but below target rates. We also revised the scale on the left to begin at 0%. July 2015 was clearly a low point, but it wasn’t quite as low as you might be led to believe based on the first chart. Depending on what we want to communicate, we might add callouts to draw our attention to certain points. It depends on the story we want to tell with the data. Okay, so we’ve looked at a couple of examples. What can we take away from this? Most importantly, good visuals can help us intuitively understand data they represent. However, we also learned that it takes effort to design a good visual. It is not enough to display “the data” – the context of the data can be just as important. And, unfortunately, we learned that you can display the data in ways that manipulate perception of the data. A picture can be worth a thousand numbers. Why is that? A well-designed visual will tell a story of the data in a single picture. If you need some help with visualizing your data, be sure to contact BlueGranite for help."
"130" "It’s that time again. It’s been two years and you’re starting to plan the dreaded Data Warehouse upgrade to SQL Server version next. You’re wondering if you can recycle that old project plan from two years ago, but your data volume has grown and the performance of your ETL isn’t what it used to be. You know you need to do some analysis of your hardware first to see if you need more horsepower. You’ve also been keeping up with the latest new features and know there are some you’re wanting to take advantage of. You think, how in the world are we going to migrate and verify the 200 SSIS packages that we’re up to now? All of this on top of an already demanding meeting schedule, new user requirements, and support requests. The stress and anxiety are starting to kick in. Do you ever catch yourself: Wishing you could magically upgrade your SQL Server every two years by snapping your fingers? Wishing you didn’t have to think about hardware, ever? Wondering if your hardware is causing your performance problems but don’t have the time, bandwidth, or access to prove it? Dreading the weekend maintenance outages where IT is patching the Windows OS or installing a SQL Server Service Pack or Cumulative Update? Daydreaming about the latest SQL Server feature, knowing you could benefit from it, but then sighing, because you know it will be more than year before you see it? Analyzing your SQL Server options in the cloud and becoming disappointed with what you find? Load your plutonium and turn the flux capacitor and time circuits on, because you’re heading to the future. On March 7th of this year, Microsoft announced the public preview of Azure SQL Database Managed Instance (Managed Instance for short). Managed Instance is a new Azure SQL Database offering that is managed at the SQL Server Instance level in the Azure Platform as a Service cloud. Unlike Azure SQL Databases that consist of a single SQL Server database or Elastic Pool, Managed Instance comes as an entire instance of SQL Server just like on-premise SQL Server 2017. Unlike other Azure SQL Server products that might require some redesign of your solution, migration to Managed Instance is easy because it is almost 100% like SQL Server on-premise. With this service, you get: Infrastructure and OS managed by Azure Database patching and upgrades managed by Azure Ability to scale up and down to meet performance expectations The latest version of SQL Server in perpetuity SQL Server Relational Database features like: Integrated security Common Language Runtime (CLR) Global Temp Tables Cross-database queries Linked servers Service Broker/Query Notifications Native BACKUP/RESTORE statements Database mail SQL Agent The latest security capabilities And a multitude of other supported features  High Availability and Disaster Recovery are built in and managed by Azure Some minor T-SQL differences exist, but they aren’t show stoppers Performance Architecture Selecting an architecture with the required performance is straightforward with Managed Instance. The architecture has four dials you can configure to obtain your optimal solution and pricing. General Purpose or Business Critical: do you want budget-oriented, balanced architecture (General Purpose), or do you have high IO requirements coupled with more complex HA/DR requirements (Business Critical)? During the Public Preview phase, Business Critical is still privately reserved. Pick your Processor Type: Generation 4 uses Intel E5-2673 v3 (Haswell) 2.4 GHz processors, attached SSDs, and a vCore that is equal to 1 physical core. Generation 5 uses Intel E5-2673 v4 (Broadwell) 2.3 GHz processors, fast NVMe SSDs, and a vCore that is equal to 1 logical processor. Number of vCores: on Generation 4 processors, you can select 8, 16, or 24 vCores. On Generation 5 processors, you can select 8, 16, 24, 32, or 40 vCores. Generation 4 processors come with 7GB of RAM per vCore, and Generation 5 processors come with 5.5GB of RAM per vCore. Storage Size: picking your storage size is as easy as moving a slider. Managed Instance is currently limited to 8TB of storage while in Public Preview. A more thorough explanation of the architecture and vCore selection options can be found here. Security and Networking Managed Instance is designed with security and networking in mind. Managed Instance is deployed specifically for use through Azure Virtual Networks (VNET) and connected to on-premise via Azure Express Route or Azure VPN Gateways. Connections to Managed Instance come through a safe and private IP address on the VNET. Managed Instance also provides a single tenant experience with dedicated resources for compute and storage, combined with all the latest and greatest security enhancements with the latest version of SQL Server. More information on security and networking for Managed Instance can be found here. Migration Migration to Managed Instance is easy. Because Managed Instance is so much like SQL Server, there is no need to rewrite your code. Additionally, Azure makes it easy to migrate, providing three different options. Azure Database Migration Service – a fully managed service for migrating on-premise SQL Server databases (or Azure VMs) to Azure using a GUI/wizard-like experience and providing zero downtime. Backup to Azure Blob and Restore – backup your current SQL Server databases to Azure Blob storage. Restore to Managed Instance by restoring your backups from Blob storage. Using BACPAC file – provides the capability to create the entire database and data into a single deployable file. More information can be found on migration here. Closing Managed Instance provides all the great features and capabilities of on-premise SQL Server without the hassle of managing infrastructure, configurations, or patching the OS and SQL Server. It also allows you to skip the bi-annual SQL Server upgrade because you’ll always be on the latest version and have access to the latest SQL Server features. For more information about Managed Instance, contact BlueGranite today. Warning: please know that you might get sad or reminisce about past SQL Server migration projects – said no one ever."
"131" "Managing your Report Server via PowerShell We have written several blog posts that explain what Power BI Report Server (PBIRS) is, how to acquire it, how it can modernize your on-premises BI solution, and how it supplements Power BI Premium implementations as a component of a hybrid BI solution. What we haven’t discussed are strategies around managing and deploying various PBIRS assets. PBIRS can be thought of as a superset of SQL Server Reporting Services (SSRS). Therefore, traditional SSRS, Mobile, Key Performance Indicators (KPIs), and Excel-based reports can all be hosted alongside your Power BI reports. With this post, we will describe a large-scale PBIRS implementation we have been working on for a client, as well as some useful DevOps strategies we’ve implemented along the way.   Background We began collaborating with a client on a brand-new data warehouse last year. BlueGranite partnered with the company to implement everything from overall architecture and design, ETL, and semantic modeling, to development of reports and scorecards. Reporting assets included Power BI, traditional paginated (SSRS), and Excel-based reports. Once we empowered the company’s key stakeholders with self-service business intelligence they grew excited about Power BI development, and began to create their own Power BI reports against the SSAS tabular models we developed. Due to the complexity of underlying Data Analysis Expressions (DAX) queries, BlueGranite continues supporting our client with ongoing SSRS report development. Today, we maintain three reporting environments (Development, UAT, and Production) with 50+ SSRS reports we developed, along with 50+ Power BI reports developed largely by business users. You can probably imagine, from a DevOps standpoint, the potential for headache. NOTE: We are not using Mobile Reports and KPIs as part of our solution. Traditional SSRS development leverages SQL Server Data Tools (SSDT) as a development environment to maintain an overall reporting solution, as well as provide a simple deployment interface for SSRS projects.  However, Power BI and Excel-based reports aren’t natively developed and maintained within SSDT. Each type of report has its own client development environment (Power BI Desktop and Excel, respectively) and is maintained as an individual object, as opposed to SSRS reports that are maintained as part of a larger project. From a deployment standpoint, this means they must be manually uploaded/downloaded from one PBIRS instance to another, one report at a time. This is on top of manually maintaining the connection strings for each environment! Luckily, there is an open-source PowerShell module called ReportingServicesTools to help us manage our report servers and deployments. ReportingServicesTools PowerShell Module The ReportingServicesTools module was originally developed by the Microsoft Reporting Services Team to enable users to perform various tasks against the SSRS report server within PowerShell. Now the module is open source and available through GitHub. This module is constantly being enhanced by the community to provide more functionality. The list of commands is available on GitHub here. NOTE: Aaron Nelson has written extensively on this topic. His session on the module at last year’s PASS Summit got us excited to try it out on our client initiative. Thanks, Aaron! Setup You can download the module from the PowerShell Gallery with this link, OR you can use the Install.ps1 script from GitHub site above.  Once you have the module downloaded, you must also have at least PowerShell 3.0 installed (but PowerShell 5+ is highly recommended) before you can install the ReportingServicesTools module. If you always want to get latest module when you are running your deployment script, you can check for updates each time using this code snippet: #Installs SSRS module (need to run as admin to do this)Install-Module -Name ReportingServicesTools; #Update SSRS moduleUpdate-Module -Name ReportingServicesTools; #Import SSRS moduleImport-Module -Name ReportingServicesTools; USE CASE: Exporting Let’s assume both BI Developers and business power users are developing and modifying reports against your Development PBIRS server. Often, we’ll want to integrate these changes into our source control repository. To export your reports from your PBIRS development environment, first connect to your PBIRS server and then iterate through all the folders/reports. Next, create an exact copy in your source control environment using the code snippet below.  Please replace localhost with the name of your report server and update the LocalPath variable with your local source control folder (in our case, we used Git). #Create new web session to your Power BI Report portal$ReportPortalUri = 'http://localhost/reports'; #Replace with your report server URL$session = New-RsRestSession -ReportPortalUri $ReportPortalUri;$LocalPath =’C:\Git\SSRS'; #Specify local folder to download assets to #Get Power BI and Excel reports from Power BI Report Server#NOTE: Since we use SSDT for managing SSRS reports, we only wanted to download Power BI & Excel reporting assets$reports = Get-RsCatalogItems -ReportServerUri $ReportPortalUri -RsFolder '/' -Recurse |Where {$_.TypeName -eq 'PowerBIReport' -Or $_.TypeName -eq 'ExcelWorkbook'}; #Export each report that was selectedforeach ($report in $reports){  $reportFolder = (Split-Path $report.Path).Substring(1)  $exportPath = $LocalPath + \"\\" + $reportFolder;   #Create folder if it does not exist  if (-Not (Test-Path $exportPath))  {    New-Item $exportPath -ItemType Directory | Out-Null;  }   #Export the report  Out-RsRestCatalogItem -Destination $exportPath -RsItem $report.Path -Overwrite -        WebSession $session;   Write-Host \"Report exported: $($report.Name)\";} USE CASE: Deploying Source control integration is only half of the battle. Often, we’ll also need to deploy these new reports and/or enhancements to a higher environment (i.e. UAT or Production). Now that your source control environment matches your Development PBIRS server, you can deploy these reports to a different PBIRS instance, such as one used for UAT or Production. #Again, we use SSDT for deploying SSRS reports. Therefore, we're only using PowerShell for Power BI & Excel reports#That said, there is nothing preventing you from deploying all report types via PowerShell. Simply remove the \"Include\" parameter below. $reports = Get-ChildItem -Path $LocalPath -Recurse -Include \"*.pbix\",\"*.xlsx\"; #Loop through selected reportsForEach ($report in $reports){   #Get the report folder from source control path   $reportFolder = Split-Path -Path \"$($report.Directory)\" -Leaf    #Deploy the report to the server   Write-RsRestCatalogItem -Path $report.FullName -RsFolder \"/$($reportFolder)\" -      Overwrite -WebSession $session;}  Enhancing Your Scripts There are many ways to enhance these scripts beyond the basic exporting and deployment of Power BI and Excel-based reports. For example, we could have just as easily modified the scripts above to include ALL report types compatible with PBIRS, not just Power BI and Excel. This would allow for a complete deployment via PowerShell, without the need to deploy SSRS reports separately via SSDT. Grid View & Manual Selections Parameterizing your script to display a grid view enables the user to select items at run-time, instead of having to update the script prior to running. This is great for items like environment selection and individual, or multiple, report selection for exporting/deploying. The example below displays the reports available on the server, so the user can select which ones they would like to export. The key items below are the Out-GridView cmdlet, along with the -PassThru parameter. $reports = Get-RsCatalogItems -ReportServerUri $ReportPortalUri -RsFolder '/' -Recurse |    Where {$_.TypeName -ne 'Folder'} |    Select @{Name=\"Folder\"; Expression={(Split-Path $_.Path).Substring(1)}}, @{Name=\"Name\"; Expression={$_.Name}}, @{Name=\"Report Type\"; Expression={$_.TypeName}} |    Out-GridView -PassThru -Title 'Select reports to export';   Figure 1 - Our PBIRS Folder contains five reports. Three Power BI, one SSRS, and one Excel report.      Figure 2 - The same five reports are shown via a PowerShell grid. We can select one, some, or all, reports to include in our deployment.  Modifying Connection Strings Another accelerator we found necessary was to use PowerShell to modify the Power BI and Excel report connection strings prior to deployment. This removes the tedious process of updating each connection string on reports that were deployed to a different environment than they were created in. This is very useful since Power BI and Excel report connections are embedded and do not have the ability to use Shared Data Sources at this time. Open the Power BI or Excel file as though it were a zip file in PowerShell and modify the connection string inside the connections file. NOTE: This is not a Microsoft supported solution, but sometimes you must get creative when managing hundreds of reports. Tidying up Source Control You can also include logic in your script to clean up your source control repository to match what exists on your PBIRS instance. For example, you can traverse through your reporting structure in source control and compare it to what is currently in your PBIRS instance. If any reports were removed on the PBIRS instance, these reports can be outputted into a grid view to allow the developer to review if they should also be removed from source control. Have questions about Power BI Report Server or just want to learn more? Contact us! We would be happy to share more examples of how you can also take advantage of this great technology."
"132" "From June 4th - 6th, 2018, Mike Cornell, Jared Zagelbaum, Matt Mace, and I attended the Spark+AI Summit 2018. This event offered a day of training sessions on June 4th and then two days of conference activities on the 5th and 6th. Hosted in beautiful San Francisco, the conference was packed with technical deep dives and demos from experts around the country. Couldn't attend? Don't worry, I've got you covered!   About Databricks If you frequent the BlueGranite blog, you're probably very familiar with Azure Databricks, the hottest new data and analytics platform from Microsoft. However, did you know that Databricks is actually a company founded by some of the original creators of Spark? Databricks’ founders started the Spark research project at UC Berkeley, which later became Apache Spark™. They've been working for the past 10 years on cutting-edge systems to extract value from Big Data.  In addition to making proprietary enhancements to the Spark system to create their own Unified Analytics Platform, the company has a commitment to open-source development and effective training. Databricks is also the host and main sponsor of the Spark+AI Summit. What We Learned Mike, Jared, and I attended a variety of sessions, from technical deep dives to Python demonstrations to talks on deep learning techniques. Personally, I enjoyed the sessions around research, which described the work that's being done (in and out of academia) in the world of Spark and distributed computing. I loved seeing what everyone else has accomplished. It certainly gave us some cool project ideas.  All of the sessions were recorded and are available here. Each day began with a series of keynotes who provided great case studies and use cases from companies such as Regeneron and Bechtel, as well as universities and research institutions. These keynotes provided great demonstrations of state-of-the-art, game-changing solutions that have been implemented in their respective industries.  BIG Announcements Databricks Delta Databricks Delta is a new data management tool that provides a single interface for combining the scale of a data lake, the reliability and performance of a data warehouse, and the low latency of streaming together in a single system. Using Delta along with the rest of the Databricks Unified Analytics Platform makes it much easier to build, manage, and put Big Data applications into production. A big talking point at the keynote for this technology was around data quality. Delta helps to make sure the data flowing through your workflows remains consistent. Plus, Delta will give you insights around changes in your data. In addition to this, Delta helps to capture and track some summary metrics around your data, as well as relevant metadata. Read more about Databricks Delta from the blog, here. MLflow As more and more deep learning or machine learning frameworks are created, the use of state-of-the-art algorithms, which are extremely desirable, becomes difficult from an integration/compatibility front. Many data science teams struggle with using these myriad tools together, tracking experiments, reproducing results, and deploying models. Databricks has announced a new open-source solution to help combat these issues.  MLflow is designed to work with any ML library, algorithm, deployment tool or language. It’s built around REST APIs and simple data formats (e.g., a model can be viewed as a lambda function) that can be used from a variety of tools, instead of only providing a small set of built-in functionality. Check out MLflow.org to get started. Databricks Runtime for Machine Learning Databricks also announced a new runtime for machine learning. This includes ready-to-use machine learning frameworks, simplified distributed training, and GPU support. For many deep learning toolkits (Microsoft Cognitive Toolkit, TensorFlow, XGBoost, etc.) or machine learning packages (scikit-learn, etc.), the environment configuration can be a tricky process to getting these libraries working optimally. With the new Databricks Runtime for Machine Learning, you will now have access to pre-configured machine learning frameworks that have been optimized for Spark.  In addition to the frameworks, the folks at Databricks have simplified the task of training models in a distributed fashion using the Horovod framework. This framework facilitates distributed training across multiple GPUs. With Databricks, you will now be able to unleash the power of GPUs in Spark! Unified Analytics Platform for Genomics For those of you who work in the fields of healthcare or life sciences, you already know the pains of dealing with huge files with crazy file types and using command line-based tools like SnpEff, BWA, or GATK4. Traditionally, these tools only work on MPI-based computing clusters. Now, resulting from a dedicated effort from Databricks, they have ported over many of the common bioinformatics pipeline functions into Spark. Plus, since Databricks is cloud-based, genomics data pipelines and analyses are now fully scalable.  Stay tuned for an upcoming blog where I'll take an in-depth look at the new genomics-based features coming to Databricks.  All the Swag... As with any conference, the Expos are quite interesting. Some booths were giving away fidget spinners, shirts, or stuffed animals. Others were giving away hot sauce (yes, seriously) or raffling off drones and books. In my opinion the best swag is, of course, stickers!  If you have any questions or want to learn more about BlueGranite's AI capabilities, contact us, or check out BlueGranite's free collection of Azure Databricks resources here. "
"133" "As adoption of Power BI grows, more and more organizations are finding value in purchasing and deploying Power BI Premium. Power BI Premium offers a different licensing structure where an organization can purchase dedicated capacity and users who only consume reports and dashboards aren’t required to have a Power BI Pro license. The move from shared to dedicated capacity brings with it: larger storage limits support for larger data sets higher refresh rates support for incremental refresh support for Power BI Embedded use of Power BI Report Server Power BI Premium requires some additional deployment planning over simply using shared capacity in PowerBI.com. Listed below are important questions to answer as you get started with Power BI Premium.   1. How will we allocate Power BI Premium capacity?  When you purchase Power BI Premium, you are purchasing an amount of capacity (virtual cores) rather than a number of nodes. If you purchased the P2 plan, you would have 16 v-cores. You could have one node with all of the v-cores, or you could have 2 nodes with 8 v-cores each. You’ll want to take into consideration memory and CPU usage based upon dataset sizes and refresh rates, as well as the need for autonomy (dedicated capacity) of departments or groups within your organization. 2. Who will manage each capacity?  Power BI Premium brings new management roles in PowerBI.com. In addition to the Office 365 admin and Power BI admin roles, there are capacity admins and capacity assignment permissions. Capacity admins can assign workspaces to their capacity and update the capacity assignment list.  Users with capacity assignment permissions can add any workspace in which they are an admin to dedicated capacity. It can be helpful to allow some groups to manage that for themselves rather than to have to submit a help ticket to IT or the BI team when they want to move workspaces in or out of dedicated capacity. If you have multiple groups on one capacity, it might be best to have a capacity admin from each of those groups. But those capacity admins will need to communicate so they don’t overcommit the capacity and cause poor performance for the other groups. 3. How will we assign workspaces to each capacity?  Not every workspace needs to be in dedicated capacity. As the number of workspaces grows, all of your workspaces may not fit in dedicated capacity. You could start out putting every workspace in dedicated capacity until it fills up and then re-evaluate, or you could come up with guidelines as to which workspaces should be assigned to a capacity. Some organizations leave all personal workspaces in shared capacity and put all app workspaces in dedicated capacity. Some organizations assign workspaces to dedicated capacity to maximize their use of Power BI Free licenses for users who only consume content. Other organizations identify important or frequently used workspaces and assign only those to dedicated capacity. 4. Should we allow users to sign up for Power BI Free licenses on their own, or block them from doing so?  Often, the deployment of Power BI Premium brings new users to your Power BI tenant. By default, any user can sign up with their Azure Active Directory credentials. Some organizations turn off that ability and require the O365 admin to assign a license to users before they can use PowerBI.com. This might be a good fit if your organization requires training on Power BI or data security before users may begin to use Power BI. Power BI Premium can bring interactive analytics to a larger audience within your organization, but it takes some additional planning and management. If you would like help planning your Power BI Premium deployment, please contact us. We would be happy to share our knowledge and experience.  "
"134" "Continuing BlueGranite's series on the Microsoft Cognitive Services APIs, let's look into Vision. Using the pre-built artificial intelligence from the Vision APIs will give your apps and other solutions the rich capabilities of different types of image and video analysis.  All About Vision The Vision capabilities use AI to automatically provide detailed explanations of what your images and videos contain. The following Vision services cover many of the common tasks that you might use to enrich your applications.   Computer Vision API Analyze image content, obtain tags and categories, identify text using optical character recognition, flag racy or adult content, crop photos as thumbnails, and more  Face API Locate and group faces in images as well as suggest gender, age, and emotions (previously a separate Emotion API)  Custom Vision Service Train and use your own custom image classifiers  Video Indexer Analyze video content to track faces, gauge sentiment, transcribe audio, recognize different speakers, and more  Content Moderator Detect and potentially filter offensive content from images and video, and flag content for human review To try any of these Vision Cognitive Services, get free trial API keys here.  Custom Vision Service Let's peer into some of the functionality for one of Microsoft's Vision offerings: the Custom Vision Service. Among the Vision APIs, the Custom Vision Service is unique because it enables developers to easily train their own image classification model. In addition to the abundant data provided by pre-trained AI from the Computer Vision and Face APIs, the Custom Vision Service lets you go a step further and tag your own images. Once you have trained a model and it's ready for production, create a personalized API endpoint. Ultimately, send additional images to the API to classify them and predict how closely they align with your custom labels. Why would you consider using the Custom Vision Service and not the regular Computer Vision API? Custom Vision is beneficial in cases where you need precise or proprietary labels for your data. For example, a manufacturer may want to label product lines using their own enterprise terminology, which the Computer Vision API is not equipped to do. In fact, consider using both APIs together. Here is a look at some of the output from the Computer Vision API and how you can enhance that output using the Custom Vision Service. Use the Computer Vision API for General Results  Use the Custom Vision Service for Personalized Tagging  Through its website, the Custom Vision Service lets you build an image classifier using a process similar to tagging images on social media. Even if you are not a data scientist, you could take advantage of Microsoft’s AI without the time and resources needed to code and productionalize a custom image model from scratch.  With a language such as Python, you can use code to perform the same tasks as the customvision.ai  website. Code gives you the advantage of greater scalability, faster changes, and the ability to reference image URLs instead of manually uploading images.    Getting Started with the Custom Vision Service To get started, go to customvision.ai and sign in with a Microsoft account associated with an Azure subscription. The first time you sign in, you need to agree to Microsoft’s Terms of Service. When adding a new project, choose between Classification and Object Detection. Classification predicts labels for an entire image while Object Detection details where tagged content appears in an image. With a generic classification project, you may optionally select targeted domains for scenarios like Food or Retail.  To train a model, you currently need at least 15 images for every tag, but precision should increase with additional images. Microsoft recommends at least 50 images per tag. With a standard project, you can include up to 250 tags and up to 50,000 training images. It also helps to include images that contain your objects of interest in a variety of settings and backgrounds. The Custom Vision Service also provides precision and recall by tag as basic measures of model performance. To use your model in production, you need a subscription key. As with the other Cognitive Services, incorporate your Custom Vision model into a solution with a basic API call.  As you can see, the Vision APIs are convenient for a variety of tasks. Whether you are a developer or data scientist, these APIs bring you advanced AI capabilities without the cost and time of training your own image models. You can always decide if you need more at a future point, but the Vision APIs provide a jump start into an area where it is time- and resource-intensive to build these types of models on your own. Also, consider looking further into all of the Cognitive Services here, and combine Vision with other sets of APIs like Language or Speech to make your apps even more intelligent. More to Come So far, BlueGranite has surveyed the Search and Vision APIs. In addition, we will explore the remaining Cognitive Services categories: Speech, Knowledge, and Language. Subscribe to our blog so that you don't miss out, and contact us if you would like to learn more about incorporating Cognitive Services into your own solutions."
"135" "What is data science? At its heart, it’s the ability to extract insight from data. Successful practitioners know that understanding basic statistics is the first step toward mastering this skill. In this post I’ll cover some beginning statistics concepts, then explain how to calculate statistics in Data Analysis Expressions (DAX), and how to create histograms to communicate your statistical findings in Microsoft’s Power BI.   Definitions and Statistical Notation Before we begin, let’s cover a few mathematical terms and how to easily communicate those terms via notation. x – A variable we’re trying to find an answer for. [Amount] is our variable in these examples. <U+2211> – This equates to SUM or SUMX in DAX. N – The Population Size of your data set. If there are 100 records in your data set your Population Size is 100. N = 100 in this case. µ – The Mean or Average of the measure in our data set. AVERAGE in DAX. s – Standard Deviation. A measure of how far away from the Mean a particular data value is. The larger the Std Dev the more spread out our data set is. P in DAX. Making a Histogram in Power BI Histograms or Bell Curves are the most common ways to display statistics about data sets. In Power BI terms, the only real difference between these is the chart type; the Histogram uses a Bar Chart while the Bell Curve uses an Area Chart.   Bar Chart Area Chart A Histogram differs slightly from a standard Bar Chart. A typical Bar Chart relates two variables; in BI speak, a Measure and a Dimension. A Histogram, however, only visualizes a single variable. The variable on the x-axis (in this case [Amount]), and the frequency of that variable on the y-axis. To get the frequency, we just need to count the rows in the data set. Row Count = COUNTROWS('MyTable') We can then create our Amount groupings. I do this in two steps. 1. Create a New Column.  Histogram Buckets = [Amount] 2. Select your new Column and add a New Group.   You can then create a new Histogram with the [bins] on the Axis and [Row Count] on the Value.  Applying Statistics to Data Now that we’ve got our Histogram, we can apply our Statistics. For our example, assume we’re looking for Outliers in our data set. An Outlier is typically defined as a data value that falls outside of 3 Standard Deviations of the Mean. First, we find the Mean of our data set. Mean:  DAX:Mean =CALCULATE (AVERAGE(MyTable[Amount]),ALL(MyTable)) We can then apply the Mean to the Histogram. Using the formula (x - µ) moves the center point of the curve to 0. Histogram Buckets = ([Amount]-[Mean])  Next, we need to find our Standard Deviation for the Population. Standard Deviation:  DAX:Std Dev =CALCULATE (     STDEV.P(MyTable[Amount])     ,ALL(MyTable)) We can then apply the Standard Deviation to the Histogram. Using the formula ((x - µ)/ s) moves the center point of the data set to 0 and divides the values in [Amount] by the Standard Deviation converting our chart into a Normal Distribution. Apply the formula to [Histogram Buckets] and change the Bin Size to 0.5 (feel free to change the Bin Size to whatever makes sense in your data set). Histogram Buckets = DIVIDE(([Amount]-[Mean]),[Std Dev],0)  Now that our data has been normalized we can easily see our Outliers (bars over 3 or under -3). For more DAX tips and tricks, be sure to check out this tutorial from BlueGranite’s blog: 5 Useful DAX Functions for Beginners; and Microsoft’s handy DAX reference. Looking to master and truly own your organization’s data? BlueGranite can help! Contact us today to learn more about our on-site Power BI training. Whatever your data requirements, we customize our analytics solution to meet your company’s needs."
"136" "Microsoft and BlueGranite have blogged recently about the release of Azure SQL Data Warehouse (Azure SQL DW) Gen2. This post is a deeper dive into the practical application of some of the specific capabilities revealed in those announcements.  To put the platform through its paces, we will need a lot of data. So, we’ll start by using Azure Data Factory V2 to pull the last 100 years of weather data from the National Oceanic and Atmospheric Administration (NOAA) directly from the Http source into Blob storage.  Next, we load this data into Azure SQL DW Gen 2 using PolyBase. At DW1000c, the smallest scale for Gen 2, using mediumrc resource class, with source data in compressed (gzip) csv format exactly as it came from NOAA, this took 33 minutes and 24 seconds. This has now given us about 2.5 billion rows to work with. (Incidentally, our table is partitioned by year.) For our first scenario, we want to visualize this data in Power BI. But, first we’ll use the massively parallel processing power of Azure SQL DW Gen 2 to reduce the size of our dataset by filtering and aggregating our daily observations, per station, into total annual rainfall by U.S. county. In my environment, this query completes in about 3 minutes, yielding about 282K rows. We can now use this data to analyze, for example, the California drought. Looking over the last 30 years, we can see that the wettest year during this period was 1998 (left), compared to the driest in 2013 (right). We can also see the areas of greatest impact.  For our final scenario, we will again visualize the data in Power BI, but first we’ll employ Azure Analysis Services (AAS) to import the full, unaggregated dataset for in-memory analytics and, in turn, connect Power BI live to our AAS tabular model. To facilitate parallel processing, our fact table is partitioned by year, just like the source. We immediately realize the benefits of the increased connection limit in Azure SQL DW Gen 2 when the queries from AAS are processed simultaneously without queueing. With this level of detail afforded to us, we can now zero-in on station-specific data and even analyze daily readings, if needed.  The last, but not least, feature of Azure SQL DW Gen 2 worth mentioning, is that as soon as our tabular model has finished processing, we can pause the data warehouse! I hope this post has given some insights into how Azure SQL DW Gen 2 makes an already great data platform even better, and provided another example of how it fits into the Azure big data ecosystem. Wondering what Azure can do for your organization? We can help! In addition to our Azure SQL Data Warehouse hands-on, instructor-led training, BlueGranite offers a wide spectrum of analytics solutions and support to help your group embrace data as a strategic asset. Contact us today to learn more."
"137" "Over the next few weeks, we will be exploring the capabilities of Microsoft’s Cognitive Services. These application programming interface (API) sets empower users with intelligent algorithms that see, hear, speak, understand, and interpret data when applied to apps, websites, and bots.  For our first exploration of Microsoft’s Cognitive Services APIs, we will be looking into the Search category. Adding the Bing Search API to your apps will give you the ability to comb through billions of web pages, images, videos, and news with a single API call. I'll also show how easy it is to create your own search engine using the Custom Search API.  All About Search When we need to search for something on the web such as a new recipe for chocolate cake or maybe some helpful articles related to some research we're doing, we probably use a search engine like Google or Bing to help us navigate to the content for which we're looking. However, given the billions of pages online, and the fact that a small percentage of popular pages typically  overshadow the rest of the web, it may be desirable to better control the results a search engine returns. This is where the Search APIs can come in handy. Using a Cognitive Services Search API will allow you to define a slice of the web from which to return results. As of today, Microsoft has seven search APIs. These APIs can handle many tasks – from searching for images and video, to browsing the news, to intelligently autosuggesting options for searches.   Bing Autosuggest API Give your app intelligent autosuggest options for searches  Bing Entity Search API Enrich your experiences by identifying and augmenting entity information from the web  Bing Video Search API Search for videos and get comprehensive results  Bing News Search API Search for news and get comprehensive results  Bing Custom Search API An easy-to-use, ad-free, commercial-grade search tool that lets you deliver the results you want  Bing Web Search API Get enhanced search details from billions of web documents  Bing Image Search API Search for images and get comprehensive results To try any of these Search Cognitive Services for yourself, get a free trial API key here. Bing Custom Search API  One of the most useful Search APIs, in my opinion, is the Bing Custom Search. This API allows you to quickly and reliably define the slices of the web from which you want results. This API is very flexible and easy to use. Simply change the parameters of the sites you want and don't want and explore site suggestions to intelligently expand the scope of your search domain. Here at BlueGranite, we have a very active blog written by many of our esteemed consultants. Did you know that many of us have personal blogs, too? To demo this API, I have created my own custom search engine to pull search results from everyone's personal blogs as well as the main BlueGranite blog. To begin, navigate to customsearch.ai and click Get Started. On the next page, you'll need to sign in with your Microsoft/Office 365 credentials.  Next, you will need to agree to the terms and conditions and click Agree.  There are only a couple of steps that you'll need to go through to create your customized search. Click Create new instance and give your new custom search a name.  Next, you'll begin typing in the URLs for the sites you want to search. For my custom search instance, I am including all the URLs for each BlueGranite employee's personal site. Note that you can adjust the rankings of each active site and even block particular sites from which you don't want search results.  On the next few tabs, the site will give you the API endpoint information if you need to embed these results in your own app. The site also generates code to easily embed the custom search into your own web page. Try my custom search for yourself:  // <![CDATA[  // ]]> (Hint - try searching for \"Power BI\" or \"SSAS\") Notice that the search engine returns relevant posts from the BlueGranite blog as well as everyone's personal blogs. Imagine using this for a specific set of sites where domain-specific and highly specialized results are needed. For example, you could create a custom search engine of your competitors' sites or maybe a custom search engine that looks for results related to a specific product. That's it! Easy, huh? In just a few clicks, I have created a custom search engine using the Microsoft Cognitive Services Bing Custom Search API. Many of these APIs work in a similar way. They are all designed to be easily consumable and flexible for your individual use cases. Check out all of the Cognitive Services here. More to Come In the coming weeks, we will be showcasing the remaining four Cognitive Services categories: Vision, Speech, Knowledge, and Language. Be sure to subscribe to our blog so that you don't miss seeing the rest of these amazingly powerful Cognitive Services APIs!"
"138" "It’s been an exciting couple of weeks at BlueGranite. Microsoft recognized us as an AI Inner Circle Partner for our premier service delivery and as a Microsoft AI preferred training partner. We’re proud of these honors, achieved through demonstrated customer value in trailblazing AI projects and our training expertise in some of the market’s most advanced AI technology.    Here are some examples of what our customers are saying:  For a cognitive services project using voice-to-text translation and sentiment analysis for call center analytics: “BlueGranite provided the highest level of expertise in machine learning, text analytics, and the full Microsoft AI platform. We now have a best-in-class system to analyze call center topics and sentiment. They were a pleasure to work with.” For a digital transformation project using geospatial analytics and AI: “BlueGranite has been a solid partner and has helped us significantly to get started on our machine learning and advanced analytics journey.” For Azure Machine Learning Services training: “Instructors delivered an amazing two-day session.”  “Very interactive [and] well done. Both presenters … were very patient and knowledgeable.”  Artificial intelligence has different meanings for different people and organizations – maybe it even conjures images of red-eyed robots coming for our jobs! But the essence of AI is machines enhancing human ingenuity with intelligent technology and inexhaustible energy to accelerate business – NOT to become our overlords. How is that acceleration achieved? We believe AI, with its capability to learn from experience, predict outcomes, and prescribe action, is a crucial component of digital transformation – enriching customer experiences, automating and optimizing operations, transforming the nature of products and services, and empowering employees.   Why Microsoft, and why BlueGranite for AI? Maximizing your AI investment requires the best technology and the best people. Microsoft has shown tremendous commitment to a modern, comprehensive AI platform with: AI Services like pre-built APIs for image and speech recognition (and many others) for accelerated application development. AI Infrastructure for scalable, secure compute for even the most demanding machine learning and deep learning models. AI Tools and deep learning frameworks for development and operationalization of custom solutions.  BlueGranite is well-positioned to maximize the capabilities of all these components. With data architects, data engineers, data developers, statisticians, and data scientists comprising our 50-member team, we’ve got the breadth and depth of expertise – specifically with the Microsoft stack – to deliver amazing end-to-end AI solutions. But again, don’t take our word for it. Take Microsoft’s. Even more exciting than the recognition? Our amplified access to Microsoft’s readiness tools, assets, and experts to further boost our expanding AI solution area. If you want to harness the vast potential of AI at your organization, contact us today. We’d love to help."
"139" "If you're a data scientist or an AI aficionado, you already know the struggles of using your laptop or workstation to train complex models. From model parameter tuning and cross-validation to handling large amounts of data, completing your machine learning work can be a slow-moving process without the proper equipment. This is where Azure can help. By using various services on the cloud platform, your data and AI team can offload the work to a highly scalable cloud environment to complete the job faster than ever before.   Scalability is Key Gone are the days when businesses must purchase millions of dollars in equipment in order to work with and innovate upon their data. The cloud has transformed the way that we can access amazing computing power by eliminating the need to be in the same building as the machines. Sure, most of us can get by working on small amounts of data on our nice laptops. But what happens when you want to ramp up on your analyses? Azure allows us to scale out our analyses to the cloud to use the immense amount of computing power there. As we all know, popular machine learning/AI languages such as R or Python have a memory barrier, which limits the amount of data you can handle at any given time. Plus, most CPUs on business-grade laptops only go up to 8 cores. This is good for some amount of parallel processing, but we can do better! Even though machine learning, AI, and advanced analytics aren't new concepts for most organizations, many companies simply don't have the on-premises setup to handle AI-based problems. They are traditionally set up to do data processing and data storage, but not anything more advanced. Azure allows users to deploy their code in the cloud in a managed environment that is pre-configured for their AI workload. Simply specify the size of compute power that is needed and let Azure do the heavy lifting.  Reasoning   Understanding   Interacting   Find new, unexpected ways to adapt and innovate based on your unique data   Interpret business and customer data in real time, and scale including text, docs, images, video, voice   Remove technology barriers for your customers and expand your employees’ capabilities  GPU Goodness The surge of cryptocurrency mining has caused a boom in graphics card prices lately. GPUs are often out of stock for the same reason. The training of certain types of machine learning models (namely, neural networks) is exponentially faster on a GPU rather than a CPU. The rise in popularity of deep learning on NVIDIA GPUs is due to the CUDA parallel computing platform. In Azure, compute resources are available with pre-configured NVIDIA graphics cards. These resources allow you to train your complex models in the cloud while only paying for the time you use it. No more investment in expensive graphics cards that will be out of date in a few years. For more information about GPUs in Azure, follow this link. In addition to Azure having GPUs at the ready, you can spin up specialized virtual machines (VMs) known as Deep Learning Virtual Machines (DLVMs). These VMs come ready with all sorts of AI-related tools pre-installed. From all the popular deep-learning frameworks, to RStudio and Anaconda, to SQL Server and Power BI, the DLVMs are great for any data scientist or AI pro! DLVMs run on Azure GPU NC-series VM instances. These GPUs use discrete device assignment, resulting in performance close to bare metal, which is perfect for solving deep-learning problems.  To spin up your own DLVM now, follow this link. Azure Batch AI Great hardware is only half of the experience you get with Azure. With Azure Batch AI, you can let Azure handle the provisioning and management of clusters of VMs while you just focus on running your AI experiments. Azure Batch AI is designed to allow you to run large AI training and testing workloads in the cloud without having to manage the underlying architecture. This supports the popular training toolkits like TensorFlow, the Microsoft Cognitive Toolkit (previously known as CNTK), Chainer, and others. In addition, you can also deploy and scale your own software stacks.  To begin, you simply describe the requirements of your job, where the inputs and outputs are located, and then Batch AI will handle the rest. From the Azure Portal, search for \"Batch AI Service\" and click Create. Then you can specify the quantity and type of Nodes that you want. Plus, you can configure the cluster to automatically scale based on the workload it's given.  Once the cluster is up and running, you can submit jobs through the command- line interface or through your own Python script. Batch AI will make it easy for you to work in parallel with multiple CPUs or GPUs and can scale to connect a large cluster of VMs together. To learn more about Azure Batch AI, follow this link. Azure Databricks You may have heard of Microsoft's latest partnership with Databricks, a cloud-based Spark platform for big data processing. This collaborative analytics platform allows for scalable analysis of your data in the cloud.  To make your Azure Databricks service, search for \"Databricks\" in the Azure Portal. Click Create and then fill out the information on the next blade to give your Databricks service a name and assign it to a subscription and resource group.  Once your Databricks service is successfully deployed, making a cluster is easy. Simply click on Clusters on the side menu and then Create Cluster. Then, specify the number and type of workers you need. Notice that there are large virtual machines (up to 256 GB of memory and 64 cores!) that can be selected. After you make your initial cluster, you can scale it up or down, depending on the workload that it needs to handle.  Using Azure Databricks has its advantages. From using a highly scalable and massively parallel Spark backend to using Python (PySpark), R (SparkR), or Scala to complete large machine learning workloads, Azure Databricks is extremely flexible. Not to mention, Azure Databricks' interface is highly conducive to collaboration for your data science/AI team. To learn more about getting started with Azure Databricks, follow this link. But Wait, There's More... The services I've outlined here are only a small part of the AI platform on Azure. Azure offers a comprehensive set of flexible AI services for any scenario and enterprise-grade AI infrastructure that runs AI workloads anywhere at scale. Plus, you can equip your team with modern AI tools designed to help you create AI solutions easily, with maximum productivity. Training complex AI models on large amounts of data is often a computationally intense task that requires the support of more advanced computing resources to complete. Traditionally, on-premise solutions to handling data don't scale well for handling AI workloads. So, why not let Azure do the heavy lifting? To learn more about how your organization can take advantage of Azure Databricks for your AI workloads, contact us!"
"140" "Microsoft recently announced the general availability of its Compute Optimized Gen2 tier for Azure SQL Data Warehouse (Azure SQL DW). This new tier brings with it more compute, concurrency, and availability for the cloud data warehousing service.  The Azure Databricks team also recently released the Azure SQL Data Warehouse connector (SQL DW connector) for Apache Spark. This connector enables even deeper integration between Azure SQL DW and the unified Apache Spark platform. These improvements in the two Azure services make them even better together for modern big data and AI platforms in Azure.  Azure SQL DW Compute Optimized Gen2 The announcement of the Gen2 tier for Azure SQL DW came with three major highlights: More processing power: Gen2 offers as much as a 5X performance improvement over Gen1 by caching more frequently used data closer to the compute resources. This is done without sacrificing the separation of compute and storage, which is a key differentiator for Azure SQL DW allowing the service to be “paused” and “resumed” as needed. More concurrent users: Gen1 provides just over 30 concurrent connections to the database. This means that for organizations with many BI and reporting users, the number of active connections must be strictly monitored and mitigated. With Gen2, 128 concurrent users are supported. This 4X improvement takes a ton of pressure off administering active sessions. More availability: With the announcement of Gen2, Azure SQL DW is now available in 33 different Azure regions, making it the most widely available cloud data warehouse platform to date. Azure SQL DW Connector for Apache Spark The Azure SQL DW connector for Apache Spark allows services like Azure Databricks to interact much more effectively with Azure SQL. With this new connector, Azure Databricks can both query massive amounts of data from and load massive amounts of data to Azure SQL DW using PolyBase. After loading data, it can also fire off additional processing in Azure SQL DW directly from Azure Databricks. This functionality allows for end-to-end, secure, big data ETL processing scenarios using Apache Spark in Azure Databricks to load data into Azure SQL DW. Better Together With the recent updates to Azure SQL DW and Azure Databricks, these two services are even better together in a modern big data analytics and AI platform than they previously were. More seamless and efficient integration and the ability to operate at previously unavailable compute scales make for even more solid combined-service use cases. Below are just a few of the use cases for using Azure Databricks and Azure SQL DW together. Batch or Streaming ETL Use Azure Databricks for processing batch and streaming data before loading it into Azure SQL DW for further processing and analysis.  Reference Data Lookup Use Azure Databricks to mashup data from Azure SQL DW with other data sources.  Machine Learning Train machine learning models in Azure Databricks and send predictions into Azure SQL DW for further processing and analysis.  If you have any questions about the recent announcements for the Azure SQL DW Compute Optimized Gen2 tier, about Azure Databricks, or how these services can help you do more with your data, please reach out to BlueGranite today. In addition to our custom analytics solutions, our offerings include Azure SQL DW Training and a Databricks Workshop, each designed to maximize your organization’s data capabilities. Want to learn more about Databricks’ vast potential? Read our recent review here."
"141" "I’ve recently devoted a lot of thought to the role structure plays in data analytics. Not just in the immediate sense that comes with being on a specific project, but in a more holistic “Why do we do what we do?” sense. Structure is a foundational element to much of what we, as designers and builders of analytical systems and solutions, do – but like anything that becomes a matter of course, it can be easy to lose sight of the “Why?” of it all.  Modern tools and technologies make it easier than ever to go and get data wherever it resides and begin gleaning insight from it – a fact which might require us to reevaluate some of the industry’s longstanding rules. If we don’t strictly need to face the often-difficult task of imposing structure on our data in order to analyze it, then what do we gain by doing so? This question led me to re-examine the reasons we devote time and energy to structuring data for analytics in the first place. I soon realized that exploring those reasons is a daunting task – and fully unpacking them goes beyond a single blog entry. To fully delve into the broader relationship between data structure and analytics, I’ll be using this multipart series to take a closer look at some individual use cases I’ve encountered. While this first entry prefaces the larger subject, more specific exploration and examples will follow. My hope is that this upcoming series will shed some light on practices that, while now venerable by the fast-paced standards of today’s technology, are still integral to our efforts today. Imparting Understanding Fundamentally, analytics is about achieving understanding. I’ve long been fascinated by etymology – or the origins of words. Studying a word’s roots can uncover concepts buried by modern context, conveying new insight. According to the Online Etymology Dictionary, “analytic” as an adjective comes to us modern English speakers mostly unchanged from both the Greek “analytikos” and the Medieval Latin “analyticus” – derived from its separate parts of “ana-“, meaning “up, back, throughout”, and “lysis”, meaning “a loosening or unfastening”. In other words, it is the creation and separation of an idea or concept from the actual thing or event we seek to understand, all for the sake of conveying greater meaning to our oft-fallible brains. It is directly related to the word and concept of “analogy”, or the use of related concepts and illustrations for the sake of understanding. This is at once both obvious and confounding – isn’t the best, most precise way to get to the truth of the matter to use as little translation as possible? To address this, consider the sciences (bear with me here, I promise it will make sense in the end) – all of which are predicated on the precise structure of mathematics. In a way, we can see each science as a layer of abstraction, an analog, built atop the foundational concept that is mathematics. Math itself is based on shared conclusions concerning what we observe to be true. If I have an apple, and you give me an apple, then I now have two apples, and so on. Now, imagine trying to describe the migratory patterns of geese in purely mathematical terms – as only a series of numbers in black ink on a white page. It can be done, and the result would be extremely precise. However, it would not be a good way to impart general understanding of the phenomena, especially to an audience not already familiar with the subject matter. In the case of migrating geese, we can better convey understanding in a visually oriented way. Instead of showcasing the lowest level of detail, the mathematical formula at the base of the migratory behavior, we can use maps and diagrams which layer on top of the other structured analogs, or the other levels of detail, to represent it in a way the human mind can easily grasp. The visual representation we show sits atop the field of biology in describing the geese’s behavior. This is underpinned by chemistry, which is a set of molecular and atomic reactions described by physics. Physics, in turn, is derived atop mathematical formulae. By using this series of logical structures, one built upon another, we can easily communicate a concept without bogging our audience down in thousands of pages of pure numbers. Enter modern data analytics and its inherent need for structure. The Structure of Data Source data is often organized for purposes other than conveying understanding. This can make it difficult to impose a structure like the one detailed above. The challenge usually lies in how the source data is written, stored, or generated. For example, highly normalized database topologies are put in place to facilitate the speed at which data is written, rather than to ensure that it can be readily read and understood. All the same, if a modern and powerful tool like Power BI can connect to a highly normalized transactional data structure, restructure and aggregate it behind the scenes, and still present that data graphically – why should we take additional time and effort to impose manual structure? The answer lies in the fact that this automatic restructuring by a tool has limits, and that there are other capabilities that we gain, as well as missteps we avoid, when we deliberately impose structure on our data. These additional capabilities and avoidable pitfalls will be outlined on a case-by-case basis within this series. Stay tuned. Part 2 is now available! Check it out here. "
"142" "The concept of artificial intelligence (AI) has been around for decades, so why has it only recently gained traction? Due to recent advances in both hardware and algorithms, what was once thought prohibitively expensive or not even possible is now possible. Not only that, but it is available at scale and feasible to incorporate into everyday business solutions.  Artificial Intelligence AI presupposes that people can create technology that imitates human intelligence. In addition to applications explicitly programmed for an outcome, AI also encompasses models that learn on their own when provided with specific inputs. Traditionally difficult tasks for a computer to perform are becoming increasingly available via AI. Solutions that involve highly cognitive skills that encompass images and vision, speech and audio, or natural language are now in the realm of possibility. For the past few years, machine learning has been a dominant discussion point. Separate from but related to machine learning is deep learning. Deep learning takes advantage of algorithms that work well for vision, speech and text. To simplify, think of AI as an umbrella that encompasses both machine learning and deep learning. AI + Cloud AI takes a lot of processing power. Easier access to high-end computing resources has enabled faster and broader innovation in AI. Perhaps more exciting than AI by itself is the combination of AI and scalable cloud computing. Cloud vendors such as Amazon or Microsoft (with its Azure platform) are in a much better position to offer the compute power required for large-scale deep learning and AI solutions. Cloud technologies have also matured to the extent where it only takes minutes to create a powerful data science environment. Many of the arguments in favor of a cloud-based AI echo the traditional reasons businesses weigh on-premises versus cloud. For solutions leveraging AI, there are clear advantages to the cloud, but none of these potential reasons are AI-specific. Cloud-based AI allows businesses to: Leverage a spectrum of technologies that cater to anyone – from developers to data scientists. Quickly scale if needed. Take advantage of a pay-as-you-go pricing model where you only pay for what you consume. Rapidly build out prototypes to prove out a concept. AI is Specialized As humans, we are well-rounded with general knowledge, but we also have specific skills. You likely would not ask someone from your marketing department to prepare your company’s financial statements (a specialized task). Even without the ability to prepare the statements, however, the marketing staff would be able to read and comprehend much of the content (a more general function). What people often take for granted as general or innate knowledge is completely unknown to various AI models. Throughout our lives, we’ve learned and can adapt to a variety of situations we have never specifically encountered before. At this stage, AI is specialized. While models still learn, that learning is heavily dependent on the training data. A deep learning model initially trained to recognize images of automobile components will not be good at classifying images of birds, let alone be useful for natural language tasks. An AI focusing on understanding speech will not work well for images, etc. A broader business solution, however, may utilize several AI solutions in combination. Outputs of one system can be used as the inputs for another. Many AI solutions are also not standalone. You may employ AI at a specific point in a larger data pipeline. The Case for AI Some people believe AI to be a panacea that in itself could solve a wide range of business problems, but that is not the case. It takes planning and effort to solve a problem, and nothing changes in that regard when you employ AI in a solution. As an “expert” trained in a specialized task, however, AI is great at handling problems where it is difficult for people to achieve scale or efficiency. AI is a component in a business solution and does not marvelously solve every problem by itself. Human decision-making and manual work will likely still be involved at many stages of an AI-related project. Before taking on any AI project, first determine the problem you would like to address. Here are some ways businesses today employ AI to increase their competitive advantage: Industry Sample Problem Solution Goal Manufacturing Equipment failure leads to costly shutdowns Increase efficiency through predictive maintenance Healthcare Labor-intensive diagnostic and administrative processes More effective patient and medical record processing Retail High inventory costs due to overstocking, or shortages due to out of stock items Optimize supply chain and logistics Education Student dropouts Identify at-risk students or likely enrollment changes This introductory AI post intentionally did not consider specific technologies, but in my next post, I will survey a set of tools that make up the Microsoft AI platform. I’ll also demonstrate how BlueGranite can help you leverage many of those components to build an AI solution. Stay tuned for the next update! If you are interested in learning more about harnessing the potential of AI at your organization, contact us today."
"143" "“Harness the power of AI through a truly unified approach to data analytics powered by Apache Spark.”– Databricks, a unified analytics platform optimized for Azure The mission of Azure Databricks is to make big data and AI simple by providing a single, notebook-oriented workspace environment that makes it easy for data scientists to create Spark clusters, ingest and explore data, build models, and share results with business stakeholders.  The analytic objective for this blog was to create a predictive employee turnover model. Traditionally, the steps for conducting such a task would require a fairly large collection of disconnected technologies and languages. However Azure Databricks offers an analytic workspace that allows for a seamless pipeline from ingestion to production. Thus, the technical objective for this blog was to test drive Azure Databricks and use an anonymized data set of HR employee information to build an employee flight-risk model.     Clusters: Spark power for processing large data sets All AI pipelines within Azure Databricks begin with creating a Spark cluster. A cluster is the computing engine necessary for conducting big data analytics. While a cluster may be composed of several computers behind the scenes, an Azure Databricks user interacts with the cluster as if it were a single computer. The Azure Databricks workspace makes creating clusters easy; users need only to make a few choices regarding the initial size and type of computing resources.   Data: connecting to data sources and ingesting data Once a cluster has been successfully initiated, it is time to ingest the data by first creating connections to the data source or sources. Azure Databricks allows for the integration of diverse data sources as if they were centralized; the platform provides a single view of a users’ data sources and fast, robust access to each data source via optimized connectors. Spark has an extensive set of data sources it can connect to out of the box. In Azure, these sources include, but are not limited to, SQL database, Azure Blob Storage, and Azure Data Lake Store. Azure Databricks also allows you to upload files to the service’s native file store, Databricks File System (DBFS).  The data used in this HR analytics project was stored in an Azure SQL Database. Azure Databricks readily connects to Azure SQL Databases using a JDBC driver. Once connectivity is confirmed, a simple JDBC command can be used to ingest an entire table of data into the Azure Databricks environment. This above bit of code results in what is known as a Spark DataFrame. More detail on this can be found in a recent blog post by BlueGranite’s own Mike Cornell; excellent use-case information and example code are provided.  Data: Spark DataFrames Spark DataFrames are very similar to R and Python data frames and tables in relational databases. The DataFrame API has the distinct advantage of creating a columnar organization of distributed data that is optimized for the analysis of very large data sets. Use of the DataFrame API allows for data analysis using familiar languages such as Python, R, Scala, and SQL.  Exploratory Data Analysis (EDA) using Azure Databricks Notebooks The Azure Databricks workspace is an integrated environment for a data scientist or a team of data scientists to explore data and build models in a self-service manner. Databricks notebooks are the foundational component of the interactive and collaborative workspace that simplifies exploratory data analysis and visualization of data. Several programming languages are supported in the notebooks including R, Python, SQL, and Scala. An end user must select a primary language for a new notebook but may choose to author code with other programming languages by using the appropriate language magic command - %[language], e.g. %sql. Such language flexibility allows data scientists to capitalize upon the unique strengths of individual programming languages for a given analytic pipeline without having to change notebooks or the workspace. Markdown and HTML also are supported in the notebooks to create non-code material for contextual information or report writing. A notebook must be connected to an active cluster in order to execute commands. Here is a subset of some exploratory data analysis code written for the “hr” DataFrame:        Exploratory data visualizations with the display() function The display(<dataframe_name>) function is an especially powerful manner by which to create informative exploratory visualizations of a data set.  Below are some exploratory visualization examples using the plot options function.     Model: build machine learning models with Spark ML For data modeling, Azure Databricks includes the Spark ML machine learning library which provides all common machine learning algorithms, e.g. classification, regression, and clustering. These Spark ML algorithms allow for parallel, distributed training of models using large data sets located on Spark clusters. The DataFrame API is the primary API for the machine learning algorithms included in the Spark ML. Using binary logistic regression, a classifier was trained using the “hr” training data with the objective to predict employee turnover. All the necessary preprocessing of the data (e.g. converting categorical variables into numeric variables) is easily conducted using the ML Pipelines API. Pipelines chain several modeling steps together, such as transformations, assembling of features, and fitting of algorithms. The Spark ML Pipelines will be familiar to users who have experience with Python’s scikit-learn library.  Conversion of categorical features:  Assemble features:  Create a pipeline:  Once an ML pipeline has been built, Spark ML supports hyperparameterization using the ML functions parameter grid builder and cross validation. With further parameterization using 10-fold cross validation, the best model for predicting employee turnover was chosen. Predictions using the test data were evaluated using the common evaluation metric, area under the ROC curve. The trained model was then saved and used to evaluate new, never-modeled HR data. This new data and the resulting predictions were saved as a DataFrame on an Azure Databricks cluster. Share results using Microsoft Power BI Numerous business intelligence tools can connect and ingest data from Azure Databricks clusters.  Microsoft’s Power BI is one such supported business analytic tool. Using an JDBC/ODBC driver, an end user can connect Power BI Desktop to an Azure Databricks cluster. The new HR data and associated predictions were brought into Power BI Desktop and a simple dashboard was created to share the HR employee flight risk results with relevant business stakeholders.  Note: for an interactive look at a Power BI dashboard for Healthcare Workforce Analytics, check out BlueGranite's Power BI Showcase.  Microsoft’s announcement of the general availability of Azure Databricks this past March received an enthusiastic welcome and inspired this HR analytics project. The experience of creating an HR data analytics pipeline within the Azure Databricks environment was made easy with the highly integrated self-service workspace. For those teams of data engineers, data scientists, and business analysts who are responsible for designing big data AI projects, Azure Databricks will definitively meet and exceed their analytic needs. If you're looking to put Azure Databricks to work for your organization, BlueGranite can help. Contact us today to talk with our team of experts and ensure success at every stage of your data development journey. Azure Databrick’s documentation: https://docs.azuredatabricks.net/"
"144" "Hopefully, you are well aware that one of the most powerful aspects of the Power BI platform is the ability it gives report authors to share, with fellow users, the solutions they create and insights they discover. Here we will review some of the established options for sharing Power BI artifacts with users outside an organization, and take a deeper look at a newer option – through Azure Active Directory business-to-business (Azure AD B2B) – that makes it much easier for report developers to share reports with users outside their organization, such as suppliers, subsidiaries, or partner organizations.  Sharing reports and dashboards within an organization is most commonly accomplished through the use of apps. Report developers can collaborate in app workspaces on report and dashboard creation, then publish those reports and dashboards as apps to be consumed by groups or individuals throughout an organization. Those consumers must either have a Power BI Pro license to access to the reports and dashboards, or the reports and dashboards must be published to Power BI Premium capacity on the tenant. But what if you need to share your insights with people outside of your organization? Enable Peer-to-Peer Sharing The peer-to-peer sharing option is one method report developers can use to share published dashboards or reports with individuals outside their organization, but only if those recipients have a Power BI Pro license. You can add individual email addresses (no groups allowed) to allow sharing with multiple individuals, and enable features like row-level security to limit the data the user sees. Besides needing a Power BI Pro license to view the content, these outside users cannot re-share your dashboards with individuals outside of your organization. Due to its limitations, this option is best suited to sharing with a small number of individuals on an ad-hoc basis, and not for large numbers of report consumers that will regularly use the reports and dashboards. Create an Organizational User Account The second option for enabling Power BI report and dashboard access outside of your organization involves something of a “workaround”. Creating an organizational user account for your tenant for an outside user allows them to use that account and password to access the Power BI service. For example, if I want to provide Bob at Contoso access to my BlueGranite Power BI reports, I need to create a duplicate Power BI account using bob@blue-granite.com for him to use to view the reports on our tenant. However, managing multiple user accounts can be complicated, and the user will need a separate account for every Power BI tenant that they need to access. This can quickly become confusing for users and complicated to manage for administrators. Use Power BI Embedded The third option for sharing dashboards and reports with individuals outside of your organization is through Power BI Embedded. Power BI Embedded is primarily focused on providing a developer platform for embedding Power BI visual elements into your own applications and websites. Power BI Embedded doesn’t require the user to have a Power BI license, or to sign into the Power BI service. Pricing for using Power BI Embedded is based on capacity and is dependent on the number of Power BI elements rendered per hour. As a Power BI Embedded developer, you are responsible for the development of your application or website, including the integration of Power BI elements, and you are responsible for developing solutions to restrict access and authenticate users to your application or website. While Power BI Embedded can be used to develop compelling applications and websites that integrate the value of visualizations and interactive data exploration into your offering, the target customers for Power BI Embedded are organizations that provide data and information as a service offering for paid customers. What if you have suppliers or customers that would derive business value from the insights that you’ve discovered? How can you make interactive dashboards and reports based on your data available to them? Azure AD B2B Eases Sharing Late last year, Microsoft enabled Power BI to be used with Azure Active Directory business-to-business. Azure AD B2B allows you to invite external users as guest users into your organization, and to grant permissions to those users to view published dashboards and reports on your tenant. Administrators can provision external users to get access, or Power BI app creators can send invitations to the external users to access the published app. Providing access to large numbers of users can be scripted through PowerShell. The permissions to apps are handled in the same way that they are with internal users, as is role-based security functionality. If you have multiple partner organizations that consume the same reports, you can use role-based security to filter the data in your reports by the user’s domain. Using Azure AD B2B means that you can share dashboards and reports with external users without resorting to the previously mentioned, more complicated methods, but it does require the correct licensing to allow for guest user access. If the user has a Power BI Pro license on their own organization’s tenant, the user can then access the content in your tenant without additional licensing. If the user doesn’t have a Power BI Pro license, you have two options for providing access: You can assign a Power BI Pro license from your organization to that user when they access the content on your tenant. Alternatively, if the Power BI app is published to Power BI Premium capacity on your tenant, the guest user can access the content without additional licensing. Beyond sharing your Power BI reports and dashboards with external partners, Azure AD B2B applies to many other collaboration scenarios where the same type of shared access to applications might be desired. For example, Azure B2B can also provision access to other internal applications and services that you use to collaborate with those external partners and can manage options for secure access, like requiring multifactor authentication and auditing and reporting.     Let’s Get Started Want to position your team for Power BI success? BlueGranite can help. Contact us today to learn the many ways we work with you to customize our training to your organization’s needs."
"145" "It’s been argued that the term business intelligence (BI) dates all the way back to 1865, when author Richard Millar Devens used it to describe how an astute banker tapped into business information to gain an edge on his competitors. Nowadays, the label encompasses everything from reporting, analytics and decision support to key performance indicators, dashboards and executive information systems. While solutions and technology have evolved significantly since Devens’ day, companies still face the same age-old problem – what resources offer the most value from their data? For a growing number of organizations the answer is Microsoft’s business analytics workhorse, Power BI.  While modern data organization procedures are fairly standardized, methods of exploring that data can range from beautifully simple to painfully complex. The organization process typically begins by: identifying data to access; choosing what the data outputs look like (e.g. reports, dashboards, etc.); and finally, deciding what questions will be asked up front. Once an organization has taken those steps, it can begin to interact with its data and get answers. Let’s take a closer look at the final step, and its importance to business.  For example, what if I’m looking at sales information and I see a problem, or a high-level opportunity, that I want to access in greater detail? My output is configured so I can investigate geography, personnel, product, time, weather, customers, and more. I could spend hours, or even days, manually exploring different combinations to try to understand something like the reason behind an increase or decrease in sales. The answer is crucial. I can only communicate my findings and take action once it’s in hand. What if I only had to identify the data set I am interested in? What if the click of a button could reveal outliers, correlations and trends? What if I was looking at my reports or dashboards and had the same high-level sales question, but did not have to manually cycle through myriad combinations to be able to zero-in on what drove changes? Power BI makes that not just possible, but easy. As Microsoft integrates machine learning and data mining into this self-service BI tool, its substantial analytics value continues to grow. One of my favorite Power BI tools is the Quick Insights feature. It applies a set of data mining algorithms to a data set, auto-generating a series of charts and graphs that reveal trends, correlations, and outliers – no set-up required. Just feed in the data to discover patterns that can broaden understanding and help formulate questions. It’s as easy as 1-2-3:  The chart those simple steps created shows me that there are outliers in Discounting:  I can then focus Quick Insights on those outliers, to uncover their origins. This action autogenerates the next series of charts, shown below:  Though it seems to be a lesser-known feature, this Power BI capability has been around for a while. If you’ve never tested it out, I suggest giving it a try.   Another exciting feature in Power BI is the latest data mining infusion. Now, when looking at a chart I’ve created with a timeline attached to it, I only a click away from understanding why a value has increased or decreased from the prior period. A series of waterfall charts (users can cycle through a few different chart types) are auto-generated across data combinations to show the origins of an increase or decrease. Even better, the narrative that is written holds insight into relative contributions, not just raw change. For example, Product A historically drives 50 percent of all Product Sales. From period 1 to period 2 there is a $2 million increase in Total Sales. An analysis of the increase might show that during that time Product A sales increased $1 million. The narrative that Power BI writes and the waterfall chart it creates reflect that $1 million of the increase is attributable to Product A, AND that the relative contributions remained the same (as historically expected). On the other hand, if Product B and C were major contributors to the $2 million increase and Product A had none, the narrative would reflect that the relative contributions of Product A, B and C have changed the most. Here is an example: Given this Ribbon Chart of Revenue across years:  I simply click “Analyze, Explain the decrease” in the bar for 2011:  Power BI delivers a series of charts (by Product, by Geography, by Channel, etc.) that explain the details around the decrease. By clicking on the chart, I can add it into my report to monitor going forward.  Microsoft’s addition of “intelligent’ features to its latest product versions is reducing the once labor-intensive, manual nature of these types of tools. While there are other technologies on the market that offer similar capabilities, they are generally expensive, take a lot of up-front work and require continued care and feeding to maintain. They also lack Power BI’s breadth of capabilities. With tools spanning BI, data manipulation, data acquisition, data mining, and self-service access, to name just a few, Power BI supports users facing an explosion of data volume and granularity, and helps them not just navigate information, but make the most of it. If you are looking to make the most your data, contact BlueGranite today. Let us help you get the answers you need to deliver value across your organization."
"146" "Intro to Data Factory v2 Azure Data Factory (ADF) version 2 (v2) is a game changer for enterprise integration and workflow orchestration. For those of you who aren’t familiar with data factory:  “It is a cloud-based data integration service that allows you to create data-driven workflows in the cloud for orchestrating and automating data movement and data transformation. Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from disparate data stores. It can process and transform the data by using compute services such as Azure HDInsight Hadoop, Spark, Azure Data Lake Analytics, and Azure Machine Learning.  Additionally, you can publish output data to data stores such as Azure SQL Data Warehouse for business intelligence (BI) applications to consume. Ultimately, through Azure Data Factory, raw data can be organized into meaningful data stores and data lakes for better business decisions.” (found here)  Data Factory v1 vs v2 ADF version 1 (v1) was designed predominantly for the “modern data platform”, i.e., data lake architecture and big data tooling in Azure. Microsoft accepted the feedback from its customers and partners to develop v2 and redefine “modern” to include existing on-premises infrastructure and batch orchestration needs. If you haven’t taken the time to explore what’s coming in v2, please take this as your wake-up call. Here are some of the exciting capabilities that v2 offers: Linked services now execute under an integration runtime that can be hosted on premises or in the cloud. A cloud-hosted SSIS integration runtime allows for lift-and-shift and scale out of on-premises SSIS solutions. Schedule triggers enable pipeline execution based on a “wall clock”. The output of the pipeline is no longer expected to be contiguous and non-overlapping. Control flow activities enable conditional workflow logic (looping and branching) like SSIS. Expressions based on trigger provided parameters. Again, very similar to SSIS. Custom Activities can run against any executable program on Windows or Linux. Version 2 custom activity –– run any executable The ability to define custom activities against any executable program in v2 means we can be platform agnostic when orchestrating data in the cloud. If a process can run on a single machine in Windows or Linux, ADF v2 can integrate it into a pipeline via the cloud-hosted integration runtime and the Azure Batch service. (If you need custom distributed jobs, those are available through HDInsight, Data Lake Analytics, and the now generally available Azure Databricks linked services). In ADF v1, custom activities are limited to .NET solutions that specifically target Framework 4.5.2, and which implement the Execute method of the IDotNetActivity interface. This means you must do a lot of refactoring if modeling off existing solutions, as well as implement additional program(s) to debug locally. Integrating with Microsoft AI platform Cognitive Services So, now that we’ve been set free to use any executable when looking for solved problems, let’s consider the possibilities for integrating modern solutions into enterprise workflow orchestration. For inspiration, I looked to Microsoft AI Cognitive services – extremely powerful AI solutions that Microsoft maintains and packages as APIs for developers to integrate into their own applications. Here I stumbled across the Translator Text API as well as a related GitHub project. With some simple refactoring of the base classes, I was able to transform this windows application built on .NET 4.6.1 into a console executable that ADF v2 can call at runtime and use to read/write directly against Azure blob storage*. I now have a batch process that can take Microsoft Office, plain text, HTML, PDF and SRT caption files and translate them to and from over 60 different languages (including Klingon). Potentially more useful than the Klingon part, you can further extend the functionality of translator text API via the Microsoft Translator Hub, so that translations specific to your business domain can be included, and the specific communication style of translation used in your enterprise can be learned as well. *Technical note: Microsoft Document Translator can be called from command line and run against local storage, meaning I could have simply installed it on a batch pool node and used dedicated attached storage instead with no refactoring required at all. I just preferred to use blob storage for this demo. Sample Solution The basic plumbing for an ADF v2 custom activity is documented here. The sample I am using is in C#, but anything that executes on Windows or Linux OS is fair game: Python, Java, Scala, etc. The key components for batch execution are to consume the activity.json, linkedServices.json, and (optional) datasets.json files that ADF v2 adds to the runtime directory when calling the executable. This is where all the system parameters and/or extended properties of your activity are passed to the executable in code. The sample solution is available here. When successfully deployed and executed, the custom activity iterates over all documents in a blob container, auto identifies the source language and translates the documents to English, writing to a translated folder in the same blob container. Here are some sample screen shots: Source folder for documents in blob storage. Sample untranslated document (in Swedish). Custom ADF v2 activity successfully ran in the visual editor. New translated folder created by the custom activity run. English translation of document. Potential Use Cases This pattern could just as easily be extended to include any of the other language APIs available in cognitive services. Likewise, we can apply this pattern to other blobs like images and sound recordings utilizing the vision or speech APIs, respectively. Some potential solutions we can now build as a batch process: Identifying who is speaking and appropriately tag the file Tag images as well as add descriptive text Sentiment and key phrase extraction from text Identify specific people and / or emotions in images Many of these tasks are done manually by third parties in an enterprise setting, or simply not done at all. The cost benefit and quality improvement of switching to a Cognitive Services API can be significant and a natural first step for bringing AI to your organization. If you’re interested in seeing how Azure and Microsoft AI can enable insights and streamline operations in your organization, please contact us. We’re happy to help."
"147" "Digital Transformation – the process of using technology to evolve – is revolutionizing how companies do business. And BlueGranite is at the forefront of helping organizations successfully navigate the conversion. We leverage leading technologies to help our clients (and our own employee team) reimagine how they achieve organizational objectives. Through this multipart Digital Transformation series, we’re exploring the many ways embracing the concept can help businesses grow.  Earlier in this series, we investigated what Digital Transformation means. Then we dove into how we can help you further engage customers. Today, let’s look at effectively empowering employees. Before we get started, here’s a recap of the 4 pillars of Digital Transformation: Engaging Customers: If we improve customer experiences, customer satisfaction, and brand image, business results will follow. Empowering Employees: If we empower employees through a high-performing culture and enable access to data and analytics assets, we can accelerate delivery, improve quality, and drive user adoption and satisfaction. (We’ll dive deeper here as we go along). Optimizing Operations: By building a better understanding of current processes –manufacturing, sales, marketing, finance, or otherwise – we can often streamline and cut costs. Transforming Products: Flexible, innovative organizations are better able to offer agile, adaptable products and services. Organizations can use technology to innovate on their value proposition to the market and their customers. We’ve heard this oft-repeated saying from progressive business leaders for decades: “Our people are our greatest asset.” Indeed, what is an organization if not the collective contributions of its employees in practicing its values and fulfilling its mission? In a highly competitive market environment, good business leaders realize that creating a culture and an organization that values employees and provides them with an environment to fully utilize their skills is crucial – it leads to improved employee satisfaction and, consequently, improved business results. As data and analytics experts, BlueGranite develops insight-driven solutions that improve business performance. One way we do that is through helping our clients empower their employees. Below are a few examples of how. Employee Skill Mapping Almost all organizations invest time and energy into defining job roles and responsibilities, but how many proactively manage skills development, or chart a path for personal development and career success? Tracking your employee’s skills and interests, monitoring their experiences, and cultivating their growth over time can offer huge rewards. Data and analytics play a vital role in enabling these business priorities. At BlueGranite, we use an internally developed tool to effectively track each of our employee’s skills and interests, as well as provide standard reporting and ad-hoc analysis. To meet the needs of our clients, it is vital that we understand our collective capabilities and the individual skills on our team. Skill mapping makes that possible, while also giving us a window into market demand and potential employment gaps – insights that inform our recruiting and staff development initiatives. This also helps to align our team’s abilities, interests, and goals, with our organization’s strategy and direction, and it helps us do the same for our clients. Employee Flight Risk Professional development is a critical function in empowering employees. So is developing a clear and nuanced understanding of what drives them, and what factors may indicate when those valued employees (who can often be difficult to recruit and hire) might be planning to leave. The concept of preventing employee attrition is similar to that of reducing customer churn, in that we leverage data and analytics for both solutions to take preemptive action to avoid negative organizational impacts. BlueGranite can empower your HR team with analytics, allowing them to better understand where to focus their time to avoid unwanted employee attrition. Not only can this help lower turn-over rates, but it also potentially increases your workforce’s job satisfaction levels. User Segmentation in Power BI Our clients are adopting Microsoft Power BI in droves. We’ve found that tailoring data and analytics training to the individual needs of an organization’s user base is one of the most powerful methods of energizing and enabling employees. While a percentage of each organization’s users are in the IT department, many more sit in the business division, working in sales and marketing, operations, HR, and so on. What does it take for those individuals to be successful with Power BI? Thriving organizations know that providing orientation and training to their end users maximizes Power BI’s benefits. While certain baseline educational assets apply to a broad majority of users, organizations have multi-faceted training needs, dictated by end user types. To that end, BlueGranite developed a tailored training curriculum customized to each client’s organizational needs. As part of each session: We develop a detailed understanding of our Power BI training audience. We leverage a brief online survey to profile the audience. We identify baseline skill levels, job responsibilities, and training objectives. And we baseline participants to ensure we emphasize particular training modules that meet their specific needs. We Can Help Fostering career development, managing flight risk, and offering in-depth digital technology training are just a few of the many ways BlueGranite can help you help you empower your employees, and take the next step in your organization’s digital transformation. We leverage data to build value across organizations. Contact us today to discover the many ways we can reinvent the way you do business."
"148" "Microsoft recently announced the general availability of Azure Databricks. This service is the fruit of a lot of hard work between Microsoft and Databricks to make the already-popular Databricks Unified Analytics Platform a first-class service on Microsoft Azure. This platform brings the power of Apache Spark into a secure, scalable workspace environment where data scientists, data engineers, and business users can explore and analyze data, build and schedule ETL pipelines, and build and deploy complex machine learning models.   Azure Databricks has the potential to be a game changer in the Microsoft Data and AI Platform. Few, if any, Azure data services cater to the variety of users, connect to the wide range of data sources, and satisfy the broad set of compute tasks necessary for ETL and AI use cases that Azure Databricks does. Single Service for a Variety of Users First, Azure Databricks provides a single workspace for a variety of users. It provides a place for data engineers to transform data and create and schedule batch and streaming ETL jobs. Data scientists can also use Azure Databricks to explore data, create machine learning models, and perform other advanced analytics tasks. Even business analysts can write SQL queries and analyze and visualize data in Databricks notebooks. They can also use tools like Power BI or Tableau to connect to Azure Databricks tables for analysis and self-service BI.  Universal Connectivity to Azure Storage Services Second, Azure Databricks seamlessly connects to all the different Azure storage options. This includes the ability to read and write to file-based storage, like Blob storage and Azure Data Lake Store, as well as relational data stores, like Azure SQL Database/Data Warehouse, and NoSQL data stores, like Azure Cosmos DB. It also connects to streaming or event data sources in Azure, such as Event Hubs or Apache Kafka on HDInsight.  Single Service for a Variety of Compute Tasks Finally, Azure Databricks provides a single service for many of the common compute tasks in ETL and AI use cases. The Microsoft Data and AI Platform already provides a collection of unique services that work together to satisfy specific data processing use cases. This may mean that a client might use Azure Data Lake Analytics (U-SQL) for batch processing of big data in a data lake, Stream Analytics for processing steaming event data, and Azure Machine Learning for creating and operationalizing predictive models. With Azure Databricks, all these different use cases and compute tasks can be developed and implemented in a single workspace. Batch ETL jobs can be developed in the workspace and then scheduled via the Databricks scheduler or with Azure Data Factory; jobs for processing streaming data can be developed and deployed to dedicated clusters within the workspace; and machine learning models can be created and deployed in the workspace to be used in batch or streaming jobs to make predictions on new data.  BlueGranite has recognized Azure Databricks as a critical part to the Microsoft Data and AI Platform. To position ourselves to be the best at helping clients implement this new service, BlueGranite is teaming up with Azure to become an official Databricks consulting partner. This partnership, paired with our existing relationship with Microsoft, helps provide BlueGranite the tools, education, and technical and sales support resources from both vendors to help clients do more with their data using Azure Databricks. If you are interested in learning more about Azure Databricks and how you can get started, check out BlueGranite's 1-day workshop and consulting offer! "
"149" "4 Ways to use Flow in an Enterprise Environment In 2016, Microsoft released a self-service online integration tool called Flow. It allows power users and IT folks to create simple data flows in an easy, user-friendly web application. In just an hour or two of familiarization with the tool, you can create functioning, useful workflows. Whether it’s sending out emails from your CRM or triggering a database query from the Flow smartphone app, there are loads of clever ways to use Flow in an enterprise.  Scenario 1 – Coordinated Communications Management wants to facilitate better communications with their employees and audience. The company maintains communication through many social media platforms, including Twitter, YouTube and Facebook. While that sounds successful on paper, managing those multiple platforms is proving more difficult than originally thought. Sometimes, a social platform is being neglected; other times, communication is inconsistent among platforms. Flow can be used to cross-post messages across a wide variety of platforms. For instance, it can automatically post updates on Twitter, LinkedIn and Facebook when a new blog entry is posted on WordPress. It can alert Twitter and Facebook followers when a new YouTube video is posted. In the process, it can also archive all your communications to a SQL or an Azure database. Of course, it also has connections to other social media platforms. The automated, trigger-based actions ensure that all lines of communication to your customers or employees are engaged, without the overhead and inconsistency of manually posting to multiple platforms. Scenario 2 – CRM Integration One of Flow’s strengths is as a customer relationship management (CRM) software integration tool. In fact, BlueGranite uses Flow internally to integrate Dynamics 365 with email and SharePoint. One of the CRM’s duties is to track sales opportunities. Every sales opportunity needs to have various files associated with it. To ensure that every opportunity has a consistent file storage place, a workflow was developed to generate a folder on SharePoint for each opportunity as it is created, and Dynamics CRM is automatically updated with that folder location. There’s no more need to create folders or hunt around for them, as they are clearly listed in SharePoint and Dynamics, and the folder names match the opportunity names. Another way to use Flow with Dynamics 365 is to keep tabs on the status of sales opportunities. When an opportunity is created, won or lost, those who need to know can be notified immediately. They can be sent emails, text messages, or even push notifications with the Flow app. We use notifications like this internally, as well. Scenario 3 – Power BI Integration, Part 1 Flow combined with Power BI makes for a powerful pair. The Power BI connector in Flow can trigger execution based on measure values using data-driven alerts. That gives Power BI the ability to react actively to the data, not just passively visualize it. That’s really putting the power into Power BI. The simplest and most obvious application is sending out emails in response to the data changes. That will notify executives and analysts of problematic situations immediately, rather than waiting for someone to check into Power BI. If you want more information on using Power BI and Flow with data-driven alerts, a colleague at BlueGranite has already covered using those in this previous blog post. Simple notifications are only the tip of the iceberg. Flow enables Power BI to interact with other systems directly. With the ability to interact with SQL Server, Azure services, generic web services or even custom connectors (in the Premium editions), it can kick off any number of automated responses, from gathering related in-depth reports for analysis, to actively controlling hardware and software systems. Since Power BI can integrate data from multiple source systems, it can act based upon a holistic measure of business needs. This is powerful stuff. Scenario 4 – Power BI Integration, Part 2 Microsoft Flow’s interaction with Power BI is not just a one-way ticket. Flow can also feed records into Power BI datasets. Flow’s triggers and actions can track lots of activity that would be otherwise difficult. With premium editions, it can do that tracking in near real-time. For instance, a customer support administrator might track traffic on a support email address with Flow. He can run it through Microsoft Cognitive Services Text Analytics for sentiment analysis or key phrase extraction and deposit the results into a Power BI dashboard. That keeps a running tab on hot support needs. This sort of tracking can be done out of the box with Flow. With some creative thinking, Flow writing to Power BI datasets really enables some interesting applications. Going with the Flow By going to the Flow website, you can see lots of additional ideas and templates for its use. They cover personal to enterprise solutions, and everything in between. Flows can be simple data dumps or include some amount of internal logic. Flow advertises over 200 connectors (with more always coming) to services both cloud-based and on-premises. If you need help going with the Flow, contact BlueGranite. We’re here to help."
"150" "Headlines such as Reuters’ “U.S. job openings at record high; qualified workers scarce” underscore the need for organizations to predict and manage employee flight risk. Many of today’s data-driven companies use predictive analytics as a mission-critical tool to mitigate the high cost of managing employee turnover by first identifying which employees are at risk of leaving the organization. If you are looking to leverage the power of advanced analytics to lower the rate of your organization’s employee turnover, it all starts with the quality of your data.   The relative worth of any good predictive analytics project begins with the data it is fed. A common phrase in the realm of data analysis is “garbage in, garbage out”, however it equally could be “quality in, quality out”. The nature, amount, and validity of the data that a predictive model ingests strongly influences the worth of the model.  Traditionally, predictive employee flight risk models are based entirely on the employee data that is tracked and stored in an organization’s human resource system. This historical data contains a wealth of information relevant to predicting employee turnover. Some of the most commonly used employee information (or from a modeling perspective, predictor variables) includes: Tenure or duration of employment Compensation level or ratio Date of, or time since, last promotion Percent of most recent pay raise Job performance rating/score Distance to/from work Developing your first predictive model of employee turnover can be a confusing and intimidating project. How much data is enough? Do I have the correct type of data? The “start small and think big” approach can address these modeling concerns. Start with the relevant data you have currently and plan for the growth of the model as additional employee information is tracked. The above list of predictor variables is a very good place to start. If your objective is to be able to predict employee turnover, you’ll need to include that employee attribute in your data set as well. That is, you need to know the employment status of each of your employees (i.e. turned-over, yes or no) since this is the outcome, or target variable, you are attempting to predict. Let’s start with these six predictor variables and run a classification analysis. A gradient boosted classification analysis was run on a demo data set consisting of 1,200 observations. Predictive employee turnover models are known as classification analyses because the results classify an employee as either yes or no for flight risk. The subset of data that was set aside for evaluating the model had 37 employees who were no longer employed, i.e. turned over = yes. With this initial set of predictor variables, the overall accuracy was 42% and specific results were:  What to make of these results? To start, the level of accuracy is poor. How should we interpret true/false positive/negative values? Intuitively, a model’s accuracy is an appealing metric by which to evaluate the worth of a model. However, accuracy is not a good evaluation metric when the analyzed data has a target variable that is not equally split (e.g. equal number of yes and no turned-over employees) and nearly all employee turnover data sets will be unequally split. In the case of predicting employee turnover, it would be better to decide which error (false positive or false negative) would be least costly when it comes to expending resources to retain current employees.   False positive: making the error of predicting an employee is likely to turnover when in fact s/he is not looking to leave the company (this may waste retention resources but ultimately the employee does not leave and costs of replacing and retraining are not spent). False negative: making the error of predicting an employee is not a flight risk when in fact s/he is looking to leave is a costly mistake. The employee is not included in any retention efforts and the company loses a valued employee. Let’s increase the quality of the data set by adding new predictor variables to the model to see if the false negative error rate can be lowered and accuracy increased. Additional quality data for predicting employee turnover can include predictor variables such as: Job satisfaction rating/score Satisfaction (rating/score) with the work environment Number of previous jobs/positions held Years with current supervisor/manager Engagement score/rating The same data set was used with 1,200 observations and a gradient boosted classification analysis was run. With this second set of predictor variables, the overall accuracy has increased to 72% and specific results were:  With this expanded set of data points, both metrics were improved upon. The model has a higher overall accuracy and a lower rate of incorrectly predicting an employee has stayed with the company when, in fact, the employee has left. These employee attributes can easily be found in most organizations’ human resource databases. However, additional quality data can be added to the model from external or third-party sources. Such external or third-party data could include: The availability of qualified employees Compensation rate/salary for similar positions with other employers Tenure at previous job(s) Health of the labor market Moreover, changes to how the model is run could be done to improve the ingestion of quality data. For example, if practical, it is recommended that predicted employee turnover models are run individually for different departments, job roles, and/or locations. If there is any reason to believe the factors that may influence an employee’s decision to look for employment elsewhere may differ based on status and/or location, it may well improve the model to run these groups of employees separately. This decision often is weighed against the need for a large data set. Classification analyses are best run with large data sets, i.e. high hundreds, thousands of observations. As the number of predictor variables in a model significantly increase, so too should the number of observations. As such, data sets for predictive employee turnover models often include multiple years of human resource employee information. Once the analysis and metric evaluation tasks are complete, it is time to deploy the model and explore the success of your organization’s retention programs and campaigns. With a model that is strategically implemented with the best quality data available, your predictive employee turnover model should be able to successfully scaffold your organization’s employee retention efforts. Want to learn more? If predicting employee turnover seems baffling, BlueGranite can help. To learn more about how our experts can work with your organization, check out our Employee Retention QuickStart offering, or reach out to us! We’re always happy to help."
"151" "The first post in our Power BI Report Server series spoke about acquiring Power BI Report Server via purchasing SQL Server Enterprise edition with active Software Assurance (SA). This second post in the series will focus on the other route, acquiring Power BI Report Server by purchasing Power BI Premium capacity within the Power BI Service.  Power BI Report Server: A Hybrid Approach with Power BI Premium When you purchase Power BI Premium, you get the best of both worlds when it comes to reporting – Power BI Service and Power BI Report Server. Power BI Service, with dedicated cloud capacity, gives you all the functionality of Power BI and cloud reporting while Report Server allows you to continue to host reports behind your firewall and develop using both Power BI Desktop and traditional SSRS.  There are many reasons for purchasing Power BI Premium. Maybe you’re already using SSRS, and would like to continue, but are ready to shift some reporting to Power BI and the cloud. Maybe there are regulations, laws, or internal policies dictating that some of your data remain on-prem. Maybe you need the dedicated cloud capacity for Power BI, and Report Server is just something that came extra with your purchase. Whatever your situation is, taking a hybrid approach to your company’s BI solution by using both Power BI Service and Report Server is a great way to account for a multitude of reporting scenarios.  When Power BI and Report Server work together, you can host some reports behind your firewall, developing in both Power BI and SSRS, and you can push some reports to the cloud, taking advantage of all that Power BI Service has to offer such as live dashboards, app workspaces, and natural language query. Report Server’s part in your Reporting Solution Often overshadowed in the Power BI stack by its glossier cloud counterpart, Power BI Report Server has its strengths, especially when it comes to addressing security concerns, flexible report development, and cloud integration. Report Server is completely on-premises, giving you total control over security. This is great for organizations that are not looking to implement an entire cloud-based solution, whether that’s due to internal or external policies governing data. Report Server can host both Power BI and SSRS reports, giving you the ability to author reports in either product. Since Report Server is built upon the SSRS framework, it’s also very easy to take existing SSRS reports and migrate them to Report Server. Because of this, any existing SSRS reports that your company relies on can continue to be used in your new solution. As detailed in the first blog post of this series, Power BI and SSRS are unique and sometimes complementary reporting tools, with individual strengths and limitations. Each tool shines in different reporting scenarios. Power BI is great for supporting quick, at-a-glance data analysis while SSRS is great for more granular reports, with intricate levels of detail and numbers.  With sleek, interactive visualizations, Power BI Desktop is an ideal choice for developing reports aimed at providing quick insights and high-level data analysis.  With the ability to create in a freeform style with endless possibilities for customization, SSRS is ideal for reports with a focus on detail and numeric results. Service and Server – Better Together In a hybrid approach, the two platforms and tools work together, but this can look different from organization to organization. Listed below are some scenarios where a hybrid implementation is a great fit. Some industries have data that cannot move to the cloud, due to either internal or external regulation. For example, in a hospital setting, despite Power BI’s HIPPA compliance, reports containing identifiable patient data often need to remain on-premises, but more general, aggregated data about operations or demographics can be pushed to the cloud for use in dashboards and collaborative workspaces. In this scenario, certain data remains on-premises, and some is pushed to the cloud. Often, the finance department of an organization relies on paginated reports, complete with large amounts of fields and functionality best served by SSRS. In this scenario, finance and its data operate on Report Server, while the rest of the organization’s departments can move to Power BI Service. Finance data can still integrate with other data in the cloud for collaboration with other departments, but the reports and the detailed, large datasets that the finance department relies on remain on-premises. In a manufacturing setting, business managers often move from plant to plant, overseeing various processes and meeting with plant managers and employees to discuss production metrics. Deploying Power BI reports and datasets to the cloud allows managers to easily access them on the go, without the hassle of a VPN connection. Other data and reports that do not need to be accessed at the plants can remain on-premises and take full advantage of being able to report in both Power BI and SSRS. Of course, with a VPN connection, on-premises reports and KPIs can still be accessed remotely and on mobile devices when needed. In many organizations, upper management needs to see aggregated, cross-department data in the cloud, so that they can collaborate in app workspaces, take advantage of Q&A and quick insights, and view dashboards. In this scenario, only the datasets and reports that upper management needs are deployed to the cloud, keeping the rest on-premises. This scenario could mean your organization only needs the minimum premium capacity because the majority of your userbase will be leveraging the on-premises Report Server, thus saving you money on capacity costs. These scenarios are just examples. Every organization is different. When thinking about your hybrid solution, keep the strengths and limitations of each reporting tool and platform in mind as you align them to your organization’s needs and requirements. Remember, too, that the Power BI reports initially developed for Report Server can easily be migrated to the Power BI Service if and when you’re ready, but paginated SRSS reports cannot be migrated to Power BI Service. SSRS does not have a PAAS (Platform as a Service) counterpart. Even with this limitation, visualizations from on-premises SSRS reports can be pinned to Power BI Service dashboards. For more information on how to do this, check out this document from Microsoft. Power BI Premium Power BI Premium offers a unified reporting solution, with a clear path to the cloud, and the ability to choose what remains on-premises. If you’ve ever explored Microsoft reporting solutions, you’ve probably seen the below chart before. It’s worth looking at again in the context of developing a hybrid solution, as it’s a great summary of what the offerings include:  The best reporting and BI solutions are adaptable, able to handle known reporting needs, and anticipate new ones. As you can see, Power BI Report Server and Power BI Service (available together with Power BI Premium), give you everything — on-premises and cloud, SSRS and Power BI. Business intelligence and analytics advance at a rapid pace. Having a collaborative cloud and an on-premises solution, coupled with the rapid release cycle of Power BI features, accounts for this, and sets your organization up for continued success. Have questions about Power BI Report Server or just want to learn more? Contact us! We would be happy to help you and your team learn how to take advantage of this great technology."
"152" " {   \"@context\": \"https://schema.org\",   \"@type\": \"BlogPosting\",   \"mainEntityOfPage\": {     \"@type\": \"WebPage\",     \"@id\": \"https://www.blue-granite.com/blog/power-bi-report-server-revisited\"   },   \"headline\": \"POWER BI REPORT SERVER: BENEFITS, HOW TO PURCHASE AND MORE\",   \"description\": \"Our goal is to provide further education on the product by focusing on topics such as: Purchasing Power BI Report Server, How Power BI Report Server modernizes your on-premises BI solution, Hybrid Power BI implementation scenarios (Service & Report Server), Power BI DevOps strategies & tips, “War stories” from an actual large-scale Power BI Report Server implementation.\",   \"image\": {     \"@type\": \"ImageObject\",     \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/Power%20BI%20Report%20Server%20Update.png?width=805&height=509&name=Power%20BI%20Report%20Server%20Update.png\",     \"width\": 696,     \"height\": 696   },   \"author\": {     \"@type\": \"Person\",     \"name\": \"Josh Crittenden\"   },     \"publisher\": {     \"@type\": \"Organization\",     \"name\": \"Blue Granite\",     \"logo\": {       \"@type\": \"ImageObject\",       \"url\": \"https://www.blue-granite.com/hs-fs/hubfs/logo-2.png?width=186&name=logo-2.png\",       \"width\": 600,       \"height\": 60     }   },   \"datePublished\": \"2019-02-27\",   \"dateModified\": \"2019-06-14\" } A modern on-premises BI solution This blog post serves as the first in a series on the topic of Power BI Report Server, an underrated addition to the overall Power BI product family. Last year, we wrote a blog post that introduced the Power BI Report Server. The post was well-received and made it evident that people are looking to learn more about the product.    Our goal is to provide further education on the product by focusing on topics such as: Purchasing Power BI Report Server How Power BI Report Server modernizes your on-premises BI solution Hybrid Power BI implementation scenarios (Service & Report Server) Power BI DevOps strategies & tips “War stories” from an actual large-scale Power BI Report Server implementation POWER BI REPORT SERVER RECAP Even with our introduction to Power BI Report Server blog post, we still received quite a few questions about what Power BI Report Server even is. Our aim with this recap is to put those questions to bed by illustrating key points. WHAT IS POWER BI REPORT SERVER? Figure 1 - SSRS + Power BI Reports = Power BI Report Server! Power BI Report Server is an on-premises product based on the SQL Server Reporting Services (SSRS) framework which allows hosting, sharing, and collaborating on Power BI Reports, all behind your organization’s firewall. The key words in that previous sentence are “on-premises”, “SSRS framework” and “Power BI Reports”. Let’s take a closer look at each of those. ON-PREMISES Unlike the Power BI Service, which is a cloud-based analytics platform hosted by Microsoft, or Power BI Premium, which is simply dedicated cloud capacity within the Power BI Service, Power BI Report Server is a product that is installed and configured on a dedicated on-premises server. Figure 2 - Power BI Report Server is 100% \"in-house\" vs. Power BI Service, which is hosted by Microsoft. SSRS FRAMEWORK As illustrated above, Power BI Report Server is also built atop the SSRS framework, thus allowing you to deploy SSRS reporting assets such as paginated reports, mobile reports, KPIs, and Excel workbooks to your Power BI Report Server. Additionally, this means you can easily migrate an existing SSRS implementation to Power BI Report Server. POWER BI REPORTS In terms of Power BI assets, only Power BI Reports can be deployed to the Power BI Report Server. Power BI assets and functionality such as Dashboards and Q&A are only available in the Power BI Service. Figure 3 - Power BI Report Server only includes Power BI Reports. YEAH, THAT’S GREAT, BUT HOW DO WE PURCHASE POWER BI REPORT SERVER? Unfortunately, Power BI Report Server cannot be purchased directly as a standalone product. There are currently only two ways to acquire Power BI Report Server. The Microsoft preferred way is via purchasing Power BI Premium capacity within the Power BI Service. This will give you the ability to install and configure an on-premises Power BI Report Server to compliment your cloud premium capacity. Acquiring Power BI Report Server via this method sets that stage for a hybrid Power BI implementation, which is a topic we will discuss further in a subsequent blog post. WE HAVE NO INTEREST IN THE CLOUD. WHAT’S THE OTHER WAY TO ACQUIRE POWER BI REPORT SERVER? The other way to acquire Power BI Report Server is by purchasing SQL Server Enterprise edition with active Software Assurance (SA). If you already have SQL Server Enterprise and active SA, you can obtain your license key from the Microsoft Volume Licensing Service Center (VLSC). For more information on SQL Server licensing or Software Assurance benefits, check out Microsoft’s SQL Server licensing guide and datasheet. NOTE: An important purchasing footnote is that although it is an on-premises report server, you must still purchase Power BI Pro licenses for any report authors that will be publishing Power BI reports to your production server(s). Therefore, if you have ten report authors, but only two individuals handle publishing to Production, then only two Power BI Pro licenses are required. This is an honor-based system from an enforcement standpoint.  Figure 4 - Very last footnote on Microsoft's Power BI pricing page. Figure 5 – These are the two ways to acquire Power BI Report Server, as it cannot be purchased directly.  Power BI Report Server is included as part of your purchase of Power BI Premium capacity within the Power BI Service. You may think of it as an “add on” to purchasing Premium capacity. Obtaining Power BI Report Server via this method is optimal for those organizations that are looking for a hybrid (both cloud and on-premises) BI architecture.   Power BI Report Server is included as part of your purchase of SQL Server Enterprise with active Software Assurance (SA). This is currently the ONLY way on-premises customers can obtain Power BI Report Server. Consider this another perk if you choose to purchase SQL Server Enterprise edition over Standard edition.   A MODERN ON-PREMISES BI SOLUTION  Figure 6 – The remainder of this post will focus on acquiring Power BI Report Server via SQL Server Enterprise with active SA. The remainder of this post will focus on organizations that: Want, or need, their BI solution 100% on-premises May have an existing investment in Microsoft BI products such as SSRS, Mobile Reports, and PerformancePoint, but want to see how Power BI fits in May have an existing investment in other vendor BI products such as MicroStrategy, OBIEE, BusinessObjects, Tableau, Qlik, etc., but leverage the SQL Server suite in their back-end architecture Don’t currently have any investment in Microsoft data and analytics, but are “kicking the tires” POWER BI COMPLEMENTS SSRS SSRS is a mature BI product that has been in use for well over a decade now. However, the main gripes against SSRS are that: It’s more static than interactive Report development is not drag-and-drop, therefore development time can be lengthy Report development typically involves an understanding of various querying languages such as SQL, DAX, and MDX Report development is typically the responsibility of IT and not consumers of the report NOTE: We have yet to see an organization use Report Builder to enable their power users to build their own paginated reports.  The beauty of Power BI is that it serves as the answer to all those SSRS gripes, and much more. Power BI reports are highly interactive by default. For example, you can have visuals cross-filter one another by simply clicking on a data point. Power BI Desktop provides an intuitive drag-and-drop interface. Power BI report development can fall to the hands of report consumers. This can free up IT resources to build and maintain corporate BI data sources such as SSAS tabular models or multidimensional cubes. We have had numerous customers ask: “Which product should we use then? SSRS or Power BI?” Our response is always, “Why not both?” If you own Power BI Report Server, why not leverage both? In general, SSRS is a great product to use when you want to produce “show me the numbers” reports. Power BI is a great product to produce highly visual reports to help you spot trends, anomalies, and provide “ah-ha!” moments that are hard to achieve by looking at a table full of numbers. GENERALLY, WHEN TO USE EACH PRODUCT SSRS Power BI  You hear the requirement:“We want to see the numbers”“We want a detailed list of…”“We want to be able to print this report”“We want to export to Excel”   You hear the requirement:“We want a dashboard”“We want charts and graphs”“We want to be able to spot trends and outliers”“We want to build our own reports”  POWER BI COMPLIMENTS THE SQL SERVER SUITE The on-premises SQL Server suite provides an end-to-end solution for all your organization’s BI needs. The SQL Server suite provides a mature set of technologies that cover areas from ETL, MDM, data quality, data warehousing, semantic modeling, and reporting. Power BI Report Server is the cherry on top of it all. What had always been lacking in the on-premises suite was a product that could allow organizations to quickly and easily visualize their data. Lastly, if you’re already leveraging SQL Server components as part of your back-end infrastructure, but use a different vendor product for reporting, Power BI Report Server serves an inexpensive alternative. We’ve worked with customers that had all Microsoft back-end components (SQL Server, SSIS, SSAS, MDS), but used products from vendors such as MicroStrategy, Tableau, and Qlik for their presentation needs. Often, their reasoning for not using Microsoft for their reporting needs was due to a lack of a self-service visualization product. With the introduction of Power BI Report Server, organizations may want to revisit their investments in other BI vendors and see what Power BI has to offer. Figure 7 - An example of a possible end-to-end, on-premises Microsoft SQL Server implementation. Have questions about Power BI Report Server or just want to learn more? Contact us! We would be happy to share more examples of how you can also take advantage of this great technology."
"153" "As features like Drillthrough open the door for Power BI reports to become more interactive – and by extension, potentially more complex - it is important that they remain intuitive to use.  For example, I have been in several meetings where a Power BI report with drillthrough and bookmarking capabilities is used to drive conversation.  As the presenter navigates through the report, she is often asked to go back to the original page to double check her selection for drillthrough or check which bookmark option she selected.  This happens for various reasons, whether it’s because the data doesn’t match what people were expecting (eek!), or because the meeting veered off topic a little and now no one can remember with certainty what was selected. All this clicking back and forth eats up valuable time and reduces productivity. The worst part? It’s easily avoidable – simply add dynamic titles to your drillthrough and bookmarked pages. The suggestion of dynamic titling in Power BI was first introduced to me by a few of my colleagues at BlueGranite, and I was struck not only by its usefulness, but by how easy it is to implement. It’s simple, powerful, and immensely valuable from a user-experience stand point. It’s something every report developer should be able to do, and I’ll show you how in this post.   On the main page, users have the option to drillthrough on Breed or Group from the table, or to click the button to access a report page bookmarked for popular running breeds.  If you drillthrough on Group for “Herding”, you’re taken to a page displaying popular herding breeds (some of my personal favorites), with the title “Herding Group”.  After admiring the adorable herding breeds, you can navigate back to the main page and click the button, “View Popular Breeds for Runners”. This takes users to the Group drillthrough page, bookmarked to display running breeds rather than breeds for a single Group. The title reflects this:  The functionality of the title on this page was accomplished with a simple DAX measure: SELECTEDVALUE  SELECTEDVALUE checks to see if a field has only one value selected, and if so, it displays the value. If not, it displays an alternate result that you set. For this page, I created the following measure: Group Title = SELECTEDVALUE('AKC Top 10 Breeds'[Group Name], \"Popular Dog Breeds for Runners\") In this measure, SELECTEDVALUE checks to see if only one Group Name is selected, and if so, displays it. If not, it displays the default text string, “Popular Dog Breeds for Runners”.  I put that measure in a card visual to create my title.  When the page is filtered to display a single group via drillthrough, it displays the Group Name:  When a user clicks the button to view popular breeds for runners, the page displays breeds spanning multiple groups, so the default title, “Popular Dog Breeds for Runners”, is displayed.  This title improves user-experience by displaying the user’s selection prominently on the page, so she can be confident of her selection and subsequent analysis of data on the page. If you navigate back to the main page again, you can also drillthrough on Breed. This takes you to a page that can only be accessed by drilling on a single breed.   Since only one value from Breed will ever be shown on this page, rather than use a DAX measure to create a dynamic title, I display Breed in a card visual for the title. Because of the functionality of the report, there’s no need for a default value here, and thus no need to create a title measure. This avoids over-engineering the report with extraneous measures, as too many measures can affect performance.  Displaying Breed in a card visual works just fine as a title for this page:  As you can see from the examples, the second part of creating dynamic titles is controlling how the user can interact with the report. For the flow of the report to work correctly, I never want users to access either the Breed or Group drillthrough pages via the tabs. By hiding the pages, I ensure that users can only access these pages by either drillthrough or the button from the main page, and thus ensure that my dynamic titles display accurately and in the intended context.  A dynamic title takes only a few minutes to implement and provides significant value to users during data analysis. Even little details can go a long way, especially when you’re trying to stay on track in a meeting, right? For more help with DAX, or report design in general, please reach out to us here at BlueGranite!"
"154" "From January 31st through February 3rd, Andy Lathrop and I attended rstudio::conf 2018. The conference offered two days of training events and two days of actual conference activities and was a lot of fun. Hosted at the beautiful Manchester Grand Hyatt right on the San Diego harbor, the convention included lots of different talks covering an array of subjects for all skill levels. Couldn't attend? Don't worry, I'll recap the best parts for you.   Next year's conference will be in Austin, Texas, January 15-18, 2019. About RStudio This section is mainly for the non-R readers in our midst. RStudio, located in Boston, is a tech company founded by J.J. Allaire, an acclaimed software engineer and entrepreneur. RStudio Desktop is the company's flagship product and is the most popular R integrated development environment (IDE) software, used by data scientists and statisticians across the planet. It allows for users to write code, see objects in the coding environment, view plots and tables, and more. At the conference, one talk covered the enhancements to the software in version 1.1. Highlights included: Connections tab: Allows the user to connect to, explore, and view data in a variety of databases. Terminal tab: Provides shell integration within the IDE so that users can now execute terminal commands without leaving the program. Object explorer enhancements: Easily navigate deeply nested R data structures such as complex lists and S3 objects. UI enhancements: Two new UI themes are available (a modern, flat theme and a dark theme), as well as Retina-quality icons throughout.  For more information on the 1.1 enhancements, see the RStudio Blog. In addition to RStudio Desktop, the company also has an array of other products, such as: RStudio Server - A browser-based IDE useful for centralizing access and computation across an organization. Shiny and Shiny Server Pro - Software that provides an elegant and powerful web framework for building web applications using R. RStudio Connect -  A new publishing platform to share Shiny applications, R Markdown reports, plumber APIs, dashboards, plots, and more in one convenient place. Many of RStudio's products are open source (AGPL v3), but offer commercial licenses with additional features. Check out the website for more information. All Things Shiny Many of the sessions at the event centered around Shiny, a web application development framework for R. Without any knowledge of HTML or CSS, R programmers can create interactive web pages for analyzing data or showing results. At the conference, there were many sessions around designing amazing Shiny dashboards, using API's with the plumber and Shiny to ingest and return data, and even returning results in real-time of machine learning model training.    I have used Shiny in the past and I must admit that it makes some really nice web apps. So, I'm excited to see the versatility in the platform grow! See the Shiny Gallery for great examples. Keeping it Tidy Another big theme at the conference was the tidyverse. Originally philosophized by Hadley Wickham, RStudio's Chief Scientist, the tidyverse is a set of packages that share an underlying design philosophy, grammar, and data structures. Hadley's one-sentence definition of tidy is \"Every column is variable, every variable is a column, and every row is an observation.\" This attempts to standardize how R functions and packages are written so that each package can work together harmoniously. It was great to see other developers (outside of RStudio) publishing and sharing packages that follow these design practices. I attended multiple sessions where developers were presenting their work and demonstrating the interoperability with the tidyverse library.  Visit the tidyverse website to learn more.  Prior to the actual conference beginning, a couple hundred of us attended a training session called \"Extending the tidyverse\". This session covered some really useful packages for creating a package of your own, testing your packages, and documenting with ease. This really served as a nice walkthrough of the end-to-end process of making a tidy-compliant package in R. Some big takeaways: The usethis package and R Project files make life easy when you want to make a package. This creates all the files necessary for publishing a package such as NAMESPACE, DESCRIPTION, LICENSE, etc. and an .Rproj file for your code.  usethis::create_package(\"~/Documents/mynewpackage\")   Test your code with testthat to make sure your functions give you what you expect. Automatically create test files that correspond to each of your functions. usethis::use_test(\"myfunction\") devtools::test()  The purrr package is nice for handling redundant function needs and you can ensure stuff won't fail. purrr::map(input, function) purrr::safely(function)  We all know documentation is important. With roxygen2, documentation is as easy as using a special comment. #' My header goes here.#'#' Some text will go here...#'#' @param var1 Explanation of var1   For further reading, check out the R Packages book by Hadley Wickham. Deep Learning with R The keynote on day 2 of the conference was given by J.J. Allaire. He and other developers have created the Keras, Estimator, and Core APIs for R, which open the door for deep learning using TensorFlow. For those of you who are unfamiliar, TensorFlow is an open-source machine learning/computational framework (initially developed at Google) and Keras is a high-level interface into many deep-learning frameworks (including TensorFlow, the Microsoft Cognitive Toolkit, and Theano). However, Keras was originally written as a Python library. Using the R Interface to TensorFlow from RStudio, R developers can train complex neural networks and other machine learning models with ease.    Yes, you read that correctly -- R & deep learning in the same sentence? No, R is still a single-threaded, memory/CPU-based language, which typically isn't conducive to deep learning. However, R is a great interfacing language that can connect to other environments like a charm. So, this new package simply operates as the liaison between R and TensorFlow. If you don't have multiple GPU's in your desktop workstation, fear not! The keynote also demonstrated the ease of use by writing the R training script locally and then deploying it out to a cloud platform to use pay-as-you-go GPU's for faster training of complex models. Personally, I find this exciting as it opens up R to be more commonly used in large-scale machine learning projects than ever before. To learn more about the Keras package for R, see the RStudio documentation. It's Really All About Stickers The data science equivalent of collecting Pokémon cards is collecting hex stickers. These are the stickerized versions of the logos for many of our favorite R packages. These stickers often find their home on the laptops of developers as a pseudo-status symbol in the R community. At the conference, it was almost a competition as to who could find and collect the most hex stickers. Even the RStudio employees were making riddles to hint to their sticker hiding places. I'd say I ended up with a pretty decent collection:  Resources Special thanks to Petr Simecek for curating the rstudio::conf 2018 resources. You can view his GitHub repository for all the links here. Want to learn more about how BlueGranite can help solve your organization's advanced analytics challenges? Contact us today!"
"155" "According to the U.S. Census Bureau, 19 percent of the of the population had a disability in 2010. And according to a 2011 Pew Internet & American Life Project, 54 percent of adults living with a disability used the internet (and that number has likely increased in the last few years). To be more inclusive in our services and products, we need to be mindful of accessibility. As accessibility consultant Ian Hamilton explains here to Creative Bloq, accessibility is catering for your whole audience, including those with disabilities. There are four categories of disability that relate to web access: Visual Auditory Motor Cognitive If you are working with a U.S. federal government agency, accessibility isn’t just a “nice to have”. Section 508 of the Rehabilitation Act requires accessibility for both members of the public and federal employees to technologies when developed, procured, maintained, or used by federal agencies. Many states adhere to this standard as well. When compliance imposes an undue burden, agencies may provide individuals with disabilities with the information and data involved by an alternative means of access that makes it usable, but that means additional cost and maintenance. Outside of U.S. government agencies, many organizations have adopted WCAG (Web Content Accessibility Guidelines) standards to ensure accessibility is not overlooked. There are several examples of Power BI reports made available to the public through the Publish To Web or Power BI embedding features. But were they designed in an inclusive way such that everyone can use them? This is especially important if you are creating reports for a government entity or an educational organization, or creating reports targeting the general public as your audience. Today I’d like to focus on creating Power BI reports that work for users who are blind or low vision. About 7.3 million adults in the U.S. had a visual disability in 2015. It may seem strange to think about data visualization for people who have low vision or even complete blindness, but they need information and read web pages just like we do. They just may consume it differently, often navigating without a mouse and using a screen reader for assistance. Although Power BI is a tool we use to deliver data visualizations (as opposed to delivering visuals through custom web pages), there are several steps we can take to make more accessible Power BI reports. The video below shows what it’s currently like to navigate a Power BI report with a keyboard and the JAWS (Jobs Access With Speech) screen reader.                                                        Screen Reader Accessibility in Power BI Video Transcript There are a few current feature gaps to note. Not to worry, as many of these are already in the backlog for the Power BI team, and Power BI accessibility will continue to improve.  Outside of the functionality within a visual to go to the next level in the hierarchy and to expand all down one level, there isn’t much interactivity available to keyboard-only users in Power BI (whether a screen reader is used or not). Users can’t select an item in a slicer or a bar in a bar chart and get updated insights based upon the visual interaction. Here are some tips to help you build a report that works well with a screen reader and keyboard navigation. Make extensive use of alt text. There is a 250-character limit, but do your best to be descriptive. If your report is largely static (not using a scheduled refresh), you can put the conclusion/message the user should take from each visual. Screen readers can’t read textbox contents, so be sure to copy them into the alt text. Don’t rely on visual interactions or filters to show important information. This may mean you have to do small multiples of visuals within a page or multiple report pages with different filter values to communicate the necessary information. You may be able to rely on alt text to describe the summary or conclusion. Images with links and hyperlinks within a table or textbox can’t be selected using the keyboard. This means using images to link to bookmarks is not yet accessible. The hyperlink text can be copied out of a table, but that takes more effort to find it and select it. If possible, use alt text to describe the link so the user can decide if it is worth the effort to navigate to it. Avoid using images or color as the only indicator of a trend or status, unless you can list the trend/status in the alt text. The accessible Show Data table will show in the sort order you have selected for the visual, and there is no way for the user to change it using the keyboard. Be sure to set the sort order purposefully. Tooltips are included in the accessible Show Data table, so feel free to use them to add more information in the same way you do for users who don’t need a screen reader. Avoid auto-playing video or audio as that conflicts with the screen reader. If you must use video or audio, provide it in a way that requires the user to start it rather than stop it. Be sure to format numbers appropriately so screen readers don’t read out a long series of insignificant digits. Avoid the use of lots of decorative shapes and images within your report page that do not relay information to users. The screen reader reads each one. When using shapes and images to call out data points, use the alt text to explain what is being called out. Your experience may vary with custom visuals. The accessible table generated from the visual is not always useful, and not all custom visuals allow tooltips. Power BI reports can be designed in a way that is accessible to screen readers. While there are currently some design trade-offs that must be made to achieve accessibility, the Power BI team will be adding more accessibility features and the experience will only improve from here. If there are accessibility features you’d like to see in Power BI, make sure to add your feature request or vote for existing feature requests on Ideas.PowerBI.com. "
"156" "Conceptually, data virtualization has been around since 1982 when computer scientist Edgar Codd proposed his “data independence objective,” separating the logical and physical aspects of database management. Given its age, data virtualization should be as ubiquitous as computer viruses, which also found their start in 1982. Instead, the programming world adopted the concept, bringing us object-oriented programming, but data virtualization languished in the enterprise data model. Over the past decade, data virtualization has struggled from an implementation standpoint due to its early lackluster performance and functionality. But for industry analysts such as Gartner and Forrester, and experts such as R20/Consultancy Managing Director Rick van der Lans (in a 2017 presentation for The Data Warehouse Institute), the recent evolution of commercial data virtualization products counters the previous stigmas. As of July 2017, Gartner’s Hype Cycle for Data Management shows data virtualization in the Plateau of Productivity along with database Platform as a Service and data quality tools. As mature as this technology sounds, most of our clients have yet to consider software data virtualization, and sometimes confuse it with data federation. Image credit: Gartner Reveals the 2017 Hype Cycle for Data Management. Retrieved from https://www.gartner.com/newsroom/id/3809163  Data federation is one function of data virtualization, but it is limited to only consolidating heterogeneous data sets of source systems. According to a van der Lans whitepaper for Quant ICT Group, data virtualization platforms should provide integration, transformation, abstraction, and encapsulation. Whereas federation provides heterogeneous integration and light transformation, but a very limited abstraction. Today’s data virtualization tools provide additional management of query optimization, metadata management, data quality, distributed transactions, and, through encapsulation, the ability to present multiple interfaces for data service-oriented architectures or third-party integrations. Performance Performance continues to draw skepticism around data virtualization platforms, but there are numerous innovations around query optimization, query pushdown, caching, and distributed architectures. Most of the top data virtualization platforms accommodate multiple methods of performance enhancement to ensure a positive user experience. Functionality Data virtualization takes a lot of integration pain points away by avoiding multiple copies of the logical data and providing complete access to data lineage, from the source to the presentation layer for business intelligence. Additional data points can be added without having to change transformation packages or staging tables. All data presented through the data virtualization software is available through a common SQL interface regardless of the source – whether flat file, Excel, mainframe, relational database management system (RDBMS), online analytical processing (OLAP), big data, or other. Beyond the Hype Gartner says data virtualization is well beyond the hype cycle and Forrester, in The Forrester Wave™: Enterprise Data Virtualization , Q4 2017, agrees, calling it “…critical to every organization in overcoming growing data challenges.” In an earlier role, one of my former CEO’s directives was to make data available and easily accessible. Many of BlueGranite’s clients have developed data strategies around similar philosophies and, from a strategy perspective, data virtualization is a great fit.  If you have evaluated data virtualization in the past, it may be time to re-evaluate. Currently one of the top platforms, Denodo, is available on Microsoft Azure for a free 30-day trial. If you would like to know more about data virtualization, or how to leverage the free trial in Microsoft Azure, we’d love to help. Contact us today!"
"157" "Every day at BlueGranite we see how Digital Transformation revolutionizes the businesses we collaborate with. Leveraging technology to reimagine customer service enriches and enhances organizations. That’s why we’re committed to exploring how companies can reap technology’s rewards. In the first post of this series, we outlined the Digital Transformation concept and introduced its four pillars: Engaging Customers; Empowering Employees; Optimizing Operations; and Transforming Products. Today we’re fully exploring how your organization can identify and shape opportunities through customer engagement. Where to Begin? Whether you’re looking to hold on to the clients you have, or want to bring new ones into the fold, a comprehensive understanding of your current customers is crucial to success. What are their defining characteristics? What are their needs? Why do they prefer your organization over your competition? There are many ways to capture and analyze this information. A customer survey is one of them. Conducted online or by mail (by an internal group, such as the marketing department, or a third party) surveys can be sent to a broad audience on a regular schedule – quarterly or annually – or tailored to a limited audience for a specific reason. Once gathered, this data can confirm or update key demographic attributes, or be shaped into a structure to ease reporting and ad-hoc analysis. But there are drawbacks. Surveys can be labor-intensive and hindered by challenges, including limited data access mechanisms and the structuring and organizing of raw data. And the dataset often only gives a simple “point-in-time” perspective. Without historical context, how do we know if we’re improving or declining in the eyes of our customers? Tackling Survey Challenges A BlueGranite client recently faced similar issues with its quarterly customer survey. A third party administers the survey, then provides the company with a “data dump” of the results in a flat file (CSV). This put the burden of organizing and structuring the data, then manually creating the needed reports, on our client. Due to limited technology, those reports were static in nature, allowed no interaction, and lacked historical trends of past quarters. Fast-forward to today: We partnered to implement a new solution that leverages Microsoft Power BI. It includes an automated integration of new data into a comprehensive dataset that powers interactive dashboards and reports. This greatly reduces the administrative overhead formerly needed to massage data and produce reports, yielding more time for analysis and action. With deeper insight and flexible reporting tools, this organization can better address customer needs to create stronger relationships and improve business results. Study Communication Channels Behavior analysis is another way to better understand your customers. There are many ways to do this – from monitoring social media activity and assessing marketing campaign effectiveness, to analyzing customer care and customer service activities. Many modern organizations use multiple customer care channels to give customers flexibility. These include traditional call centers and email, and more recently, instant messaging or chat options. Companies can study this feedback to enhance their understanding of customers. Who is using the call center versus email versus chat, and why? What communication channels best address which issues? Do certain methods have better outcomes? Uncover Issues, Retain Customers One of our clients is engaged in an initiative to improve insight into its customer care function. This project is broad and deep, requiring complex methods to integrate large volumes of data from multiple systems, and artificial intelligence to parse the data to find patterns and uncover new insights. The solution automatically identifies common customer care topics, and establishes a baseline to facilitate trending of these topics over time. Leveraging this information will help resolve issues more efficiently, leading to higher customer satisfaction rates, and will ultimately improve business outcomes. Our clients’ examples show the value of data and analytics in improving relationships with existing customers, but what if we could use technology to proactively identify customers who might be on the verge of leaving for a competitor? The topic of customer churn has been around for decades – the cell phone carrier wars, as referenced in this Forbes piece, is an often-cited example – but with today’s technology, addressing this problem is much easier and no longer requires a massive upfront investment. Tested Methods Tackle Challenges By understanding common business objectives, like reducing customer churn, we can use proven reference solution templates to discover opportunities and quickly address associated challenges. Microsoft provides partners such as BlueGranite with a variety of these templates. We then use them to help our clients discover the best way to harness data to solve problems and get results, while also providing a stable, proven foundation for building a production-ready solution. A consumer-packaged goods leader recently expressed concerns about potential market share erosion, something on the minds of many major businesses in a tough business climate. We helped this client confront these concerns using a verified customer churn template to find, in stack-ranked order, customers most likely to not reorder. This solution allows their team to proactively communicate with those customers to discover ways to improve relationships – not just maintaining, but growing market share. Work With Us Contact us to learn how our proven methods deepen your understanding of your customer base. Whether you need assistance with Power BI, advanced analytics, or anything in between, we’re always ready to help you uncover new insights. Stay tuned for future posts in the series! Next time, we will explore the benefits of Empowering Employees."
"158" "When the Power BI team released the hotly anticipated Bookmarks functionality in October, users were quickly drawn to the brilliant method of using bookmarks to toggle between two visuals. This concept was demonstrated in a Guy in a Cube video and in the Power BI Desktop Update October 2017 release video.   The basic idea is that the report author overlays two images. Each image has a link to a separate bookmark. When clicked, the \"button\" appears to toggle back and forth between content as various visuals are shown or hidden in a bookmark. For example, the author might create a bookmark for a bar chart and a separate bookmark for a line chart, and then add images linked to the bookmarks to move between them. With proper planning, using images to toggle between two options works well.   What if the author decides to make changes or add other options after putting bookmarks in -- perhaps a third visual with another chart type? One of the main disadvantages of using Power BI's built-in Image functionality to toggle is that the images are static. More user interactions in the future create additional work outside of Power BI, both to find or edit images and then to replace the existing ones in the report. In addition, any time the author adds a new object to the report, they must update all affected bookmarks. As advantageous as bookmarks are to the Power BI storytelling experience, you can see why they require forethought and planning to avoid rework. HTML to the Rescue! An alternate method to using static images is to use Power BI's built-in Shapes. Like images, Power BI allows shapes to link to bookmarks. In this case, transparent shapes are overlaid above a single HTML Viewer custom visual that contains the toggle image. The HTML Viewer accepts a column from the data model as input, so the toggle source is data-driven instead of static. Images or icons can be easily swapped out or added in a data source, rather than the report. If an existing report requires additional changes, this method cuts down (but does not eliminate) the number of editing steps.  How Does It Work? Perhaps a user does not like the look or color of the toggle images and wants to change them. Using static images, this change requires adding the new images in the report and updating all of the bookmarks accordingly. With the HTML Viewer and data-driven images, however, you only need a quick data change and report refresh. In summary, the following components are required to make the solution work: One HTML Viewer custom visual added through the Store Two or more transparent rectangle shapes with links to bookmarks Two or more primary visuals that function as toggle targets, e.g. column chart and line chart Looking at it step by step, add the data to your report in Power BI Desktop, and then build and overlay the two visuals you want to toggle between.  Add another data source containing the HTML data with two or more records you want to switch between. It may help to have an extra field for a label as well. This is a disconnected table with no relationship to the primary report data.   Obtain the HTML Viewer custom visual from the Store and add the HTML column to it. Note that when you add the data to the visual, all toggle images appear.  Add a Filter for the soon-to-be bookmarked image selection (Bar in the example). Position and resize the HTML Viewer visual as needed, and hide that visual's Title.  Add two rectangle shapes and position them next to each other on the toggle. Do not overlay them.  Make the two rectangle shapes transparent by switching Fill to Off and changing the Line Weight to 0pt in the Format Shape options.  Hide the second visual (the line chart in this example) in the Selection pane and then make a bookmark. Hide the first visual and show the second visual in the Selection pane, and then make a second bookmark. Since the shapes were not overlaid, there is no need to hide any of the other supporting objects.   Click on the left shape and add a Link to the first bookmark. Then, click on the right shape and add a Link to the second bookmark. This creates the toggle effect when you click on the \"off\" side of the image once you publish the report to Power BI Service.   If you then update your HTML image source URLs and refresh your Power BI report, there is no additional work needed to alter the two images.   Publish to the Power BI Service and see your bookmarks in action! You can also more easily extend beyond the limitation of a two-image toggle using this method as well. In the following example, three transparent shapes overlay a single HTML Viewer set of icons. You can add as many selections as needed! Note how the color changes so that the currently selected visual is grayed out. Follow BlueGranite on Twitter @BlueGraniteInc and let us know if you would like the sample PBIX file!  As with many things in Power BI, there are multiple ways to accomplish a goal. While the existing image toggle works well, it may sometimes be beneficial to extend the functionality using the more dynamic method outlined above. When creating bookmarks, content authors now have one more tool in their toolkit to help users navigate through a data story."
"159" "To start 2018 off right, we are welcoming the new year with an addition to BlueGranite’s Meet our Team blog series. This time around, we’d like to introduce Solution Consultant Bret Myers – a Michigan State University graduate with a knack for turning data into valuable insights.   With a background in computer science, Bret spent years working as a developer in the Microsoft BI stack for a large medical device manufacturer. He joined the BlueGranite team about three years ago to further his data and analytics talents. When asked what he likes most about working in the industry, Bret said:  “I really enjoy helping clients use their data to uncover information about their business that they might not have had before. The value that this knowledge can bring to an organization, when talking about millions of dollars OR records, is incredible – especially considering sensor and IoT data are growing at exponential rates for many industries.” In addition to the opportunities to work in SQL and to help companies maximize their data, Bret said he also enjoys BlueGranite’s atmosphere. He credits the people, and BlueGranite’s startup culture. Working with what he describes as “incredibly intelligent” team members, engaging in a healthy work-life balance, and having control over his own schedule are the keys to a rewarding career.   Bret said that the emphasis by BlueGranite leadership on exploring, learning, and professional development is encouraging. He explained that to expertly serve clients, consultants strive to be proficient in many technologies across several industries; professional development at BlueGranite has been crucial to doing it successfully. He added that BlueGranite’s continual growth and expansion of service offerings also inspire his ongoing education in the data and analytics field. As for Bret’s 2018 predictions for the business intelligence industry, he’s confident that the growing flood of data – coming from everything from smart phones to water pumps – combined with advanced analytics will continue to allow companies to uncover previously unheard-of insights. Every day in the data and analytics world, we continue to see advances, whether it’s more and more data from smart devices, customer churn data predictions, or even forecasting demand for an organization’s services – all things that couldn’t be easily discovered or measured 10 years ago. While Bret enjoys using the latest technology to solve problems for our clients, he also appreciates an adventure. An outdoorsman at heart, you will likely find him snowboarding, playing guitar, rock climbing, or traveling in his free time. Want to learn more about our team and how BlueGranite can assist your organization in making the most of its data? Reach out to us – we’re always ready to help! "
"160" "As the year comes to an end, we’d like to take a moment to reflect on our appreciation for our clients and partners, our 2017 accomplishments, and to share some of the best of our business insights from the past 12 months. This proved to be a record year for BlueGranite – we’re thankful to our staff, clients and partners for that growth, and for our continued business development. We added new team members, expanded our capabilities, released a new logo, and our staff gave back to communities across the U.S. We’ve also continued to foster strong client relationships through our innovative industry-specific solutions.   Exciting Year-End Highlights This year we added 11 new members to the BlueGranite team. Thanks to our newest associates, we now have a nationwide presence, increasing our ability to regularly connect face-to-face with our clients. In addition to our team expansion, we are pleased to announce that we spurred business growth by 14 percent this year. Fortunately, our progress allows us to give back to our local communities. BlueGranite initiated our Day of Service program this year where we encourage team members to spend a work day in June volunteering for a local nonprofit organization of their choice. Across the country, employees came together to volunteer their time and service to 8 charitable U.S. organizations – rehabilitating housing for families with little to no income; implementing and upgrading tech at a kids’ summer camp; helping support local military personnel and their families; and working to feed the hungry in over 19 counties, as well as those who have trouble getting out of the house or are too weak to prepare food, and bringing a mobile grocery to communities with little access to fresh food. In addition to donating our time in 2017, BlueGranite started the BlueGranite Give Back program: our team members nominate, then vote for, a nonprofit organization to support in December. We chose the Bronson Children’s Hospital this year, one of four Michigan hospitals that do wonderful work helping children and their families recover from serious illnesses. Thanks to our team members’ contributions, and BlueGranite’s donation match, we raised almost $5,000. Our new logo release in October was another exciting BlueGranite first. After 15 years of growth under our former branding, we wanted to visually mark our evolution. Through research, dialogue, and team collaboration, we created a new, up-to-date design that represents BlueGranite’s passion for data, our expertise in building effective solutions, and our delivery of value across organizations. We also expanded our Azure AI and Cognitive Services and Advanced Analytics industry-specific solutions in 2017. Additionally, we drove tremendous growth in our advanced analytics client projects – a trend we look to continue in 2018. 2017’s Top Blog Content Every year our writers create a wealth of valuable blog content – this year proved to be no different. Check out some of our top 2017 posts for a look back at helpful tips and the latest and greatest analytics industry advancements. 5 Useful Data Analysis Expressions (DAX) Functions for Beginners, by Greg FoleyWant to start using DAX functions? We know from experience that it can sometimes be daunting, but it doesn’t have to be. Greg’s tips explain filter, calculate and related functions, and more, help you start strong with DAX. 3 Reasons to Consider Microsoft’s R Tools for Visual Studio, by David EldersveldMicrosoft announced a preview of R Tools for Visual Studio (RTVS) in 2016 – but it left some users confused about when and where you might use RTVS if you are already using R Studio. David does a great job here exploring useful ways to incorporate RTVS under the right circumstances. Introducing Power BI Report Server, by Josh CrittendenMicrosoft made the Power BI Report Server publicly available in June as a part of its Power BI Premium announcement. Josh details Power BI Report Server’s functions, key features, and limitations. ImpoRting and ExpoRting: Getting Data Into and Out of R, by Colby FordColby’s quick demonstration and walkthrough of reading and writing common flat files, importing and exporting data, and working with SAS files is full of tips, tricks, and code that you can try out in R yourself. Four Tips for Using Azure Data Factory to Load Your Data to Azure Data Lake Store, by Meagan LongoriaMoving your data to Azure Data Lake Store with Azure Data Factory? Start with Meagan’s blog post – especially if you are getting into ADF with a SQL Server Integration Services (SSIS) background. What a Year! BlueGranite had a record year in 2017 – and we couldn’t have done it without you! From the BlueGranite team, we thank you for your support and wish you a wonderful holiday season and happy New Year!"
"161" "Microsoft recently announced a new data platform service in Azure built specifically for Apache Spark workloads. Azure Databricks is the fruit of a partnership between Microsoft and Apache Spark powerhouse, Databricks. The service provides a cloud-based environment for data scientists, data engineers and business analysts to perform analysis quickly and interactively, build models and deploy workflows using Apache Spark. Below are some of the key reasons why Azure Databricks is an excellent choice for data science and big data workloads. Reason #1: Speed Anyone familiar with Apache Spark knows that it is fast. It can run up to 100x faster than Hadoop MapReduce when running in-memory, or up to 10x faster when running on-disk. Azure Databricks is even faster! The team at Databricks provides a series of performance enhancements on top of regular Apache Spark. These include caching, indexing and advanced query optimizations. The benchmarking data below, from a recent post by Juliusz Sompolski and Reynold Xin on the Databricks Engineering Blog, shows that these optimizations contribute to a performance increase of up to 8x over other, similar big data SQL platforms. Tack that on to the already 10 to 100x performance gains, and one can see the obvious processing efficiencies this engine provides.  Reason #2: Security Azure Databricks integrates directly with Azure Active Directory (AAD) out of the box, with no custom configuration. This differs greatly from Apache Spark on Azure HDInsight, where AAD integration is a premium feature requiring considerable configuration using Apache Ranger. After creating the Azure Databricks service and initializing the Databricks workspace, users with access can simply go to the workspace URL and log in using their AAD credentials.  Once inside the Databricks workspace, administrative users can navigate to the Admin Console where they can easily add, delete and manage users in the workspace. They can even invite external users (users not in the same AAD) to the workspace, as long as the user belongs to another AAD.  Reason #3: Collaboration Collaboration is the third reason to choose Azure Databricks for data science and data engineering workloads. Azure Databricks provides a platform where data scientists and data engineers can easily share workspaces, clusters and jobs through a single interface. They can also commit their code and artifacts to popular source control tools, like GitHub. Within Azure Databricks, users can spin up clusters, create interactive notebooks and schedule jobs to run those notebooks. Using the Azure Databricks portal, users can then easily share these artifacts with other users. This allows users to create and build models together in the same notebook in real time, to re-use data assets, libraries and compute resources across the same cluster, or to re-use and monitor scheduled jobs.  Data engineers and data scientists that use popular source control tools like GitHub and Bitbucket to manage their code can continue to do so with Azure Databricks. This will allow companies that have adopted enterprise-wide, platform-independent source control processes to continue using their established methods. Azure Databricks makes it easy to link and sync artifacts like notebooks to a Git repository where they can live, even if the Azure Databricks workspace goes away.  Azure Databricks, the exciting new Azure service, helps companies innovate more effectively and efficiently on top of big data. If you are interested in learning more about this service, and how it might fit your company’s data platform, contact us, or check out our Azure Databricks Resources. "
"162" "One of the most exciting tales in modern analytics is the continued maturation of business-led analytics solutions, or Self-Service BI. Tools like Microsoft’s Power BI continue to make leaps and bounds forward, putting powerful and easy-to-use analytics tools into the hands of those who rely on insights from their organization’s data every day, all while helping to straddle traditional obstacles in the self-service space such as solution scalability, collaborative workspaces, and centralized oversight. But what about governance? When, where, and how does the traditional IT or IS department step in?   While a tool like Power BI may help ease some aspects of governance, such as offering usage statistics and the ability to manage published content available to consumers from a single portal, other aspects can remain elusive: when should a data source become an IT-managed asset? When should a self-service data model become an IT-hosted enterprise model? Should they ever? To what extent should we favor “one version of the truth” over the “democratization of data”? Consider, if you will, the English Garden. The English Garden In the early 18th century, a new approach to garden-building was sweeping across the green spaces of Europe, replacing the formally rigid and symmetrical styles of the older French methodologies: the English Garden[1]. Seeking to embrace the natural contours and intrinsic “wildness” of the land, this new approach was a reimagination of the original landscape as an idealized pastoral form of nature, rather than a strict and symmetrical recreation of something apart from it[1]. Often including bodies of water, groves of indigenous trees, and traditional floral plantings, these new gardens were an offering of the best that both nature and traditional formal gardening had to offer. To set up the analogy then, let’s think of an organization with no IT management of analytics artifacts (data sources, data models, or reports) as being like a natural landscape. While abundant, it is also untamed. Not just anything can grow there, because there is no higher governance than nature’s own rules – survival of the fittest. Conversely, consider an organization in which IT policy governs and locks down all data sources, models, and reports. We can think of this as being like the old French-style symmetrical gardens. While there is an appeal (and a few advantages) to the rigid perfection, there is also an “unnatural” quality to it, as it has effectively replaced the land it grows on with an inflexible vision of what should grow there – survival by design. A Traditional Give-and-Take For a long time, IT’s control of analytics assets not only made perfect sense – allowing for focused and accountable application of data quality and cleanliness standards, security requirements, and captured business logic – it was also nearly essential. The tools required to clean, combine, transform, and centrally store data needed large amounts of computing power and very specific skill sets in order to be effective. So too with building and distributing reports, where knowledge of esoteric data-centric languages like SQL and MDX (multi-dimensional expression language) were necessary prerequisites to report design, not to mention the inherent complexity of the report-building tools themselves. These advantages and necessities came to present a metaphorical chasm within organizations everywhere, between those with the resources and skillsets to curate the data on one side, and those who need to make use of that data every day in order to do their jobs, on the other. As organizations began to create and retain more data than ever, this chasm grew wider and deeper. Greater volume, variety, and velocity of data, along with the attendant use-cases for their efficacy translated to significant demands for even the best staffed and equipped IT department. This then became our traditional garden: highly symmetrical, well-tended, and quite restrictive; often becoming quite independent of the surrounding landscape. It was into this gap that Self-Service BI sought a niche, eroding some of the traditional necessities that once made centralized IT control of data and analytics a foregone conclusion. Power BI in particular represents one of the most thorough realizations of this vision available, bringing an intuitive yet robust platform for report design and data acquisition, and the vast capabilities of cloud-based computing, to make manageable what were once formidably large and complex data sets. This “democratization of data” meant that those who most needed to make use of it had the means to potentially do so, without the risk of losing requirements in translating them to an IT department, or denial of service due to a lack of resources. Yet while some of the hard requirements to making use of data diminish with a self-service tool, what of the benefits that IT-governed data confer? Indeed, without some manner of overarching vision, the “democratization of data” can quickly begin to resemble a wild and overgrown landscape of competing and overlapping initiatives. Important standards around data quality and security can become nigh impossible to apply uniformly, let alone audit. In understanding the strengths and weaknesses of each platform, we can begin to tame the wilderness and build a better garden. Visions of Harmony Just as an English garden seeks to incorporate the natural contours and features of the surrounding landscape, so should a good analytics practice. The traditional give and take of enterprise, or IT-managed, BI and self-service BI need not be at odds, but rather can work together to form a more natural and complete practice. From the perspective of IT, introducing self-service BI to the organization can serve as a path-finder to data sources, use cases, and business logic not yet considered or curated within the enterprise. Seeing captured data directly and the logic employed is often far superior to slogging through hours and hours of abstract interviews, all while risking that something could get lost in translation. Additionally, self-service BI can help drive adoption of existing enterprise data assets by shining a light on the benefits conferred by such. There’s nothing like experiencing first-hand the challenges and rewards to data acquisition and modeling for opening the door to an appreciation of those assets at enterprise scale. From an organizational perspective, self-service BI helps introduce a common language by which analytics needs can be clarified and met. Without some degree of governance, the importance of having “one version of the truth” – or a common set of data and logic across all relevant areas of the organization – becomes immediately and obviously important. In this way, the introduction of governance from IT becomes a welcome means to taming the wilderness, rather than an imposition of abstract laws and requirements that can often seem completely out of sync with the surrounding environment. Additionally, IT skillsets that once represented an inescapable barrier to entry for report development can now present a route toward the useful enrichment of a business-led analytics initiative, rather than act as gatekeepers to it. If you are looking for more information or need help finding the right solution for your BI and governance needs, contact us! [1] English landscape garden. (2017, October 29). In Wikipedia, The Free Encyclopedia. Retrieved 15:47, December 14, 2017, from https://en.wikipedia.org/w/index.php?title=English_landscape_garden&oldid=807708902"
"163" "While BlueGranite offers a variety of technical training courses, one of our most popular continues to be a multi-day, onsite Microsoft Power BI exploration course. Over the past few years, Microsoft has transformed both self-service and enterprise analytics through its Power BI platform. But due to frequent updates, the pace of innovation can be difficult to manage. That's why we keep our training material current. Our team of experts created the content, and keeping it up to date is our priority. We believe so strongly in its merits, we even use the course for internal training.   With a wealth of outside Power BI resources available, why do so many businesses take advantage of our our traditional, onsite, classroom-style training?  Self-directed learning has its benefits. It's convenient to use books and online courses to try to keep technical skills up to date. But nothing replaces the strong benefits of taking a formal course, attending an event with others, or working under the mentorship of a more seasoned professional. What You Don't Know Can Hurt You In a study and article in 1999, Justin Kruger and David Dunning explored the “tendency of the average person to believe he or she is above average”1. Self-perception is complicated. Individuals tend to think that they know more than they truly know, and this reflects in how they assess their abilities, a phenomenon now known as the Dunning-Kruger Effect. Too many employees working under the assumption that they are already experts, or those who do not position themselves to benefit from outside guidance, can be detrimental to an organization. However, an experienced person typically recognizes gaps accurately and strives to overcome them, rather than plateauing in their skills under the mistaken belief that they already know all they need to know. Steps to Success How can you prevent bad outcomes while your team learns Power BI? Even in the age of online courses and so many self-directed resources, in-person training sessions with peers still offer a variety of benefits over other modes. 1) Cater the Experience to Your Team An onsite instructor can more easily adapt to the needs of an audience. That might involve providing more depth on certain concepts that are of particular interest to you, answering questions as they are asked, giving immediate feedback and guidance, or putting you in position to begin combining Power BI with your own data and unique business needs. 2) Quality of Work With more Power BI exposure, it becomes easier to distinguish a good data model from a bad one, or a well-designed report from a quick collection of visuals. From the outset, we expose you to content created by professionals well-versed in data visualization and data modeling best practices, who fully explain the reasons behind specific implementations. 3) Explore Your Knowledge Gaps Through exposure to a variety of concepts and demonstrations, you will be able to better understand what areas you can focus on going forward as you gain experience with Power BI. No one is an expert in every single facet of Power BI. Rather than treating these gaps as weaknesses, they are opportunities for future learning. 4) Collaboration On-site classes also offer the valuable advantage of peer collaboration. Your organization can reap rich benefits from student questions, answers and dialogue - gains likely unacheivable with self-directed learning. You can even use class time to discuss specific business requirements with the other attendees, ultimately using your own data to build reports in class that matter to you. Our Trainers Are Practitioners In addition to some of the general benefits, BlueGranite’s instructors primarily work in the field. We are not full-time trainers. We come from diverse technology backgrounds, but we all have one thing in common: we regularly implement Power BI and other Microsoft technologies on client projects ourselves. Our real-world experience enhances BlueGranite’s Power BI training course. Most trainers also have a background in working with Microsoft Power BI precursor technologies, and have deep data modeling, DAX language, and related proficiencies, by virtue of applying them in the field for years. We Recognize the Audience We also recognize that there are many user types, all interacting with Power BI in different ways. Some will gradually become experts at creating content, while others will only consume reports. Some may go deep into data preparation or modeling, while others only want to view insights. Some may be daily users, while others may only occasionally use it. Training a broad audience like this can be difficult, but our content approaches different types of users in different scenarios. We provide a broad survey on the first day of training but do not shy away from advanced concepts as the course progresses. We draw a distinction between an audience of Consumers, versus Power Users, and cater the content appropriately. Bring Your Own Data Finally, BlueGranite’s training often finishes with something we call “Bring Your Own Data”. After your team has spent time learning concepts with a sample dataset, you can apply much of what you have learned to building reports that directly matter, and often have immediate impact on your business. Okay, Let's Get Started By coaching you through the Power BI ecosystem, exposing you to the broad landscape, and then helping you to explore any gaps in knowledge, BlueGranite helps your team quickly progress in Power BI proficiency. Rather than suffering the potential consequences of mistaken assumptions about existing skills, or slowing progress through self-directed learning, many organizations choose to gain proven Power BI proficiency from our team. You can too. Interested in learning more about how BlueGranite can help your organization better take advantage of Power BI? See the Power BI Training page on our website. Our training is also listed on Microsoft AppSource. 1. Justin Kruger and David Dunning. \"Unskilled and Unaware of It: How Difficulties in Recognizing One's Own Incompetence Lead to Self-Inflated Assessments,\" Journal of Personality and Social Psychology 77, no. 6 (1999): 1121-1134."
"164" "It is important to understand your audience to effectively communicate your ideas and information during a speech or presentation. Building effective and powerful dashboards for an audience (or a group of report consumers) requires an approach similar to giving a speech since dashboards are also a way to communicate data. Knowing this – have you ever taken the time to learn about your audience's communication styles?  There are countless ways to classify communication styles. To narrow things down, we’ll use part of the Myers-Briggs Type Indicator. The four Type Indicators we will explore for designing dashboards are Sensing, Intuition, Thinking, and Feeling [1]. Here's a quick overview of how each type of communicator generally consumes a dashboard: IntuitionIntuitive users are generally looking for the bigger picture. This makes them an ideal group of dashboard consumers as they prefer overviews rather than extensive detailed reports. When presenting data to the Intuitive user, provide the summary levels first and include additional details through click-throughs or expanding actions. SensingIn contrast with the Intuitive users, people who are Sensors tend to like to see the complete picture. Instead of presenting at a summary level, try providing basic components that will show a complete context underneath the summary information. This will save Sensor users from clicking down into the details to validate and orient themselves each time they view a dashboard. ThinkingAnother set of the Myers-Briggs Type Indicators are the Thinker and Feeler. Thinkers, commonly known for taking an analytical approach, focus on the data, numbers, and facts. When Thinkers express information, it is in the form of specific data values. If having to choose between displaying the value or an indicator, you would be best to show the specific value. Sometimes, you can spot Thinkers according to their requests for data labels within chart and graph elements. FeelingThe opposite of the Thinker is the Feeler who has a more personal communication style. Instead of asking “What does the data say?”, feelings resonate more with this type. When designing for this type, use simple indicators to interpret business rules and convey data, rather than overwhelming a dashboard with numbers. When a Feeler sees data labels on charts and graphs, they will typically request the data labels be turned off to declutter and simplify the dashboard. Communication Style Examples Let's compare how these communication styles impact a sample dashboard design using Microsoft Power BI. In this example, we have survey ratings from customers for a chain of restaurants. Each survey is composed of six questions asking the customer to rate their experience on a scale of 1 to 5. To compare the communications styles, we will answer a simple question: “What are the top restaurant locations?” Thinking and Feeling types are easy to contrast as Thinkers want to see the numbers and Feelers want to see an indicator. On the left chart below (Figure 1) you will see the list of restaurant locations sorted by their overall average survey rating. This is okay for a Thinker, but a Feeler will desire a visual indicator as shown on the right chart below (Figure 2). As Figure 2 only shows stars, Thinkers will still be wondering what the exact rating is.   Figure 1 - Thinker Figure 2 - Feeler So what happens when you have an audience that is mixed? One solution is to find a compromise that satisfies both communication styles. In this case, the compromise is to combine the two and show the values alongside the stars as shown below (Figure 3) while not overwhelming the dashboard with a wall of numbers. Figure 3 - Thinker / Feeler compromise and Intuitive Intuition and Sensing types contrast in the level of detail they prefer to see. Here, the overall rating is composed of six different ratings. The Intuitive user will generally be content with the overall rating as shown in Figure 3, above, and will click through to more details on only the locations that are of interest or concern. Sensors will appreciate the chart below (Figure 4) that includes the six ratings that compose the overall rating. Figure 4 - Sensor Minding Your Audience in Dashboard Design You will most likely have a mixed audience outside of a dashboard built for a single user. In these cases, consider narrowing the audience of the dashboard to key individuals, finding an acceptable compromise, or dividing the dashboard into more focused topics that will reduce the mix of audience. Another area to keep in mind is that these communication styles are a person’s primary tendencies which can relate to the individual strongly or moderately. Individuals may not have a preference between Thinking/Feeling or Sensing/Intuitive. Also, these communication styles are not the only design they can comprehend since they only indicate preferences. These preferences should be considered when it enhances (rather than dilutes) the dashboard to effectively answer a question and prompt action. Understanding various communication styles enables you to refine your dashboards enhancing their effectiveness. Designing a dashboard will become less about your preferences and more about those of the audience. In addition, when you receive feedback, you can use it as an opportunity to understand why a user is requesting changes to a dashboard you designed in a specific way. Whether you are looking to develop new dashboards or enhance current ones that are difficult to consume, BlueGranite is here to help, train, and mentor. For more information and to learn more about how we can work with you and your organization, contact us! References:[1] The Myers & Briggs Foundation. (2014). MTBI Basics. Retrieved December 4, 2017, from http://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/ "
"165" "Predictive maintenance could reduce maintenance costs 10-40%, reduce downtime by 50%, and lower equipment and capital investment by 3-5% by extending machine life – McKinsey[1]  In business, few people like surprises. Wall Street can punish a company’s stock price if the numbers are not in line with estimates. Shocks in supplier or customer behavior can derail the best strategic plans. In manufacturing and other industries that depend on reliable machine performance, there may not be anything more disruptive than unexpected breakdowns. They can create expensive, emergency actions like rush delivery, overtime, or acting under dangerous conditions to restore normal operations.     What would it mean to your business if you could predict the probability of failures before they happen, with enough notice to prevent them? Predictive maintenance solutions offer the ability to assess the performance of in-service equipment, identify patterns preceding failure, and automatically deliver alerts to responsible parties. This allows organizations to save time and money with advanced warning signs about maintenance needs, and preemptive equipment repair. Predictive maintenance modeling techniques also offer a buffet of interesting predictions to choose from, customized to your needs. For example, if it usually takes 2 days to schedule a repair crew once a failure occurs, you might configure your solution to alert you of a pending breakdown 3 or 4 days in advance to achieve zero downtime. Even better, you can predict not only the probability of failure, but also which root cause (i.e. particular component) is most likely within a future time period. This helps manage both inventory and crew costs. Pretty cool!  Here’s a glimpse of some of the expected business value of predictive maintenance: Proactively manage maintenance needs, rather than waiting to react to the problem until equipment fails. Reduce unscheduled downtime, waste, and rework. Decrease supply chain costs by optimizing inventory. Reduce holding and shipping costs for spare parts and rush orders.  Better utilize and optimize your existing assets and resources – people, technology and equipment – to avoid costly disruptions in operations.  As the old saying goes, “If this was easy, everyone would be doing it.” Predictive maintenance is a modern solution that performs best with modern data and analytics tools. A key aspect of effective predictive maintenance solutions is combining connected/IoT-enabled devices with advanced analytics to translate data from physical assets to insights from predictive models. And the more data, the better: failure and error data, general machine and operator information, and repair history all contribute to more accurate predictions. Given these considerations, here are some of the key challenges to a useful predictive maintenance solution: Business-focused objectives and data: predicting an outcome for which you can take action; if you only have data that captures which day a failure occurred and it’s important to predict which hour it will occur, you have a problem Complexity and diversity of data: combining at-rest (e.g. maintenance history) and in-motion (e.g. failures and errors from telemetry) data into a single source connected to an analysis system Data and analytic pipelines: storing, scheduling, moving, and creating useful data sets for analysis Data science: considering multiple operational factors simultaneously for predictions, not just static thresholds of single measures Delivering insights: bringing the right information to the right person at the right time, especially with real-time streams    What’s the best way to deal with these challenges? I discussed some solution options in a recent webinar, but here’s a summary. Assuming you have clearly defined business objectives, use a modern data platform that’s equipped to handle streaming and Big Data sets and is agile when it comes to moving that data around, combining it, and using it in machine learning models. With the Microsoft Azure IoT Suite and Cortana Analytics Suite, you can connect and monitor your devices and analyze their data in real time. These analytic technologies, along with data platform tools like Azure Data Lake, work seamlessly together in both development and production to construct a predictive maintenance workflow, regardless of data size or complexity. You’ll also want a data scientist who is familiar with the particulars of predictive maintenance modeling. Microsoft has provided an excellent primer here. Finally, a dashboard tool like Power BI provides interactive reporting of predictive results to the people who can take action. As a Microsoft partner, BlueGranite is well-positioned to deliver predictive maintenance solutions using the Azure platform. Get started quickly with a predictive maintenance preconfigured solution during a 3- to 4-week proof-of-concept engagement. For more details and to learn more, check out our Predictive Maintenance offer page. References: McKinsey, “The Internet of Things: Mapping the Value Beyond the Hype” Microsoft, “Creating Business Value with Predictive Maintenance.\" "
"166" "In the world of data and analytics, the BlueGranite team has a lot to be thankful for. Whether we are working to support relationships with our partners (Microsoft, Hortonworks, and most recently, Profisee), our clients, or our team, we are constantly learning and growing as an organization. As the holidays and the end of 2017 draw near, we find it is a great time to reflect on recent successes and express thanks for our industry, our community, and our company.   We decided the best way to show our thanks would be to ask around and get feedback from our experts. Here are some fun and interesting insights from the BlueGranite team on what they are most thankful for: Matt Mace, Founder/CEOI’m thankful for a lot at BlueGranite – our team, who could work for any organization in the world but chooses to work here, contributing their best; our clients, who give us the opportunity to utilize our talents and passions; and our partners, who believe in us and our ability to help them succeed. I’m especially thankful for all of the people who have contributed to our company over the years, helping us grow and develop into who we are today.  Melissa Coates, Solution ArchitectI am thankful for working at BlueGranite because it's a very supportive environment which provides me challenging projects and learning opportunities. My colleagues are smart, helpful, and inspirational to be around. And, I work from my home office the vast majority of the time which helps me be balanced, healthy, and happy (my dog is pretty pleased too because she gets a walk whenever I need a brain break).  Robert Hutchison, Senior Solution ConsultantOver this past year I’ve had a great deal to be thankful for, but people top the list. I am thankful for the mentors in my life who are willing to share their time and wisdom, family and friends who love unconditionally, colleagues who work diligently and constantly teach me something new each day, and clients who make delivering insights from data exciting.  Leo Furlong, PrincipalI am thankful for the opportunity to work on challenging and innovative projects where I get to add immediate ROI for my customers. I am also thankful for workplace football analogies that seem to never fully resonate with my IT coworkers. #FootballAnalogyFail  TJ Polak, Senior Solutions ConsultantI'm thankful for the ability to be a part of the team at BlueGranite and to work for an organization that truly values its employees. I am lucky to work alongside some of the brightest and most talented consultants in the industry, who both inspire and challenge me to do my best every day, and to continue to learn new things.  Mike Cornell, Senior Solutions ConsultantI am thankful for doing big data engineering with Apache Spark in Azure. Lots of new functionality, tools, and adoption this year for these platforms make it an exciting and challenging space to be in.  Although we have included technology advancements, our organization’s culture, and our team – we are most thankful for you! There is a lot to appreciate in this company, but our collaborators, clients, and partners help us continue to grow and provide the latest data and analytics news and updates. We’re looking forward to seeing what the rest of 2017 and months following bring our way! If you want to learn more about BlueGranite and keep up on what’s happening in the data and analytics industry, subscribe to our blog! If you’ve got some additional questions or are looking to chat with an expert, please contact us and we’ll be happy to help."
"167" " Recently, Microsoft and Databricks made an exciting announcement around their partnership that will soon result in a cloud-based, managed Spark service on Azure. Currently, some select customers are allowed into a \"private preview\" mode of the service, and over the next few weeks, a \"gated public preview\" will ensue for around 150 clients. In January 2018, the service will be available for everyone to try. While the full details are not known about the partnership or full features of the platform, here is how Azure Databricks will likely enhance your Big Data capabilities in the cloud.    +   What is Databricks? Databricks is a company that was started by the team that originally created Spark at UC Berkeley. They have created a Unified Analytics Platform that aims to be the single system for everything from analytics workflows to Spark integration to security. Databricks boasts various benefits of their Unified Analytics Platform such as: UNIFY ANALYTICS WITH APACHE SPARK - Eliminate the need for disparate tools. STREAMLINE ANALYTIC WORKFLOWS - Reduce deployment time to minutes. INCREASE PRODUCTIVITY OF DATA SCIENCE TEAMS - With Databricks, they’ll be 5x more productive. REDUCE RISK - Enable innovation with out-of-the-box enterprise security and compliance. [Source]  Apache® Spark™ on Databricks is said to have a 5x performance gain over that of the open-source version.  Looking at the Databricks' Feature Comparison page, there are quite a few features that could likely make it into the Azure version in the near future.  CLOUD OPTIMIZATION: Tuned Apache® Spark™ clusters High availability for Spark Streaming Built-in file system COST MANAGEMENT: Autoscaling Apache Spark clusters Multi-user cluster sharing BUILT-IN EXPLORATION TOOLS: Notebooks with real-time collaboration + revision history Publish notebooks as production dashboards   BUILT-IN PRODUCTION TOOLS: Spark job monitoring alerts One-click deployment from notebooks to Spark Jobs APIs to build workflows in notebooks SECURITY: Access control for clusters and notebooks Permission-based job and workflow execution Authenticated SQL server  Expect a Familiar Azure Experience Azure already has a managed Hadoop™ offering known as HDInsight. You can spin up a custom HDInsight cluster with your specifications from the Portal. Then, you pay for the time that you have your cluster running. Support for HDInsight is provided by the Microsoft Azure support team. As for Azure Databricks, the experience will be very similar. Simply spin up an Azure Databricks cluster directly from the Portal and Azure will do the setup work for you. No licensing is required other than your Azure subscription. For support, Microsoft and Databricks will have a seamless system for users to get help with their individual needs. Since the service is within Azure, you will go through Microsoft for support, which will now be fully integrated with the Databricks expert support team. Want to learn more about how you can take advantage of this exciting announcement at your organization? Contact BlueGranite!"
"168" " BlueGranite is pleased to announce that it is now a Systems Integrator with Profisee to provide our clients with powerful Master Data Management (MDM) solutions. Profisee is a leader in the MDM technology space and is positioned as a rising niche player in the 2017 Gartner Magic Quadrant. The Profisee platform is a cloud and on-premises SQL Server based MDM solution that extends Microsoft’s out of the box Master Data Services (MDS) product to include enterprise MDM capabilities like: Data Stewardship and Governance Golden Record Management (GRM) Data standardization and enrichment via integration with third party providers like Melissa, Loqate, Bing, and Google Real-time bi-directional data integration Event Management Enterprise Workflow Why Partner with Profisee? BlueGranite is a trusted advisor for our clients around Microsoft’s Data Platform and Artificial Intelligence solutions in Azure and on-premises. Microsoft provides innovative technology for enabling BlueGranite to create powerful business solutions for our customers. Though Microsoft is a leader in the Corporate and Self-Service Business Intelligence (BI), Big Data, Analytics and Visualization, Machine Learning, and Internet of Things (IoT) software categories, there isn’t an enterprise offering that cleanses, enriches, and standardizes organizational master data. SQL Server’s MDS product provides some capabilities in this area, but the scope and scale of this tool is limited. BlueGranite has closed this feature gap with our Profisee partnership. By adding Profisee to our Microsoft toolset, BlueGranite can create better business solutions for our customers. Profisee’s powerful capabilities, coupled with SQL Server MDS, provide a true enterprise MDM solution. Common Use Cases for Customer and Product Data Two common MDM use cases are for the improvement of customer and product data. A description of the business needs and value created for these use cases is below. Cleaning Customer Data to Maximize Lifetime Value Cultivating rich and long-lasting relationships with customers is key to most organizational strategies. Successful organizations recognize that keeping existing customers is cheaper than finding new customers, and that increasing overall Customer Lifetime Value by even small percentages adds significantly to the bottom-line. This is proven by the popularity of programs like 360o View of the Customer, Customer Churn Reduction, “Customers who bought this also liked” Recommendation Engines, Inbound Marketing, or even Social Media Outreach programs. While the importance of customer data isn’t usually disputed, many organizations have issues with their customer data and don’t spend materially to fix the problem the right way. Some common issues are: Duplication between different software platforms and those that aren’t fully integrated (CRM, ERP, Billing, Support, etc.) Duplication due to churn and reacquisition of the customer or name/address changes Duplication due to mergers or acquisitions of other businesses Poor data quality including addresses, categorization, missing information, or free form text A lack of data ownership and process maturity for ensuring data quality No capability to enrich customer data from outside data sources The good news is that these issues are curable. BlueGranite, using the Profisee Platform, can create solutions to solve these problems. Data from source systems can be brought into Profisee to be cleansed and enriched with industry leading third party web services. Through this process, customer identities are verified, and names are corrected and standardized. In addition, customer addresses are also cleansed, corrected, and standardized. In many cases, the customer data is also enriched with missing information like latitude and longitude, email, phone number, area classifications, or demographic data. After cleansing, the data from different sources can be merged and matched with rich and high-performing fuzzy matching algorithms to remove duplication for creating consolidated customer master records (also known as golden records) that can be used for analytics (Business Intelligence, Visualization, and Artificial Intelligence) and the popular business programs mentioned above. Profisee also supports creating business rules on the customer data, enables user notifications, provides high-fidelity tools to interact with and edit customer data, and finally synchronizes the data back to the source system. True Product Margin with a Consolidated Product Master “How many products did I sell, and what is my product margin?” These are hard questions to answer for companies operating multiple ERP systems across several business units. Whether you’ve grown by acquisition or your company simply runs different software platforms, creating a consolidated Product/Item Master is an important task for measuring product performance and is usually a challenge for many organizations. Different ERP systems often have different item numbers, descriptions, SKUs, and configurations, and some do not keep track of vendor part numbers. Trying to consolidate/map them all for the first time can be daunting. An additional challenge exists in setting up the business processes for maintaining the consolidation over time and synchronizing the data across systems. If you’re facing any of these challenges, BlueGranite can help. Using the Profisee platform, the data from different ERPs can be merged and matched with rich and high-performing fuzzy matching algorithms to link the same products between systems. Once the products are linked, rules can be implemented on the data to ensure that critical information is defined for each product. To make sure that the Product Master doesn’t get disconnected after your initial consolidation, Profisee provides real-time integration capabilities that keep all systems synchronized and clean. From a business process perspective, Profisee also provides the ability to create application workflows so that an approval process can be introduced to create new products for the organization. With your consolidated Product Master in place, you can create meaningful, high-value analytics on product popularity and profitability, or even create machine learning solutions like demand forecasting, inventory stock-out prediction, or a product recommendation engine. We are excited about our new Profisee partnership and the added value we can bring to our customers using this software. Profisee’s powerful capabilities, coupled with SQL Server MDS, provide a true enterprise MDM solution. Contact BlueGranite today to learn more about how to clean your customer, product, or other master data using the Profisee platform."
"169" "In October of this year, Power BI introduced an exciting new preview feature: bookmarks. Bookmarks, and the selection pane introduced alongside them, provide a solution to what was previously one of Power BI’s biggest pain points: the inability to configure and save specific, individualized views of report pages. Now, views of pages configured by slicers, filters, sort directions, and the new show or hide options in the selection pane can easily be saved as bookmarks.   Before we get started, I recommend checking out the Power BI report highlighted throughout this post online using Power BI Service, or trying the embedded version below. When a report like this one is viewed online, especially in full screen mode (using the bottom right corner), it becomes an immersive, intuitive, app-like experience.  Note: Since bookmarking is still in beta, bookmarks cannot be created in the service, but they can be used there. Bookmarks in Power BI differ from those in Tableau in both behavior and general use. In Tableau, bookmarks are copies of configured pages that can be imported and exported from workbook to workbook. Unlike Power BI, if the original changes, for example by having a new measure or chart added to it, the bookmark will not reflect that change. Once a bookmark is created, it is no longer linked the original. In Power BI, the two are linked, and live in the same report, so any change to the original is reflected in the bookmark and vice versa. We'll explore this functionality, its advantages, and its uses further below. These new features not only deliver some of the most requested functionality in Power BI, but also give developers new ways to produce creative and interactive reports. Bookmarked pages and the selection pane (which gives developers the ability to show or hide visualizations; see Figures 1c and 3b below) open the door for developers to create Power BI reports that are app-like in nature. This blog post will showcase a basic example of how to create a report that looks and feels more like an app. Before diving into development, bookmarks need to be enabled in Power BI Desktop. To enable bookmarks (or any preview feature), navigate to “Preview Features” in the Options dialog box. Check the box next to “Bookmarks.” You will be prompted to restart Power BI Desktop to finish the process. Figure 1a Once Power BI Desktop restarts, you can utilize bookmarks and the selection pane by selecting them in the View ribbon at the top of the screen. Figure 1b You will now see the Selection pane and Bookmarks pane to the right, next to the Visualizations and Fields panes that you’re used to.  Figure 1c Maybe it was word association with “bookmarks”, or the fact that people frequently ask me for book recommendations, but for this example, I created “What Should I Read Next,” a Power BI report designed to guide readers through the process of picking their next book. Using data from Goodreads.com and HowLongToReadThis.com, simple visualizations, and the new bookmarks and selection features, I created a report that behaves, looks, and feels more like a custom application than a standard Power BI report.  A link to this report is available toward the end of this post. The report has four main components that interact via bookmarks: the welcome page, the library page, a table view page, and individual book pages. For the sake of brevity, I’m not going to go into detail on how I developed every part of the report, I’m only going to cover the portions that relate to the new features, bookmarking and the selection pane, and how I achieved the app-like look and feel. The first screen users see is a welcome page, much like you would see in any application. I added text to give users direction, created “buttons” by combining text with simple, empty rectangle shapes, and added a clipart image of books. Figure 2a Users can click any of the buttons or the books to navigate to different, bookmarked versions of the library page: Figure 2b The library page is like the welcome page in that it uses images and shapes to link to other bookmarked report pages. When clicked, each book in the library will take users to a page for that book, complete with a synopsis, rating, food and drink pairing, external links, and a link back to the library page: Figure 2c The library page also has buttons at the bottom that will take users back to either the welcome page, or to a page with all the book data in a table: Figure 2d This page is bookmarked to appear sorted by number of pages. It also has the stacked book image that appears throughout this report, and when clicked, takes users back to the library. So how do you set up bookmarks to create this type of report navigation? Easy! To demonstrate, let’s walk through the creation of the historical fiction bookmark together. When I click, “Historical Fiction” on the welcome page, it takes me to this bookmarked view of the library page (the original version is shown in Figure 2b). Note: When developing in desktop, you need to hold down ctrl + click to have your linked images behave like links. Once published to the service, users can simply click. Figure 3a I created this view by hiding all the books not in the historical fiction genre. To show or hide a visualization (in this case, an image), simply click the eye icon next to it in the “Selection Pane”. When the eye is clicked, it toggles to a dash, and the image is now hidden on the page. Figure 3b Note: When you first put an image, shape, or text box on the report, it will be given a generic name in the selection pane. To give it a unique name, enable the image title in the format pane, name it, and then disable it again. Now the title doesn’t show above the image in the actual report, but it does show in the selection pane, making your report development just a little easier.  Once I had the view of the library that I wanted, I saved it as a bookmark, by clicking “Add” in the bookmarks pane. By default, it was named “Bookmark”, so I renamed it to something more meaningful, in this case, “Historical Fiction”.  Figure 3cNote: You can rename, update, or delete a bookmark at any point by clicking the ellipsis next to it. With the historical fiction bookmark created, I returned to the welcome page to add a link to it there. I selected the visualization that I wanted to point to it (the rectangle around the “Historical Fiction” text, not the text itself), and formatted it to be a link: Figure 3d In the format pane, I turned the “Link” toggle to on, changed the type from “Back” to “Bookmark” and then selected “Historical Fiction” from the bookmark drop down menu.  Now, when users hover over that rectangle, a hand appears, indicating that they can click and follow it to a new page. From here, I repeated this process for every view of the report pages I needed to make my “app” function the way I wanted it to. I added bookmarks for the welcome page, each different view of the library page based on the various selection options, one for each book page, and one for the sorted table page. By the time I was done, I had a long list of bookmarks: Figure 3e Although it looks like a lot, it wasn’t much work once I had the main pages developed. It was simply a matter of displaying and hiding visualizations, sorting and filtering data, saving off different bookmarks, and creating links to them. It’s simple development that creates a next-level, interactive report for users. At any point while exploring “What Should I Read Next”, the user can swiftly and intelligently navigate between pages to find exactly what she’s looking for.   After publishing a report with bookmarks to the service, simply enable bookmarking and the selection pane under View in the top ribbon: Figure 4a Bookmarks have many applications. They’re ideal for creating business presentations, tailoring reports to individual users or specific datasets, and creating reporting experiences like the one demonstrated in this post. With the introduction of bookmarks and the selection pane coming hot on the heels of drillthrough in September, Power BI is more dynamic than ever. To learn more about bookmarking, the selection pane, and other enhancements to Power BI, check out Microsoft’s Power BI Blog, or contact us here at BlueGranite. We’re happy to work with you and your organization to help you get the most out of your data. Sources:https://powerbi.microsoft.com/en-us/blog/power-bi-desktop-october-2017-feature-summary/https://www.goodreads.com/https://www.amazon.com/https://www.howlongtoreadthis.com/http://www.columbuslibrary.org/ "
"170" "“Deploying data platform solutions to Microsoft Azure is easy,\" they say. “Deploy Hadoop in 5 clicks or less!\" or \"Create a Data Lake in under 3 minutes!” While such claims are powerful and show the flexibility and simplicity of managing the infrastructure within Azure, they don't really speak to deploying complex, productionalized solutions that many large organizations are looking to implement.   Solid solutions looking to enter modern production environments need to support two things up front: DEV and/or TEST sandboxes: Utilizing a non-production environment to test, make changes, and implement new features is an essential function. A sandbox ensures a safe and proper testing solution without impacting the day-to-day production environment. Deployment Automation: Just as important as being safe and secure, software deployments need to be automated. Solutions that automate and deploy your applications across various environments improve the productivity of both the Dev and Ops teams. In addition, deployment automation ensures every build deploys the same, reduces human error, and allows more frequent updates addressing customer needs sooner, keeping organizations competitive. On DevOps: “I know all about DEV/TEST environments surrounding DevOps. Our organization includes it for all IT deployments, including our data warehouse.” And that’s a fantastic start - managing multiple environments and deployments in an on-premises environment is important. However, with the proliferation of the cloud-based and platform-as-a-service (PaaS) products, there are a huge number of new options for building solutions. With that, managing code and environments in those solutions can be substantially different than what you may be used to in on-premises environments. Here are a few tips to consider while creating new Azure Data Platform projects.  1) Create a subscription for Dev/Test When you create an Azure tenant, you’re automatically given an Azure subscription to use within that tenant. But you can create additional tenants to separate resources and security models. Azure Dev/Test subscriptions are a powerful tool to help you create and test your projects, while controlling costs. Pricing within Azure Dev/Test is reduced for many of the common services that we use in data platform projects.  Learn how to deploy Azure Dev/Test subscriptions with this link. 2) Use a Cloud-based Source Control system Even with platform-as-a-service implementations, someone still must write code to build and implement the solution. Often, enterprises have an in-house method for storing, versioning, and managing source code. For Azure Data Platform projects, this system should live in the cloud with the solution. The Microsoft Visual Studio Team Foundation Server (TFS) is a great place to store your Data Factory JSON files, HDInsight Hive and Spark projects, and even your PowerShell automation scripts and ARM templates. In addition to being used for source control, TFS can be used to automate the build, test, and deploy process. If your Data Platform includes custom applications to connect to data APIs, or serve User Interfaces for data entry, you can configure TFS for Continuous Deployment to ensure timely, stable builds and deployments to your Azure-based infrastructure. Learn about Continuous Deployment with TFS with this link.   3) Use Visual Studio for development While most of the Azure Data Platform services offer web-based development environments, a better choice for enterprise development is to use Visual Studio. Not only does Visual Studio integrate directly with TFS for easy management of source control (which is a great tie-in with the previous tip), Visual Studio also offers a set of features that enable the management of Dev/Test environments. Azure Data Factory projects, for example, support multiple build environment options. This is important because using Visual Studio for this enables you to deploy new pipelines to your DEV/TEST environment without hard coding any values. Removing the need to \"hard code\" settings is important to enable stable deployment practices. Azure HDInsight also has a rich set of project templates that can be used to expedite development and deployment of data platform projects. Learn how to manage multiple Azure environments within Visual Studio with this link. 4) Only use Dev/Test resources when needed One of the most important reasons to go the cloud is to save money. The main source of cost savings in the cloud is related to the fact that we only have to pay for infrastructure when we are using it. Usually, organizations are not developing solutions 24 hours a day. Testing doesn't happen continuously. It only happens at the end of a sprint and may be limited to only a few days. Using Azure Automation (PowerShell) and Azure Resource Manager (ARM) templates, you have complete control of which features and products are running within your Azure subscription at any given time. For example, an HDInsight cluster used for development might cost $15 an hour to run. You have to pay this charge even when the cluster is idle. If you know that development does not occur after 6 p.m. and doesn't start up again until at least 9 a.m. the next day, you can create an ARM template and Automation script to deactivate the cluster in the evening. When the cluster is deactivated (deleted), you don't have to pay for it. Managing cost by only running infrastructure when you need it is something that only the cloud can give you. Learn how to manage Azure with Automation and ARM templates with this link. 5) Utilize local sandboxes whenever possible Just like with custom .NET applications, most developers like to develop their solutions using a local sandbox to ensure that they can test new code without impacting any other solution. With Azure Data Platform projects, this isn't always possible. Some of the products within Azure, like Azure Data Lake Analytics, support local mode execution. Others, like Azure Data Factory, don't have a local mode, meaning all development will need to occur within an Azure Dev/Test Subscription. Sometimes, you can get creative and find other ways to enable local sandbox development. If you’re building Hive, Pig, or Spark applications with HDInsight, you could consider using a local copy of the Hortonworks Sandbox that matches the HDP version of HDInsight your organization is targeting. While this won't provide a 100% equivalent development environment, it will be close enough to write, test, and debug your application scripts prior to testing in the Azure environment. It may also mean the ability to further reduce cost by only requiring the HDInsight cluster to be running to test scripts already vetted in the local sandbox. If you don't have enough local resources to run the HDP Sandbox on your machine, you can even host it as a single virtual machine (VM) in the Azure cloud. While you'll have to pay for this VM, it will be cheaper to run per hour than an HDInsight cluster which consists of multiple virtual machines. Learn more about HDP Sandbox in Azure with this link.   Developing Azure Data Platform solutions may require some modification to your current DevOps processes, however Microsoft has given us the tools to build cloud-based platform solutions while retaining a level of trust that builds and deployments will be safe and successful. Don't be afraid to try creative workarounds like running a local HDP Sandbox to emulate HDInsight to meet your organizational goals for development and deployment practices. Just because you're moving to platform-as-a-service-based solutions, doesn't mean you have to sacrifice the safety and security of your current DevOps processes. For more information on these, and other tips for succeeding with Azure Data Platform projects, contact us today."
"171" "As a long-time Microsoft partner, it’s been interesting over the years to watch Microsoft identify, adapt, and capitalize on the various megatrends in the technology field. They’ve had some stumbles, like any large organization, but the current industry buzz and recent financial results indicate that they’re on the right track. That’s certainly our perspective at BlueGranite, where we’ve seen waves of innovation from Microsoft, and the ever-growing adoption by our clients of Microsoft’s tools and technologies for data, analytics, and insights.   At the recent Microsoft partner conference, Inspire, CEO Satya Nadella reaffirmed Microsoft’s belief in Digital Transformation as the defining megatrend of this generation. I was intrigued and impressed by his presentation, which inspired my question \"What does this mean for BlueGranite?\" This is the first in a series of blog posts on Digital Transformation. We’ll begin by offering our perspective as data and analytics consultants and practitioners, and then work into specific examples on each of the Digital Transformation pillars. So, what is Digital Transformation? Simply stated, it is the process by which organizations leverage information (digital) technology to reimagine the way in which they serve their stakeholders and provide value in the marketplace. Digital Transformation can be evolutionary or revolutionary (disruptive), depending on the industry and the commitment level of organizations looking to embrace these innovations. So, why is Digital Transformation a trend right now? We’re at a unique point in time where a confluence of factors are enabling this transformation to occur. For example, increasing data volumes – as in, the amount of data being created and stored (cheaply) - provide a rich opportunity for organizations to mine that data for deeper insights about their respective markets and customers. New sources of data, such as sensor data (Internet of Things), and the ability to easily integrate data from outside one’s organization enable new types of analysis that were previously very difficult or nearly impossible to perform. Another key factor in enabling Digital Transformation is the rapid advances in computing power which have stimulated the maturation of artificial intelligence (AI) technologies. These advances enable forward-thinking organizations to change the way they design and build products with augmented reality, and interact with their customers through bots, just to highlight a few examples. And last, but not least, the very real culture shift and changing demographics of the modern workforce, increasingly made up of “digital natives” who have grown up with technology and have expectations of rich digital experiences in the workplace that they’ve experienced in their personal lives. Data and technology are pervasive, and in this new world, leading organizations are taking advantage of these factors to transform their organizations. Now that we’ve defined Digital Transformation and outlined some of the factors that enable this trend, let’s dive deeper into the pillars of Digital Transformation, which are: Engage Customers Empower Employees Optimize Operations Transform Products These pillars provide us with a model to conceive and deliver new and/or improved business outcomes that we can measure and rally around as an organization. Let’s explore each one. Engage Customers Customers are the lifeblood of any business, and cultivating and strengthening those relationships largely determines whether an organization will flourish and grow, or stagnate and struggle. Broadly speaking, we can think of the term “customer” as representing the external stakeholders served by your organization. In a commercial context, customers may be individual consumers who purchase your products and services. They could also be business customers in organizations that provide goods and services within a B2B setting. And in the public sector, customers might be better termed constituents or citizens. Regardless, Digital Transformation in this context is about improving the “face” of your organization by whatever customer touchpoints you have available. If you improve customer experiences, customer satisfaction, and brand image, business results will follow. Empower Employees Great organizations focus on building a culture of high performance and investing in their people. Empower Employees is the pillar that looks inward at the various business units, departments, teams, and individuals, and asks, \"How can we leverage digital technology to better serve our people?\" From a data and analytics perspective, we strive to provide our clients with solutions that deliver “the right information, at the right time, in the right format.” If I’m a mobile worker, that means mobile information delivery is likely a priority. What information would I like at my fingertips when I’m on the way to service equipment or visit a customer? If I’m an executive in the corner office, what information is important to me to see “at-a-glance” to help me start my day? We also look at Empower Employees through the context of creating data and analytics assets to meet the information needs of the organization. How do we serve (often) overlapping needs of IT administrators, BI professionals, and data scientists in an organization? As Microsoft partners, we have a rich set of tools and technologies to leverage and align to these roles based on team structure and organizational goals and objectives. If we empower each of these parties, we accelerate delivery, improve quality, and drive user adoption and satisfaction – all key measures of data and analytics success. Optimize Operations As consultants, we often describe opportunities and initiatives through the lens of “people, process, and technology.” Optimize Operations is the pillar that brings all this together in ways that transform an organization “from the inside out.” It often starts with building a better understanding of current processes, whether they are in the areas of human resources, manufacturing, sales, marketing, finance, or otherwise. We help organizations build comprehensive data repositories that represent a “single version of the truth” and serve as the foundation to enable analytics and insights. This could take the form of a predictive model to help us understand the useful life remaining of our products, or in a retail setting to gain a better understanding of which customers are likely to churn (and when). Identifying patterns in the data allow us to act and change outcomes before it’s too late. Transform Products The last pillar in Digital Transformation is Transform Products. This term could be refined to include both products and services, as the key point is in how organizations are leveraging technology to innovate on their value proposition to the market and their customers. The healthcare industry is filled with examples of digital innovation – anywhere from fitness trackers/wearables, to devices designed to promote and monitor medication adherence. Not only do the physical aspects of these products and services improve in performance, but they also often create new markets that are only recently possible thanks to advances in digital technology. In summary, Digital Transformation is a megatrend that provides all of us with a model to identify and pursue innovative solutions that make a meaningful impact for our clients. As this blog series continues, we’ll explore specific client examples that illustrate these points, and provide you with the inspiration to look for Digital Transformation opportunities within your own organization. Can’t wait for the next post? Contact us, and we’ll be happy to continue the conversation."
"172" "Note: This blog post was originally published on 11/1/2017 and has been updated on 1/23/2019.--------------------------------------------------------------------------------------------------------------  Over time at BlueGranite, we have observed some customer confusion around when Azure SQL Data Warehouse is most appropriate to use. This blog post and the accompanying decision tree below are meant to help you answer the question: Is Azure SQL Data Warehouse the best technology choice for your implementation? Azure SQL Data Warehouse (SQL DW) is a cloud-based Platform-as-a-Service (PaaS) offering from Microsoft. It is a large-scale, distributed, MPP (massively parallel processing) relational database technology in the same class of competitors as Amazon Redshift or Snowflake. Azure SQL DW is an important component of the Modern Data Warehouse multi-platform architecture. Because Azure SQL DW is an MPP system with a shared-nothing architecture across distributions, it is meant for large-scale analytical workloads which can take advantage of parallelism. The distributed nature of Azure SQL DW allows for storage and compute to be decoupled, which in turn offers independent billing and scalability. Azure SQL DW is considered an elastic data warehouse because its level of compute power can be scaled up, down, or even paused, to reserve (and pay for) the amount of compute resources necessary to support the workload. Azure SQL DW is part of Microsoft’s ‘SQL Server family’ of products which also includes Azure SQL Database and SQL Server (both of which are SMP, symmetric multiprocessing, architecture). This commonality means that knowledge and experience will translate well to Azure SQL DW, with one notable exception: MPP architecture is very different from the SMP architecture of Azure SQL Database and SQL Server, thus requiring specific design techniques to take full advantage of the MPP architecture. The remainder of this post discusses some of the most important things to consider when making a decision to use Azure SQL DW.  The following comments highlight each of the items in the decision tree above: Q1: Have you justified that a relational data warehouse solution is supported by business requirements and needs? The most common justifications for a data warehouse implementation include: Consolidate and relate multiple disparate data sources. Data is inherently more valuable once it has been integrated together from multiple sources. A common example cited is the 360-degree view of a customer which could align customer master data, sales, open receivables, and support requests so they can be analyzed together. Centralize analytical data for user data access. A data warehouse is most often thought of as supporting corporate BI efforts (typically thought of as standardized corporate reports and dashboards). It can also play a big role in self-service BI efforts by providing consistent, cleansed, governed data. Realistically, a \"single version of the truth\" can never be 100% met, but effective governance and master data management can increase the odds that the data warehouse provides consistent and accurate data for all types of analytics throughout the organization. Historical analysis. The data warehouse supports historical reporting and analysis via techniques such as periodic snapshots and slowly changing dimensions. A common scenario is a customer's sales representative has changed this quarter, or a department rolls up to a different division now. The flexibility to report on either \"the way it was\" or \"the way it is\" can offer significant value - and is rarely available from standard source systems. User-friendly data structure. It is valuable to structure the data into a user-friendly dimensional model which really helps the largest portion of the user base. Other techniques such as friendly table and column names, derived attributes, and helpful measures (such as MTD, QTD, and YTD), contribute significantly to ease of use. Time investments here should encourage data analysts to utilize the data warehouse, leading to consistent results and, in turn, saving time and effort. Minimize silos. When a business-driven analytical solution (often referred to as shadow IT) becomes critical to running the business, that is a signal that it's time to promote the solution up to a centralized system so that it can be supported more fully, integrated with other data, and made available to a larger user base. The data warehouse can take advantage of business user efforts and continue gaining in maturity and value, as well as minimize silos and “one-off” solutions. Multi-platform architecture which takes advantage of existing investment. If your existing data warehouse does bring value for certain use cases, it is not economically feasible to retire it or migrate everything to another architecture (ex: Hadoop or data lake). Instead, we recommend a multi-platform architecture in which the data warehouse is one, albeit important, component. For instance, using a data lake for data ingestion, exploratory analysis, staging for a data warehouse, and/or archival from the data warehouse, are all complementary to the data warehouse which can handle serving much of the curated, cleansed data. Tip: A data warehouse is most advantageous when is deployed alongside other services, such as a data lake, so that each type of service can do what it does best. -------------------------------------------------------------------------------------- Q2: Are you comfortable with a cloud-based Platform-as-a-Service solution? Azure SQL DW is a service offering for the public cloud and the national (sovereign) clouds. It is a PaaS (Platform-as-a-Service) solution in which the customer has no responsibility for, or visibility to, the underlying server architecture. The storage and compute are decoupled, which is a very big advantage of Azure SQL DW. Costs for processing power (compute) are based on a consumption model, which is controlled by data warehouse units (DWUs for Gen1, and cDWUs for Gen2) that can be scaled to meet demanding data loads and peak user volumes. The persisted data is required to be stored on Azure premium storage, which performs better than standard storage and thus is more expensive.  Tip: As a PaaS service, Microsoft handles system updates. For Azure SQL DW, customers may specify a preferred primary and secondary day/time range for system maintenance to occur. -------------------------------------------------------------------------------------- Q3: What kind of workload do you have? Azure SQL DW is most appropriate for analytical workloads: batch-oriented, set-based read and write operations. Workloads which are transactional in nature (i.e., many small read and write operations), with many row-by-row operations are not suitable.  Tip: Although ‘data warehouse’ is part of the product name, it is possible to use Azure SQL Database for a smaller-scale data warehousing workload if Azure SQL DW is not justifiable. Keep in mind that Azure SQL DW is part of the SQL Server family; there are some limitations and feature differences between Azure SQL DW, Azure SQL DB, and SQL Server. -------------------------------------------------------------------------------------- Q4: How large is your database? It is difficult to pinpoint an exact number for the absolute minimum size recommended for Azure SQL DW. Many data professionals in the industry see the minimum “practical” data size for Azure SQL DW in the 1-4 TB range. Since Azure SQL DW is an MPP (massively parallel processing) system, you experience a significant performance penalty with small data sizes because of the overhead incurred to distribute and consolidate across the nodes (which are distributions in a “shared nothing” architecture). We recommend Azure SQL DW for a data warehouse which is starting to approach 1 TB and expected to continue growing.  Tip: It’s important to factor in realistic future growth when deciding whether to use Azure SQL DW. Since the data load patterns are different for Azure SQL DW (to utilize PolyBase and techniques such as CTAS which maximize MPP performance) versus Azure SQL DB or SQL Server, it may be a wise decision to begin using Azure SQL DW to avoid a future migration and future time redesigning data load processes. Do keep in mind though that it is a myth that you can provision the smallest size Azure SQL DW and expect it to perform just like Azure SQL DB. -------------------------------------------------------------------------------------- Q5: Do you have firm RPO, RTO, or backup requirements? Being a PaaS offering, Azure SQL DW handles snapshots and backups each day. The service automatically creates restore points throughout each day and supports an 8-hour recovery point objective (RPO) over the previous 7 days. Once a day the service also automatically generates a geo-redundant backup, with its recovery point objective being 24 hours. Customers also have the capability of creating a user-defined restore point as of a specific point in time. The retention period for a user-defined restore point is still 7 days, after which it is automatically deleted.  Tip: Backups are not taken when the compute resources for Azure SQL DW are in a paused state. -------------------------------------------------------------------------------------- Q6: Do you plan to deliver a multi-tenant data warehouse? A multi-tenancy database design pattern is typically discouraged with Azure SQL DW.  Tip: Although features which can be important to multi-tenancy (such as row-level security and column-level security) are available, you may instead want to evaluate using elastic pools in conjunction with Azure SQL Database for multi-tenant scenarios. -------------------------------------------------------------------------------------- Q7: What kind of data model represents your data warehouse? A highly normalized data warehouse structure does not completely preclude you from using Azure SQL DW. However, since Azure SQL DW takes significant advantage of clustered columnstore indexes (which utilize columnar compression techniques), Azure SQL DW performs substantially better with denormalized data structures. For that reason, following sound dimensional design principles is strongly advised.  Tip: Modern reporting tools are more forgiving of a substandard data model, which leads some data warehouse developers to be less strict with dimensional design. This can be a mistake, particularly if there are many users issuing self-service queries because a well-formed star schema aids significantly in usability. -------------------------------------------------------------------------------------- Q8: How is your data dispersed across tables in the database? Even if you have a large database (1-4 TB+), table distribution is another consideration. An MPP system such as Azure SQL DW performs better with fewer, larger tables (1 billion+ rows) versus many small to medium-size tables (less than 100 million rows).  Tip: As a rule of thumb, a table does not benefit from being defined as a clustered columnstore index until it has more than 60 million rows (60 distributions x 1 million rows each). In Azure SQL DW, we want to make the effort to use clustered columnstore indexes (CCIs) as effectively as possible. There are several reasons for this, but one key reason is because CCI data is cached on local SSDs and retrieving data from cache improves performance significantly (applicable to Azure SQL DW Gen2 only). -------------------------------------------------------------------------------------- Q9: Do you understand your data loading and data consumption patterns extremely well? Being a relational database, Azure SQL DW is considered “schema on write.” Since Azure SQL DW is also a distributed system, distribution keys inform the system how it should allocate data across the nodes. The selection of a good distribution key is critical for performance of large tables. In addition to distributing data, partitioning strategies are different in Azure SQL DW versus standard SQL Server. It is extremely important for Azure SQL DW developers to deeply understand data load patterns and query patterns in order to maximize parallelization, avoid data skew, and to minimize data movement operations and shuffling within the MPP platform.  Tip: PolyBase can be used one of two ways: (1) for loading data into Azure SQL DW (in fact, it’s recommended), or (2) for querying remote data stored outside of Azure SQL DW. PolyBase is the recommended method for loading data in Azure SQL DW because it can natively take advantage of the parallelization of the compute nodes, whereas other loading techniques do not perform as well because they go through the control node. Usage of PolyBase for querying remote data should be done very carefully (see Q15 below). -------------------------------------------------------------------------------------- Q10: Are you comfortable with ELT vs. ETL data load patterns, and with designing data load operations to specifically take advantage of distributed, parallel processing capabilities? The principles and patterns for loading a distributed MPP system are very different from a traditional SMP (Symmetric Multi-Processing) system. To utilize parallelization across the compute nodes, PolyBase and ELT (extract>load>transform) techniques are recommended for Azure SQL DW data load processes. This means that migration to Azure SQL DW often involves redesigning existing ETL operations to maximize performance, minimize logging, and/or utilize supported features (for example, merge statements are not currently supported in SQL DW; there are also limitations with respect to how insert and delete operations may be written; using CTAS techniques are recommended to minimize logging). New tables added to Azure SQL DW often involve an iterative effort to find the best distribution method.  Tip: Although PolyBase does significantly improve the performance for data loads because of parallelization, PolyBase can be very challenging to work with depending on the data source format and data contents (for instance, when commas and line breaks appear within the data itself). Be sure to include adequate time in your project plan for development and testing of the new data load design patterns. -------------------------------------------------------------------------------------- Q11: Do you have staff to manage, monitor, and tune the MPP environment?  Although Azure SQL DW is a PaaS platform, it should not be thought of as a hands-free environment. It requires monitoring of data loads and query demands to determine if distribution keys, partitions, indexes, and statistics are configured well.  Tip: Azure SQL DW does have some emerging features offering recommendations. Integration with Azure Advisor and Azure Monitor is continually evolving, which makes it easier for an administrator to identify issues. Specifically, Azure SQL DW Gen2 utilizes “automatic intelligent insights” within Azure Advisor to display if issues exist related to data skew, missing statistics, or outdated statistics. -------------------------------------------------------------------------------------- Q12: Do you have a low number of concurrent query users? Queries are queued up if the maximum concurrency threshold is exceeded, at which time queries are resolved on a first-in-first-out basis. Because of concurrent user considerations, Azure SQL DW frequently has complementary solutions in a multi-platform architecture for handling different types of query demands. We often see Azure Analysis Services and/or Azure SQL Database as the spokes in a hub-and-spoke design.  Tip: The number of concurrent queries executing at the same time can be as high as 128 depending on the service tier (i.e., the pricing level) and based on resource class usage (because assigning more resources to a specific user reduces the overall number of concurrency slots available). -------------------------------------------------------------------------------------- Q13: Do you have a lot of self-service BI users sending unpredictable queries? A data warehouse is primarily intended to serve data for large queries. Although a tool such as Power BI supports direct query with Azure SQL DW, this should be done with some measure of caution. Specifically, dashboards can be troublesome because a dashboard page refresh can issue many, many queries all at once to the data warehouse. As noted in Q12, for production use of Power BI we often recommend using a semantic layer, such as Azure Analysis Services, as part of a hub-and-spoke strategy. The objective of introducing a semantic layer is to (a) reduce some of the query demand on the MPP (reducing data movement when unpredictable queries come in), and (b) reduce concurrent queries executed on the MPP system, and (c) include user-friendly calculations and measures which can dynamically respond as a user interacts with a report.  Tip: It is certainly possible to use Azure SQL DW directly with an analytical tool like Power BI (i.e., with Power BI operating in DirectQuery mode rather than import mode). However, usage of DirectQuery mode should be tested thoroughly, especially if the user base expects sub-second speed when slicing and dicing. Conversely, if the expected usage is more data exploration where query response time is more flexible, then it’s possible direct querying of Azure SQL DW from a tool such as Power BI will work. The Gen2 tier of Azure SQL DW introduced adaptive caching for tables which are defined as a clustered columnstore index (CCI). Adaptive caching increases the possibility that self-service user queries can be satisfied from the cached data in Azure SQL DW, which improves performance significantly. Another option to potentially consider: Power BI Premium now also has aggregations which can cache data in Power BI’s in-memory model for the first level of queries, requiring a drillthrough to Azure SQL DW only when the user gets to a lower level or less commonly used data. -------------------------------------------------------------------------------------- Q14: Do you have near-real-time data ingestion and reporting requirements? Distributed systems like Azure SQL DW are most commonly associated with batch-oriented data load processes. However, capabilities continue to emerge which support near real-time streaming data ingestion into Azure SQL DW. This works in conjunction with Azure Databricks streaming dataframes, which opens some interesting new scenarios for analysis of lower latency data (such as data generated from IoT devices or the web).  Tip: When using Azure Databricks for streaming data, it is the front-end for the ingestion stream before being output to Azure SQL DW in mini-batches via PolyBase—meaning this can be classified as a near-real-time system, but you should expect there to be some latency. Also, keep in mind that Azure Databricks can also be effectively utilized as a data engineering tool for data processing and loading to Azure SQL DW in batch mode (the JDBC Azure SQL DW connector from Azure Databricks does take advantage of PolyBase). -------------------------------------------------------------------------------------- Q15: Do you have requirements for data virtualization in addition to or in lieu of data integration?  PolyBase in Azure SQL DW currently supports Azure Storage (blobs) and Azure Data Lake Storage (Gen1 or Gen2), which can be used for very selective data virtualization and data federation needs. Data virtualization refers to querying the data where it lives (thus saving work to do data integration to relocate the data elsewhere).  Tip: When using PolyBase for data virtualization (i.e., querying remote data stored in Azure Storage or Azure Data Lake Storage), there is no pushdown computation to improve query performance. This means that Azure SQL DW needs to read the entire file into TempDB to satisfy the query. For queries which are issued rarely (such as a quarter-end analysis, or data to supply to the auditors), it’s possible to use virtualized queries effectively in very specific situations where expectations for query response speed is not of utmost importance. -------------------------------------------------------------------------------------- Q16: Do you anticipate the need to integrate with multi-structured data sources? It is very common for a data warehouse to be complementary to a data lake which contains multi-structured data from sources such as web logs, social media, IoT devices, and so forth. Although Azure SQL DW does not support data types such as JSON, XML, spatial or image, it can work in conjunction with Azure Storage and/or Azure Data Lake Storage (Gen1 or Gen2) which might provide additional flexibility for data integration and/or data virtualization scenarios.  Tip: When connecting to external data, PolyBase currently supports reading Parquet, Hive ORC, Hive RCFile, or delimited text (such as a CSV) formats. Parquet has emerged as one of the leading candidates for a data lake storage format. -------------------------------------------------------------------------------------- Q17: Do you intend to scale processing power up, down, and/or pause to meet varying data load and/or query demands? One of the best features of a cloud offering like Azure SQL DW is its elasticity of compute power. For instance, you could scale up on a schedule to support a demanding data load, then scale back down to the normal level when the load is complete. The Azure SQL DW can even be paused during times when no queries are sent to the data warehouse at all, during which the storage of data is safe, yet there are no compute charges at all (because storage and compute are decoupled). Utilizing scale up/down/pause techniques can prevent over-provisioning of resources, which is an excellent cost optimization technique.  Tip: When an operation to scale up or down is initiated, all open sessions are terminated, and open insert/update/delete transactions are rolled back. This behavior is to ensure the Azure SQL DW is in a stable state when the change occurs. The short downtime may not be acceptable for a production system and/or may only be acceptable during specific business hours. Also, keep in mind that the adaptive cache (available with the Gen2 tier) is cleared when a scale or pause occurs, requiring the cache to be re-warmed to achieve optimal performance. -------------------------------------------------------------------------------------- If you're exploring the best data analytics architecture for your firm's needs, BlueGranite would love to help. Contact us today to discover how our team designs solutions that fit your company and your budget."
"173" "Microsoft SQL Server 2017 became generally available in October. Although there are many new enhancements across the entire technology stack, this blog post will showcase and highlight potential use cases of some of the new features available in SQL Server Analysis Services (SSAS) 2017. For a complete list of all the new features available in SSAS 2017, check out Microsoft’s documentation here!   ATTENTION: To explore the new features on your own, you must use the latest version of SQL Server Data Tools (SSDT). Additionally, your Tabular models must be set to compatibility level 1400 for SQL Server 2017 or Azure Analysis Services. Modern Get Data Experience AKA Power Query Enhancing your Data Warehouse New to SSAS 2017, Microsoft has introduced the same “modern Get Data experience” you may recognize from Power BI Desktop and Excel.  NOTE: This “Get Data” technology is known internally as Power Query. The language behind Power Query is called M. Microsoft dropped the Power Query name, at least externally, to be less confusing to users. Figure 1: The \"Power\" Query Editor The “Get Data” feature provides developers with another method to not only acquire data for their semantic models, but to cleanse, transform, and enhance it, as well. Traditionally, SSAS would “sit on top of” a mature data warehouse residing in a relational, on-premises database such as SQL Server. However, over the years the product has evolved to allow connectivity to a variety of data sources, from file-based sources such as Excel and text files, to Azure components such as SQL Data Warehouse and Data Lake Store. SSAS 2017 takes this a step further by not only allowing connectivity to additional sources, but by providing the ETL capabilities of Power Query to shape, manipulate, and append your incoming data as well. USE CASE: Enhancing the Data Warehouse Contrary to product marketing campaigns and the latest trends in the analytics space, the Enterprise Data Warehouse is not a thing of the past. In fact, the data warehouse should be the main engine that drives a company’s decision making. That said, the data warehouse can always be enhanced further. For example, let’s say your company is planning to expand into foreign markets, and they want to see currency exchange rates along with their sales data. As an SSAS developer, you can leverage the new “Get Data” engine to pull in historical exchange rates from the web and integrate this with your historical sales data. Having ETL capabilities in SSAS essentially fast-tracks your development efforts. You’re no longer dependent on integrating data points such as publicly available datasets directly in your data warehouse. You can bridge the data directly in SSAS. Another common use case is creating calculated columns in SSAS. Calculated columns are an effective way to enhance your tabular models, but they may not always perform well when filtered upon. For this reason, it may be best to derive your new column prior to populating your tabular model. Figure 2: Adding a custom column with the M language during the data import process, as opposed to creating a calculated column with DAX NOTE: Get Data/Power Query and the M language are topics that warrant their own blog post (okay, series of blog posts). I wanted to bring attention to this new feature of SSAS 2017, but it is out-of-scope to discuss the plethora of powerful ETL capabilities that this feature truly provides. Ragged Hierarchy Improvement Modeling Financial Data with Hide Members Property SSAS Tabular models allow parent-child hierarchies, but previous versions struggled to properly display ragged hierarchies. A ragged hierarchy is a type of parent-child hierarchy that has a variable and indeterminate depth. The textbook ragged hierarchy example is the manager-employee relationship in an organization. In one direction, an employee typically has a direct manager. Conversely, that manager may have other employees he/she oversees. In turn, these employees may be managers themselves, thus creating a ragged hierarchy. USE CASE: Modeling a Chart of Accounts Another ragged hierarchy example we encounter often in the BI space is displaying a company’s Chart of Accounts (COA). A COA is a hierarchical listing of accounts that categorize transactions against a company’s general ledger. It is used to segregate a company’s assets, revenue, expenditures, liabilities, etc. in a meaningful way. Each company defines their own unique COA, resulting in indeterminate depth and creating a ragged hierarchy. SSAS 2017 introduces an improved way to browse ragged hierarchies by allowing you to hide blank members. To demo this, I developed a Tabular model on top of the AdventureWorks DW database we all know and love. This sample database is perfect because it includes an Account dimension for us to work with. As with any parent-child dimension, there is a field for the current record’s key, as well as a separate field for the parent record key. Figure 3: Sampling of the Account dimension inside the Adventure Works DW database I need to bring this relationship to life by building the COA hierarchy path for each record. There are many ways to implement this, including leveraging a recursive common table expression (CTE) inside of SQL Server itself, but hey, this is a blog post on SSAS, so I used DAX to do the heavy lifting inside of the Tabular model. Creating the COA hierarchy path is one half of the battle. The next piece is to split the values, or levels, of our hierarchy into individual fields. Since ragged hierarchies do not have a fixed depth, I calculated the maximum depth of the entire COA hierarchy to determine how many level fields were needed. Figure 4: Using DAX PATH functions to visualize the COA hierarchy. NOTE: This is not a blog post on DAX, but rather new features of SSAS 2017. For a great in-depth walkthrough of how I leveraged the various PATH functions in DAX to visualize my hierarchy, check out this post by the SQLBI guys. They are the renowned experts in all things DAX. After creating new level fields in my Accounts table, I created a simple hierarchy aptly named “Chart of Accounts”, comprising the six level fields constructed above. The screenshots below contrast the default display from older versions of SSAS with the new Hide Members property set to Hide blank members in SSAS 2017. As you can see, the hierarchy is much cleaner to work with in SSAS 2017.  Figure 5a: Hide Members set to Default  Figure 6a: Hide Members set to Hide blank members   Figure 5b: COA hierarchy by default Figure 6b: Hide blank members property in action. A simple, yet long-awaited, new feature! Detail Rows “Just show me the numbers” SSAS, at its core, is an analytical reporting product. It provides users a semantic layer that contains complex logic composed of key business entities and advanced measures. Users can leverage products such as Power BI and Excel to easily gain quick insights into their business via a drag-n-drop interface. USE CASE: Drill into the Details With analytical reporting, you are analyzing your data at highly aggregated level. For instance, let’s say you are analyzing your company’s sales by month for a specific year. You may find that a given month is an outlier in the data set and want to dive deeper into the transactions that contribute to the amount. You know you want to see details about individual orders including their transaction date, the products that were sold, and the customer they were sold to. In other words, you simply want to “see the details”. Finance teams are often asked to provide these kinds of transaction details to auditors to identify precisely how they arrived at a value. Traditionally, the problem with these types of requests is that there is nothing analytical in providing a list of individual transactions. Technologies such as Excel pivot tables are not optimal to provide such a detailed list. However, SSAS 2017 introduces a new feature called Detail Rows. With Detail Rows you can define an expression on a given measure that will allow users to see the details behind that single data point! The series of screenshots below shows an example of this new feature in action. Figure 7: I created a new measure in my Tabular model called Sales Amt. In the properties menu for my Sales Amt measure I wrote a DAX query for my Detail Rows Expression that will retrieve my desired fields. Figure 8: In Excel, I constructed a Pivot Table against my Tabular model that slices the Sales Amt measure by Month. As you can see, for the year 2013 our sales steadily increased, but we had a dip in February. I want to see the details behind this number. Figure 9: I right-clicked on my February data point and selected Show Details… Figure 10: This created a new worksheet that displays the transactions that contribute to the February data point on my Pivot Table. And wouldn’t you know, those are the same fields I specified in my Detail Rows Expression at the beginning. NOTE: There are additional capabilities that go along with the new Detail Rows feature. This includes the ability to set an expression for an entire table in your model, as well as the new DETAILROWS DAX function. Please refer to the Microsoft documentation listed at the beginning of this post for more information. Bonus Feature – Object-Level Security I would be remiss if I didn’t mention Object-Level Security because it’s awesome! We can now define security on specific tables and columns in addition to the row-level security tabular models already provided. We can even make it so sensitive objects such as a salary and/or social security number field aren’t discoverable by malicious users. Read more about it here! Still looking for help? BlueGranite experts can assist with the strategy and implementation of SSAS at your organization. To learn more about the process and what to look out for during implementation, contact us!"
"174" "SQL Server 2017 has some significant new features being introduced, but will it make much of a difference for BI professionals? That’s the refrain on everyone’s mind for every new release of SQL Server, and, really, for any product. Is it worth the pain and hassle of upgrading? We’re looking at the top new features in SQL 2017 and what they mean for the BI professional.   The big news, which is perhaps the most surprising development, is the announcement of SQL Server 2017 on Linux. Who would have ever thought that Microsoft would release a flagship server product on Linux? Well, for BI professionals, its applicability is going to be niche as it only includes the database engine and SQL Server Integration Services (SSIS) components. So, for a primarily Linux shop, it can be used in conjunction with Power BI to host an enterprise data warehouse. It can also be used as an ETL server running primarily SSIS, particularly in conjunction with the new SSIS scale-out feature. However, SQL Server 2017’s lack of Analysis Services (SSAS) and Reporting Services (SSRS) support means it cannot function as a standalone BI server. It also lacks PolyBase and Stretch support for a hybrid environment. However, to clients, it’s virtually indistinguishable from SQL Server on Windows. While it’s not ready for most BI environments just yet, it’ll be good to keep an eye on it for future releases. Data scientists will appreciate the new Graph processing features added to the database engine. Graph processing uses the concept of nodes and edges to model objects and their relationships. For instance, it would be useful in establishing flight routes or in analyzing social network activity. The node and edge tables implicitly link together. Those tables are used with specialized queries to simplify certain unwieldy and inefficient SQL queries. In any case, using the Graph processing features should simplify and accelerate analysis of complex or recursive relationships. While there’s no single SSAS Tabular development that stands out, it has a grab bag of smaller features added. Here are the two biggest: Object-level security is the headline. Administrators can now set permissions on tables and columns in addition to the row-based security introduced earlier. That can completely prevent users from seeing or accessing certain tables or columns. With both row and object security, administrators can provide a robust security model. SSAS 2017 also incorporates the Get Data and Power Query/M features from Power BI. That will allow a tabular model to reference a much larger variety of data sources directly and perform quite a bit of ETL on its own. For some users, it may allow them to use SSAS tabular in environments where a data warehouse is not available. With these additions, there’s rarely a need to implement a multidimensional model. The final important feature is another one for the data scientists. SQL Server R Services now becomes SQL Server Machine Learning Services. Why the name change? R Services needed a rename now that it also supports Python in addition to R. Python is one of R’s principal alternatives in the data science world, and now users do not have to learn R if they are already using Python. Machine Learning Services integrates the SQL database engine with the external R and Python engines, increasing the storage performance of R or Python scripts. It also allows those R and Python scripts to be embedded in SQL scripts and stored procedures. For a more in-depth blog post on this feature, check out this blog post. Now, those three are not SQL 2017’s only new features. Here are a few quick highlights of the rest: Automatic database tuning can now detect and fix potential performance issues. Master Data Services has performance increases throughout. SSIS added Scale Out support to Always On support, allowing for high performance and high availability in clustered environments. Object-level security in Tabular allows administrators to restrict users’ access to tables, columns and measures. Increased support for DAX in SSRS and SSDT So, where does that leave the IT decision makers? Should they make the jump and endure the pain that upgrading entails? For BI purposes, it appears that the big winners with this release are the data scientists. Python and Graph processing could significantly improve their workflows. For others, the decision might be a little murky. Still having trouble deciding? Our team at BlueGranite can help you develop a strategic plan, including relevant SQL Server features, to match your analytic goals. Contact us for more details!"
"175" "\"Increasing customer retention rates by 5% increases profits by 25% to 95%.\" - Harvard Business Review[1] Reading this statistic in Frederick F. Reichheld and Phil Schefter's \"The Economics of E-Loyalty\" article from the Harvard Business Review, I was surprised that such a minor tweak in retention could have such a major impact on profits. The economic experts went on to explain that loyal customers buy more and refer new customers, solidifying the importance of long-term customer relationships.   While we know winning a new customer's business is difficult and costly, keeping long-established clients has its own set of challenges. Predicting and understanding customer churn take some of the guesswork out of customer retention. By studying behaviors of clients who have churned in the past, and looking at current customers that are behaving similarly, we can take action to positively affect their retention. This can be achieved by establishing a targeted plan that nurtures loyalty in every one of your customers. Key Benefits, Facts, & Insights Boost Profits: Selling to existing customers is easier and more cost effective rather than selling to new ones. Retain More Customers: Proactively launch campaigns and strategies to abate customer attrition. Win Back Business: Identify the root causes as to why customers leave and establish re-acquisition strategies. Avert Loss: Customer loss is substantial, long-reaching, and can impact everything from revenue to opportunity for competition. Where to Begin... Whether your customers are people or businesses, and no matter if you're selling products or services, the problem is still the same: Keeping current customers while continuing to grow with new ones. This means understanding who is no longer a buyer and why they've chosen to leave. Once you have the answers, you can work on your retention practices to prevent similar losses in the future. Knowing who is no longer purchasing from you is easy. The mystery lies in knowing which active clients you may lose. Predicting which of current customers are exhibiting behavior like your \"previously-churned\" customers is the best way to tackle the problem. However, doing this by hand would prove to be highly complex. Using the power of Machine Learning and Microsoft Azure, your historical customer data will be put to work to accurately predict future churn. We're using our Customer Churn Solution to help organizations utilize their historical customer data to target at-risk customers and find opportunities to bring back those who've left.   The first step is to define what \"churn\" actually means for your organization. Often, \"churn\" is defined by a period of inactivity. Additionally, defining your success criteria is very important to this step. Once we help you gather sufficient historical data to make a satisfactory prediction, we can then begin creating the predictive model. This also includes model validation and tuning to get the most accurate churn prediction from your data. As a last step, we will consult with you around implementing this churn prediction into your organization, both from a data architecture front and reporting/training standpoints, too.  Putting the Predictions to Use Our goal is to give your company a targeted set of actions to improve customer retention. Once an accurate model has been trained, all of your past and present customers will be run through the predictive model. This could give one of three results. See the table below: If the prediction matches the actual status of the customer, then there is no action necessary. However, if an active customer is predicted to be a churned customer, this will imply that this customer is at-risk of churning. Alternatively, if a customer is currently inactive (meaning that they previously churned), but the prediction is that they are an active customer, this may indicate that they are a good target with which to attempt to reinstate business.  BlueGranite's Customer Churn prediction also enhances reporting by providing account managers with a list of clients to target with retention efforts, and identifying the best former prospects to try and win back. These crucial insights maximize customer preservation and accurately pinpoint marketing efforts. Want to Learn More? Interested in learning more about customer churn, but not sure where to go? Here are a few more resources to point you in the right direction: Check out BlueGranite's Customer Churn Solution Offering for more details on how we can help you implement this type of prediction tool. Register for our October 26th Customer Churn Webinar. Contact us to learn how BlueGranite experts can help to design a solution to meet your unique needs. References: [1] Harvard Business Review - https://hbr.org/2014/10/the-value-of-keeping-the-right-customers "
"176" "Among the many exciting developments surrounding Power BI this summer, one that may have been easy to overlook was the release of interactive R visuals. For anyone looking to extend Power BI beyond its built-in or custom visual options, R is a good choice due to its variety of packages. For data visualization, R can be approachable whether you want an advanced statistical plot or a basic map.    What’s Interactive? The first generation of R visuals in Power BI only created and displayed a static image. With interactive R visuals, the output is HTML rather than a static image. This means that you can create and use visuals that largely work like standard Power BI visuals. In-visual interactivity such as zooming or hovering is now possible. Even with interactive R, there are some caveats. First, many standard custom visuals and all native Power BI visuals permit user selections to filter or highlight content in other visuals on the report. In contrast, an interactive R custom visual is interactive unto itself. It is a step up from static R visuals that only render an image, but there is no way to click on content in an interactive R visual and filter other report visuals. Second, interactive R is only available as a custom visual and not using the R visual in Power BI itself. Rather than scripting your R code directly in Power BI Desktop like you can with a static visual, you need to develop interactive R visuals using the Power BI Custom Visual API. What Does It Take to Build? After the announcement of interactive R visuals, I created a preview version of a basic table visual that I called rDataTable. It was an interactive R custom visual that uses R’s DT package, which itself binds to JavaScript’s DataTables library. It offered built-in search across all columns, column-specific filter in the visual itself, dynamic paging, and more. You may wonder about the level of effort involved to create this table.  Rapid development is one of the strongest advantages of R visuals in Power BI. This is true whether you create static or interactive R visuals. To obtain what you see in the screenshot as well as the functionality outlined above, it only took a few lines of R code. This is due to the impressive level of innovation surrounding the htmlwidgets package and the many other packages that combine R with JavaScript to produce interactive content in today’s R ecosystem. Four Considerations Here are four broad considerations when creating interactive R custom visuals for Power BI: Audience: Will you be keeping your visual for personal or team use, or do you plan to deploy it to a wider audience? The level of initial effort — not to mention potential maintenance and support — varies substantially depending on whether you build a visual for a small team, a large organization, or open-source and release it for everyone in the Power BI community. Deployment: Does your organization use Power BI Service or Power BI Report Server on-premises? Are you building it for yourself and can get away with having it only in Power BI Desktop? As with static R visuals, interactive visuals have a dependency on the R packages available in your environment. Power BI Desktop: You are limited by your local R workspace. If you don’t have a package you need, install it locally to get your visual to work. Power BI Service: Interactive and static R visuals should work as long as packages are supported by Microsoft. Mobile reports should render interactive and static R visuals. While there are hundreds of packages available to Service users, only a few cater to interactive HTML output. If you plan to create a visual for wide distribution in Power BI Service, consider supported packages that create render HTML such as Plotly, timevis, networkD3, Highcharter, or DT. Power BI Report Server: No R visuals are currently supported. There is no mobile support by extension. Data: How many records do you need to represent in your R visual? As with static R visuals, interactive R visuals allow for up to 150,000 records. Note that this is still a heavy advantage over standard custom visuals, which plateau at 30,000 records. Format options: How much customization do you want to allow users, if any? Most custom visuals give report builders the courtesy of changing options like data colors, font size, and more. It may be quick work to write a few lines of R code with formatting hardcoded. Once you add custom formatting, however, the effort grows significantly. You will be no longer be exclusively working in R. Instead, you will venture into TypeScript and JSON. Decisions like allowing users to customize colors will therefore substantially increase the effort to build a visual not simply by expanding time but also by broadening the required skillset. Once you have an idea of who will use your visual, how and where it will be used in reports, what type of data it will display, and how much user customization you may want to consider; you’re in a suitable position to plan your interactive R visual development in more detail. How Can I Get Started? To get started with custom visuals in Power BI, see the Microsoft documentation on GitHub. In addition to the basics of setting up your development environment, the docs lay out basic steps to work with interactive R in the “Creating R Powered Custom Visual with HTML Output” section. If you are looking for even MORE help, I would recommend checking out BlueGranite's Powe[R] BI eBook on using R custom visuals within the tool. BlueGranite also has the expertise to help guide you through Power BI development and training. Please contact us today!"
"177" "We have some exciting news to share: after 15 years under our old brand, we are updating the BlueGranite logo. For more than 10 years, we’ve helped our clients create brilliant data and analytics solutions and we felt that it was time for our logo reflect that.   When we started out in 1996, a mountain rendering represented our business. Since then, our logos have evolved along with our company. After 15 years under our most recent banner, we decided it was time to embrace that evolution and design a new logo – one reflecting our standing as a consulting team focused on data and analytics, and signaling our reputation as your strategic trusted advisor and partner. Out with the old, in with the new After 21 years of growth and change, we found ourselves wanting to visually get back to our roots. Through some thoughtful discussions and careful consideration, we decided we wanted to design a logo and tagline that: Illustrates our skills using data to drive insights and analytics across organizations Demonstrates our team’s deep expertise and passion for what we do Outlines our collaborative approach to helping clients get essential insights Appeals to the many industries we serve, from retail to healthcare and all those in between Though we loved our old logo, we felt it was time for a change. Research, dialogue, and team collaboration led us back to our original vision of a mountain, albeit with a modern twist and some new colors to catch the eye. The up-to-date design represents our ability to help clients embrace data as a strategic asset, build effective solutions, and deliver value across the organization. During this re-branding journey, we will continue to create awareness and support our clients’ efforts in data architecture, business insights, and advanced analytics. You may notice continued updates to BlueGranite’s website, social media pages, demonstrations and more as we embrace our new, refreshed look. We hope that you like our new BlueGranite logo! We will continue to provide you with updates on the latest and greatest technologies through our blog, webinars and educational resources. As always, if you have any questions or want to discuss your goals, please contact us!"
"178" "This week at the MS Ignite conference, Microsoft announced the public preview of SQL Server Integration Services (SSIS) in Azure. We're long-time fans of SSIS here at BlueGranite, and we've embraced Azure as our first-choice platform for new solutions and deployments. We’ve all been waiting – rather impatiently – for this announcement, and we're excited to start busting out some control flows in the cloud.    If you’ve been avoiding Azure or annoyed that it doesn’t have many of the Microsoft BI tools your team knows so well, this announcement is for you.   If you've embraced Azure Data Factory (ADF), but loathe diving into custom activities and code for things you know you could get done quickly in SSIS, this announcement is for you.  If you’ve been creating killer analytics solutions in Azure but struggling to get operational support from your BI team, this announcement is for you.  Is it ready for my production ETL?  Not yet. The capabilities to support a production ETL are on the way. What’s missing?   Deeper Integration with Azure Data Factory Without getting too technical (plenty of time for that later), SSIS in Azure is built on top of Azure Data Factory. As of today, you can deploy packages and SSIS projects to run as a managed Azure Service. You can’t, however, execute SSIS packages from an ADF pipeline or otherwise communicate with SSIS from ADF.  Enterprise and 3rd party component support Right now, only the standard SSIS components available in the SSIS Standard license are available. Use a custom component to generate hashes for dimension processing? Not supported yet. Extracting data from Dynamics and using third-party source components? Not supported yet.   What's available today? <U+202F>  The feature set isn’t complete, but this release targets the needs of two similar sets of customers: those looking to migrate all of their SSIS workloads into Azure and those looking to start deploying new SSIS integrations to Azure and maintain their existing work on-premises.   Managed SSIS in Azure will be available in the East US and North Europe Azure data centers. The Azure-SSIS Integration Runtime (IR) will be available in six different sizes and support only the Standard Edition Features. The SSIS IR can be joined to a VNet, but only one in classic mode. And during Public Preview, the service will have 24/7 support to keep the service running healthy while you try it out.  What’s a public preview?  You can think of a public preview as a more advanced version of a beta. The service is mostly stable but the product team is still working out the kinks, completing available features and deploying new ones. The preview period of a service is the best time to start “kicking the tires” and getting a feel for how it might fit into your architecture.  The Public Preview period is also a great time to get to know the product teams, give them feedback and let them know what’s important to you. They’re looking for those passionate, early adopters to help drive their roadmap.  Luckily for you, we’re some passionate early adopters ourselves. If you want to learn more about the public preview, give us a call and we will be happy to chat about the new features. "
"179" "We recently discussed options for securing internal access to Power BI reports and dashboards. That post also explained how to apply role-based security filters to a dataset, ensuring users only see the information appropriate for their roles. Dynamic row-level security (DRLS) is one of the most effective and efficient ways to restrict data views across an organization. Using DAX functions, DRLS filters the dataset based on the Power BI service user’s log-in credentials. This allows Power BI report authors to easily create filtered data views and skip the hassles of creating multiple security roles in the model and managing the assignment of users to these roles.   However, using dynamic role-based security in dashboards and reports has some limitations. One is that the data will be filtered to show only the data associated with the login credentials of the user. In some scenarios, users may want access to data associated with different user credentials. Circumstances such as these require that a filter instead be applied to a hierarchy in the data model. As an example, let’s look at a model with a security role based on geography.  This security role filters dataset to the North American continent. This means that report consumers assigned to the Geography Role can view North America continent data. These users can also view data for the countries within that continent.  The geography table below has columns for both continent and state, enabling users to crossfilter the data for that continent.  There are often times when there isn’t an option in the data model to crossfilter as shown above. Developers typically address this issue by creating additional dimension tables to support crossfiltering, then adding those tables to the data model. In the example below, we use a helper table to add categories to the product list. By creating a product subcategory table, and adding keys to the products table, we can define product groupings by subcategory.  Additional dimension tables are often used to apply an organizational hierarchy to a fact table. Suppose that a company wants to look at sales based on organizational hierarchy. Ideally, they want to be able to drill up from an individual salesperson’s results through the organization – sales manager, district manager, regional manager, country manager and so on. Creating additional dimensional tables is one way to accomplish this. Though this approach may enable users to drill up and down through organizational data, without security roles it may also give users access to all the company’s sales data – information typically restricted to upper-level management and employees governed by insider trading restrictions. Creating security roles configured to limit access for groups of employees to specific data, then assigning users or security groups to those roles once the report has been published to the Power BI service, is one way to secure data access. However, you might be thinking that this approach sounds complicated, especially in a larger organization, or in one where the combination of employees, managers, and executives is not clearly hierarchical, and you would be correct. Maintaining the security roles, and users assigned to those roles, can be time consuming and error prone. Here’s where we come back to dynamic row-level security. It’s one of the most efficient ways to assign users or security groups to specific roles after publishing a report to the Power BI service. The process filters data based on the Power BI user’s log-in credentials, passing those credentials to the data model through a DAX expression used in a security role. Let’s see what that looks like:  When using this role in a published report, the Power BI service passes the user’s email log-in address to the model, filtering the data accordingly. To test the functionality, we can manually substitute an employee email address in double quotations for USERPRINCIPALNAME, as shown below:  When viewing the report without the security role applied, we see the following data:  However, when we use the View as Roles function in the Security group on the toolbar, we see data filtered by that email address:  However, our fictional support organization has a hierarchy, and the user above (sanne@contoso.com) has a manager. When we change the hardcoded value to her manager (rpatrick@contoso.com), we see the following:  The dynamic row-level security role filtered the data to only show those tickets assigned to rpatrick@contoso.com. This is by design, as the USERPRINCIPALNAME function filters data to only show tickets assigned to that specific user. As discussed earlier, most organizations want to see a rollup of metrics by organizational hierarchy. In this case, the manager (rpatrick) has five employees that report to him. To accomplish this goal, and use dynamic row-level security in our report, we need to implement the PATH function in the model. The PATH function is used with parent-child hierarchies, like those typically found in organizational charts. When used in a calculated column, the function creates a delimited text string with the identifiers of all the parent values for each child value in the table. In our example, the user “sanne” reports to the user “rpatrick”, who then reports to another manager, and so on. The PATH function allows us to map the parent-child hierarchy from the lowest level to the highest level in the organization. The function automatically generates the complete hierarchy, even though each employee only has their direct manager’s ID associated with their email address.  As you can see below, once we add the calculated column with the PATH function, the complete hierarchy for each employee is generated:  The next step is to create an additional column in the model for each level in the organizational hierarchy, and to get the email address for the corresponding employee ID. This is necessary for the use of dynamic row-level security, since it needs the email address as the value for USERPRINCIPALNAME. Our example organization has four hierarchy levels, so we will create four additional calculated columns that capture the email address for each hierarchy level, and get the corresponding email address using the following function: Org Level 4 = LOOKUPVALUE (    'Organization'[Email - Work],    'Organization'[Employee ID],    PATHITEM ( 'Organization'[Organizational Hierarchy], 4, 1 )) The PATHITEM function determines which level in the PATH string should be retrieved. In the DAX example above, the 4th level in the hierarchy is being returned. After we’ve repeated the process for the other three levels in the hierarchy, we’ll have a table that looks like the following:  Now that we have the parent-child hierarchy with email addresses implemented in the model, we can modify the role to use dynamic row-level security in the context of a hierarchy with the following expression: [Email - Work] = USERPRINCIPALNAME()|| [Org Level 4] = USERPRINCIPALNAME()|| [Org Level 3] = USERPRINCIPALNAME()|| [Org Level 2] = USERPRINCIPALNAME()|| [Org Level 1] = USERPRINCIPALNAME() Using the OR operator (II) in the expression means that the role will evaluate the user’s email address against each level in the organizational hierarchy. We can again test the functionality by using the hardcoded email addresses in the role. For example, if we replace USERPRINCIPALNAME() with “sanne@contoso.com” as shown below, we’ll see all the data shown for her level in the hierarchy. As a member of the lowest level of the parent-child hierarchy, only the tickets assigned to her will be shown. However, if we use her manager’s email address “rpatrick@contoso.com”, the data is filtered to show all of the tickets assigned to him and any employee that reports to him:  This approach enables dynamically filtered data based on the email address of the user accessing the report, and limits the dataset to only that employee and his or her subordinates. This can greatly reduce the number of Power BI reports and security roles that an author needs to create, and can significantly simplify the management and governance of the groups assigned to security roles once the report is published to the Power BI service. In this example, a single report and a single role can be utilized to provide the appropriate filtering of data for anyone in the organization. In addition, if the employee data is being imported from an official HR source, whenever new employees are added or the organizational structure changes, the report will automatically be updated to filter the data correctly. Creating the correct level of access and security for your Power BI reports, dashboards, and datasets can be complicated. If you have a question or want to know more about how BlueGranite can help your organization with Power BI solutions, contact us!"
"180" "When new versions of a software product are announced, there’s always a grab bag of new features. Some of those features turn out to be a godsend for users everywhere, but many of them are only useful for specific applications, or, worse, are worth little more than a bullet point on marketing materials. One of the new features of SQL Server 2017 is the addition of Python support to its Machine Learning Services. Is it a boon or a bust? Read on to find out.   Earlier this year, Python joined R as a supported language in Microsoft’s Machine Learning Services module. Python support will be available on every edition of SQL Server 2017, including the free Express Edition. Guido van Rossum created the straightforward, easy-to-learn Python language in 1991. It has a wide variety of library packages to handle all sorts of tasks, including a significant number of actively maintained data science-specific libraries. In fact, it now rivals R in data science. Like R, its intended use is in data science workloads. When embedded in SQL Server, Python scripts can directly use the SQL Server engine to access and store datasets. The increase in storage efficiency is a huge boon when working with large swaths of data. Data scientists already specializing in Python now need not learn R, with its relatively steeper learning curve. Like R integration, Python scripts can be embedded within scripts and stored procedures, giving an easy transition to production use of data models and the like. However, Python was developed as a general-purpose language. It is no one-trick pony. An enterprising developer can use its features to encapsulate heavier logic or specialized features that previously required deploying a common language runtime (CLR) stored procedure. Migrating that functionality between servers or environments is simplified too, as it’s all contained within a SQL script. From data-cleansing functions to web service access, the world is your oyster. This is where Python really comes into its own. Here are a few examples to whet your appetite: Email handling – You can ingest emails, perform sentiment analysis, store the contents in SQL Server, and even reply within the context of a stored procedure or SQL Agent job. Real-time currency conversion – Retrieve real-time currency exchange rates from a webservice and use those rates to convert from the base currency. Data Quality tools – Easily create text formatting, duplicate detection and fuzzy matching algorithms to improve data quality. Use specialized hardware from within SQL Server – Feed data directly from SQL Server to GPUs for heavy-duty floating-point computation and efficiently store the results back in SQL Server. This can be especially useful in a data science context. Now that Python is part of SQL Server 2017, it can be used in nearly any situation. On-premises, it can be run independently or embedded within SQL Server. In a hybrid environment, PolyBase or stretch databases can be added to the mix to store data both locally and in Azure. In the cloud, it can run against Hadoop or Spark clusters. Now that SQL Server supports both R and Python, it begs the question, “Which is better?” There have been endless discussions about that very topic within the data science community. Proponents of R cite its development by statisticians and laser-focus on data analysis. It often can accomplish common analytical tasks with few lines of code. Critics of R suggest that it is harder to learn and less capable for data preparation tasks. Proponents of Python praise its ease in learning and debugging. Its general capabilities make it a flexible jack-of-all-trades, letting it handle all aspects of the task. Critics suggest that its general outlook makes achieving some goals less efficient. The real answer to the question “which is better” is the language you’re comfortable with, that gets the job done. And, now that Python support is on par with R support in SQL Server, there’s no penalty or benefit in choosing one or the other. Have a question about advanced analytics or want to learn more about how BlueGranite can help your organization? Contact us!"
"181" "Some organizations embark on the self-service BI journey because it’s popular with data analysis or perhaps because competitors have an established program and they fear being left behind. Others do it to offload IT requests to the business, permitting expensive resources to focus on long-term projects. Organizations might also do it “by accident”, discovering that self-service BI grows organically whether they planned for it or not. Regardless of how self-service BI initiatives started at your company, if you end up “owning” them (whether you are an IT person or not), it is worth considering the associated costs beyond licensing. Actively managing these costs can positively impact your organization, and ultimately your career. Efficiency Costs Efficiency costs are the costs incurred to ensure reports are highly consumable by the target audience and that they can be trusted. These costs tend to rise when reports are confusing, or when they offer conflicting numbers compared to other reports. There are several ways to categorize these costs, explained below. Dataset Redundancy This can occur when multiple data models query the same source in the same way and possibly define the same business calculations across departments, which can imply redundant data refreshes across reports.  In some cases, this redundancy can even happen across reports built by different people in a single department. Given that it is common for data analysts to work independently, redundant dataset modeling occurs often and can increase the price tag of self-service BI initiatives when slow queries with the same result set are executed concurrently during scheduled data refreshes, impacting source system resource capacity. Additionally, since the same calculation can be defined across reports, there is always a risk of creating rogue copies. This means that the same calculation name with a different formula (and output) can negatively affect adoption if a user’s trust in the reports is diminished. Competing Solutions A competing solution tends to happen while IT is focused on broader, long-term projects that seek to satisfy many reporting use cases. These take time to develop, given their large scope. Meanwhile, business users may use self-service BI tools to create tactical, short-term solutions of their own that contain “tribal knowledge” that the IT-led corporate solutions sometimes lack. When the time comes for those expensive IT-led solutions to be deployed to production, some users can be reluctant to adopt a BI solution given that their own reports satisfy their needs – and more importantly, they feel they have control over them. The price tag of such an experiment can be very high, given abandoned IT solutions can imply multiple layers of work, each costing thousands (or even hundreds of thousands) of dollars. In other scenarios, business users are simply unaware of the existence of enterprise solutions and datasets that may satisfy their needs. They could be building their own self-service BI reports while other existing solutions they were not informed of may have already been deployed. Inaccurate Report Scope This situation refers to when well-meaning self-service BI authors create models that are too generic and cover too many use cases in hopes of targeting a wide audience of disparate users. Unfortunately, this approach can increase efficiency costs as report performance can suffer due to excessive data volumes and broad data definitions. Performance issues alone can cause users to abandon reports. I often say a report that attempts to answer too many questions will not properly answer any of them. In this scenario, users may be forced to spend additional hours creating more specialized reporting - adding cost to the initiative. It is also common for analysts building reports for themselves, or for a small audience (their boss, for example), to take the opposite approach of building a single data model per report. This approach can be too narrow, and increases efficiency costs due to redundancy (see bullet point above). Ineffective Report Layouts Reports that are difficult to read, confusing, and overly decorated can increase cost by causing users to spend too much time trying to understand what the report says (or intends to say), or by abandoning the report completely. Lack of Systematic Testing Systematic testing is a process inherent to IT development. In contrast, self-service BI users may be business experts with no IT background and might not be accustomed to rigorous unit testing. A “random check” or “spot check” of numbers may not be sufficient to ensure accuracy across all report filtering conditions, and without appropriate testing, efficiency costs can increase when key users detect inaccuracies leading to decreased usage. Undefined Development Environment One of the many strengths of self-service BI tools is the flexibility for rapid authoring. Report creators often want to experiment with new calculations or visualizations. This, coupled with the fact that many self-service BI initiatives do not designate a separate authoring environment (as in, development and production are the same), can create user confusion at best, and improper decision making based on “test” calculations at worst. Reports that generate initial excitement may end up suffering low adoption if the original author modifies the “production” copy directly too frequently. Operational Costs Operational costs are incurred to keep the self-service BI initiative running beyond a license bundle purchase. Similar to efficiency costs, operational costs can also be defined in a number of categories. Incorrect License Allocation When some users (but not all) plan to author/publish reports, an organization might erroneously purchase authoring licenses for its entire user base. To prevent this, consider assigning authoring licenses to report creators, and read-only licenses to consumers. This begs the question: do you know who your authors are? How about your consumers? Some companies, like BlueGranite, have strategies in place to understand the user population and segment it into dataset developers, report authors, or consumers. This can help align licensing with actual use. Unplanned Team Assignment Some companies mistakenly treat all users as if they are in the same bucket. In reality, some authors are more driven by visualization best practices, while others are more interested in data preparation, modeling principles, and calculation development. Making this distinction is significant, considering many companies want to follow a “shared dataset” approach to self-service BI. This implies a person (or team) is developing the data models, while other people (or teams) develop the reports on top of the shared model. Although some users may claim expertise in both areas, typically one area is dominant. These groups have different interests and motivations that benefit from specialized training, support and a process definition to empower them. When companies instead treat authors as belonging to the same category, costs can increase by expecting capabilities from the wrong audience, or by teaching a skill to the wrong user. Lack of Report Ownership Report prototypes often start as experiments. Some stick and become valuable, given their ability to solve reporting needs. However, once reaching that point, not all report authors have the interest or capabilities to own them moving forward. Without a proper ownership migration strategy in place, supporting these tactical solutions becomes expensive, since IT or business teams may struggle to properly maintain them. No Usage Analysis Often, companies do not measure usage behavior for self-service BI reports. This increases operating costs, as IT may spend hours maintaining solutions people no longer use and could be missing important user adoption cues. Champion Mentality Some users are not only business experts, but also achieve self-service BI tool mastery. They are aware of their ability to produce value quickly and their technical prowess surpasses their peers’ capabilities for report creation. With their skills and enthusiasm, they are a fantastic asset to kick-start a self-service BI initiative. However, as an initiative matures, these experts can become an operational cost risk because they can default to be as self-sufficient as possible. This circumvents the IT department instead of collaborating with it, leading the team to re-invent solutions that may already exist or be in the works. Opportunity Costs Opportunity costs are those incurred due to the loss of potential gain, had other alternate approaches been taken. Prototyping As mentioned, self-service BI tools can be used to develop flexible reports quickly. IT has an opportunity to use this capability during the requirement-gathering phase by using self-service BI tools to confirm assumptions around reporting, data modeling, and data preparation. These discoveries can influence blueprints for data warehousing or enterprise data modeling work. In other words, self-service BI tools can be used to “fail cheaply and quickly” to save on expensive future refactoring efforts. By not planning for a prototyping phase using self-service BI tools, IT misses a tremendous opportunity to avoid high refactoring work down the road due to inaccurate requirements. Undefined Integration Lifecycle Leading reporting technologies allow for upgrading self-service BI models to enterprise BI models. For example, it is possible to migrate Microsoft Power BI models into SQL Server Analysis Services Tabular. As part of a managed integration process, developers can expand the reach and accuracy of enterprise models by: Comparing popular data models developed by the business to those created by IT Merging valuable query and calculation objects into the enterprise BI models Oftentimes, however, businesses rolling out self-service BI programs are either unaware of these capabilities or do not plan on using them methodically. In these cases, organizations are missing out on an opportunity to harvest users’ knowledge and leverage ideas of how to enhance enterprise capabilities. Sometimes the opportunity cost for integration is so high that, over time, it can diminish IT’s ability to remain relevant as a provider of analytical capabilities when enterprise solutions become out-of-sync with business needs. Addressing hidden costs Regardless of how well engineered your back-end data layer might be, these hidden costs can eat up most of, or all, the value of your self-service BI initiative. Companies that seek to manage them typically do so through an established program that accounts for the following: Auditability: Usage metrics analysis and adoption management Training: User segmentation, gap analysis and roadmap Requirement discovery: Tactical and strategic managed prototyping Integration Life Cycle: Model harvesting via compare, merge and certification Process: Environment separation, team structure and SLAs Want to learn more about self-service BI and how you can improve your organizational strategies? Check out our eBook How to Implement a Successful Self-Service BI Program – and if you still have questions or just want to chat, contact us!"
"182" "Microsoft R Server is an enterprise-class tool for hosting and managing parallel and distributed workloads of R processes on servers. Organizations that need to process large amounts of data or perform complex processing on the data benefit the most from a parallel architecture like Microsoft R Server. It uses the RevoScaleR package, which makes parallelization easy.   In genomics research, we often interact with large amounts of data from complex pipelines in a diverse array of formats. Luckily, Bioconductor helps make this process simpler by packaging up common sets of processes in ready-to-use R code. Harnessing the power of both Bioconductor and Microsoft R Server together can help streamline the processing of your genomics data. All About Bioconductor  Bioconductor is an open-source, open-development software project to provide tools for the analysis and comprehension of high-throughput genomic data. It is based primarily on the R programming language. In other words, it's an extension of R that is specialized for bioinformatics and genomics analyses. Installation Once R (either base R or Microsoft R Server) is installed on your local machine, installing Bioconductor is simple. Open R and use the following commands to grab the latest version of Bioconductor.    source(\"https://bioconductor.org/biocLite.R\")biocLite()    Once the biocLite script has loaded, you can now call any desired packages.    library(\"BiocInstaller\")biocLite(\"RforProteomics\", dependencies = TRUE)    Now you are ready to use over 1,000 Bioconductor packages! Workflows The field of genomics is very broad, but Bioconductor will often have a solution for every area. The Bioconductor site is rich with workflow examples to help connect the dots for your research. Check out Bioconductor's help section for a list of the available workflows. Here are a few of my favorites: Sequence Analysis - Import fasta, fastq, BAM, gff, bed, wig, and other sequence formats. Trim, transform, align, and manipulate sequences. Perform quality assessment, ChIP-seq, differential expression, RNA-seq, and other workflows. Variant Annotation - Read and write VCF files. Identify structural location of variants and compute amino acid coding changes for non-synonymous variants and predict consequence of amino acid coding changes. High Throughput Assays - Import, transform, edit, analyze and visualize flow cytometric, mass spec, HTqPCR, cell-based, and other assays. Transcription Factor Binding - Find candidate binding sites for known transcription factors via sequence matching. Cancer Genomics - Download, process, and prepare TCGA, ENCODE, and Roadmap data to interrogate the epigenome of cultured cancer cell lines as well as normal and tumor tissues with high genomic resolution. Package Database Didn't see a workflow that exactly fit your needs? No problem! Bioconductor has a nice list of available packages that will help you find the right one for the problem at hand. Using the search box shown below, I searched for the term \"eQTL\" (Expression Quantitative Trait Loci) to find packages related to that topic.  For more information, you can check out the package list here. Enhancing your Workflow with Microsoft R Server Whether you are using the pre-made workflows or ended up creating your own, you can likely speed up processing time by running your Bioconductor/R scripts in parallel. Microsoft R Server and RevoScaleR make this easy. Let's take the Annotating Genomic Variants workflow, for example. .vcf files are often very large and sometimes difficult to process or summarize due to their size. Using the VariantAnnotation::locateVariants function from Bioconductor makes this process more automated. We can use this function to identify where a variant falls with respect to gene structure, e.g., exon, utr, splice site, etc. We use the gene model from the TxDb.Hsapiens.UCSC.hg19.knownGene package loaded earlier.    ## Use the 'region' argument to define the region ## of interest. See ?locateVariants for details. cds <- locateVariants(vcf, txdb, CodingVariants()) five <- locateVariants(vcf, txdb, FiveUTRVariants()) splice <- locateVariants(vcf, txdb, SpliceSiteVariants()) intron <- locateVariants(vcf, txdb, IntronVariants())  all <- locateVariants(vcf, txdb, AllVariants())     If we want to start summarizing the variants, we could use sapply to repetitively perform some operation over the entire data object. Take a look at the highlighted lines below.    aa <- predictCoding(vcf, txdb, Hsapiens)idx <- sapply(split(mcols(aa)$QUERYID, mcols(aa)$GENEID, drop=TRUE), unique)sapply(idx, length)## Summarize variant location by gene: sapply(names(idx),      function(nm) {         d <- all[mcols(all)$GENEID %in% nm, c(\"QUERYID\", \"LOCATION\")]         table(mcols(d)$LOCATION[duplicated(d) == FALSE])     })  ##            125144 162514 23729 51393 7442 84690 ## spliceSite      0      2     0     0    1     0 ## intron          0      0     0     0    0     0 ## fiveUTR         0      2     0     1    3     5 ## threeUTR        0     25     2     1    2     0 ## coding          0      5     0     3    8     0 ## intergenic      0      0     0     0    0     0 ## promoter        1     23     0     0   15    11    This code easily summarizes the .vcf file in a few seconds. However, the NA06985_17.vcf.gz file is only a small (35MB) sample from human Chromosome 17. What if you were to use multiple files to assess a sample population such as the ones available from 1000 Genomes? sapply might take a while... We can use the RevoScaleR package to parallelize the summarization function in the code. By using rxExec to distribute the processing over multiple cores of a processor or even multiple nodes on a Hadoop cluster, we can speed up the processing time tremendously. In the sample code below, we use the rxExec function to split up the processing by GeneID.    ## Summarize variant location by gene using rxExec from Microsoft R Server: vcflocationsummary <- function(nm) {         d <- all[mcols(all)$GENEID %in% nm, c(\"QUERYID\", \"LOCATION\")]         table(mcols(d)$LOCATION[duplicated(d) == FALSE])     }rxExec(vcflocationsummary, rxElemArg(GENEID))     Note: this is only sample code. To fully use this workflow, visit Bioconductor's workflow variants. Try it Out! Now that we have explored how easy it is to speed up your genomics workflows using Microsoft R Server, you can try it out for yourself. Pick a workflow that fits your needs and then use it. Once you start seeing where the processing bottlenecks are, think about using RevoScaleR's parallelization functions to speed things up. Look for loops and apply functions as prime candidates for distributed processing. First time using Microsoft R Server and the RevoScaleR? Microsoft's documentation is a great place to start. To compare the RevoScaleR functions, read  Explore R and RevoScaleR in 25 functions. If you still have questions, please reach out to us and we will be happy to help!"
"183" "Power BI Security is one of the most valuable features. Understanding the best way to do this can be more complicated – especially if you only want to provide access to a specific user group, or you only want to share a portion of report data.   For example, a report creator may want to share a report containing sensitive financial data with the finance team and executive leadership only. An HR specialist might want to share an HR report that includes detailed salary information with the HR team only. In these cases, access to reports and dashboards should only be made available to specific groups or individuals. On the other hand, a report that shows company sales data could be of great use to sales managers and individual salespeople, but perhaps the data should be secured to show only the sales data for the regions or accounts that pertain to the individual viewing the report. Finally, it might be useful to share analysis with partners or suppliers that are not part of the organization. Each of these cases requires a different approach to ensure the right access is being provided to consumers of the Power BI artifacts. Before we look at the different options available to address these scenarios, it’s important to understand the concepts of access, sharing, and security, as they are often confused. This is understandable, as these concepts are often used in conjunction with the development and deployment of Power BI solutions, and some terminology in use in the product (row-level security, for example) can contribute to misunderstandings. As we dive further, we will cover how to provide access to Power BI artifacts to specific people in your organization, and how to restrict views of data within reports and dashboards to specified users. The implementation will require the use of some security features, but the complete security model for Power BI is out of scope. For more information on how Microsoft secures Power BI instances, see this documentation. One simple way to think about access to Power BI dashboards and reports is to compare it to taking a flight on an airplane. To get to a destination, a traveler will need to have a ticket for that flight. The ticket (along with identification such as a driver’s license or passport) has the necessary information for the traveler to pass through various checkpoints to get on the flight, and will also have information about what types of services the passenger has access to once on board the flight. This is similar to the process for a user accessing an organization’s Power BI service via authentication of user name and password. Once an authorized user has provided the necessary account information and password, they will be logged on to the Power BI service that hosts the workspaces, apps, dashboards, and reports for that organization. Just because a traveler has passed through airport security doesn’t mean they have the ability to get on any flight in the airport… the airline ticket is for a specific flight! Similarly, an organization can use permissions to ensure that users who have appropriate permission to the organization’s Power BI service only have access to the dashboards and reports that are relevant to their role. This is access control… if the user doesn’t have the right permissions, they can’t access (or even see in the list of available reports/dashboards) the content. Power BI report and dashboard authors can define which organizational user groups should have content access, and whether to grant permissions for report creation collaboration, or solely for report consumption.  Now that the traveler has boarded the plane they have a ticket for, another aspect of ticketing comes into play. Depending on the ticket, they might be sitting in first class, business class, or economy. So while the traveler on the first-class ticket might have access to a broader menu of meals, entertainment options, and conveniences, the economy traveler might only have access to a subset of those services. Thus, while all travelers are on the same flight, their experiences might be quite different. This is similar to the use of role-based security in Power BI dashboards and reports. If role-based security is being used, a report or dashboard creator can ensure that while a group of consumers of a report or dashboard are all interacting with the same artifact, members of different roles will only see the data that is relevant or permitted for members of that role. Managers or executives might be assigned to a role that allows them to view all of the data in the dataset, while individual salespeople might only have access to their region or customers. The process for creating role-based security is managed by creating roles within Power BI desktop. A role uses data within the model to define access. For example, to restrict sales data to only sales representatives in a specific region, the workspace owner would create a role for each region, and then use a DAX expression to restrict the members of that role to the data filtered by that expression. As an example, the image below shows creating a role that filters the data by North America:  Once the dataset is published to the Power BI service, the owner assigns users to the role or roles that were created, as shown below:  While this method can reduce the number of reports and dashboards that need to be created for an organization, it does require careful management to both create the necessary roles in the report, and to add the users and/or groups to the role in the service. Using a SQL Server Analysis Services Tabular model to standardize the datasets behind your reports and dashboards is one way to minimize the work of creating and maintaining multiple security roles. It’s as easy as building and enabling roles and role filters with the SSAS tabular data source for use across multiple Power BI reports and dashboards. Lastly, you can use dynamic security to restrict data access. By enabling row-level security in Power BI reports, based on a user’s log-in credentials, you set the parameters of what data they see. Continuing with our air travel analogy, the use of dynamic row-level security is similar to the information provided to the flight attendants regarding things like special meals. Travelers might be on the same flight, and in the same class and row of seats, but one might receive a vegan meal and the other a gluten-free meal, based on the information they provided to the airline. Dynamic row-level security is accomplished in Power BI by using user tables in the model and the USERNAME() or USERPRINCIPALNAME() functions in the DAX expression when creating security roles. The Power BI service passes the credentials of the user logged into the service and viewing the report or dashboard to the function and uses that value to filter the data, as show below:  Creating the correct level of access and security for your Power BI reports, dashboards, and datasets can be complicated – but it doesn’t have to be! If you have a question or want to know more about how BlueGranite can help your organization with Power BI security, contact us!"
"184" "Big organizations have multiple departments responsible for providing analytics, including marketing, sales, finance, supply chain, and logistics. Due to the nature of large companies, any type of analytical technology must come through IT. Many organizations are trying to take advantage of the cost savings and speed of analysis that Infrastructure as a Service (IaaS) and Platform as a Service (PaaS) can provide, however, the common IT approach hinders or completely blocks the potential of cloud services for analytical business units outside of IT.   Having witnessed this situation while working in a corporate environment and again as a consultant, this common problem is becoming ubiquitous in the Fortune 500 world. Analytics teams will come in with business cases and instead of changing process and adjusting governance, IT tends to continue to charge down the old path of creating a requisition and trying to be agile within a non-agile process. The next issue comes with scaling: As analytics projects can vary greatly in size and scope, simply provisioning a one-size-fits-all compute engine is not possible. Having the ability to spin up a project-specific cluster and then tear it down when done is where the cloud shines, but not when IT must create another requisition to dismantle the cluster. One example comes from working with a manufacturing company attempting to build a sensor analysis across its network of plants. The process monitored the sensor output of multiple machines on multiple lines to look for outages or excessive cycle time. To spin up the appropriate infrastructure, the U.S.-based team had to requisition the Poland-based team to provision the hardware. The process could take a week or more before any work could be done. Similarly, any change required within the design of the infrastructure meant starting the process all over again from the beginning. From a personal experience, as then-VP of Data Warehouse and CRM, I was confronted with having to transition from a data warehouse appliance to an on-premise Hadoop solution, due to the company’s comfort level with cloud-based solutions. With the current appliance, I was told that in order to double my performance, the vendor would have to remove my half-rack appliance and bring in a full rack, requiring down time for infrastructure changes and data transition. The Hadoop solution did provide flexibility for future growth by simply adding additional nodes; however, there was still lead time required to expand and very little opportunity to scale back if needed. Another sticking point was the necessity of building a disaster recovery cluster in a co-location, due to bandwidth and cost of duplicating the cluster. In hindsight, having a cloud-based solution, that can scale up or down based on need, and that includes geographic redundancy, would have been a much better choice. Plus, by having multiple technology options from a traditional relational database management system, data warehouse appliance and HD Insight (Hadoop), analysts can easily transition their skill set and choose the best option for the workload. Luckily, IaaS/PaaS providers have taken note and started offering solutions where provisioning will become a thing of the past. Microsoft’s latest cloud-based solution, Azure Data Lake Analytics, solves this by offering a scalable and always available compute engine. “Process big data jobs in seconds with Azure Data Lake Analytics. There is no infrastructure to worry about because there are no servers, virtual machines, or clusters to wait for, manage, or tune. Instantly scale the processing power, measured in Azure Data Lake Analytics Units (AU), from one to thousands for each job. You only pay for the processing that you use per job.” Data Lake Analytics. (n.d.). Retrieved August 8, 2017, from https://azure.microsoft.com/en-us/services/data-lake-analytics Recently data warehouse appliance vendors have changed direction, turning away from selling more units, and declaring on-premise appliances end-of-life, making the transition to the cloud a requirement. So, if you find yourself in the position to rethink your infrastructure and want to take advantage of redundancy, scalability, and technical flexibility, I highly suggest looking to the Azure cloud. The cloud brings tremendous flexibility and speed to analytics, but a change of process and governance will be required to make it effective in large organizations. However, change is good when it positively affects the company’s bottom line and provides solutions that are just not possible with evolving workloads. With the combination of both service offerings, the result is literally Architecture as a Service and, with the right controls in place, a limitless canvas for current workloads and those yet to be defined. Don’t fear the cloud. Prepare, adapt and reap the rewards. If you are looking for help with transitioning to the cloud and building a successful governance strategy, contact us!"
"185" "As modern analytics continues its exciting evolution, it’s more important than ever to be mindful of some of the foundational elements that allow an organization’s analytics strategies to mature and evolve. Today, advanced analytics, data lakes, and cloud-based solutions dominate our industry headlines, and while these things will no doubt prove indispensable to any venerable analytics practice, many of us are still faced with learning to walk before we can learn to run – and the time-tested data warehouse remains today just as relevant a cornerstone to a budding analytics practice as it did a decade ago.   Why a Data Warehouse? There’s just no getting around it: before stakeholders can turn their data into insights, that data needs to be structured in a way that lends itself to being easily understood – a central prerequisite to any analytics solution. Turning data into insights is, and should be, the ultimate goal, but it is difficult to build effective and advanced solutions without having a strong model to stand on. Of the many data models designed to help organizations build a durable analytics platform, the Kimball dimensional model is the gold standard. It pushes data modelers to focus on building something that is fast, and, more importantly, something that is easily understood. After all, if an analytics solution does not impart understanding and insight, then what is it doing? While the technical requirements to building a data warehouse are generally well known and understood at this point by the industry, we should know that lurking behind IT’s requirements checklist is a far more critical (and almost completely non-technical) prerequisite: organizational consensus. Organizational Consensus As consultants, we have a saying in the Business Intelligence/Analytics space when it comes to centralizing data as part of an analytics solution, and that is: “We want one version of the truth.” In other words, for the audience served by your analytics solution – and by extension the data warehouse within that solution – we want to use one set of terminology, one set of “rules,” and/or one set of agreed-upon use cases. How else do we arrive at “truth” if we are not using a common language to describe an agreed-upon definition of the process or organization that we seek to analyze? Quite simply, we cannot. In fact, as the scope of a solution grows in size and in complexity, it becomes more critical to have a single truth in order to be successful. Indeed, simply deciding the scope of the solution itself should help organizations come to a consensus, but it isn’t always that simple when there are questions such as: Should the data warehouse merely be a massive structured repository of as much atomic-level data as we can collect? Should we seek to instead build data “marts,” defined as collections of use cases and intended to serve well-defined target audiences? There is no one-size-fits-all answer here. Rather, these questions can only really be answered through meaningful discovery and conversation, ultimately allowing teams to arrive at a point of mutual agreement. Arriving at Consensus Somewhat unique to analytics solutions is the unvarnished way in which data can paint a picture of a process, or an organization, or a way of doing business. Much as the weather doesn’t particularly care what the forecasters are calling for, raw data begins to tell a story that is – especially as we continue to procure it from more and more sources – largely agnostic to any one interpretation or point of view. For example, data generated by a process of selling goods or services may ultimately end up in the hands of both sales personnel and accounting staff – often undergoing quite a bit of disparate conditioning and massaging along the way in order to suit the particular needs of each division. When tracking toward a data warehouse as a singular repository of data, what will be the agreed-upon structure, naming, and rule sets that the data warehouse should incorporate? The answer here is not particular to some hypothetical question like the above, but rather to point out that it’s still up to people (the stakeholders and the consumers), and the establishing of process, to determine these answers before any sort of computer architecture can be expected to. Know your audience. Know the use cases being presented. Bring those with both relevant knowledge to the processes being discussed, and an open mind to change, to the table. Give them a chance to be heard. And only then worry about knowing your source systems, or what keys your tables should use. After all, improved insight often brings about change, as we very rarely open our eyes only to discover that we’ve been doing things in the dark perfectly all along. Do you want to learn more about building and maintaining your data warehouse? Contact us today! We are always happy to provide ideas and insights for you and your organization."
"186" "An On-premises Solution for Hosting Power BI Reports The Power BI Report Server was made generally available on June 12th, 2017 as part of the announcement of Power BI Premium. To this point, the Power BI Report Server has been largely overshadowed as it often finds itself as a sub-bullet of the Premium subscription model. In the sections to follow, we will discuss what the Power BI Report Server is, review its key features, discuss how you can get your hands on it, and lastly, provide our 30-day impression of working with Power BI Report Server with an actual customer.    WHAT IS POWER BI REPORT SERVER? The Power BI Report Server is a new, standalone product that allows organizations to host Power BI Reports on an on-premises report server. Additionally, the Power BI Report Server is built upon the same architecture as SQL Server Reporting Services, allowing you to also use the server to host paginated reports, mobile reports, and key performance indicators (KPIs), thus creating a “one-stop shop” for all your reporting assets.  WHY ON-PREMISES? As discussed in the recent Microsoft whitepaper, Planning a Power BI Enterprise Deployment, there are several reasons why customers still want, and even need, an on-premises solution to host Power BI Reports. Complement the Power BI Service Highly Regulated Customers  Integrate into existing investment in Reporting Services Suffice it to say that even in 2017 many organizations cannot, or are simply reluctant to, deploy their BI assets to the cloud. The Power BI Report Server serves as a solution to these customers who have been longing for an on-premises option for Power BI. WHAT ARE THE KEY FEATURES? The Power BI Report Server only offers a subset of functionality found in the Power BI Service, however, as stated above, it is built upon the same platform as SQL Server Reporting Services (SSRS). Simply put, you can think of the Power BI Report Server as a superset of SSRS. The list below highlights the key features of Power BI Report Server: HOST POWER BI REPORTS ON-PREMISES This can’t be stated enough. Finally, an on-premises solution for sharing and collaborating on Power BI reports with your colleagues! As an added bonus, the Power BI Report server comes with all the functionality found in Reporting Services, thus allowing you to also deploy paginated reports (.rdl), mobile reports (.rsmobile), and KPIs to the server, thus supplementing your Power BI reports. This is something that not even the Power BI Service can lay claim to. This also means that if you already have a Reporting Services report server, you can easily migrate/upgrade your report server database to be compatible with Power BI Report Server. NOTE: Your report server must be at least SSRS 2008 or greater.   LEVERAGE CUSTOM VISUALS The Power BI Report Server can render custom visuals, which adds almost 100 more visualizations to your toolbelt while authoring reports! VIEW REPORTS ON-THE-GO WITH POWER BI MOBILE You can connect to your Power BI Report Server via the Power BI Mobile app! Power BI Mobile currently supports iOS, Android, and Windows mobile platforms. INTERACT WITH REPORTS VIA THE WEB Power BI report rendering allows for the same functionality you would expect if you’re familiar with the Power BI Service. Functionality including slicers, cross-filtering, tooltips, and underlying data exploration are all available. EXPORT REPORT DATA TO CSV Perfect for those who simply “want to see the numbers”. WHAT ARE THE LIMITATIONS? Having an on-premises solution for hosting Power BI reports is a great start, but we would be remiss if we didn’t discuss some key limitations. LACK OF POWER BI SERVICE FEATURES As its name implies, the Power BI Report Server can only be used to host Power BI reports. Cool features found in the cloud offering, such as dashboards, streaming data, and “Q&A”, are not available. Only reports authored in Power BI Desktop can be hosted on the report server. TWO POWER BI DESKTOP CLIENTS Speaking of Power BI Desktop, the Power BI Report Server is intended to be used in tandem with a new Power BI Desktop client that is optimized for on-premises development. This will only be an issue if you have a hybrid Power BI implementation in which you are leveraging both the Power BI Service and the on-premises Power BI Report Server. For example, you may use the Power BI Report Sever for hosting sensitive reports that should remain behind your corporate firewall. Technically, you can still use the default Power BI Desktop for report authoring, however, you should be extremely careful not to introduce any features in your report that are not yet supported by Power BI Report Server. Otherwise, your report may not render as expected when viewing from the web. NO PREVIEW FEATURES While we’re on the topic, Power BI Desktop preview features are not supported by the Power BI Report Server. That means features like custom report themes will have to wait. In fact, the new Desktop client optimized for on-premises doesn’t even give you the option to turn on preview features. Thus, another reason you should stick to the Power BI Desktop client that is suited for your situation. LACK OF EXPORT OPTIONS Currently, you only have the option to print a Power BI report from the report server. Options to export the entire report as an Excel file, image, or PowerPoint slide to be used in presentations are lacking. CONSTRAINED TO ANALYSIS SERVICES Power BI reports must connect to an Analysis Services data model. The good news: both Tabular and Multidimensional models are supported. The bad news: those are the only data sources that are currently supported. WHAT DOES THE PRODUCT RELEASE CYCLE LOOK LIKE? Unlike SQL Server Reporting Services (SSRS), which follows the same release cycle as SQL Server, the Power BI Report Server will follow a quarterly release cycle (four times per year). This will coincide with a Power BI Desktop release that will be optimized for the on-premises server. While a quarterly update is nowhere near as frequent as the monthly Power BI updates we’re accustomed to, it is still very much a welcomed timeline as opposed to following the SQL Server product release cycle. HOW CAN YOU OBTAIN POWER BI REPORT SERVER? Your organization can acquire Power BI Report Server one of two ways: Purchase a Power BI Premium subscription Via active SQL Server Enterprise Software Assurance (SA) If you choose to go the Power BI Premium route, you are permitted to install Power BI Report Server on-premises with the same number of cores as specified in your Premium capacity subscription. This is a good option for those organizations that are looking to implement a hybrid Power BI architecture (both cloud and on-premises). Acquiring Power BI Report Server via active Software Assurance is the recommended approach for organizations that plan to stay on-premises only. NOTE: An important caveat of Power BI Report Server is that although it is an on-premises solution for hosting Power BI reports, you still must purchase Power BI Pro licenses for any report authors. HOW DOES IT FIT IN WITH EXISTING MICROSOFT PRODUCTS? The product comparison chart below can be found on the SQL Server Reporting Services Team Blog. I’ve included it in this post because it does a fantastic job explaining how the Power BI Report Server bridges the gap between traditional Reporting Services and the Power BI Service (cloud).  Additionally, the image below shows what a Power BI Report Architecture may look like in your organization.  30-DAY IMPRESSION Power BI has done a terrific job building enthusiasm in its products and establishing a foothold in a data visualization market that was once dominated by the likes of Tableau and Qlik. However, one of the chief complaints of Power BI was that it didn’t have an on-premises offering. I have had actual discussions with IT directors who favored Tableau over Power BI simply because, with Tableau, they could have an on-premises solution. The craziest part was that they even liked Power BI’s development features and interactivity better than Tableau’s! With the release of the Power BI Report Server those conversations will be a thing of the past. We have been using the Power BI Report Server for over a month now at one of our largest customers. Simply put, it rocks! The ability to set up a “one-stop-shop” reporting solution for our customer has been very well received. The fact that users don’t have to navigate to one site to view paginated reports, and another site to view Power BI reports, is a huge win. Our goal was always to implement SSAS Tabular models for our semantic layer as part of a new data warehouse initiative, so we have no complaints over the lack of data source variety. However, we would love to see a future introduction of features such as custom report themes and Power BI report subscriptions. Overall, the Power BI Report Server has exceeded our expectations and our customer is very happy with its features. Have questions about Power BI Report Server or just want to learn more? Contact us! We would be happy to share more examples of how you can also take advantage of this great technology. "
"187" "Power BI alerts are very important therefore a data-driven organization has quickly taken action to address changes in their business. However, in order to take action, users need to know when business conditions have changed. Dashboards published to the Power BI service can be configured to send alerts to users when the underlying data changes to meet limit conditions set on gauge, KPI, and card visualizations. Let’s take a look at how data alerts work in the Power BI service.   The dashboard above monitors support tickets for an internal IT support organization. The dashboard includes visuals that represent:  Average satisfaction of employees by seniority Number of open tickets by support agent Average severity Days to close Satisfaction   Priority for closed tickets The number of open critical severity tickets Critical severity tickets by senior management Average age of open critical tickets  While some of the metrics on this dashboard are useful for understanding long-term trends and satisfaction with customer support, the critical tickets, especially those opened by senior management, need to be closely monitored and handled within a service level agreement (SLA). With Power BI service, once the dashboard is published to users, individuals can create an alert to notify them if the value of the card changes beyond a threshold. To create an alert, hover over a tile in the dashboard and then click the ellipsis in the top right of the tile. This will open a context menu.  Clicking on the Bell icon (shown above) will open the Manage alerts pane for that visualization.  Clicking on + Add alert rule enables users to create an alert for that tile. Users can then give the alert a title, set the rule for any measure in the visualization, set the condition and threshold for the alert to trigger, set the notification frequency, and decide whether the alert will only show in the Power BI service notification center or if it will also trigger an email to be sent to the user setting the alert. For this dashboard, we will create an alert if any member of senior management opens a critical ticket.  Each visualization can have multiple alerts. For example, you might want to create additional alerts on the tile above so that an additional alert will trigger if the number of open tickets rises above 5. While the use of alerts in the Power BI service is helpful in keeping a user aware of changes to the dashboard data, there are some limitations that can be problematic. The first is that alerts are created and managed by each user of the dashboard. So, for users to receive alerts, each one of the dashboard users must configure them individually. In addition, each user must set the parameters for the alerts individually. If an organization has specific targets in mind for alerts to be created, then all users creating the alerts must be sure to use the same parameters. Finally, the method for delivering alerts is limited to a notification in the Power BI service, or an email that will be sent to that user only, and on an hourly or every 24-hour schedule. In summary, the alert function in the Power BI service is generally designed for individuals to set their own alerts to notify themselves of changes to the data they care about. But what about instances where the alert parameters should be uniform across all the users of a dashboard? Or if an organization wants to configure alerts for several users without each individual having to configure their own alerts? Or if notification methods other than a message in the Power BI service notification center or an email to a single user are desired? Fortunately, Microsoft has another service that can address all of these scenarios with ease, and can offer much, much more. Microsoft Flow is a cloud-based automation service that enables businesses to create workflows in a simple, drag-and-drop interface – and better yet, it is included with an Office 365 subscription. Flow is ideally suited for automating processes and tasks within and between various services. While there are many services that can be used with Microsoft Flow, we’re going to focus on how we can use it to turn Power BI alerts into actions that will drive your business. After logging into Flow, click on Templates in the toolbar and type ‘Power BI’ into the search bar.  As you can see below, there are already quite a few templates that use Power BI data alerts to trigger complex workflows. For this example, let’s use ‘Send an e-mail to any audience when a Power BI data alert is triggered’. Before we begin, let’s note that Flow has templates available to send the message to other services, like Microsoft Teams, Slack, or GitHub, and can even create a task in Wunderlist or send a text message to a phone.  After clicking on the template (lower left corner), you’ll be prompted to sign into the services being used in the flow if you haven’t done so already. Once you have signed in, click continue to open the flow editor. You will notice that this template has two items: a trigger and an action.  The trigger item in this flow fires when a data alert in the Power BI service is activated. When you click the dropdown list, you should see a list of all the data alerts published to your dashboards and can choose the alert you want to use in the flow. Remember, by using Flow and the Power BI trigger, you can use a single data alert in the Power BI service to send alerts to as many people in the organization as you want, rather than having each user of the dashboard set up the alerts and define the conditions for the trigger individually. Once you have chosen the alert you want, you can move to the Send an email action. You can add individual emails, distribution lists, security groups, and others in the “To” field. Again, this allows you to deliver notifications to users in an organization in a more targeted fashion than having individual users set up their own alerts. In addition, Flow allows you to customize the subject and body of the email that will go out to users, and you can include “dynamic content” in those fields of the email. Dynamic content is information provided by the Power BI service, which in this case could include the title of the alert, the value of the tile that triggered the alert, the threshold for the alert, and the URL to the tile. The user receiving this email would have a great deal of information about why they were alerted.  Using the example of the Support Dashboard above, we can configure a flow to send a customized email to the lead engineers email group notifying them when a critical ticket has been opened by senior management. Microsoft Flow also provides configurations that cause actions to repeat until value thresholds are met, or conditional statements to add additional actions to a flow. For example, we could include a conditional statement in this flow that would send an email to the senior engineers unless a certain threshold was reached, at which point an email notification would also be generated for support team management. Alternatively, we can add parallel branches to the flow which would trigger additional actions, perhaps creating a Wunderlist task and an item in a SharePoint list in addition to sending the email, or even sending a text message to a manager’s phone.  In conclusion, while the Power BI service provides alerting functionality that enables individual dashboard users to set alerts and notify themselves when values change, using Power BI alerts with Microsoft Flow enables organizations to create sophisticated workflows that can notify groups of individuals, create tasks and work items, and set conditions to trigger additional actions if thresholds are reached. Using Power BI and Microsoft Flow together can help move your business from an environment where users are simply being made aware of changes to business data to one where data alerts trigger workflows that can help them quickly take the correct action. Creating the right metrics, alert thresholds, and Microsoft Flows to drive business action can be complicated. If you have a question or want to know more about how BlueGranite can help your organization use Power BI alerts and Microsoft Flow, contact us!"
"188" "As more and more organizations embrace data-driven decisions and the value of data democratization, their success in doing so will depend on getting their data management strategies right. For corporate and traditional Business Intelligence scenarios, many companies already have Master Data Management strategies in place. However, when it comes to self-service BI scenarios, there is often no real strategy and end users are left with gaping holes in their data quality as the usage increases over time. Let’s discuss how you can enable data democratization within your organization, as well as provide a tool for your power users to maintain data quality in their reports.   Missing the relevant master data information for the transaction data is a common data quality scenario. Here, I'll walk you through how to identify, get notified, and resolve this. We will use Power BI to identify the issue and display in a dashboard, Flow to notify the data steward or subject matter expert of potential issues, and PowerApps to easily resolve the issue. In this example, we have some product-related information (master data) stored in an Excel sheet in OneDrive for Business:  The transaction data is also stored in an Excel sheet in OneDrive for Business:  You’ll notice that Product ID - 5 is not present in the master data for Product. Follow the steps below to recreate the solution. Step 1 Create a Power BI report that displays the number of Product IDs in the transaction table that is missing the master data information. The model is straightforward for this scenario and you can even connect Power BI directly to the Excel files stored in OneDrive for Business.  Step 2 The next step is to create a DAX measure that will display the number of Product IDs present within the Sales table and not present in the Product table.     MissingProductID =VAR MPD =    DISTINCTCOUNT ( Sales[Product ID] )        - CALCULATE ( DISTINCTCOUNT ( 'Product'[ProductID] ), Sales )RETURN    IF ( MPD > 0, MPD )     Step 3 Now, follow the steps below to create a simple Power BI report: Using the measure MissingProductID, create a card to display the total number of missing product IDs. You will need to create a table to display the missing product IDs. To do this, add a table visual using the Product ID from the Sales table and the MissingProductID measure. Additionally, you will need one table to display the data as it would appear for an end user. Create the table (just like step b) using the Product ID from the Product table and the Sales table. Lastly, add in the MissingProductID. We can now see that there is one missing product ID (number 5). You can also see that the sales attributed to the missing product ID are 50 dollars. Step 4 You will now need to publish this report online and pin the card you just created to a new dashboard. Set a data alert on the card (if you aren’t sure how to do this, read this article on setting data alerts in Power BI) with the condition (threshold above 0) and frequency (at most once an hour) as shown below. Emails are not enabled as we will make use of Flow for that. Click on Save and close.  Step 5 For the next step, go back to Manage Alerts (found under the ellipsis icon of the card on which we set the data alerts) and select the alert that we just made (shown below). Click on “Use Microsoft Flow to trigger additional actions” found at the bottom of the alert. This will take you to Microsoft Flow and will open a template for triggering Flow with Power BI data-driven alerts. Select the alert we just created:  Step 6 Create a new step which will be an action to send an email, and add some content in the subject and the body of the email draft. Note that we are providing the link of the dashboard in the body of the email:  Step 7 So far, we have used Power BI to identify the issue and Flow to notify the power user of the issue with customized email content (rather than the generic data alert message from Power BI). Our next step is to make use of PowerApps to create an App that can help the power user make the required corrections or add missing product IDs in the Product Excel file. To do this, log in to PowerApps with your username and password. Click on the Apps menu on the left, and select Create New. You should see the screen below where you can select OneDrive for Business as your data source.  Add a new connection to OneDrive, point it to the Product.xlsx file, and choose the Product table as shown below:  Click on the Connect button below after selecting the table, and your basic app should be ready in a few seconds. Step 8 You should see three screens in the app that you created – Browse screen – where you can see all the Product IDs and selected details Detail screen – to show further details of the selected Product ID Edit screen – where you can add or edit Product IDs You can follow this link to learn how to edit this app further. For the purpose of this post, we are just going to use the basic app as it is.  Step 9 Save the app to the cloud under the name DataQChecker: Step 10 After that, embed this app into the Power BI dashboard that we created using the steps from this article. Now your dashboard should look similar to the example below where you have an interface to directly add or edit product entries. Step 11 In this step, we are going to simulate new data coming in by adding two more entries in the Sales table for Product IDs 10 and 11: We know Product IDs 10 and 11 are not present in the Product table. Once the dataset is refreshed (could be a scheduled refresh or in this case, Excel files in OneDrive for Business can sync automatically), we should get an email as shown below:  By clicking on the dashboard link, we should see the dashboard with three missing product IDs:  Step 12 Lastly, go ahead and add the three missing product IDs using the Data Quality App (right there in Power BI!). Refresh the dataset manually to see the results.  It really is that easy for self-service BI users to maintain the data quality of their analytics solution with the use of Power BI, Flow, and PowerApps. Maintaining data quality is vital to the success of the analytics solution as well as increasing adoption, and Microsoft provides all the tools required. For a more detailed look at Microsoft Flow alerts, check out this post from Nathaniel Schrar: From Alerts to Actions: Power BI Alerts and Microsoft Flow. If you have questions or want to learn more about how to maintain your data quality, please contact us."
"189" "Many companies are implementing modern BI platforms including data lakes and PaaS (Platform as a Service) data movement solutions. Microsoft offers Azure Data Factory and Azure Data Lake in this space, which can be used to efficiently move your data to the cloud and then archive and stage it for further integration, reporting, and analytics.  There is good documentation on Microsoft Docs to help you get started with Azure Data Factory (ADF), but there is quite a bit to learn, especially if you are getting into ADF from an Integration Services (SSIS) background. Here are four tips for moving your data to Azure Data Lake Store with Azure Data Factory. Use service principal authentication in your linked service used to connect to Azure Data Lake Store. Azure Data Lake Store uses Azure Active Directory for authentication. As of April 2017, we can use service principal authentication in an Azure Data Factory linked service used to connect to Azure Data Lake Store. This alleviates previous issues where tokens expired at inopportune times and removes the need to manage another unattended service account in Azure Active Directory. Creating the service principal can be automated using PowerShell. The Azure Data Factory Copy Activity can currently only copy files to Azure Data Lake Store, not delete or move them (i.e., copy and delete). If you are using Azure Data Lake Store as a staging area for Azure SQL Data Warehouse and doing incremental loads using PolyBase, you may want to load only the changes that have occurred in the last hour or day into a staging area. You are probably using dynamic folders and file names in your Azure Data Factory datasets to organize the data in the archive area of your data lake (perhaps by subject area, source system, dataset, date, and time). If you need to keep just the last set of changes to the source data in your staging area, you can remove the date and time portions of the filename to have one file per dataset. If you use the same filename and location, you can replace a file by overwriting it (removing the need to delete). Although your first inclinication might be to copy the data to staging and then archive, if you write your data to your archive area first and then copy it to staging, you can make sure your pipelines can be successfully re-run for previous time slices without having to re-query your source system. If you find that you do indeed need a way to move or delete files in Azure Data Lake Store using Azure Data Factory, you can write a custom activity to accomplish this.  Use configuration files to assist in deploying to multiple environments (dev/test/prod). Configuration files are used in Data Factory projects in Visual Studio. When Data Factory assets are published, Visual Studio uses the content in the configuration file to replace the specified JSON attribute values before deploying to Azure. A Data Factory config file is a JSON file that provides a name-value pair for each attribute that changes based upon the environment to which you are deploying. This could include connection strings, usernames, passwords, pipeline start and end dates, and more. When you publish from Visual Studio, you can choose the appropriate deployment configuration through the deployment wizard. While you can use the Azure .NET SDK to programmatically generate Data Factory assets, you can also automate the tedious creation of your pipelines with BimlScript. BimlScript is better known for automating SSIS development, but it can be used to automate ADF development as well. Exporting many tables/query results from a source system can mean hundreds of activities and datasets in your ADF project. You can save hours, even days, by using BimlScript to generate your datasets and pipelines. All you need is the free Biml Express add-in for Visual Studio and a bit of metadata. By querying the metadata in your source system or creating a list in Excel, you can generate pipelines with copy activities that copy data from each source table to a file in your Azure Data Lake Store all in one fell swoop. BimlScript is XML with C# mixed in where automation is needed. You can manually write one dataset and one pipeline and then use them as a template. Identify the parts of your datasets and pipeline that change for each table you want to export to ADLS and replace those values with a Biml code nugget. This could include the names of pipelines, activities, and datasets as well as source tables and schemas, pipeline execution frequency, and the source data column used for time slices. Automating this development process with BimlScript produces the same project files that would have been created manually, with no evidence of automation and no vendor lock-in requiring you to use BimlScript when you create more pipelines or alter the existing pipelines. Want to know more about Azure Data Factory? Feel free to contact us and we will be happy to answer your questions. "
"190" "Last month, BlueGranite encouraged team members to spend a work day volunteering for a local nonprofit organization of their choice. The work was rewarding and gave everyone a chance to give back to the communities that they care about. Here’s a quick summary of some of the organizations we worked with and how we were able to lend a hand.   Volunteering at Open Doors Although most BlueGranite employees work remotely, there were lots of opportunities to get together in groups and make a difference across the country. In Kalamazoo, Mich., we had almost half of our team (myself included) come together to help a local nonprofit called Open Doors. Open Doors' almost 50-year mission is to provide a safe haven for those with little to no income, help area individuals and families find sustainable work, and to build and maintain realistic budgets, to ultimately prevent homelessness. Additionally, by offering a Residence Community with affordable housing and support, as well as area shelters, Open Doors helps people remove themselves from difficult situations to get back on their feet. So what did our group of volunteers manage to accomplish in a day? We spent most of our time working outside at one of the women’s shelters. Between scraping off old paint, priming two garages, cutting back a long line of overgrown bushes, some general yard work, and washing many, many windows, we had our work cut out for us! It felt great to give back and help the small maintenance team at Open Doors get so much work accomplished in a single day.   Second Harvest Food Bank of Metrolina BlueGranite’s Charlotte, N.C., team chose to spend its time volunteering for the Second Harvest Food Bank of Metrolina. With a reach that spans 19 counties and includes multiple charitable agencies, the food bank annually distributes more than 50 million pounds of food in its fight against hunger. Our four Charlotte team members helped Second Harvest pack bread into boxes and prepare the loaves for shipment to local soup kitchens and shelters. Project Angel Heart Meagan Longoria dedicated her day to the Denver-based Project Angel Heart charity. The agency's donors, sponsors and volunteers prepare and deliver custom meals at no cost to people with life-threatening illnesses, helping those who have trouble getting out of the house or are too weak to prepare food.   Aside from delivering these much-needed meals, Project Angel Heart also tries to remind each meal recipient that they are cared for and appreciated with decorative and cheerful delivery bags. Meagan was able to contribute to that effort by spending her day decorating those delivery bags and getting them ready to brighten someone’s day when they need it most. Camp Barakel Jim Bennett spent his day of service working with Camp Barakel, a summer camp for kids located in Northern Michigan. Jim has been volunteering for the camp for many years, and was able to help the organization with quite a few tasks – some of which included painting, making changes to their firewall, working on their accounting system, and training Camp Barakel staff on how to use a CRM system that Jim helped to implement last year.  Citizen Soldier Connection A group of three near Colorado Springs, Colo., volunteered with Citizen Soldier Connection, an organization that provides friendship and support to soldiers and their families. The BlueGranite team spent its day of service helping set up and take down the gear needed for the local Colorado Springs Western Street Breakfast, a huge celebration and fundraising event intended to support local military personnel. Each year, they serve around 10,000 people breakfast!  As much as they all enjoy breakfast, BlueGranite’s group of volunteers got their hands dirty instead by helping to unload hay bales, placing them in the street for breakfast attendees to sit on while they enjoy their meal. Once the breakfast concluded, the group helped to break down and load the serving tables, chairs, and the stage into trucks. They also spent a good amount of time stacking hay bales for loading, and cleaning up leftover debris on the streets and sidewalks.  Fresh Truck On the Northeast side of the U.S., in Boston, Marc Mingrone and TJ Polak spent their day of service volunteering for a nonprofit organization called Fresh Truck. The Fresh Truck mission is a unique solution to a lack of grocery stores and fresh food in several areas of the Boston region. The mobile grocery store brings fresh food to communities with little access to fruits and vegetables. Throughout the day, TJ and Marc helped the Fresh Truck team deliver food all over Boston to those who have difficulties finding it in their own neighborhoods. Feeling inspired? We are too! That’s why we set aside time every year in June to work with our local communities and give back. We’re looking forward to next year already! If you want to volunteer for an organization in your community, try checking out VolunteerMatch to find nonprofits in your area that could use a lending hand. "
"191" "In my 17 years of database development, a few projects stand out from requirements so elaborate that I really had to stretch my imagination. One such assignment required the importation of comma separated value (CSV) files where the fields may not be in the same place and new fields may appear without warning. As you can imagine, this wouldn’t have been difficult if the source was a semi-structured format like JSON, but in this case, it was good old CSV.   Thanks to the new functions within SQL Server 2016, this once-daunting task becomes easily doable when you combine dynamic SQL statements and String_Split. String_Split is a unique function where you can take a list of strings and parse them into rows. See the example from MSDN below:     SELECT valueFROM STRING_SPLIT('Lorem ipsum dolor sit amet.', ' ');     value Lorem ipsum dolor sit amet. If you are already thinking that this will provide you with a separate row for each field name of the CSV, you are headed in the right direction. String_Split can provide a glimpse into the format of any CSV file, and with the help of the Pivot operator and dynamic SQL statements, you can build a process to handle changing file structures or the addition of new fields without having to rewrite your code. Technical Requirements In this instance, sensor data is coming from a 3rd party vendor, and after it is processed by a proprietary tool, a CSV file is cut for the client to consume. Per the specifications, only the 1st field is consistent and will contain a date, but all other fields may be in a different order or include never-before-seen fields. Another important piece of information to make note of is that all fields, aside from the date, will be floats (large decimals). date sensor1 sensor2 sensor3 sensor4 19-06-2017 12:00:00.500 0.1 5.0 600.0 1700.0 File Example 1 date sensor5 sensor1 sensor3 sensor17 20-06-2017 12:00:00.500 6.5 21.0 168.5 2.0 File Example 2 High-Level Approach Let’s walk through each step of the process, using the file examples above as a guide. Below is a summary of the steps we will take with more details to follow further down. Step 1: Import CSV into a single-column staging tableStep 2: Parse 1st row into a staging metadata table (field names)Step 3: Compare staging metadata to historic metadata and add new fields if necessaryStep 4: Assemble dynamic pivot query to parse and insert values into staging tableStep 5: Assemble dynamic query to insert into production table   Step 1: Import the CSV into a staging table so that all the data is in 1 column. This can be done by using an obscure delimiter that doesn’t appear in the file like ‘|’ or ‘~’. The point is to keep all the data together so that String_Split can be used on the entire row of data. Step 2: Capture the first row of data into a variable and use the String_Split function to capture the Fields into a staging metadata table (example code below). The staging metadata table is very important when creating the dynamic Insert / Select statements to persist the correct order of the fields since they can vary.     DECLARE @Header VARCHAR(5000), @FieldName VARCHAR(5000);     SELECT @Header = DataTXT     FROM stage.sensor     WHERE RowID = 1           AND FileID = @FileID;     WHILE     (         SELECT CHARINDEX(',', @Header, 1)     ) > 1         BEGIN             SELECT @FieldName = LEFT(@Header, CHARINDEX(',', @Header, 1)-1);             INSERT INTO stage.metadata             (FileID,              FieldName             )             VALUES             (@FileID,              @FieldName             );             SET @Header = SUBSTRING(@Header, CHARINDEX(',', @Header, 1)+1, 5000);         END;     INSERT INTO stage.metadata     (FileID,      FieldName     )     VALUES     (@FileID,      @Header     );     Step 3: After capturing the fields from the incoming file, you should compare them to the existing metadata fields expected in the resulting table that the data will be inserted into. The following code creates dynamic SQL statements that can be executed against the production table to add additional fields. Since it’s a given that the fields are all floats except ‘date’, then you don’t have to worry about conversion issues. However, we worked out a solution that added an additional field to the metadata table indicating the data type for future flexibility.     DECLARE cur_fields CURSOR FORWARD_ONLYFOR SELECT       s.FieldName,       'Alter Table dbo.sensor ADD ['+s.FieldName+'] FLOAT' AS Remediation    FROM stage.metadata AS s    WHERE NOT EXISTS    (        SELECT           *        FROM dbo.metadata AS m        WHERE s.FieldName = m.FieldName    )          AND s.FileID = @FileIDDECLARE @FieldName VARCHAR(500),        @sqlAlter  NVARCHAR(1000); OPEN cur_fields; FETCH NEXT FROM cur_fields INTO @FieldName,                                @sqlAlter; WHILE @@FETCH_STATUS = 0    BEGIN        INSERT INTO dbo.metadata_history        (           FileID,           FieldName        )        VALUES        (@FileID,         @FieldName        );        INSERT INTO dbo.metadata        (           FieldName        )        VALUES(@FieldName);        EXEC sp_executesql           @sqlAlter;        FETCH NEXT FROM cur_fields INTO @FieldName,                                        @sqlAlter;    END;     Steps 4 & 5: The last 2 steps are accomplished using 1 procedure as shown in the code below. The first part will use the metadata tables to pull out the fields necessary for the Select and the Insert. A dynamic SQL statement is then created to parse the data within the CSV, skipping the header row. The final dynamic SQL statement creates a Merge based on some other client requirements, but a simple insert would also work depending on your situation.     DECLARE @sqlCMD       NVARCHAR(MAX),        @sqlHeader    NVARCHAR(MAX),        @sqlStage     NVARCHAR(MAX),        @Fields       NVARCHAR(MAX),        @SourceFields NVARCHAR(MAX),        @SetFields    NVARCHAR(MAX); SET @Fields = '';SET @SourceFields = '';SET @SetFields = ''; SELECT   @Fields = @Fields+'['+FieldName+'], ',   @SourceFields = @SourceFields+'source.['+FieldName+'], ',   @SetFields = @SetFields+'target.['+FieldName+'] = source.['+FieldName+'], 'FROM stage.metadataWHERE FileID = @FileID      AND FieldID > 0; SET @Fields = LEFT(@Fields, LEN(@Fields) - 1);SET @SourceFields = LEFT(@SourceFields, LEN(@SourceFields) - 1);SET @SetFields = LEFT(@SetFields, LEN(@SetFields) - 1);TRUNCATE TABLE stage.sensor_parsed;SET @sqlHeader = 'WITH RawData(RowID, FieldID, Date, SensorValue) AS (SELECT RowID, ROW_NUMBER() OVER(PARTITION BY RowID ORDER BY RowID) AS FieldID, LEFT(DataTXT, CHARINDEX('+''''+','+''''+', DataTXT, 1)-1) AS Date, value FROM stage.sensor s CROSS APPLY string_split (SUBSTRING(DataTXT, CHARINDEX('+''''+','+''''+', DataTXT, 1)+1, 5000), '+''''+','+''''+') WHERE FileID = '+CONVERT(VARCHAR(10), @FileID)+' AND RowID >= 1 and isnumeric(value) = 1), Result(RowID, Date, FieldID, FieldName, SensorValue) AS (SELECT r.RowID, Date, m.FieldID, m.FieldName, r.SensorValue FROM stage.metadata AS m JOIN RawData AS r ON m.FieldID = r.FieldID where m.FileID = '+CONVERT(VARCHAR(10), @FileID)+') ';SET @sqlStage = 'insert into stage.sensor_parsed (RowID, Date, EquipmentID, FieldID, FieldName, SensorValue) select RowID, Date, FieldID, FieldName, SensorValue from Result where SensorValue is not NULL';SET @sqlCMD = @sqlHeader + @sqlStage;EXEC sp_executesql   @sqlCMD;SET @sqlCMD = '';SELECT   @sqlCMD = 'merge dbo.sensor as target using (SELECT '+CONVERT( VARCHAR(10), @FileID)+', [Date], '+@Fields+',  getdate() as [CreateDT] from (select [RowID], [Date], [FieldName], [SensorValue] from stage.sensor_parsed ) as s Pivot(Max([SensorValue]) for [FieldName] in ('+@Fields+')) as p) as source ([FileID], [Date], '+@Fields+', [CreateDT]) on (target.[Date] = source.[Date] and target.[FileID] = source.[FileID]) when matched then update set '+@SetFields+' when not matched then insert([FileID], [Date], '+@Fields+', [CreateDT]) values (source.[FileID], source.[Date], '+@SourceFields+', source.[CreateDT]);';EXEC sp_executesql   @sqlCMD;     Summary These project requirements really drove the direction of this process, resulting in a SQL table that can expand horizontally as new fields come in through the CSV. However, there are endless scenarios making for an even simpler process, assuming requirements differ from the above-mentioned project, and you have the freedom to consider other storage methods, such as JSON. String_Split is a fantastic new tool for dealing with edge cases and in this engagement, it provided a fairly elegant solution which would have been next to impossible in prior versions of SQL Server. Have questions about SQL? Our team of experts would love to chat with you!"
"192" "If the predictive analytics team of your organization is already using Microsoft R Server, you may have missed an exciting feature that helps with the operationalization of your models: Web Services. A story we often hear from our clients is that once a data scientist creates a satisfactory model, it is often difficult to productionalize the model for use across the organization. In other words, packaging the model to be accessible - or callable - by other individuals and multiple systems can be challenging. Luckily, with Microsoft R Server, this process is simple. Your entire data science team can publish models such that anyone you wish may consume them through a web service. Enhance Your Analytical Workflow In many organizations, struggling to operationalize the work of the data science team is a common problem. Despite the team building exceptional models with great business impact, it often takes far too long to implement their solutions. R, while a great language for machine learning and statistical analyses, does not have an innate ability to productionalize a predictive model beyond saving the model objects to the user's local machine. Traditionally, it has been a very manual and labor-intensive task to put a predictive model into production for continual use. I've even heard of companies having to hard-code linear regression coefficients into SQL Server to calculate predictions. In some cases, models have to be recoded into another language (.NET, for example) to work with other systems. This does not have to be this difficult.  Operationalize your analytics with R Server This is where Microsoft R Server comes into play. Microsoft R Server makes publishing your model as a callable web service simple; breaking the barrier from model testing to its use in production. Using only a few lines of code, a published model can be called by practically anything using an application program interface (API). With Microsoft R Server as your deployment engine, your data science team will require far less attention from the IT infrastructure team (besides the initial server setup, of course).  Operationalize your analytics with R Server For more information about the features of operationalization, follow this link. Requirements Web services are created using the mrsdeploy package, which is included in Microsoft R Server 9.x and Microsoft R Client 3.x. For remote execution of a model, participating machines (nodes) can have one of the following configurations: Two machines running the same version of R Server 9.x. (They may even be on different supported platforms, such as one Linux and one Windows.) One machine running R Client 3.x and one machine running R Server 9.x, where the R Client user remotely logs into the R Server instance. (Execution is always on the R Server side.) The requirements for remote execution include: An R Integrated Development Environment (IDE) such as RStudio or Visual Studio with R Tools. Authenticated access to an instance of Microsoft R Server. Configuration and Security R Server offers two types of configuration for operationalizing analytics and remote execution: One-box configuration: One web node and one compute node run on a single machine. Set-up is a breeze. This configuration is useful when you want to explore what it is to operationalize R analytics using R Server. It is perfect for testing, proof-of-concepts, and small-scale prototyping, but might not be appropriate for production usage.  Configuring R Server to operationalize analytics with a one-box configuration   Enterprise configuration: Multiple nodes are configured on multiple machines along with other enterprise features. This configuration can be scaled up or down by adding or removing nodes. This is likely the more permanent implementation of R Server for your organization as it utilizes enterprise-grade security while allowing for scalability above that of the one-box configuration.  Configuring R Server to operationalize analytics (Enterprise Configuration)  For more information about enterprise configuration, follow this link. Security should always be a top priority when it comes to your data. Luckily, R Server's operationalization feature offers seamless integration with popular enterprise security solutions like Active Directory LDAP or Azure Active Directory. You can configure R Server to authenticate using these methods to establish a trust relationship between your user community and the operationalization engine for R Server. Your users can then supply simple username and password credentials in order to verify their identity. User access to the R Server and the operationalization services offered on its API are entirely under your control as the server administrator.  In addition to authentication, you can add other enterprise security around Microsoft R Server such as: Secured connections using SSL/TLS 1.2. (Strongly recommended for all production environments.) Cross-Origin Resource Sharing to allow restricted resources on a web page to be requested from another domain outside the originating domain. Role-based access control over web services in R Server. For further security documentation, follow this link. Create a Web Service Step 1 - Ready your model Web services offer fast execution and scoring of arbitrary R code and R models. They can contain R code, models, and model assets.   Step 2 - Authenticate with R Server Before you can publish to the server, you must first login. Depending on your authentication method, you will either use remoteLogin or remoteLoginAAD to do so. Use the remoteLogin function in these scenarios: Authenticating using Active Directory server on your network Using the default administrator account for an on-premises instance of R Server     > remoteLogin(\"http://localhost:12800\", username = \"admin\", password = \"\",diff = TRUE,session = TRUE,commandline = TRUE)     Use the remoteLoginAAD function if you are authenticating using Azure Active Directory in the cloud.     > remoteLoginAAD(endpoint, authuri = https://login.windows.net,tenantid = \"<AAD_DOMAIN>\", clientid = \"<NATIVE_APP_CLIENT_ID>\", resource = \"<WEB_APP_CLIENT_ID>\", session = TRUE,diff = TRUE,commandline = TRUE)      Step 3 - Publish your model to the server as a web service After you've authenticated, use the publishService function in the mrsdeploy package to publish a web service.     # Publish a standard service 'mtService' version 'v1.0.0'# Assign service to 'api' variableapi <- publishService(\"mtService\",code = manualTransmission,model = carsModel,inputs = list(hp = \"numeric\", wt = \"numeric\"),outputs = list(answer = \"numeric\"),v = \"v1.0.0\")      Step 4a - Call your model through the web service with R Once your model has been published to the server as a web service, you (and others) may now consume it. Using the getService function, you can retrieve any services with which you are allowed to interact.     # Get service using getService() function from `mrsdeploy`.# Assign service to the variable `api`api <- getService(\"mtService\", \"v1.0.0\")# Start interacting with the service by calling it with the# generic name `consume` based on I/O schemaresult <- api$consume(120, 2.8)# Or, start interacting with the service using the alias argument# that was defined at publication time.result <- api$manualTransmission(120, 2.8)     For more code examples, follow this link. Step 4b - Get your Swagger on. (Using RESTful APIs with .JSON) In addition to calling the web service from within R, you can also  access these RESTful APIs by using a Swagger .JSON file. Use a Swagger code tool to generate an API client library that can be used in any programming language, such as .NET, C#, Java, JavaScript, Python, or Node.js. The API client simplifies the making of calls, encoding of data, and markup response handling on the API.  How to Integrate Web Services and Authentication into your Application R Server provides several Swagger templates each defining the list of resources that are available in the REST API and the operations that can be called on those resources. Additionally, another unique Swagger-based .JSON file is also generated for each and every web service version that is published. For more information on Microsoft R Server web services for use in app development, follow this link. As you can see, in just a few lines of code, your model transforms from just an object on one person's machine to a callable service for everyone. This capability helps your predictive analytics team productionalize their work and cause a greater impact in the organization. If you are a data scientist looking to take your work to the next level and put your models in motion, a great place to start is with Microsoft R Server. You can read more about it on Microsoft's post, Get Started for Data Scientists. Still have questions? Contact us!"
"193" "Much can change for the better when an organization decides to build a business intelligence strategy. But how does a company begin to adopt a BI-centric mentality, and why should they want to? A closer look the almost decade-long journey of a major nursing and assisted living organization, with more than $3 billion in annual revenue, answers some of those questions. Let’s take a look at how they managed to make a series of crucial business changes and examine the huge financial benefits to their business. July 2017 marks BlueGranite’s ninth year of partnership with the above-mentioned organization. With hundreds of skilled nursing and retirement communities across the U.S., the company had a stockpile of data available for exploration when we initially began working together. The desire for continual innovation has fueled this organization’s growth, as well as our partnership. What began with a small initial engagement using SQL Server SharePoint BI has since grown exponentially. Over the years we have helped the organization implement a data warehouse, BI analytics solutions, and provided new development, support, maintenance, and enhancements. Now, over 1,200 users have access to the BI solution BlueGranite built and fully maintains. While all of this progress and growth has been positive, it didn’t happen overnight. The client’s initial situation is described below: Their Traditional Reporting Tools could no longer support their needs for a proactive approach to managing the business. The organization was dealing with delayed and inconsistent reporting. Performance management was being delivered through the form of a quarterly scorecard that was created individually at each facility. By the time data was finally able to be consumed, it easily could have been a few months old. Because of the delay in information, they were unable to make proactive, data-driven decisions. After understanding their initial setup, we worked with the client to get a clear vision of their wants and needs. Overall, they were looking for a business intelligence solution that provided data quickly and efficiently, had low licensing/operating costs, was simple to use, and was something that their IT department was already familiar with. With that information, we came up with a plan to make great improvements to the organization’s day-to-day management. That plan included: Leveraging our experience building Microsoft Business Intelligence solutions that integrate financial, clinical, resident, labor, and HR data from over 20 different sources together in one easy-to-access place. Providing timely data refreshes – monthly, weekly, daily and hourly –  for corporate and individual facility/community levels. Enabling a distinct view into the past, present and future predictions of operational performance. Creating a scorecard containing the most important KPIs for the business so that they could use their data to gain quick insights and make informed decisions. Giving the organization the ability to easily pinpoint on-target and off-target data trends. Building Green-Yellow-Red “traffic light” icons in order to provide fast, at-a-glance information. Delivering upcoming financial projections based on accurate and timely data. Providing leaders with the ability to discern opportunities and quickly address areas of concern before they became a bigger issue. When we started our work, we took a step back and broke everything out into phases so that we could take a strategic initiative and set realistic timeline expectations for everyone involved. In Phase 1, we pulled three different data sources (accounting, time tracking, and basic care statistics) into one place so that the client would have a single source of truth in their reports. We followed up with Phase 2, which consisted of expanding on additional data sources, such as government data sets, resident/family satisfaction surveys, etc. A major issue was that the client was only able to see their financial data after a month had closed, posing problems with scheduling and finding ways to manage labor costs. By bridging historical account data to live performance, BlueGranite was able to make a positive impact on their day-to-day insights. For example, PDF reports were being generated after each month closed, and they were cumbersome to deal with, plus didn’t provide any real understanding for the business. Implementing a live report with daily updates and linking back to historic data provided company leaders with the data they needed to proactively manage the business and make fast, informed decisions. The outcome of building this solution had a lasting impact on the organization, especially when it came to saving on annual overtime labor costs. Specifically, leading and lagging indicators (which help tell an organization where they are and how they did) played big roles in the success of the solution. While their lagging indicators – such as financial performance – were helpful in letting the client know how they performed every month, implementing leading indicators opened new doors into how they planned for staffing needs. Before the solution was in place, the client was unfortunately having to spend extra funds on labor to ensure that they were always compliant with the government’s laws and regulations targeted at healthcare facilities such as this one. By tracking the predicted daily population at each care facility, management was able to proactively adjust schedules and ensure that they had enough staff available to meet the law. This helped the organization prevent overstaffing (and overpaying) for additional employees, as well as ward off federal fines for being understaffed. Once the client had access to timely and accurate data, they saw a multimillion-dollar reduction in annual overtime costs. If the cost savings were not enough, the organization also experienced a number of other benefits from making the upgrade. Their new centralized and automated business intelligence system gave clarity and valuable insights into their data when it came to timeliness and accuracy. This accuracy lead to an enviable return on investment thanks to better and faster decision making, plus they were able to reduce their IT costs by 90%. Data search time was also cut down by 80%, and the licensing cost for the new solutions ended up costing hundreds of thousands of dollars less than competing solutions. So, what does happen when organizations adopt a BI-centric mentality? For one, companies can stop reactive behavior and start to take a proactive approach to making important decisions. As time passes and the solution advances, one could argue that organizations can begin to take a predictive approach and plan for the future with the help of advanced analytics and data governance. Additionally, we see the BI-centric mentality drive savings and improve day-to-day business for our clients – something many businesses want but might not know how to achieve. Want to know more about how BlueGranite can help you achieve your advanced analytics goals? Contact us!"
"194" "Mastering when to use a calculated column versus when to use a measure – a concept that is often misunderstood when working with Data Analysis Expressions (DAX) in Power BI – is key to reporting success. Choosing the wrong one can result in anything from poor report performance to incorrect results.   By understanding how both calculated columns and measures are evaluated, the instances in which we would use one or the other, and by structuring your code to take advantage of the strengths of the DAX language, you will be able achieve faster reporting and (if your code is appropriate for the scenario) more accurate results.  In general, we use calculated columns and measures to perform arithmetic operations on top of the data that we already have in our model. A simple example could be having a sales table with a Unit Price column and an Order Quantity column, and the desire to know the Order Total for each line, or in aggregate. Let’s work through some common business scenarios with the following Power BI file, available here. Understanding Calculated Columns Figure 1 When trying to get a grasp on calculated columns, it is essential to know why evaluation context is significant. Evaluation context is the “environment” under which the formula is evaluated. Although calculated columns can compute aggregate values, the evaluation context is performed by default for each row, which suits calculated columns for some computations, but not others. In the example above (Figure 1), if we would like to know the Order Total for each line, which could represent an individual order from a customer, it would be appropriate to use a row-by-row calculation, which would make a calculated column a good choice. The DAX expression for the calculated column would look like this:     Order Total ='Sales'[Unit Price] * 'Sales'[Order Qty]           With calculated columns, data in the column is stored in the xVelocity in-memory database, meaning that a calculation is made before the model is ever queried by the user. From a performance perspective, calculated columns are positive given that there is a smaller virtual memory requirement as the user interacts with the report. The downside is that calculated columns take up more storage in your database. Calculated columns can give users other advantages as well, such as the ability to view values in the column, or use of the calculated column in a slicer or in a measure to filter data under certain criteria. Let’s say we want to use our Sales table (in the provided sample file) to classify items as low, medium, or high priced (Figure 2). We can use the following DAX expression in a calculated column to accomplish this, which will give us the ability to add a slicer and see only the category that we are interested in, as shown below:     Cost Bucket =IF (    'Sales'[Unit Price] < 50,    \"Low\",    IF ( 'Sales'[Unit Price] < 100, \"Medium\", \"High\" ))     Figure 2 Here is an example with the “High” Cost Bucket calculated column slicer selected: Figure 3 Interpreting Measures In general, measures are used to calculate aggregates, such as the sum or average of a column. Measures are calculated at the time of your query, which means that they aren’t stored in your database, but use processing power to execute a query at the time of your request. Because measures are not stored in memory, they are generally faster, although it is important to recognize the tradeoff between utilizing in-memory storage or processing power in an instance where either a measure or a calculated column could be used. Continuing with the example above, a DAX expression where it would be appropriate to use a measure could be an instance of when you want to calculate the sales for all of your high-priced inventory (Figure 4). Note that we are using a calculated column, Cost Bucket, within this measure. In DAX, it is considered best practice to “reuse” metrics in this manner. If you were to redefine what you would consider low, medium, and high-priced, these changes would carry through to this measure. For this measure, the expression would look like this:     Sum of High Orders =CALCULATE ( SUM ( 'Sales'[Order Total] ), 'Sales'[Cost Bucket] = \"High\" )     Figure 4 It is also useful to know how measures are evaluated in Power BI. Let’s break down the steps using the example above: Examine the evaluation context. In the example above (Figure 2), the DAX engine is selecting high-priced inventory, or where 'Sales'[Cost Bucket]=\"High\". Apply the evaluation context to the underlying table. In the example above (Figure 3), this means that Power BI is filtering 'Sales'[Order Total] only for high-priced inventory. Perform the aggregate operation. In the example above, this means that we are summing the Order Total, or SUM('Sales'[Order Total]). Return results. As we can see above, the measure, Sum of High Orders (Figure 4), matches the results when we total the rows of the calculated column, Order Total (Figure 3), in the table. Although there are additional technical and performance considerations for when to use a calculated column and when to use a measure, the high-level differentiation between the two are: Use a calculated column when you want to evaluate each row Use a measure when you need an aggregate If you are interested in learning more about measures or calculated columns, be sure to check out Microsoft’s Power BI Documentation. Additionally, if you have a question or want to know more about how BlueGranite can help your organization with Power BI, contact us!"
"195" "I recently got to work with a big-city airport to help them architect and begin implementing a modern data platform on the Azure Government Cloud. The client had the following set of requirements/restrictions: Needed to use the Azure Government Cloud Was willing to use/purchase SQL 2016 licenses Wanted users to consume Power BI reports Had lots of data they wanted to land and archive, but not necessarily bring into a data warehouse just yet Some of the highlights and challenges of the final implementation are covered below.  Working in the Azure Government Cloud Working in the Azure Government Cloud poses an interesting challenge. It usually falls behind the commercial cloud in functionality and services due to hurdles surrounding a more stringent certification process. For example, a typical commercial cloud analytics Azure platform might look something like the diagram below:  But as of now, June 2017, Data Factory and Data Lake Store are not available in the government cloud. Another drawback? The broad Power BI offerings under the commercial Azure cloud umbrella are considerably narrower in the government version. So, to accommodate the client’s requirements, and make the most of a more restrictive environment, we employed a little creativity. That resulted in an architecture that looks more like the following diagram:  Blob Storage for a Data Lake Because the Azure Data Lake Store was not available in the government cloud, we used Blob storage to create a data lake. This provided an efficient means to land and archive data. Additionally, the size of the data (both current and future) did not come anywhere near the limits imposed by the storage service. Blob storage also integrates well with all the other Azure Data platform services. The downside to using Blob was the fact that it does not support Azure Active Directory for authentication. The concern here was minor though, as the client did not anticipate many (if any) users needing to go directly to the flat files stored in the data lake. Using SQL 2016 for the Data Warehouse With some of the key Platform-as-a-Service options, like Azure Data Factory and Azure Analysis Services, not available in the government cloud, SQL 2016 on a virtual machine became very appealing as the core of the data warehouse. The SQL database engine was used to house the data mart, and by staging the data in Blob storage, PolyBase could be utilized to load the data warehouse. This allowed the client to a) store most of the raw staging data using cheaper file storage as opposed to storing it in the SQL database, and b) allowed a way for users to utilize SQL and external tables to browse the data lake archive. In-memory columnstore indexes were also applied to help serve up data to the reporting layer. In the absences of Azure Analysis Services in the government cloud, Tabular Analysis Services was used on the SQL 2016 VM. This worked great, as the client was already using Power BI, and was optimistic about the ease of translating their Power BI models into Tabular, server-side models. SSIS was used for automation. The client would have liked to use Data Factory, but again, it wasn’t available in the government cloud. SSIS is a mature, well-proven product that met any use-case the client had. A complication with using SSIS (as opposed to Data Factory) was that it meant the client had to have some sort of site-to-site VPN connection established in Azure to pull from on-premises data sources (or have disconnected processes push data into Blob storage). This would have been different with Data Factory as a gateway would have been all that was necessary. Finally, the client opted to use the new Power BI Report Server to publish and consume SSRS and Power BI reports. This was ideal for them because they currently use SSRS and want to begin using Power BI, and their data models would be housed in Tabular Analysis Services (which at the time of the post was the only data source supported). As all their initial use-cases were a pure publish-reports-to-consumer scenario (as opposed to the need for collaborative, self-service functionality from the portal), the scope of functionality in the Power BI Report Server was sufficient. They also understood that if their use-cases changed over time, they would be able to easily migrate to the Power BI Cloud Service. Final Thoughts When architecting solutions in the Azure Government Cloud, you must be aware of the differences in the availability and functionality of services compared to the commercial cloud. This can make it more challenging to put together the best possible solution. Hopefully this example helped illustrate that an effective and efficient data platform can be put together in the government cloud without sacrificing functionality and security. BlueGranite specializes in the challenge of taking your company’s unique use-cases and requirements and putting together an optimal architecture to support both your functional and financial needs. Contact us today to learn more about how we can help."
"196" "Power BI Premium is an immensely powerful Business Intelligence ecosystem that enables users to quickly connect to, analyze, and visualize a wide range of data. As businesses realize the value of Power BI’s capabilities and increasingly adopt and rely upon its services, they may begin to think about how they can upgrade their capability or improve functionality in Power BI.   Microsoft recently announced the introduction of the Power BI Premium service, a capacity-based offering that gives organizations greater flexibility to allow users to access, collaborate upon, and distribute content. Let’s take a look at the features of this new offering and the instances in which it could be an appropriate upgrade for your business. If your firm is considering Power BI Premium, there is a good chance that there are some users in your organization already making use of Power BI Pro. The biggest difference between the two offerings is that Power BI Pro utilizes user-based licensing, while Power BI Premium uses capacity-based licensing. Simply stated, when you sign up for Power BI Pro, you pay per user, per month (currently retails at $9.99), while with Power BI Premium, you pay for a dedicated power bi premium capacity for the month. This enables your business to publish reports broadly across an enterprise without having to worry about if individual recipients are licensed Power BI users. Let’s review some of the features currently available in Power BI Pro: All of the features present in the free version of Power BI, including connecting to and preparing data, creating reports, and publishing to the Power BI service Collaboration via app workspaces and distribution of reports via peer-to-peer sharing and Power BI apps Usage metrics and governance of how data is accessed and consumed Analyze in Excel Row-Level Security And now, Power BI Premium features allow businesses to: Share reports across an enterprise without requiring recipients to be individually licensed Scale performance: dedicated hardware and capacity, maintained by Microsoft, for your organization to allocate, scale, and control according to your business needs Less restriction on workspace sizes (Currently up to 100 TB for your workspace, depending on Premium capacity purchased) Less restriction on refresh rates (Currently up to 48x refreshes per day) Ability to maintain Power BI reports on-premises with Power BI Report Server Support for Power BI embedded solutions with APIs for custom development (Separate SKUs are also available for embedded-only scenarios) Additionally, you should be aware of some of the limitations of the service: Typically, Premium will only be cost effective for organizations with large deployments or a large number of read-only users Power BI Premium requires a Power BI Pro subscription for authors Power BI consumers (readers) can’t share in Power BI Premium. Sharing (even in Premium) requires a Pro license When upgrading to Power BI Premium, your business can improve the functionality of Power BI to closer match its needs. Here is an example in which Power BI Premium may be an appropriate solution: Let’s imagine you are part of an organization of 10,000 people. 500 of your users will require authoring and collaboration, while another 5,000 users will consume BI content multiple times a week, and your remaining 4,500 users will only consume content occasionally. In this scenario, the 500 self-service BI authors could use Power BI Pro to collaborate and create reports, while Power BI Premium can be used to distribute this content to the other 9,500 individuals in your organization who only need to consume this information. The total cost of this solution is $29,970 per month. When comparing this to the cost of paying for 5,500 individual Pro licenses  at $54,945 per month, or even 10,000 Pro licenses at $99,900 per month, you can see the cost savings associated with the Premium service.  Conversely, if you are part of a small organization of 300 people with a combination of 50 users who require authoring and collaboration, 125 users who consume BI content multiple times a week, and an additional 125 users who consume content occasionally, the total cost of a Power BI Premium solution would be $5,495 per month. Comparing this to the cost of paying for 175 individual Pro licenses at $1,748 per month or 300 Pro licenses at $2,997 per month, it might not make sense for your organization to invest in Premium. To see how either of these scenarios apply to your business, you can experiment with the Power BI Premium calculator to forecast various costs and performance prices.  Lastly, let’s look at some features that were not available in Power BI Premium at launch, but are on the road map for future releases: Support for larger datasets Incremental refreshes so that only the newest data is loaded into Power BI Pin datasets to memory, increasing responsiveness and load times for select reports Dedicated data refresh nodes to improve performance Read-only replicas to handle larger query loads Geographic distribution for optimal performance Power BI Premium provides dedicated resources for running Power BI offerings at your organization, giving you dependable performance and larger data volumes. With capacity-based licensing, Premium enables the widespread distribution of content without requiring your organization to purchase per-user licenses for anyone who consumes BI content. If you have further questions about Power BI offerings and what they can do for your business, contact BlueGranite."
"197" "Power BI Apps are a new way of sharing reports and dashboards – recently announced in the May 2017 Power BI update. In the future, Apps will replace Content Packs and are an evolution of sharing content with a large user base of consumers.  Here are some of the most important differences between the existing Content Pack sharing option and Power BI Apps: Power BI Apps have a 1-to-1 relationship with an App workspace. You cannot ‘exclude’ reports from being published if they exist on the workspace. This can help trace App components more easily compared to Content Packs. Content Packs can be copied, whereas Power BI Apps can’t. Although copying may be helpful to users who want to edit the reports and create their own versions, the downside is that the copy is disconnected from the original and there is no way to re-sync them. With an App, users always have the ‘latest and greatest’ version of the reports (but no copies are allowed at the moment). Previously, consumers would often get lost trying to find Content Packs as the Power BI service would place them on the user workspace. With Apps, there is a new link on the Power BI service navigation menu that takes you specifically to apps you have connected to. Now, within one workspace, users are not mixing reports that they may have created on their own with those that have been shared with them through an app, simplifying the navigation experience.  App authors can customize the color and logo of a pack so that it becomes more attractive and informative to users.  When should you use Power BI Apps? Well, it depends. To answer this question, let’s step back a moment and review some recent developments. In the May 2017 update, Microsoft announced a new licensing option for Power BI called “Premium”. This is a capacity license that is ideal for organizations with a broad group of consumers. The assumption is that some activities (such as data preparation, modeling and publishing) are not necessarily of interest to all individuals who want to benefit from Power BI. The distinction between “Power Users” (or even “Super Users”) and “Consumers” is real; hence being able to carve out licensing that targets a broad consumption of reports built by a smaller set of authors at an enterprise scale, is significant. At the same time, many Power BI authors have been wondering how can they make a distinction between development and production environments. Although companies don’t always proactively plan for this while preparing to roll out Power BI, a phase of prototyping is normal in the Power BI development life cycle.  With Power BI Apps, you can follow this process efficiently. A Super User (model author) will create an App workspace and invite other collaborators (Power Users) to develop reports on top of the model. Once the reports and dashboards are complete, they can be “packaged” into an App and published for a larger base of consumers. This development cycle is appropriate for a number of reasons: Currently, in App Workspaces, you cannot add security groups or distribution lists. Due to that, you have to add individuals (one at a time) when defining the group membership. Given that for each dataset/report/dashboard you would normally have a small group of ‘collaborators’, the app workspace model works well here. Power BI Apps, in contrast, can be shared with entire groups of people (AAD security groups, for example). This makes it easy to deploy to hundreds or thousands of consumers. Power BI Apps can be assigned to consumption capacity (under Premium licensing), so organizations avoid paying for every user accessing the report or dashboard. Deployed Power BI apps are ‘read only’: They cannot be edited or modified by consumers. This is appropriate when the target audience of the App is consumers (as opposed to Super Users or Power Users). Given that dashboard/reports that exist in App workspaces can be modified and consumers do not see those changes until publishing (or re-publishing), they serve as “staging” areas for development and QA prior to deployment to production. Row-Level Security is applied only to read-only users. This should not be a problem for apps, given they benefit mostly consumers, who have read-only access. On the contrary, collaborators with editing rights on the App workspace will not see RLS applied to them – which is OK as they are helping to build the reporting asset. Current Issues with Apps As with every production evolution, it is not perfect. Here are two issues we have seen with the current offering: There does not appear to be a way to know if the report published as an app is different from the one in the workspace. Knowing the last published date does not appear to be available either, even through Audit Logs. Dashboards/reports that exist in Apps cannot be ‘personalized’ by users to create their own filtered versions of reports. It is also worth noting that there is another way of sharing dashboards (outside of workspaces or apps/content packs): “Peer-to-peer shares” involve a single dashboard and, in a similar way to Apps or Content Packs, they can be used to distribute content to large audiences (except peer-to-peer shares cannot be distributed to O365 groups, whereas Content Packs can). Although the name may imply you are sharing with a single individual, in reality they allow you to share with entire groups of AD users. Want to know more about Power BI Apps? Feel free to drop us a line and we will be happy to answer your questions."
"198" "(The Preview Edition) I have a confession to make. I love movie previews. LOVE them. Many people complain when they go to the theater to see their long-awaited summer blockbuster, only to be greeted with 20 minutes of movie trailers. I am not one of those people. I get lost in each two-minute mini-movie, and always find myself wanting more. The same can be said about Microsoft’s monthly Power BI Desktop update.  Each month, Microsoft unveils several long-awaited features and enhancements to its Power BI Desktop product. These new features often serve as the solution for customer requests and technical requirements that we encounter on Business Intelligence projects. In this post, we will look at three Power BI Desktop features that are still in preview, and discuss how they can help in your next BI initiative. Enabling Preview Features Before you can begin exploring the features that are in preview, you must first enable the desired preview features within Power BI Desktop. To enable preview features, follow the steps below: Open the Power BI Desktop app and navigate to the File drop down on the navigation ribbon. Navigate to Options and Settings <U+21D2> Options. In the Options window that appears, under GLOBAL, select Preview features towards the bottom. Your screen should now look like the screenshot below. Select any of the preview features you would like to enable and click OK.  SPOILER ALERT! I have only enabled the preview features we will be discussing throughout the rest of this post. Shape Map The Shape Map visual is the oldest preview feature discussed in this post, but it’s still a good one. Unveiled almost a year ago in the June 2016 update, the Shape Map allows you to make regional comparisons on a static map. While the traditional Map and Filled Map visuals leverage Bing Maps for precise geocoding, the Shape Map is used for relative comparisons. As an example, let’s suppose you work for a company that serves customers across all 50 states. You want the ability to plot your sales territories on a map, while also providing key sales metrics as tooltips. As shown below, the Shape Map provides a no-frills map for your regional comparisons, yet its simplicity perfectly satisfies the requirements that were given. While working with customers, we often find that many prefer these simpler mapping features for deliverables, such as executive-level dashboards and company infographics. In the example below, we used the USA: states map to plot our sales territories, but the Shape Map also natively supports maps for other countries, including Canada, Mexico, Australia, and France. For an in-depth reading on Shape Maps, check out this documentation on the Power BI website.  Shape Map example that shows a company’s four sales territories across the US. Tooltips were added to provide key sales metrics about each state when you hover over them. Report Themes In the March 2017 Power BI Desktop update, Microsoft released the long-awaited Custom Report Theme feature. Report themes allow you to apply a custom color theme to your entire report. Often, customers want the ability to apply a corporate color theme across their reports. Others want to apply different color themes by department. And some customers just want something different than the default theme Power BI provides. With the new Report Theme feature, we can import a custom color theme into our Power BI report with just a few clicks. The first step, however, is to identify the colors we want to use and create a simple JSON file that will contain the metadata about the colors in our custom theme. If you’re thinking to yourself, “Great. I don’t know anything about JSON!” do not fret. The Power BI team has provided a sample template you can leverage, so all you have to do is copy and paste the color codes you want. The screenshot below shows a sample JSON report theme. Example of what your JSON report theme could look like. Not feeling creative? No worries, the Power BI community has you covered with pre-built themes you can import into your reports. You can check out the gallery here. If you’re interested in creating your own theme for corporate branding purposes there are some useful sites on the web to help you get started. Color Combos provides an easy-to-use Grab Colors tool that lets you type in the website of your choice. The tool will then read the website’s HTML and CSS files and return all the HEX color codes that it finds. You can then pick and choose which colors you want to use in your custom report theme JSON file. At BlueGranite, we have personally used this tool to help us create a corporate report theme for a customer that was very well received. Once you have your JSON file with your desired color HEX codes, you can then import the theme into Power BI Desktop. On the Home tab of the Desktop ribbon, you will find the Switch Theme button (again, the custom report theme feature must be enabled to see this button). Once you click on the button you can choose to import your desired JSON file or revert to the default Power BI theme.   The screenshot below shows a sample Sales report I created using colors found on BlueGranite’s website! For an in-depth reading on custom report themes, check out this documentation on Power BI’s website.  Sample report showing colors from a custom report theme that was imported into Power BI Desktop. New Table & Matrix Visuals Additionally, in the March 2017 Power BI Desktop update, the Power BI team gave the Matrix visual a long-awaited facelift. They then carried over this update to the Table visual in the May 2017 update. Rather than updating the existing visuals themselves, new Matrix and Table preview visuals were added to the Visualizations pane for test-driving the new functionality. Table preview visual, with the Matrix preview visual beside it. The original Table and Matrix visuals are still available as well. Some of the new features added to these visuals include the much-needed cross-filtering/highlighting functionality, as well as word wrapping, and a stepped layout for drilling down hierarchies. Table or Matrix cross-filtering/highlighting works just like it does on other visualizations. You can click on an item within your table or matrix and use the Edit Interactions functionality to dictate how it will filter or highlight other visualizations. Building off of our previous example, I added the new Matrix preview visual to my report. I then selected “Texas” within the matrix to cross-filter and cross-highlight the other visualizations within the report.  If you look closely at the Matrix visual, you’ll also see the new stepped functionality at work. Notice the city names slightly indented beneath each state. This helps both for readability, and to conserve some real estate on your report.       Matrix with stepped layout disabled Same matrix with stepped layout enabled Hopefully you enjoy these preview features as much as I do. Microsoft follows a monthly release cycle for Power BI Desktop, so be sure to keep an eye on their site to learn about the latest features released each month. Honorable Mentions Discussing every Power BI Desktop feature currently in preview is outside the scope of this post, but here are additional BlueGranite blog posts that cover several more features we couldn’t cover today: 5 New Power BI Features that will Accelerate Self-Service BI Adoption Power BI with ArcGIS Maps Closes a Critical Gap 3 Helpful Power BI Features You Might Not be Using If you have any questions or want to know more about how your organization can utilize Power BI, contact us!"
"199" "Next in BlueGranite’s Meet our Team blog series is Jon Trapane – a sports fan at heart who enjoys discovering new ways to apply data and analytics for better business insight.   Jon, a Staff Consultant at BlueGranite, has been with our team since early 2016. He received his Bachelor of Business Administration in Computer Information Systems degree from Western Michigan University. Before BlueGranite, Jon spent some time working in digital marketing and assisted with web page design and running social media accounts. Jon chose to get into the data and analytics industry because it is something that has always been interesting to him. Even when he was younger, he would spend time tracking his baseball-themed video game statistics in an old score book to try to figure out if he could improve his performance. That interest in data and analytics grew with Jon and now he gets to work with it every day.  Initially, Jon started off his Staff Consultant career in BlueGranite’s training program where he was able to follow a structured course at his own pace. He enjoyed having the freedom to work through various online classes, mixed with mentoring sessions where he could ask questions and get help. Since advancing through the training program, Jon has spent time working with clients to implement new features and updates to their existing advanced analytics solutions. Additionally, Jon is helping build an end-to end-data warehouse solution with other BlueGranite team members – a project he had been looking forward to starting. When asked why Jon enjoys working at BlueGranite, he had a lot of exciting things to say. Here’s what stood out the most: “One of the biggest reasons I enjoy working at BlueGranite is the autonomy and level of trust each employee is given. There is a lot of room to make your own decisions and express yourself with our open-door policy. This is especially true when it comes to communicating the kinds of project work you prefer to focus on as well as what direction you would like to see your career move in. I think it’s great that to have that kind of support and flexibility from your employer, and really makes work overall more enjoyable and fun.” Jon also had some interesting predictions about the BI industry. He believes that it is going to continue to grow, especially when you consider the sheer amount of data that is being created on a daily basis by businesses. Jon also thinks that there is going to continue to be a shift towards storing data in cloud solutions opposed to on-premises solutions – this is because of how difficult and (sometimes) expensive it is becoming to maintain physical storage areas. In his free time, Jon enjoys volunteering for the local Kalamazoo Area Runners group, as well as participating in a variety of sports. Currently, he likes to dabble in baseball, golf, skateboarding, and bowling. Want to learn more about the BlueGranite team and how we can help? Reach out to us – we’re always happy to chat about data and analytics!"
"200" "This is the first post of a new series: Modernize your Business Intelligence Architecture with the Cortana Intelligence Suite. For part two, please follow this link.  If your Business Intelligence (BI) and Big Data solutions are still on-premises, you should really be analyzing the benefits of migrating to the cloud.  Or, if you analyzed your cloud options last year and weren't ready, then it is time to take another look - things really are moving that fast.    Microsoft Azure is quickly advancing and has created an improved collection of Platform as a Service (PaaS) data platform tools through the Cortana Intelligence Suite.  Through this collection of independent, but tightly integrated products, customers can create robust and scalable BI solutions that are entirely in the cloud and fully accessible to employees and customers in a secure manner.  As you go through and analyze your options, the top 5 things you’ll want to consider are listed below. 1. Agility/Time to Value The number one reason to move to a cloud architecture is because of enhanced agility and time to value.  In an on-premises world, corporations must perform comprehensive solution sizing projects, go through long procurement cycles to purchase expensive hardware, sign Enterprise Agreements in order to purchase software licensing, and then pay someone to put it all together in the correct way.  If that wasn’t enough, you then have to do this every 3 years to upgrade and migrate your solution to keep up with new technologies.  If you're tired of allocating your team’s resources to the triennial on-premises refresh cycle, then the cloud might be right for you.  The Azure Data Platform tools can be deployed in minutes, which allows your team to immediately begin creating business solutions, not worrying about hardware planning and refresh.  Your team can also instantly throttle the solution up or down depending on the horsepower needed during its phase of life.  This agility is valuable in situations when your data growth increases, or your solution becomes more popular. 2. Lower Total Cost of Ownership Comparing cloud and on-premises pricing models is challenging.  When looking at your all-in price for an on-premises solution, you have to factor in things like licensing, hardware, real estate, electricity, support cost, security, deployment cost, and depreciation.  Cloud pricing is much different.  You are typically paying an hourly price for a predetermined performance level.  Performance levels can potentially even be flexed up and down depending on usage patterns, and sometimes paused depending on the tool being used.  The equation for each firm will be different, but the cloud will almost always end up the winner by a small, but meaningful margin. 3. Innovations and Continuous Feedback Microsoft is now operating in a product development cycle where all new product features are being pushed to the cloud prior to going to on-premises solutions. On-premises software refreshes generally happen about every 2 years.  The new features and capabilities getting pushed to the cloud products are typically getting released every month.  Additionally, customers are able to influence the priority of the product teams by voting and contributing feedback via sites like the Azure Feedback Forums.  While this may not seem that important, it is.  The fixes, innovations, and new capabilities being pushed monthly add real business value and allow your teams to create solutions that are impactful to businesses and at a reduced cost. 4. Flexibility In almost every industry and trade, having the right tool for the job is imperative for success.  The Cortana Intelligence Suite has a collection of tightly integrated tools that each provide different capabilities for different jobs/requirements your company might face.  Each tool can easily be deployed and integrated in minutes, which gives your team the right tool for the job without having to provision their own on-premises solutions.  As we'll explore more in the next blog post of this series, this Swiss Army knife platform has a tool for just about everything, including real-time streaming solutions, Big Data processing, and Dare Warehousing at scale.  5. Security Most people would agree that security is the most unfun aspect of a solution to consider, but it is arguably the most important.  Security is held in high regard in Azure which is reflected in the Azure Trust Center, as Azure has the most certifications out of any cloud provider.  The data platform tools in Azure are integrated with Azure Active Directory (AAD) to provide authorization and data-level security, encryption of data in motion and at rest, enable IP restrictions, auditing, and threat detection.  While putting your data in the cloud for the first time can be very scary for most business leaders, Microsoft makes the transition easy with its focus on security.  There's a lot of cloud marketing out there that says migrating to the cloud is as easy as flipping a light switch.  This series isn't one of them.  Migrating your BI solution to the cloud isn't always a perfect match to on-premises, and it isn't always straight forward, but we believe it is worth it.  At BlueGranite, we help migrate our customers to the Azure cloud, as well as help them build green-field solutions starting out in Azure.  We also eat our own dog food, so to speak, as our BI solution has been entirely in the Azure cloud going on 3 years now.  In this blog series, I'll walk you through the considerations for migrating/building in the Azure cloud.  Stay tuned and check out my next post to gain an understanding of what the Cortana Intelligence Suite is and what capabilities it has. "
"201" "As more companies look to utilize advanced analytics on Big Data platforms, it can be daunting for a data scientist to keep up with the myriad data sources and formats. I learned R with smaller datasets – using mostly Excel spreadsheets, and .csv or SAS files (see blog post here from my colleague Colby Ford). Formats like that are great for departmental solutions, development processes, and training – and they’re not going away anytime soon.   As enterprises look to extract maximum value from ALL their data, advanced analytics professionals need to become familiar with other data formats, especially those found on modern platforms like Hadoop. This article will provide a data science perspective on what some of these data sources are (namely Hadoop Distributed File System (HDFS) and Hive), why they’re important, and point to some resources for getting started with sample data. If you’ve been using R with large datasets, or an enterprise interpreter like Microsoft R Server, you might be getting to know your friendly neighborhood data architects a bit better. Let’s all say it – the data science tribe is getting bigger, and that’s a very good thing. Before I started my current role, I knew that the Hadoop thingy was shaped like an elephant, and that was the extent of my Big Data knowledge. These days, I’m talking with customers that want to train machine learning models in R with 100TB of historic data, so I must be able to discuss Big Data and the associated advantages it brings to analytics like distributed storage and computing. Thankfully, I work with a talented data platform team that has really helped me learn – but all this is to say that data science discussions involve an increasingly diverse set of technologies and skill sets. Before we look at specific data formats, let’s start with a brief and simple overview of why a platform like Hadoop is important in the context of data science. Why Hadoop?    Hadoop offers the ability to distribute data files on storage among many computers (HDFS), perform computations with each machine simultaneously on a large number of files (parallel/distributed processing) – AND get back a coherent result as if it were a single operation. Storing and processing data in very large volumes is usually cheaper and faster in a Hadoop environment compared to a traditional data warehouse technology like Oracle or SQL Server. Additionally, there is no requirement for structuring the data in HDFS in the same way a relational database enforces an associated schema. You can just throw your .csv, .txt, image files, etc. into folders with minimal organization and worry about making sense of it later. Hadoop is an open-source project from Apache with commercial distributions from the likes of Cloudera and Hortonworks.  What’s MapReduce and Spark?   Think of these technologies as the software framework within Hadoop that performs all the complicated processing needed for distributed computing. Spark represents the next generation of computing on Hadoop compared to MapReduce, providing advanced capabilities like machine learning, stream processing, and in-memory computing. Writing MapReduce or Spark applications from scratch can be complicated, so there are a variety of APIs or interfaces to other tools, like R, available for users. How do I query and use data that’s distributed on Hadoop in R? Great question! And also the purpose of this blog. With files all over the place and no required structure or schema, getting a dataset useful for modeling might seem difficult. Thankfully, there is a technology called Hive that provides a very user-friendly, SQL-like language called HiveQL for querying file systems, such as HDFS, that integrate with Hadoop and avoids writing Java MapReduce code directly. Since most people that want to use Hadoop already know SQL from working with relational databases, it’s a very nice tool to create familiar data representations like tables in the world of big, distributed, and unstructured data. Hive employs schema-on-read design – which means that structure is applied to the data during reading or execution of the query, rather than having to decide when the data was written or stored. It’s like using cookie cutters to create the exact shape and variety of cookies you want, rather than buying a whole bunch of the same cookies – YUCK! This provides tremendous flexibility in how the data can be used. It also provides storage and data management advantages as the Hive query can be saved as a lightweight metadata object, rather than having to write the complete results to a file. More simply, Hive doesn’t actually store any data, it just helps us structure and use it much more efficiently. There are a variety of ways to use Hive tables in R. One is SparkR from Apache. This R package is available only with the Spark distribution (not on CRAN), which makes getting started a pretty big investment. An easier way is through Microsoft’s HDInsight on Azure – a fully managed Hadoop-as-a-Service offering that’s easy to deploy (even for a few hours just to play) and provides an option for R integration via Microsoft R Server. R Server allows you to directly import Hive data as Spark data frames that take advantage of Microsoft’s high-performance machine learning algorithms. In preparation for this post, I followed the tutorial for getting started with R Server on HDInsight to deploy a Hadoop cluster, and then followed the instructions in the section for accessing data in Hive. Within 15-20 minutes, I was up and running with a cluster and had experimented with the sample Hive data! For an even deeper tutorial, check out this post from Microsoft. Let’s see how easy it is: In the code snippet below, the code including ‘hiveData <- RxHiveData(…)’ brings the HiveQL query results of an existing hive table named ‘hivesampledata’ into an Rx data source object in R. Rx data sources – part of the RevoScaleR package in R Server - can be created from a variety of sources such as ODBC, .csv, text, parquet, and others. The advantage of this format is that it’s just the metadata for the query – like a pointer to the data location and query structure. It has a very lightweight memory footprint in R, even for massive datasets.     '''#..create a Spark compute contextmyHadoopCluster <- rxSparkConnect(reset = TRUE)''''''#..retrieve some sample data from Hive and run a modelhiveData <- RxHiveData(\"select * from hivesampletable\",            colInfo = list(devicemake = list(type = \"factor\")))rxGetInfo(hiveData, getVarInfo = TRUE)rxLinMod(querydwelltime ~ devicemake, data=hiveData)'''     The next line, ‘rxGetInfo(…)’ returns summary information like variable names, data types, and number of rows for the query. Finally, the line including ‘rxLinMod(…, data = hiveData)’ trains a linear regression model using the hive data. It’s interesting to note that rather than having to fit all the data in memory, R Server intelligently streams the Hive data as needed from HDFS and allocates it among the computing nodes in the Hadoop cluster for distributed processing – super cool!  Hopefully this article has been helpful to understanding the value of using Hadoop data in R. For more information about Microsoft R Server, please see our recent webinars here and here. For more information on Hadoop, please visit our resource center. "
"202" "Since the time Power BI became more widely available in mid-2015, Microsoft has relentlessly continued to add new features every month. Microsoft’s vision and execution was appropriately recognized by Gartner in February 2017 and rewarded with the Leader position in the Magic Quadrant for Business Intelligence and Analytics Platforms. While there has been no lapse in new technical components for improving Power BI, the last couple of months have been exceptionally good in terms of features that will improve the adoption of self-service business intelligence. In this article, we’ll review 5 new features that I believe are game changers in improving self-service adoption rates.   1. Connecting to datasets in Power BI Service from Desktop In the April 2017 update of Power BI Desktop, Microsoft released the ability to connect to datasets in the Power BI service. This is very significant because it helps keep the pristine data model in one place as well as gives users the ability to create reports on top of their data without touching the original data model. To get a better idea of this new feature, let’s look at the old scenario before this update was released. As an example, let’s say we have a Power User – Zach, in the sales department, who is well versed with Data Analysis Expressions (DAX) and creating data models. Zach took data from different sources and included DAX calculations within his reports to satisfy some common requirements from users, then published the Power BI file. Another user – Miranda, usually just consumes reports but occasionally needs to create new reports based on changing requirements. Miranda does not know DAX and creates new reports based on the existing data model that Zach provided to her (note: creating reports in Power BI is mostly ‘drag and drop’ and hence has a very minimal learning curve, while DAX requires a bit more expertise). In this situation, Miranda does not have editing rights to the original Power BI file and she would need to save a copy of the file to make her changes. The problem with this approach is that the copies quickly get out of sync with the original Power BI file and become inaccurate – if Zach discovered a metric that he created was using the wrong logic and made an update in the original, Miranda’s copy will be out of sync and she wouldn’t get notified of the issue. With the new update, Miranda can create reports based on the original Power BI file and all changes made by Zach will automatically be reflected in Miranda’s reports. You can read more about this feature here. 2. Report level measures for live connections to Power BI Service datasets & SSAS tabular models In the May 2017 update for Power BI Desktop, Microsoft released the ability to create new DAX measures for live connections to a tabular Analysis Services server or a Power BI service dataset. This feature is huge in terms of accelerating adoption across organizations because it encourages users to improve their Power BI skills as well as makes the maintenance of Power BI reports much simpler. Thinking back to our example above, let’s assume that Miranda has been using Zach’s model for some time and she is slowly improving her DAX and Power BI skills. At this point, she feels comfortable creating simple DAX measures but doesn’t want to recreate all of Zach’s work when creating her own model. Previously, Miranda would have had to save a copy of Zach’s file and add a new measure, or ask Zach to create the measure for her. Now she can add the new measure directly to her report and keep it linked to the original model. The new measure will only be available in Miranda’s reports, so Zach will not have to worry about seeing random measures in his original model. If you want some more information on this update, check out this link. 3. Quick Measures for common calculations Starting with the April 2017 update of Power BI Desktop, Power BI can now use Quick Measures to quickly and easily perform common, powerful calculations. Using a dialog box, users can select common calculations like aggregates, time-intelligence based functions, running totals, etc. and Power BI will generate the DAX calculations behind the scenes, shown below.  This feature will enable users with minimal knowledge of DAX to create powerful calculations with much less effort. Users can also take advantage of this functionality to jump-start or expand their DAX knowledge as they can see the DAX generated for the calculation. In the scenario mentioned above, a user like Miranda will be much more comfortable creating calculations such as quarter-to-date, running totals, etc. with this technique, and she will easily be able to add it to her report. Viewing the calculations will also aid Miranda in understanding DAX in general. You can find more about this new feature here. 4. Easily distribute dashboards and reports to large audiences with Power BI apps As of May 2017, Microsoft released a preview version of Power BI apps, which enables organizations to easily deploy a collection of purpose-built dashboards and reports to a large number of business users and empower them to make data-driven decisions. With the earlier version, it was difficult to share and maintain permissions of multiple dashboards within the same set of users. Now, with Power BI apps, business users can easily install these apps from Microsoft AppSource and access them via a web portal or their iOS, Android, or Windows devices. Users can easily find and return to the content since it’s all in one place. Power Users (like Zach) can update and delete dashboards, then push out their updates to end users by re-publishing the app. Microsoft is also planning to add more features to Power BI apps like directly pushing apps to end users through alerts, email subscriptions, external sharing using AAD B2B, etc. For more information, check out this article. 5. Easier access to critical intelligence with Power BI Premium Additionally in the May 2017 update, Microsoft announced a new way of licensing Power BI. Power BI Premium builds on the existing Power BI portfolio with a capacity-based licensing model that increases flexibility for how users access, share, and distribute content. The new offering also delivers additional scalability and performance to the Power BI service. The existing method of licensing (Power BI Pro, roughly $10 per user per month) works well for small to medium companies but can be quite expensive for larger organizations. With Power BI premium, cost is based on capacity, so organizations will only have to pay for what they use. Power BI Premium also introduces the ability to maintain BI assets on-premises with Power BI Report Server. Find out more about this change in this whitepaper from Microsoft. All in all, the recent updates to Power BI are proving to be very useful and should continue to improve over time. If you have any questions or want to know more about how your organization can utilize Power BI, contact us!"
"203" "Next up in BlueGranite's new Meet our Team series is Jim Bennett – a data and analytics enthusiast with over 18 years of experience developing customized and integrated solutions for clients.   Jim is a Principal at BlueGranite and has been with the team since 2014. In his early days, Jim received his bachelor's degree in computer science, with concentrations in mathematics and philosophy, from Western Michigan University. Jim began his career as a database administrator, allowing him to branch out in many ways and gain extensive experience with moving, transforming, and integrating data into different systems. From there, Jim began working with data and analytics, eventually leading him to a BI firm in Ann Arbor, Michigan, where he filled the role of senior business intelligence architect prior to joining the BlueGranite team.  Jim has a lot of reasons for enjoying his work with data and analytics, but what stood out the most was his passion for transforming data. Here’s what Jim had to say: “I enjoy the automation and efficiency that comes with working with data. I like to take on the challenge of improving and transforming data into something that is usable and brings insight to our clients. This also keeps my work exciting since I get to apply this passion to a lot of different scenarios and circumstances in unique client environments.” Overall, Jim’s favorite kind of project work involves building end-to-end solutions because he gets to use a lot of different technologies to accomplish his goals. Jim likes getting to start from scratch and build out an entire environment to help clients implement best practices and the right tools from the very start of the project. Lately, Jim has been working on complex self-service and enterprise BI projects that have been largely focused on data visualization and analytics consumption. When asked what he likes about working at BlueGranite, here’s what Jim wanted to share: “BlueGranite is unique in the sense that we care a lot about helping our employees set aside time and budget for professional development. I really like working for a company that encourages career growth and developing learning plans to ensure continued success. You don’t find that too often in the BI consulting industry, which speaks to why we have such an amazing and talented team.” Jim also had some predictions for the future of our ever-growing industry – he believes that there is an increasing demand for analytic professionals to deliver more advanced solutions due to the importance being put on data in recent years. This also means that there is a rising need for good architecture, the implementation of best practices with data, and strong design. In Jim’s opinion, this is where BlueGranite is able to maximize clients’ potential and give them the best insights into their data.  When Jim isn’t at work, he likes to take time to participate in various professional organizations in order to keep up on the latest technologies. Some of the professional organizations that Jim belongs to include the Professional Association for SQL Server (PASS), as well as three special interest groups within the Association of Computing Machinery (ACM), including: Special Interest Group on Knowledge Discovery in Data (SIGKDD) Special Interest Group on Management of Data (SIGMOD) Special Interest Group on Algorithms & Computational Theory (SIGACT). Outside of professional organizations, Jim enjoys traveling with his family, as well as volunteering at his church, and for a camp in Northern Michigan. Want to know more about BlueGranite and the rest of our team? Contact us!"
"204" "Whether you work in bioinformatics, computational biology, genomics, proteomics, or pharmacology, your data needs often differ vastly from a traditional business user. The file types and volume of data with which you are interacting are typically unique to these industries. The common thread among them is the importance of visualizing the data. With Microsoft Power BI and its ability to integrate with R, you can easily see your data as well as the results of your analyses. Here, we will look at a few examples of visualizing data from bioinformatics-related areas using only R and Power BI.   As you may know, in bioinformatics, we encounter many odd file types. Often, these files can't be opened in a tabular viewer such as Excel or put into a traditional database table because their structure just doesn't fit. However, using R, we can use parsers to analyze and display our data. We can even blend this data with traditional data sources to enhance filtering capabilities and more. Survival Analysis In our first example, we will implement a survival analysis of tumor DNA profiles. Common industries and uses for survival analyses include: Pharmaceutical - Understand the length of time a drug compound stays in a patient's system. Clinical - Track how long individuals live with a certain disease. Population Genetics - Gain insight into gene fixation or understand the duration of a trait in a population.   In the image below, you can see a sample Power BI dashboard that shows the tongue sample data from the KMsurv package in R. This data tracks the deaths of individuals with two different tumor DNA profiles over time. To understand the differences in death rates between groups, a survival plot is the obvious choice. Power BI does not have an innate survival plot built in, but using ggplot2 within R can yield nice, custom graphics.  In this image, you can see the two charts on the left that were generated by the survival and ggplot2 packages. The table and filters on the right are generated by the included functions in Power BI. The code below is used in the R script editor after adding an R script visual to the Power BI dashboard.  1) Survival Analysis Chart 2) Cumulative Hazard Chart  3) ggsurv Function for Graphing Survival Analyses    library(survival)library(ggplot2)attach(dataset)tongue.surv <- Surv(time[type==1], delta[type==1]) #[Insert ggserv function here] surv.fit2 <- survfit( Surv(time, delta) ~ type)ggsurv(surv.fit2) + ggtitle('Lifespans of different tumor DNA profiles') + theme_bw()     library(survival)library(ggplot2)attach(dataset)tongue.surv <- Surv(time[type==1], delta[type==1]) #[Insert ggserv function here] haz <- Surv(time[type==1], delta[type==1])haz.fit <- summary(survfit(haz ~ 1), type='fh') x <- c(haz.fit$time, 250)y <- c(-log(haz.fit$surv), 1.474)cum.haz <- data.frame(time=x, cumulative.hazard=y) ggplot(cum.haz, aes(time, cumulative.hazard)) + geom_step() + theme_bw() +  ggtitle('Nelson-Aalen Estimate (Cumulative Hazard)')    ggsurv <- function(s, CI = 'def', plot.cens = T, surv.col = 'gg.def', cens.col = 'red', lty.est = 1, lty.ci = 2, cens.shape = 3, back.white = F, xlab = 'Time', ylab = 'Survival', main = ''){ library(ggplot2) strata <- ifelse(is.null(s$strata) ==T, 1, length(s$strata)) stopifnot(length(surv.col) == 1 | length(surv.col) == strata) stopifnot(length(lty.est) == 1 | length(lty.est) == strata) ggsurv.s <- function(s, CI = 'def', plot.cens = T, surv.col = 'gg.def', cens.col = 'red', lty.est = 1, lty.ci = 2, cens.shape = 3, back.white = F, xlab = 'Time', ylab = 'Survival', main = ''){ dat <- data.frame(time = c(0, s$time), surv = c(1, s$surv), up = c(1, s$upper), low = c(1, s$lower), cens = c(0, s$n.censor)) dat.cens <- subset(dat, cens != 0) col <- ifelse(surv.col == 'gg.def', 'black', surv.col) pl <- ggplot(dat, aes(x = time, y = surv)) + xlab(xlab) + ylab(ylab) + ggtitle(main) + geom_step(col = col, lty = lty.est) pl <- if(CI == T | CI == 'def') { pl + geom_step(aes(y = up), color = col, lty = lty.ci) + geom_step(aes(y = low), color = col, lty = lty.ci) } else (pl) pl <- if(plot.cens == T & length(dat.cens) > 0){ pl + geom_point(data = dat.cens, aes(y = surv), shape = cens.shape, col = cens.col) } else if (plot.cens == T & length(dat.cens) == 0){ stop ('There are no censored observations') } else(pl) pl <- if(back.white == T) {pl + theme_bw() } else (pl) pl } ggsurv.m <- function(s, CI = 'def', plot.cens = T, surv.col = 'gg.def', cens.col = 'red', lty.est = 1, lty.ci = 2, cens.shape = 3, back.white = F, xlab = 'Time', ylab = 'Survival', main = '') { n <- s$strata groups <- factor(unlist(strsplit(names (s$strata), '='))[seq(2, 2*strata, by = 2)]) gr.name <- unlist(strsplit(names(s$strata), '='))[1] gr.df <- vector('list', strata) ind <- vector('list', strata) n.ind <- c(0,n); n.ind <- cumsum(n.ind) for(i in 1:strata) ind[[i]] <- (n.ind[i]+1):n.ind[i+1] for(i in 1:strata){ gr.df[[i]] <- data.frame( time = c(0, s$time[ ind[[i]] ]), surv = c(1, s$surv[ ind[[i]] ]), up = c(1, s$upper[ ind[[i]] ]), low = c(1, s$lower[ ind[[i]] ]), cens = c(0, s$n.censor[ ind[[i]] ]), group = rep(groups[i], n[i] + 1)) } dat <- do.call(rbind, gr.df) dat.cens <- subset(dat, cens != 0) pl <- ggplot(dat, aes(x = time, y = surv, group = group)) + xlab(xlab) + ylab(ylab) + ggtitle(main) + geom_step(aes(col = group, lty = group)) col <- if(length(surv.col == 1)){ scale_colour_manual(name = gr.name, values = rep(surv.col, strata)) } else{ scale_colour_manual(name = gr.name, values = surv.col) } pl <- if(surv.col[1] != 'gg.def'){ pl + col } else {pl + scale_colour_discrete(name = gr.name)} line <- if(length(lty.est) == 1){ scale_linetype_manual(name = gr.name, values = rep(lty.est, strata)) } else {scale_linetype_manual(name = gr.name, values = lty.est)} pl <- pl + line pl <- if(CI == T) { if(length(surv.col) > 1 && length(lty.est) > 1){ stop('Either surv.col or lty.est should be of length 1 in order to plot 95% CI with multiple strata') }else if((length(surv.col) > 1 | surv.col == 'gg.def')[1]){ pl + geom_step(aes(y = up, color = group), lty = lty.ci) + geom_step(aes(y = low, color = group), lty = lty.ci) } else{pl + geom_step(aes(y = up, lty = group), col = surv.col) + geom_step(aes(y = low,lty = group), col = surv.col)} } else {pl} pl <- if(plot.cens == T & length(dat.cens) > 0){ pl + geom_point(data = dat.cens, aes(y = surv), shape = cens.shape, col = cens.col) } else if (plot.cens == T & length(dat.cens) == 0){ stop ('There are no censored observations') } else(pl) pl <- if(back.white == T) {pl + theme_bw() } else (pl) pl } pl <- if(strata == 1) {ggsurv.s(s, CI , plot.cens, surv.col , cens.col, lty.est, lty.ci, cens.shape, back.white, xlab, ylab, main) } else {ggsurv.m(s, CI, plot.cens, surv.col , cens.col, lty.est, lty.ci, cens.shape, back.white, xlab, ylab, main)} pl}   Power BI connects to the .csv output of the tongue sample dataset. By using this data for the table, multi-row card, and filters, as well as the R visualizations, everything on the dashboard can interact, blending both Power BI graphics as well as R-generated charts. Protein Structure Analysis If you're familiar with protein structure prediction, functional prediction, or proteomics in the slightest, you've most likely heard of the Protein Data Bank. When a protein's structure is determined experimentally, it's structure is uploaded to the Data Bank as a .pdb file. However, .pdb files are an odd format in that, while they are text-based, they are not tabular and can't be transformed into a database table. In this example, we demonstrate how a custom R script can fetch .pdb files from the Protein Data Bank, visualize the B-factors (temperature values) of the residues, and also query BLAST to find possible matches (similar sequences) for your protein of choice. The data that feeds the charts comes from the web (Protein Data Base and BLAST) whereas the table in the dashboard is loaded via a .csv file of PDB IDs, Protein Classifications, and Descriptions. The code below is used in the R script editor after adding an R script visual to the Power BI dashboard.  1) B-factor Chart 2) BLAST Matches Chart   library(bio3d)selection <- paste0(dataset[1,])pdb <- read.pdb(selection)ca.inds <- atom.select(pdb, \"calpha\") # Simple B-factor plotca.inds <- atom.select(pdb, \"calpha\")plot.bio3d( pdb$atom[ca.inds$atom,\"b\"], sse=pdb, ylab=\"B-factor\")    library(bio3d)selection <- paste0(dataset[1,])pdb <- read.pdb(selection)aa <- pdbseq(pdb)blast <- blast.pdb(aa)top.hits <- plot(blast)top.hits  Both scripts use the bio3d package to connect to both the Protein Data Bank and NCBI BLAST sites. Both get their query protein ID by the table that has been passed through via the filter in Power BI. Gene Expression The UCSC Genome Browser houses sequence/annotation data as well as gene expression data. While these files are often text-based, they are often quite large and hard to interpret at face-value. Visualization of gene expression data as a heatmap is a common way to understand the data. By visualizing the data in this manner, you can understand the expression values as they relate to the individual tissue samples. Power BI does not have heatmaps as a built-in visualization, but you can generate the graph by using ggplot2.  The code below is used in the R script editor after adding an R script visual to the Power BI dashboard.  1) Gene Expression Heatmap   library(tidyverse)library(ggplot2)library(reshape2)data_clean <- dataset %>% filter(uniprot_id != \"NA\")#Heatmap Plotmelted_data <- melt(data_clean)ggplot(data = melted_data, aes(x=uniprot_id, y=variable, fill=value)) +  geom_tile()+theme(axis.text.x=element_text(angle=45, hjust=1))   The data is loaded into Power BI from a .tsv file. In Power BI, we prefilter the tissue samples that display to keep the visualizations simple and readable. Plus, adding in the uniprot_id and chromosome variables as a filter will allow users to select the proteins or chromosomes of their choosing. These filters also work with the R-generated heatmap as well. Conclusion With the above three examples, I hope to have demonstrated the flexibility and enhanced functionality that becomes available by using R within Power BI. From displaying survival plots to retrieving protein structure files from the web to blending data and filtering functionality with gene expression data, Power BI can display many of the various types of data that you may encounter in bioinformatics. Custom visualizations are very simple to generate. As long as you have the package installed in R, most static visualization functions work seamlessly in Power BI. So, the next time you need to analyze and visualize your scientific data, look to Power BI to make it easy! If you have any questions or want to know more, contact us!"
"205" "Utilizing real-time streaming tools, such as the Azure IoT Hub or Event Hub, can give organizations quick insights into their data and allow for more accurate and quick decision making. In this post, I will highlight some real-time streaming solution examples, as well as the business value gained by their implementation.    Real-time Application Insights In our first example, let’s consider a company that provides a web or mobile application to its internal or external customers. By adding just a few simple lines of streaming code, data can easily be sent asynchronously to Azure. With this code, key application events such as logins, daily usage, or even new data entries are streamed in real time to Azure where you will be able to perform real-time analysis, apply machine learning, or send alerts to key members of your team. Continuing with the examples above, some key actions that may add value for your team include: Email or SMS Text Notifications: Send a notification to a customer who still has items in their shopping cart, with details on what they added during their last visit to the website. Recommendations Engine: Send a follow-up email to your internal team or customers – running data through a machine learning model with a recommendations engine to provide just that, recommendations on correlated products or services. Missed Sales Opportunities: Real-time streaming can be set up to determine missed sales opportunities due to low inventory or product issues. Someone can then take immediate corrective action to prevent further profit loss. Leads on High-Value Products/Services: Internal alerts or workflows can be triggered based on product interest. If a customer is looking for a highly-valued product, or an item/service with a high margin, a sales team member can get a text message or email with suggested actions. Fraud Detection: If your application accepts coupons, you can even begin checking for fraud in real time. When a coupon code is used multiple times in the same location or with the same IP address, a general manager can get an alert to the problem. Real-Time Monitoring There are many real-time machines, devices, equipment, and gateways that already exist or are being deployed in operations and that have been outfitted with sensors. Additionally, there are many that have been approved for Azure IoT! You can check out the full list here. As mentioned earlier, a small amount of code is added to your device so that the device's sensor and telemetry data can be sent to Azure. Azure is an amazing tool in that it can ingest data from thousands of devices simultaneously and in real time. Through this process, Azure can provide the following benefits: Basic threshold detection Real-time alerts Machine learning model building with big data and deployment for real-time detection Device management, state, tracking, and firmware updates Remote control of devices by utilizing two-way communication Business Scenarios Next, we will explore some business use-case scenarios to show a few real-life examples of how organizations are taking advantage of device connectivity, shown below. ManufacturingLet’s say a manufacturing firm is using stationary asset monitoring and utilizing sensors on its equipment to monitor air temperature, vibrations, changes in the product environment, etc. In this scenario, the organization could use this data to monitor their equipment for any potential failures. If or when a potential failure event is detected through thresholds or Machine Learning code, the device can automatically be triggered to shut down via Azure and an alert can be sent to the management team. Fleet Management In this case, buses, trucks, cars, and planes can be outfitted with GPS devices and sensors to monitor engine temperature, revolutions per minute, accelerations, braking velocity, fuel use, etc. Management can then use this data to check for and track reckless driving, utilization of fleet, and can create plans to increase asset utilization. Process ManagementIn a retail setting, a company can use data in many ways, including tracking inventory RFID tags, arrival times and distribution at queues, scan rates, and even detecting theft. By utilizing this data, a brick and mortar store can improve its customer satisfaction rates with workforce management optimization – having the right people available to its customers at the right place and the right time. Additionally, with this kind of data monitoring, real-time inventory analysis can happen almost instantaneously. In short, there are almost endless ways that real-time streaming solutions can be useful to an organization. If you have questions or want to learn more about how to stream data in real time, consider checking out BlueGranite’s upcoming webinar on the topic! Additionally, we are happy to help, so if you’d like to speak with one of our advanced analytics experts, contact us."
"206" "When Gartner released its Business Intelligence and Analytics Magic Quadrant earlier this year, Microsoft again landed at the top of its class for the Ability to Execute and Completeness of Vision. A big contributing factor for Microsoft’s continued ascension in the BI and analytics space is the evolution of Power BI. In this post, we will review three Power BI features that aren’t highly advertised, but could prove especially useful as you plan and prepare for a Power BI implementation at your organization.    On-Premises Gateway: Map User Names If you’re already acquainted with the Power BI Service, then you probably already know about the On-Premises Gateway, which allows Power BI to issue live queries or scheduled refreshes against a high-performance, on-premises database such as SQL Server Analysis Services. However, did you know that when connecting to your SSAS data models, you can also define user name mappings between the Power BI Service and SSAS? This can be a useful feature if your Power BI Service login name differs from your local Active Directory User Principal Name (UPN). We have often used this feature with clients when developing Power BI Proof-of-Concepts (POC). Often, clients in the exploratory phase of purchasing Power BI want to determine how the Power BI Service will interact with their existing SSAS data models. To demonstrate this functionality, a client first installs the On-Premises Gateway on their internal server. Next, we use our BlueGranite Power BI Service account to add the newly installed gateway. Finally, we use the Map user names feature to map BlueGranite email addresses to local Active Directory accounts that were created by the customer. By creating these mappings within the Power BI Service, we are now able to issue live queries from Power BI reports against the client’s local SSAS data models. This allows us to demonstrate all the cool features of Power BI Service using a client's own data! The screenshot below shows this simple example in action. Additionally, check out this useful Power BI documentation on working with on-premises SSAS models.  Using an Image as a Filter – Chiclet Slicers A well-known feature of Power BI (and even Excel Pivot Tables) is the concept of slicers. Slicers are visual filters that allow the end user to easily select and change data filters on the fly. For example, a Power BI report page may have a simple year slicer that allows the user to toggle what year’s data they want to view. Traditional slicers are limited to text-based values, but what if the need arises to create a slicer based on an image or logo? This is where Chiclet Slicers come into play! The Chiclet Slicer is a custom visual add-in to Power BI that was developed by Microsoft. The Chiclet Slicer behaves like a traditional slicer, but it allows you to leverage images that serve as filters on the report canvas. To illustrate an example, let’s say we have recently implemented a customer satisfaction survey for our ecommerce business. Our customer satisfaction survey has a simple scoring scale (1-5) with an appropriate smiley face emoticon that represents how satisfied they were with our service. To view our data, we want the ability to easily toggle between the survey scores by simply clicking the appropriate smiley emoticon for each score. We can easily create this functionality by leveraging the Chiclet Slicer. To use a Chiclet Slicer, we will need three things: A category, a value, and an image. NOTE: Ordinarily, you would want to maintain this data in the data model in accordance to your enterprise standards, but for the sake of a simple example, below are the steps we took to create a Chiclet Slicer. We first created a simple Excel worksheet with three columns, one for each component of the Chiclet Slicer. You’ll notice we used actual URLs for the images as Power BI naturally works well with hyperlinks. We already have a customer survey results table whose records reference our scoring scale. We imported the worksheet below into our Power BI Desktop workbook.  Next, we downloaded the Chiclet Slicer custom visual from the Power BI Visuals Library page.  We then imported the newly downloaded Chiclet Slicer visual in to Power BI Desktop. Lastly, we added the Chiclet Slicer visual to the report canvas and configured the components. Below is the final output of our customer satisfaction emoticon slicer!  Dashboard Tiles – Custom Links The last feature of this post will walk through how we can add custom hyperlinks to tiles on a Power BI dashboard. If you are familiar with Power BI Dashboards, then you have probably realized that the default “click” action on a dashboard tile is to drill through back to the original report from which the tile came. However, when designing your dashboards, you can also set your own custom links for individual tiles. A great use case for this feature is to create a custom link to execute a Reporting Services (SSRS) report that will provide the details behind the value(s) in a tile. This will allow your dashboards to continue to provide an aggregated view of your data, but still provide the flexibility to allow users to drill through to the details for further analysis. Let’s look at an example. Continuing with the Customer Satisfaction Survey theme, we created a simple Power BI dashboard that uses a card visualization to prominently display the average customer survey score for all customers in the current year.  We wanted to create a custom drill-through action by having the tile execute a detailed SSRS report whenever the tile was clicked on. Simply follow these steps to set the custom link on the tile: Hover your mouse over the tile and click on the ellipsis that appears in the upper right-hand corner Next, click on the pencil icon that says “Tile Details” Once on the Tile Details screen, you can check the check-box that says “Set custom link” This can be found under “Functionality” as shown in the screenshot below  Specify the URL you want the tile to open when a user clicks on it Specify whether you want the custom link to open in the current browser tab or to open a new tab  We created a detailed SSRS report that provides a complete listing of all customer survey results for the current year. By adding interactive sorting to the Survey Score column, we can quickly leverage this report to identify poor survey scores and figure out how to best improve the situation for our customers.  If you have questions about Power BI or want to know more about how we might be able to help, please feel free to reach out to us!"
"207" "A common approach to assess ROI on any BI tool deployment is through adoption metrics. Tool usage is a tangible way to ensure business users are extracting value from the investment. Industry organizations like The Data Warehousing Institute (TDWI) often refer to highly adopted BI deployments as “Pervasive BI” – something many companies strive for but many struggle to achieve.   Actively monitoring usage is a key component of Adoption Management – a critical factor in Power BI deployment success. Power BI offers a way to do this via Audit Logs, a capability available to tenant administrators. Working with your log data To access audit logs, you must first turn on this capability on the Admin Portal by going to your settings, as shown below. Note that to query the logs, you will be redirected to the O365 Admin Center Security & Compliance portal. From there, you can manually retrieve information regarding Power BI activities – for example: Who viewed a Power BI dashboard, and when Who is sharing dashboards When was a specific dataset deleted from your Power BI service When was a Power BI group created When manually searching the log, you can download usage data in CSV format. You can also automate the downloads via PowerShell using a specified login through Exchange Online. While loading the CSV file in Power BI, make note that log data is in JSON format. To serialize into a table, you can use a Query Editor transform:  As you expand the columns, ensure you click on “load more” within the column selector. Given that by default Power BI will not load columns unless explicitly selected, simply clicking “OK” will not expand the “ItemName” field which gives the Power BI asset name (report/dashboard/etc.) that we are tracking activities for. What kind of questions can I answer via Audit Logs? Currently, log data is centered around item usage (as opposed to by user type: consumer vs. power user, for example). As such, questions related to dashboard or report popularity can make for great uses of log data. Increased popularity can be used as a direct measurement of Power BI adoption. For example, you can see the 5 most popular dashboards by creating a simple running total DAX calculation, as shown below:  If sharing is done via direct dashboard shares, one could also analyze which drilling path (from dashboard to report) is the most popular. This is because reports do not show up on the user menu when a dashboard has been shared directly (only the dashboard shows up), yet users can still drill into the report from a dashboard tile. Consumer behavior By enhancing audit log reporting with a few calculations, we can compare the adoption behavior of consumers within specific lines of business.   By comparing individual audit logs on a specific report or dashboard, we can often see some consumers that – within a single line of business – display higher adoption rates than others. This is not necessarily intuitive, as you would think all business users with the same role would have a similar adoption profile. It is recommended to define an ideal usage profile, by user role.  With it, lots of insights could be gained by comparing the actual usage versus target usage behavior:  Note that higher usage (beyond target) may not necessarily translate into desired adoption behavior. For example, a user that accesses a report on a daily basis even when an automated data refresh is scheduled weekly might be experiencing issues consuming the report (for instance, they may have trouble retaining information due to a highly busy report layout). Audit logs provide a starting point from which we can have an informed conversation with the business regarding their satisfaction with the report consumption experience. Prototyping vs. Consumption It is common for Super Users and/or Power Users to build reports on behalf of consumers. In this scenario, picture an adoption profile in which the Super/Power User will have usage activity spike during tactical prototyping, followed by a decline once the report is published to production (via Content Packs or direct dashboard shares). While their usage declines, you would expect increasing consumer usage signaling a positive adoption rate, as shown below:  Although audit logs do not provide information to separate user types (consumers versus power users), a Power BI report using audit data can easily be enhanced to categorize users based on known profiles. Cycles of prototyping should normally be followed by spikes in adoption. Differing usage patterns can be a signal of adoption problems. For example, the usage pattern below could emerge when Super Users modify the model and/or report too frequently, which can confuse consumers who may have shared some initial excitement, but are no longer sure what to expect next time they access the report.  Audit logs do not dictate a method for tracking usage or adoption.  However, as you plan for deployment, you likely have assumptions on how you intend super users, power users and consumers to leverage Power BI reporting. By explicitly defining expected usage patterns, the BI team can corroborate adoption, and proactively take corrective measures when log data indicate user behavior that does not conform with expectations. If you have questions about BI or want to know more about how we might be able to help, contact us!"
"208" "At BlueGranite, we have a passion for data, strengthening our company culture, and showing appreciation for our team. This is the first post in our new Meet our Team series, giving you a chance to get to know the friendly faces behind our brand.   Meet TJ Polak – a Senior BI Consultant at BlueGranite. TJ has been with BlueGranite since 2011, coming from a BI developer role at a large retail company. TJ grew up in Rhode Island and currently lives in Massachusetts, just south of Boston. He received his undergrad from Bryant University, studying Information Technology. TJ also has his master’s degree in computer science from Boston University with a concentration in data management. If you can’t tell from his education and work history, TJ has an appetite for data and analytics. Here’s what TJ had to say about why he enjoys his work: “I have always had an analytical way of thinking, and it seemed like a great career choice for me. I like the challenges that come along with figuring out how to make data tell a story for our clients. I also enjoy helping our clients find insights in data that they might not have known existed because they were using it wrong or it was difficult to work with.” TJ’s favorite kind of project work involves data acquisition and preparation. He (semi) jokingly refers to himself as an “old school data warehouse guy” because he likes data modeling and ETL work above anything else. Lately, TJ has been on client projects where he has helped to start up and manage entire data management programs from scratch. He has also helped client teams put together programs for data management and governance, in addition to training their staff in developing data cubes and reports. Why BlueGranite? In addition to doing what he loves, he also gets to do it for a company where he enjoys working. When asked about what he likes about working for BlueGranite, TJ raved about our team, culture, and the importance we put on work-life balance. “I love getting to work with my team every day. Everyone is incredibly smart, and even though we are all remote and spread out across the country, the team makes themselves available to help whenever they’re needed. BlueGranite does a great job hiring top-notch talent and building our sense of community and connections with each other.” On top of working with a great team, TJ also enjoys having flexible hours and being able to work from his home office. He feels that it helps him to manage his time more efficiently and live with much less stress compared to a typical office job. In TJ’s free time, he enjoys volunteering at his local animal shelter and with Big Brothers Big Sisters. He also likes spending time with his family and two dogs, Gunther and Klaus. Additionally, he is an avid sports fan and is looking forward to spring to work on his golf swing. Outside of work and family, TJ participates in some local user groups including a Microsoft BI User Group and a SQL Server User Group. Predictions in the Industry Overall, TJ predicts that the BI industry is moving towards self-service. Consultants must wear many hats to help unlock the potential in the ever-increasing amounts of data our clients rely on for insight and decision making. When TJ got into the industry, he said that it was all BI focused and usually only involved building solutions off of a data warehouse. Now, it tends to be much less about data modeling and more about getting data to work properly when it is lacking any kind of structure. As the industry continues to grow, TJ feels we will continue to move away from general standards and will need to get more creative with the way we solve problems for our clients – a challenge we are eagerly anticipating! Want to know more about BlueGranite and the rest of our team? Contact us!"
"209" "Last week, our monthly webinar series covered Distributed Computing and What's New in R Server 9.0.1. If you missed the session, you can find the recording here. We received a number of questions during the presentation and wanted to take the opportunity to provide some insightful answers for the audience.   If I am just getting started with R, how should I go about choosing between open-source and Microsoft R? It really depends on your needs, but luckily both open-source R and Microsoft R Open are free to get started. Either solution will allow you to practice working in the environment, and once you get comfortable in R and you starting running into the limitations with R on large datasets (the ones that won’t fit in your computer’s memory), you can look into purchasing R Server, or spin up an R Server on Azure in HDInsight or on the Data Science Virtual Machine. When thinking of SAS vs. R: as a SAS user how difficult or easy would it be for me to learn and start using R? Both SAS and R do very similar things. However, SAS code is written in procedures where R is more of a script. Since you’re already familiar with the algorithms/functions you use in SAS, learning the syntax of R really isn’t that bad. Plus, the cost savings of R over SAS are tremendous! Check out the example below for a quick comparison.     SAS   R   Microsoft R   Linear Regression   proc reg data=mydata;model y=x1 x2 x3;   lm(y~x1+x2+x3,mydata)   rxLinMod(formula: y ~ x1 + x2 + x3,myxdfdata)  What use cases do you see being used with the R server technology? I think the use cases depend on when you want to take your R practice to the next level: Larger, collaborative data science teams Larger amounts of data (tweets, click stream data, genomics, etc.) a.k.a. “Big Data” When operationalizing the models you make is crucial to providing business insight. Also, when you want to easily maintain, update, and rerun them. Is there a benefit to using R if my dataset is in the range of two to four million rows of data? In short, it depends. Two to four million rows might not be outside of the memory limit of normal R, but it is approaching it for many machines. The added value of R Server won't necessarily be in its power to handle more data, but in its ability to run computations on that data in a more reasonable timeframe. Millions of rows may take a long time to train the model or predict upon, but using R Server will definitely cut down on that time. Do you have any virtual instructor-led R training? While BlueGranite does not currently offer virtual training opportunities for R, our 3-day Microsoft R Training session takes place at your location and features hands-on, instructor-led lessons for up to 10 of your firm's attendees. A BlueGranite senior consultant will facilitate hands-on labs and provide material for your team on the fundamentals of R programming for data ingestion, exploratory data analysis, model building, evaluation, and operationalization. Attendees from your company will learn how to write effective R code that can be operationalized in production. Additionally, there are many online resources that could help you get started. I would recommend looking into free courses online with edX as a beginning point and perhaps exploring an in-person training session in the future. Azure seems to be the way everything is moving – would you agree? I think so because of the flexibility and expandability of the Azure environment. Why would your organization want to pay thousands (or millions) on a big server system that will be out of date in a few years when you can just pay monthly only for what you use on Azure? Plus, the ease of setup is an added bonus. Instead of having to hire a crew to come in and setup Hadoop on your on-premise server, just spin it up on Azure in less than 30 minutes. Thank you to everyone who joined us for the webinar! If you have any more questions or want to know more about R training opportunities, feel free to reach out to us. "
"210" "Across all the industries and geographies that BlueGranite serves, today’s biggest analytics trends revolve around how to make the best possible uses of increasing amounts of data as cost-effectively as possible. The challenges of gathering, organizing and storing large datasets are exacerbated by the need to secure them while simultaneously identifying and delivering timely insights. And let’s not forget the need to do all of this while yielding more value than these capabilities cost to design, implement and support.   The analytics market is hot right now. The seemingly simple and universal goal of producing favorable ROI results often get obscured by the never-ending march of industry buzzwords and new product and service names. I am going to try and demystify some of the trending concepts that many of our clients have been asking about – offering some quick insights on what they are and how to use them, as well as helpful links to additional information that you can explore. Cloud vs. On-Premises Analytics Capabilities: What are some reasons why I should consider cloud-based analytics capabilities in conjunction with (or instead of) traditional on-premises solutions? Pay only for services being used, while they are being used, rather than paying up-front for all the capabilities of a software suite, whether or not you plan to use every feature. Eliminate long-term capex commitments by using flexible opex instead, which can be dropped or restarted on demand.   The list of pros and cons can go on and on, but it ultimately will depend on your business needs.             Check out this blog post on why thinking cloud first could benefit you in the long run. Cloud-based Data Exploration and Visualization: What is the Power BI service, and what potential benefits does it offer over competing cloud and/or on-premises capabilities? Appeal of an inexpensive yet robust capability that is stable yet always upgrading (monthly desktop releases, more frequent server releases). Native support is available for Mobile BI on iPhone.    Some of our clients who looked at Power BI two years ago are surprised when they see how much it       has evolved since then. Notably, Power BI was recently included in the upper-right Leader’s portion of   Gartner’s Magic Quadrant. Embedded BI: How can I embed the insights from my analytics engine at the point where they are needed in decision making? Seamlessly delivering information via embedded BI enables you to trigger and inform good decisions within the natural flow of your key processes. If this is something you are struggling with, check out this blog post on Embedding Power BI Reports into Your Applications which describes ways to do this within Power BI and custom applications.   For example, the process can direct a manufacturing shop floor system to adjust an output setting         based on sensors detecting an anomaly on the production line. It can even give coaches and trainers     real-time guidance on how to help athletes adjust their performance to avoid injury based on data         streaming from wearable sensors. The Internet of Things: The last statement above are just two examples, but much of the growing raw quantity of data being generated today is coming from distinct data sources that aren’t human. The Internet of Things not only includes data sources (such as streaming sensors, point-of-sale systems, manufacturing shop floor machines, and medical devices), but it also includes automated data consumers that remain online and hungry for data on a 24/7 basis, ready for an inbound alert or trigger from a sensor, or perhaps from sets of sensors that reach an actionable threshold defined via machine learning. Machine Learning and Data Science: What is R and how can I use it for machine learning and data science that makes a difference? R is an open-source programming language that supports computational statistics, visualization and data science. R is also used for statistical programming, developing and training models to identify cause-and-effect relationships that support predictive analytics. And if that wasn't enough, R is now supported on SQL Server 2016 and Azure via R Services, and on the standalone R Server platform. BI and Analytics Governance: What do I keep hearing about governance, and why does it matter in the context of BI and analytics in my organization? Implementing and supporting successful analytics depends on far more than the selection of a leading tool. At the end of the day, widespread, successful adoption of the system for actual value, trust in the data for decision making, and a belief in the underlying data and management processes are all crucial. Effective governance puts these in place. For more information, check out this whitepaper on the importance of Power BI governance. Self-Service BI: The goal of self-service BI is to empower users to create their own dashboards and reports using an organized and governed information architecture supported by IT. This lets IT leverage finite technical experts for topics that require technical skills (data integration, etc.) and moves the non-technical work (choosing a visualization, picking colors, etc.) into the hands of the users. This combines the benefits of speeding delivery of user requirements while focusing IT ROI where it drives the most benefit. To learn more about getting this process started, check out this post on how to plan a Power BI rollout. Parallel Processing for the Data Warehouse: Imagine applying the benefits of parallel processing to your analytics environment. Microsoft offers both Analytics Platform System (APS) for customers who would like to do this on-premises and the cloud equivalent called Azure SQL Data Warehouse. Our experience is that these can yield tremendous (50:1) performance gains, but they require solid implementation and key differences in SQL query logic to take advantage of the strengths of splitting work up among multiple worker nodes. Data Storytelling: What is data storytelling, and what kinds of scenarios make it valuable beyond mere collections of dashboards and reports? See this article from Jen Underwood, a longtime industry expert. Text analytics and sentiment analysis: How are text analytics and sentiment analysis practically applicable to my operation? What are some ways in which I can structure text data for analysis? See this blog article by David Eldersveld on how to bring structure to your data. Ensure that you understand these concepts and utilize them for your organization’s success, and you won’t be left behind by your competitors. For more information, here’s a CIO magazine article that lays out other market trends in analytics.  If you have an interest in learning more about these topics, check out BlueGranite’s offers! Additionally, if you have any questions or comments, feel free to drop us a line and we will be happy to help however we can."
"211" "Learning how to embed Power BI reports and dashboards into various applications has become popular with BlueGranite clients as it allows for faster insights into data and reports. In this example, we will review a quick and relatively easy way to embed reports within a demo environment.   When working with clients, I begin by using OAuth 2.0, which is an industry standard when it comes to authorization. Additionally, the example application is set up with Azure Active Directory. To register your application with Azure Active Directory, check out Power BI’s developer applications here. Once there, enter the information for your application and desired permission settings. Then, select the “Register App” button at the end and be sure to save the Client ID and Client Secret it generates. You will need them later and there is no way to retrieve the Client Secret after leaving this page.   Within Azure Active Directory, you will need to use a bit of code to initiate the exchange sequence described below. The example ASP.NET Core code snippet (shown below) handles security communication between your application and Power BI, ensuring that only authorized users can view embedded reports. This code snippet is for a controller in an MVC app called “Report”.      public class ReportController : Controller    {        // Fill in the Client ID, Client Secret, and application port number in the fields below        private string ClientID = \"{Client ID goes here}\";        private string ClientSecret = \"{Client Secret goes here}\";        private string RedirectURL = \"http://localhost:{port}/Report\";        public async Task<IActionResult> Index()        {            if (Request.Cookies.ContainsKey(\"token\"))            {                // If we have a token go ahead and use it                ViewData[\"token\"] = Request.Cookies[\"token\"];            }            else if (Request.Query.ContainsKey(\"code\"))            {                // If we have a code, we need to exchange that for a token                string strToken = await GetAccessToken(Request.Query[\"code\"], ClientID, ClientSecret, RedirectURL);                Response.Cookies.Append(\"token\", strToken);                Response.Redirect(\"/Report\");            }            else            {                // No token or code so have the user login and get a code                GetAuthorizationCode();            }            return View();        }        public void GetAuthorizationCode()        {            Dictionary<string, string> paramList = new Dictionary<string, string>();            paramList.Add(\"response_type\", \"code\");            paramList.Add(\"client_id\", ClientID);            paramList.Add(\"resource\", \"https://analysis.windows.net/powerbi/api\");            paramList.Add(\"redirect_uri\", RedirectURL);            string strUrl = QueryHelpers.AddQueryString(\"https://login.windows.net/common/oauth2/authorize\", paramList);            Response.Redirect(strUrl);        }        public async Task<string> GetAccessToken(string authorizationCode, string clientID, string clientSecret, string             redirectUri)        {            TokenCache TC = new TokenCache();               string authority = \"https://login.windows.net/common/oauth2/authorize\";            AuthenticationContext AC = new AuthenticationContext(authority, TC);            ClientCredential cc = new ClientCredential(clientID, clientSecret);            AuthenticationResult result = await AC.AcquireTokenByAuthorizationCodeAsync(authorizationCode, new                           Uri(redirectUri), cc);             return result.AccessToken;         }    }  You might notice in the code snippet above that we use security tokens. An easy way to understand security tokens is to liken them to personnel badges used to secure buildings. The first time that you visit the building, you might have to present your driver’s license or some other form of identification (similar to OAuth2). After proving who you are, you receive a security badge for the building. Now, the next time you visit, you will only need to show your security badge and won’t have to dig out your driver’s license each time. In short, the security token is used every time a user visits the page. An important thing to note about the above code snippet is that a security token will expire after a 1-hour period and then a new one is needed. You will want to keep this in mind when preparing your application. In addition to the code snippet, you will also need to add in code for the front-end embedding process. For this, you will need to get the Report ID you wish to embed. You find this by browsing to your Power BI application and running the desired report. The URL will look something like the following: https://app.powerbi.com/groups/me/reports/{report  guid}/ReportSection You will need to use the report GUID as the report ID in the code snippet for the front-end embedding.  An example of the code is shown below:  <script type=\"text/javascript\">    window.onload = function () {        var iframe = document.getElementById(\"iFrameEmbedReport\");        // Fill in the Report ID from your workspace below.  Note that if you have a report                           // from a group you have to append \"&groupId={Group ID goes here}\" to the iframe.src                           // string below.  It would then look like:                           // iframe.src = \"https://app.powerbi.com/reportEmbed?reportId={Report ID goes here}&groupId={Group ID goes here}\";        iframe.src = \"https://app.powerbi.com/reportEmbed?reportId={Report ID goes here}\";        iframe.onload = postActionLoadReport;    }    function postActionLoadReport() {        var m = {            action: \"loadReport\",            accessToken: \"@ViewData[\"token\"]\"        };        message = JSON.stringify(m);        iframe = document.getElementById(\"iFrameEmbedReport\");        iframe.contentWindow.postMessage(message, \"*\");;      }</script><style>    #iFrameEmbedReport { width:95%; height:95% }</style><iframe ID=\"iFrameEmbedReport\"></iframe>  By using the Active Directory Authentication Library (ADAL) provided by Microsoft, you can save yourself a lot of heavy lifting and let the library handle most of the work. I recommend this tactic because not only will you eliminate the time spent decoding, but if, in the future, Microsoft were to update or change any of the authentication codes, it would be fixed automatically. This will prevent you from needing to rewrite your code and fix errors. So, how does it work? To start off, a user will log into an application and, if needed, provide consent (more on consent a little later). After logging in, an authorization code and the client secret (which somewhat functions like a password) are handed over to Azure Active Directory. Assuming that the application-provided information is correct, Azure Active Directory will then return a security token in order to grant access and allow for embedding between two applications. An example of this would be embedding a Power BI report within an internal application at your firm. This process is explained visually below: This process can work very well with a custom-built or internal application, however, if you are considering integrating with a third-party application such as Salesforce, there is a downside with the login process. Since Salesforce has its own login, users will have to log in twice – once to access Salesforce and once to your Power BI environment each time. Additionally, it can take a lot of custom developer time to work with Salesforce and get it to communicate properly between applications. Additionally, in an ideal production environment, a single page will handle your authorization process and different pages in your application will handle different reports for where lines of communication get redirected. What about giving consent? When logging into an application accessing Azure Active Directory for the first time, users will usually have to give consent. User consent is set up by default and allows users to grant Azure Active Directory access to applications. In simple terms, it is a step for users to have control over what applications they are authorizing to perform the tasks it is trying to complete – in this case, accessing a Power BI report. An admin can override this default setting if they find it appropriate. Depending on the settings that your system admin has put in place, consent will only have to be given the first time a user logs in, or not at all if your admin has granted consent across your entire user group, also known as Admin Consent. Additionally, Admin Consent can be set up on an individual application basis, so that one application can have Admin Consent granted while another application can rely on User Consent. Sometimes we find that an admin within an organization has disabled user consent for the entire company. This can cause issues when getting set up for the first time. Usually when this is the case, a user will see an error stating that they cannot give consent and that their admin needs to grant consent for them. Since user consent has been disabled, that only leaves the path for Admin Consent which will then require an admin to grant access. To summarize, Admin Consent needs to be granted once for the application and User Consent needs to be granted once for each user. Using Embedded Reports The most common reason I hear for wanting to embed data from another website would have to be enhancing reporting and dashboard viewing capabilities. You can, of course, embed more than just reports and dashboards – datasets are popular too. As an example, let’s say you have a Power BI report that you want to be able to see displayed in a custom application within your organization. Once your authorization with Azure Active Directory has been established, you can embed your Power BI report within your application by using an iframe or other custom HTML code. This makes it easier for you and your users to see your application data and reports side by side in a single instance, as opposed to looking at two applications at one time. Although the authentication process can sometimes be confusing to get started, once established it will make accessing reports and dashboards through other applications much easier for your team. If you need help with Power BI or just want to chat about opportunities for enhancement, please reach out to us!"
"212" "Getting started with Data Analysis Expressions (DAX) can be intimidating, but becoming knowledgeable in just a few basic functions can help you to unlock many new insights into your data. While it is easy to create visuals in Power BI or Pivot Charts, we often desire to view data for specific time frames, or with specific filters applied or removed, that we are unable to accomplish with our raw data.   If you’ve ever run into this problem, then having some basic knowledge of DAX can enable you to create new views of the data in your model. As a relative newcomer to Power BI and DAX, I find myself using the 5 following DAX functions most often. Let’s work through some common business scenarios with the following Power BI file, available here (note: must have Power BI to open):  Report View Data View Date Table Sales Table SalesGeography Table    Relationships View 1. FILTER The FILTER function is used to return a subset of a table or expression, as shown below.  FILTER(<table>,<filter>) Let’s say that you want to get a count of items sold at the premium level, which you define as anything over $100. We will use the COUNTROWS function, which counts the number of rows in the specified table, along with the FILTER function to accomplish this: Count of sales orders over 100 = COUNTROWS(FILTER('Sales', 'Sales'[Sales] > 100)) The first parameter, 'Sales', identifies a table or an expression that results in a table. The second parameter, 'Sales'[Sales] > 100, represents a Boolean, or true/false expression that is evaluated for each row of the table. In this expression, we are passing the Sales table to the FILTER function and asking it to return any sales that are over $100. The FILTER function is never used as a standalone function, but is used in conjunction with other functions. In the example above, we use the FILTER function to return a subset and then count the results. 2. ALL  The ALL function is used to return all of the rows in a table, or values in a column, ignoring any filters that may have been applied. ALL(<table> or <column>) In the Report View above, we have a report with multiple cards and a page-level filter that excludes sales in Germany. We would like to keep this filter, but add a card visual that shows the total number of items sold, ignoring any filters placed on the rest of the report. The following expression that incorporates the ALL function can help you to achieve this:  Count of all sales orders = COUNTROWS(ALL('Sales')) In this example, we pass the 'Sales' table to the ALL function, asking it to clear any filters that may have been placed on it. Like the FILTER function, the ALL function is not used standalone but in conjunction with other functions. In this case, we use the ALL function in conjunction with the COUNTROWS function to get a count of all sales records. The ALL function accepts either a table or a column and clears any filters that may have been placed on them. 3. RELATED  The RELATED function returns a related value from another table (example shown below). RELATED(<column>) So far, we’ve worked with functions that can help you to return a subset or clear any filters on a table or column. We would now like to filter our sales for only the United States, but don’t have all of the data we need in one table to accomplish this. Fortunately, we have the RELATED function, which we can use to retrieve values from one table to another through an established relationship. Given that there is a many-to-one relationship between the Sales table and the SalesGeography table, respectively, we can use the following expression that incorporates the RELATED function to return a count of sales orders for only the United States: Count of sales orders in the US = COUNTROWS(FILTER(ALL('Sales'), RELATED('SalesGeography'[Countries]) = \"United States\")) 4. TOTALYTD / TOTALQTD / TOTALMTD Time intelligence functions in DAX enable you to manipulate data using time periods, including days, months, quarters, and years, and then build and compare calculations over those periods. TOTALYTD(<expression>,<dates>[,<filter>][,<year_end_date>]) Continuing from the examples above, let’s say that you would like to see the total sales to date for this year. The following expression that incorporates the TOTALYTD function can enable you to easily do this:  Total sales this year = TOTALYTD(SUM('Sales'[Sales]), 'Dates'[Dates]) The first parameter, 'Sales'[Sales], identifies the column that you would like to aggregate. This could also be an expression that returns a scalar, or singular value. The second parameter, 'Date'[Dates], is a column that contains dates. Time intelligence functions are immensely useful functions that eliminate the need for complex code in calculating aggregations over commonly used periods of time. 5. CALCULATE  The CALCULATE function evaluates an expression in a context that is modified by specific filters. CALCULATE(<expression>, <filter1>,<filter2>…) Let’s say you are now interested in tabulating all sales for all areas. While you could create some piecemeal expressions to accomplish this, you can easily and cleanly accomplish the same thing utilizing the CALCULATE function. The following example, which uses the CALCULATE function, can accomplish this: Sum of sales all countries = CALCULATE(SUM('Sales'[Sales]),ALL('SalesGeography')) The first parameter, SUM('Sales'[Sales]), identifies the column that you would like to aggregate. The second parameter, ALL('SalesGeography'), represents a Boolean that removes any filters that may have been placed on the SalesGeography table. Notice that this ignores the page-level filter that excludes sales in Germany. The CALCULATE function is one of the most powerful and useful functions in DAX. It is helpful to think of the CALCULATE function as a supercharged “IF” statement. A couple of rules apply to the CALCULATE function: The filter parameters cannot reference measures, and expressions cannot use any functions that scan or return a table. The CALCULATE function is typically used with aggregation functions, and although the filter parameters are optional, at least one is typically used. It’s possible to create some very sophisticated queries in DAX, but being well versed in just a few functions can help you to unlock many interesting insights into your data. I would recommend checking out Microsoft’s DAX reference for more information. If you still have questions or just want to chat about Power BI, contact us and we will be glad to help!"
"213" "SAS Enterprise Guide and Microsoft Azure Machine Learning are products used by analytics pros, statisticians, and data scientists alike, but how do they differ? Would your organization benefit by choosing one product over the other?  Let's start by defining what each product has to offer:  SAS Enterprise Guide is a graphical user interface and project management software, offered as part of SAS’ suite of products. It serves as the point-and-click front end for the SAS programming language. SAS Enterprise Guide is installed locally in your organization.    Microsoft Azure Machine Learning is a relatively new service offered by Microsoft on their Azure cloud platform. Azure Machine Learning is also GUI-based and is used for constructing and operationalizing machine learning experiments on Azure. This is hosted in the cloud, connected to your Azure account. The User Interface & Managing Projects One similarity that you will notice between the two products is that they are both workflow-based. You will select different objects or actions and connect their boxes to perform tasks. Here are some key differences in the user interfaces:  SAS Enterprise Guide   Azure ML   SAS Enterprise Guide stores its projects in .egp or Enterprise Guide Project Files. This allows for you to save different versions of your workflow as separate files and you can even share the files with others in your organization (as long as they have access to the same data that you used in the workflow).    Azure ML is hosted in the cloud. This means that the experiments you create are saved to your Azure Machine Learning Workspace in your Azure account. To share an experiment, you must share its entire Workspace from within the Azure Machine Learning Studio.   Sourcing your Data  SAS Enterprise Guide   Azure ML   For SAS Enterprise Guide, the server data sources you can select from are a bit limited, but there are still plenty of flat files that can be imported:  Data stored on the SAS Server in libraries  OLAP Servers  Excel Files and Access Databases  Text Files (.csv, .txt, .tab, and .asc)  HTML Files   You have the option to source your data from many places:  Azure SQL Database or On-Premises SQL Database  Azure Blob Storage, Azure DocumentDB, or Azure Table  Data Feed Provider (OData)  Web URL via HTTP  Hive Query  Manually-Entered Data (copy and paste from a .csv, .tsv, .arff, etc.) Plus more…    Utilizing Pre-Defined Algorithms  SAS Enterprise Guide   Azure ML   SAS Enterprise Guide has many statistical prediction algorithms and machine learning algorithms already pre-defined in the menu.    Regression Linear Regression Nonlinear Regression Logistic Regression Generalized Linear Models   Multivariate Correlations Canonical Correlations Principal Components Factor Analysis Cluster Analysis Discriminant Analysis   Survival Analysis Life Tables Proportional Hazards   Time Series Basic Forecasting ARIMA Modeling & Forecasting   In addition to these algorithms, SAS Enterprise Guide also includes many statistical functions (like ANOVA and t-Tests) as well as data manipulation functions. For decision trees, neural networks, other clustering algorithms, and more advanced text analytics, other SAS products must be used. Namely, SAS Enterprise Miner, SAS Text Miner, and SAS Sentiment Analysis.   Azure ML also has a plethora of algorithms that are pre-configured for use in every experiment.  Anomaly Detection One-Class SVM PCA-based Anomaly Detection  Regression Ordinal Regression Poisson Regression Fast Forest Quantile Regression Linear Regression Bayesian Linear Regression Neural Network Regression Decision Forest Regression Boosted Decision Tree Regression  Two-Class Classification Two-Class SVM Two-Class Averaged Perceptron Two-Class Logistic Regression Two-Class Bayes Point Machine Two-Class Decision Forest Two-Class Boosted Decision Tree Two-Class Decision Jungle Two-Class Locally Deep SVM Two-Class Neural Network   Multi-Class Classification Multiclass Logistic Regression Multiclass Neural Network Multiclass Decision Forest Multiclass Decision Jungle One-v-All Multiclass   Clustering K-Means   Time Series Time Series Anomaly Detection   Text Analytics Vowpal Wabbit   Azure ML also has other statistical functions (like correlations and data summarization) and data manipulation functions as well. As you may have noticed there aren’t any time series functions like ARIMA. There also aren’t very many text analytics functions other than Vowpal Wabbit. You’ll have to code these for yourself in R or Python. You can also use the Microsoft Text Analytics API for other text analytics functions like Sentiment Analysis.   It’s also worth noting that, while Azure ML seems to have a more extensive list of algorithms to choose from, Enterprise Miner groups many items under one function. For example, Poisson Regression, explicitly available in Azure ML, is available under the Generalized Linear Models function in SAS. While Azure ML is prone to making individual algorithms their own packaged function, be aware that SAS Enterprise Guide may only allow for many options under one function.    Writing Custom Code  SAS Enterprise Guide   Azure ML   For Enterprise Guide, you can create a code object in your workflow and write your own SAS procedures. This is fully integrated with the rest of your workflow.    In the Azure ML workspace, you can choose to write your own R or Python scripts. This is also fully integrated with the rest of the experiment’s workflow. For R and Python, many common packages are already installed and can be referenced from within your custom script. For packages not installed, you can always upload the .zip file of the package and reference the package in an alternative way in the R script.   Cost  SAS Enterprise Guide   Azure ML   To get Enterprise Guide, you will need to get SAS Console first. This is the base system that is required for all other SAS components. You must go through a SAS Account Manager to purchase a SAS license. The cost(s) associated with getting a SAS license are often a highly-guarded secret and can be fully dependent on your company size, industry, prior history with SAS, number of users, and even the account manager that you deal with. However, your organization can expect a cost of >$100,000 for a SAS license per year. You will also have to pay extra for the Enterprise Guide and the server parts, too.   The cost to use Azure Machine Learning is dependent on how much you choose to use it. There are two price tiers for the service: Free and Standard. For the Free tier, you can procure one workspace with the ability to use up to 10GB of storage at no cost. For the Standard tier, you pay by the number of users that need access to the service and by the number of hours each user uses the tool. This cost is still very low, ranging from around $9.99 per month for minimal use to the extreme of  $729.99 per month if the user were to run experiments 24/7 for the entire month.  Still need help? Choosing the right solution for your organization can be tough, even with endless information on product options. If you are looking for help in making an informed decision, contact BlueGranite today! Our analytics experts and experienced consultants will be happy to help point you in the right direction for your firm. "
"214" "Last week, we featured Data Lakes and Data Warehouses in our monthly webinar series. In case you missed it, you can check out the recorded session here. Throughout the presentation, we received several questions ranging from implementation to training recommendations. Below, you'll find some insightful audience questions and answers from our presenters, Josh Fennessy and Merrill Aldrich.   Do you have any suggestions for handling schema changes that happen in Data Lake files? Specifically, the kind that wreak havoc with 'schema-on-read' tactics such as a PolyBase external table? Merrill: This definitely is a problem, and in my mind, it has two parts. To start, it seems like the governance over the warehouse has to manage which sorts of files can change safely and which must not, because the downstream processes are “non-production” or more forgiving versus being tied into a whole lot of important code. For example, analysts might be frustrated if queries against the lake files break, but if the public uses them through, say, some website feature, it could be much worse. Secondly, the mechanics of actually encoding the change in the reading application (perhaps PolyBase) seem like they would have to be similar to a change in a source query for SQL – that is, sadly, a bit of old fashioned dev work. Josh:  Managing file formats is a big challenge with Data Lakes. It's one of the reasons that we choose to use a staging area to help move the data to the \"Raw\" layer for permanent storage. There are a couple of approaches to help mitigate this problem. First, I would recommend that you store data with differing formats in different folders. Many of the compute tools that are used in a Data Lake look at all files in a folder, so it's important to have them organized appropriately. Secondly, for savvy programmers, there are options to build flexible processing. With Azure Data Lake Analytics, for example, there is a flexible schema extractor than can be used to deal with files that have different column counts. This blog post from Microsoft does a nice job of highlighting how to handle this problem in more detail. What advice do you have when moving data from a Data Lake to a Data Warehouse? Merrill: In order to get familiar with the options, I would start with reading a bit about Sqoop, PolyBase, Azure Data Factory, and even SSIS. There are many different tools available today so it might take a short research effort to match the best one to your needs. Here are a few resources to help you get started: PolyBase Guide Apache Sqoop Software Download Azure Data Factory Guide SQL Server Integration Services Josh: In addition to the typical batch-type movement using Sqoop, Azure Data Factory, or even SSIS, you can also consider building a Lambda Architecture for dealing with data in motion AND batch data. Technologies like Kafka, Spark Streaming, Azure Event Hubs, and Azure Stream Analytics are all pieces that can be used to build solutions to move data from the Data Lake to the Data Warehouse. Reactive platform systems like Microsoft Flow also offer interesting possibilities to manage communication between the two systems. Our Data Warehouse model has a Staging area, a Raw area and a Structured Analytics area, however, we do not have a Sandbox. Are we still on the path to a Data Lake? Merrill: Well, if your Data Warehouse is a database system, then perhaps not – though the features you describe are useful in any case. A Data Lake typically can manage raw files, and can use a variety of tools to query and mine those files. If the Data Warehouse you refer to is a file store, then you may be on the path to a Data Lake. Josh: One of the biggest factors that differ between a Data Lake and a Data Warehouse is the approach to loading and consuming data. With a Data Warehouse, we follow a 'schema-on-write' approach, meaning, we apply schema to our data as it's ingested into the system. On the other hand, with a Data Lake, we follow a 'schema-on-read' approach. We DO NOT apply any schema to the data when it is ingested in the Data Lake, but rather when we query or consume data, we have to define the schema of our data for the job we are executing. So, if your current environment follows more of a schema-on-read pattern – meaning you ingest data in its raw format and apply schema later on when you are running queries – then you're probably on the path to a Data Lake. If not, however, and you're fitting data into pre-defined database tables, then it's more of a Data Warehouse approach. When organizing the Data Lake into areas for Staging, Raw, etc., would you classify these as top-level containers? Or do you actually build separate Lake Stores? Josh: The answer to this question depends on what technology you are using for your Data Lake. If it's an on-premises Hadoop cluster, then we will often have multiple HDFS environments, each managed by a set of HDFS NameNodes. If you're dealing with a platform-based solution like Azure Data Lake, it can make more sense to use top-level folders with appropriately designed security to manage user access. If you are using Azure Blob Storage as your Data Lake storage platform, then you're probably going to be best served using a unique Storage Account for each area, as there are account size limits for Blob Storage. How would you recommend triggering movement between the Staging and Raw areas? Josh: Typically, in a Data Lake most of the data movement between areas is managed with batch processing. There are times when we would choose to use a Lambda Architecture to collect and do some basic analysis on data in motion. When using a Lambda Architecture, we will often land data in the Staging Area in real time, but later process the data in to the Raw and/or Curated areas using a batch process. As data moves through Staging>Raw>Analytics, when and where would you match and merge customers together when gathering and analyzing customer lifetime value from multiple disparate sources? Merrill: By convention, the staging and raw areas are mainly for untransformed, raw copies of source data, so it seems like if you are correlating, merging, and so on, that it would be an analytics function. That said, if it benefits the business to provide that match and merge to a large audience in a repeatable form, it could certainly be automated on the way into the analytics area. Josh: Additionally, customer matching and cleansing is something we will typically do as we bring data into the Curated Layer – this process would probably feed the Data Warehouse. Can I store relational data (SQL Server, Oracle, etc.) in a Data Lake? Merrill: Not the data directly (as in the database files), but you can definitely export data out to defined files and store those in the Data Lake. For example, if you have a system that does a poor job of retaining history as data changes, and that history has value, you could extract snapshots on a daily or monthly basis of important point-in-time data and put the results in a Data Lake. If relational data is stored, then how can I query the data out of it? Merrill: PolyBase is one example of a way to query or join the data in files in the Data Lake with data in the database. The retrieval of the file-based data will not have the same level of performance as local data to the SQL Server, but this can work for historical data or analytics. Josh: PolyBase is a truly unique piece of technology in that it allows us to effectively merge Data Warehouse usability with the massive storage capacity and flexibility of a Data Lake. As Merrill stated above, it can be a great way to join together data that is stored in the Data Warehouse with additional data in the Data Lake. What toolsets and training do you recommend for an organization getting started with Data Lakes? Josh: If you're just getting started, I highly recommend considering a cloud-based solution. As we said during the webinar, cloud platforms, like Azure Data Lake, allow you to get started VERY quickly. We can deploy a Data Lake Storage account in just a few minutes and start uploading data right away.  PolyBase also offers a lot of flexibility in the compute layer with three major platforms for working with Azure Data Lake Store: Azure Data Lake Analytics is a \"cluster-less distributed computing\" platform that allows you to write data processing jobs in U-SQL without all of the complexity of managing a cluster. HDInsight is a Hadoop-as-a-Platform offering that brings the power and maturity of the open-source Hortonworks HDP distribution to the cloud. HDInsight is a great choice when you need to use a variety of tools to process your data, but don't want to rely on a full-time administration task to keep things running. Azure Data Warehouse is a distributed SQL Platform that works much like on-premises SQL Server, but also includes PolyBase connected to Azure Data Lake Store. Microsoft has done a good job of creating learning content as well. Learn Analytics is a great site that offers multiple training opportunities for Data Lake and Analytics in general. The Azure Documentation is pretty well done too! Microsoft maintains a large number of projects on GitHub that can also provide opportunities to take advantage of project templates to get started quickly. What do you think about an Azure Data Lake solution? Josh: Azure Data Lake is a great solution! While young, it is maturing nicely and we are excited to see how much in grows in the next 12 – 24 months. What are the pros and cons for Google, Amazon, and Microsoft Data Lakes? Josh: Most of our work at BlueGranite is with the Microsoft cloud, so unfortunately, we are not fully up to speed on all of the features of Amazon and Google. All three vendors offer competitive pricing on hyper-scale storage and have flexible options for computing platforms. All vendors have Hadoop-as-a-platform solutions available as well. I think Microsoft is in a unique position when it comes to Enterprise integration in that Azure Active Directory Sync allows for same-sign-on or, in some cases, even single-sign-on access to Cloud Resources. Azure Data Lake has Active Directory security built in, so it's pretty easy to manage user access to Data Lake Resources. PolyBase is also another HUGE differentiating factor. It's the 'missing link' between the Data Warehouse and the Data Lake, plus it provides opportunities for tight integration between on-premises infrastructure and cloud platforms. Do you have some high-level business use cases for when you would use a Data Lake versus a Data Warehouse?  Josh: Yes! Here are some examples of Data Lake solutions we have implemented for our customers: Manufacturer Explores Ways Big Data Can Boost the Bottom Line Universal Parts Supplier Hones Work Methods with Data Lake Century-old Charity Looks to Map Future through Big Data Another example you can check out to get your idea-engine going is this blog post on the Top 6 Use Cases to Help You Understand Big Data Analytics.  Thanks to everyone who joined us for the webinar! If you have more questions for Josh and Merrill, or just want to chat about your Data Lake and Data Warehouse environment, feel free to drop us a line. "
"215" "Last week, Microsoft was recognized as a leader in the Gartner Magic Quadrant for Business Intelligence and Analytics Platforms for the 10th year in a row. I remember back in the summer of 2013 when Power BI was first announced by Amir Netz at the Microsoft Worldwide Partner Conference in Houston, Texas. I was sitting about ten rows back, and the demo we witnessed that day was truly awesome. The Elvis music use case that Netz showed is still one of my favorite Power BI demos! As the presentation went on, I saw an extremely visual, interactive solution that would eventually revolutionize how traditional Microsoft Excel power users could interact with and shape data. At the time Netz presented, Power BI had PowerPivot, but as an Excel add-in it was clunky at best. With that first announcement of Power BI, Microsoft set a new course to a true cloud-based BI offering for all users – consumers, power users, and IT pros alike.  With Power BI, slicing and dicing would no longer be the domain of SQL Server AS cube developers (like us!)  During Netz’s presentation, one of my Microsoft partner frenemies texted me and asked, “Is BlueGranite going out of business because of this announcement?” At the time, I found myself asking the same question! Fortunately, the demand for data and analytics has never been stronger. Our company has grown lock-step with the maturity of Power BI, which is why the latest announcement from Gartner has been so personal to all of us at BlueGranite. As a Microsoft data and analytics partner over the past 10+ years, we have been witness to a couple of interesting market cycles. First, Microsoft BI grew from an afterthought for corporate reporting and dashboard projects into the leading combination of SQL Server SSIS/RS/AS and SharePoint for on-premises dashboards and BI solutions. Then, in 2010, Microsoft’s then-CEO Steve Ballmer announced that Microsoft was “all in” with the cloud. It was a visionary statement, but left us drifting along for a few years in the BI space. While the rest of Microsoft was moving to Office 365, CRM Online, and Azure, we were left with a (newly) outdated on-premises platform with SQL and SharePoint. Fast forward a few years to today, after a stream of monthly updates and new features from the Power BI team, and it’s amazing to see Microsoft’s dream of leading with BI in the cloud finally realized. Gartner has it right – Microsoft is the leader in both vision and execution. We see it every day with our clients who are moving not a few hundred users to Power BI, but tens of thousands. This has created countless new ways we can add value for our clients. Power BI is uniquely a first-class ad-hoc self-service BI tool, but it's also an enterprise data visualization tool that can replace outdated legacy platforms. Power BI is the tip of the iceberg for countless data integration, advanced analytics, and cloud data platform solutions to help businesses harness value from their data. Congratulations, Microsoft! And here’s to another 10 years of both leadership and growth in data and analytics!"
"216" "Building a Data Lake is not a small task. It requires a large amount of storage, distributed across many servers, all working in sync to provide fast, reliable access to your data. Building a distributed computer system is much more complex than deploying the single-server solution that many of us are more familiar with.  In a recent post, Mike Cornell encouraged thinking big, but starting small – that encouragement applies to deploying a Data Lake as well. For many IT professionals, our first impulse might be to design a large, complex, and expensive hardware-based solution. However, when it comes to building a Data Lake, implementation, organization, and ease of data egress holds a higher position than specific technology requirements. Cloud providers, like Microsoft, offer great solutions for deploying a Data Lake that is agile, scalable, and performs well with various distributed compute engines. Some of the most important reasons to think cloud first are: Implementation Efficiency In the introduction, I alluded that deploying an on-premises Data Lake is complex. Let’s take a look at what it can involve. The first question you’ll need to be able to answer is “How much data do you want to be able to store in your Data Lake?” Before you answer, think about the next 3 years – how much data do you think users will want to bring in to analyze in the Sandbox? What about Data Warehouse archival? Do you know all of the data sources that users might need to analyze in the future? It’s common to estimate a number, say 100TB, and then decided to double it, 200TB in our case, to account for the unknown. Once you have an idea of how much storage you want to aim for – you'll need to triple that number. Why do we need to triple the storage limit? Because modern distributed file systems are so resilient, and because they replicate files stored in the Data Lake a minimum of 3 times (if you’re lost, check out this post on data lake organization). This ensures that if any single node were to fail, two other copies of the data are available for immediate use. Our storage target is now up to 600TB. Further, you’ll want to increase your storage amount by a factor of 20 percent to account for space used during data transformation and staging during data processing. Our final storage requirement for the fictitious solution is 720TB. Not a small amount… To build an on-premises environment capable of 720TB of storage, we’ll need to do some calculations. Modern servers designed for Data Lake usage contain around 60TB of usable attached storage. This means we’ll need 12 servers to hit our target of 720TB of storage for our Data Lake. How long does it take to procure 12 servers in your environment? Once they are procured, how long does it take to install them in your data center, install operating systems, and test hardware to make sure it’s ready for use? Finally, how long should you plan for the software installation, configuration, and testing for the Data Lake platform? In my experience, this process, from beginning to end, can take anywhere from weeks to months. Now, let’s compare this with a cloud deployment. Microsoft Azure offers a product named Azure Data Lake Store (ADLS). ADLS is a cloud-based implementation of HDFS. It’s failure resilient, integrates with existing Active Directory security, is POSIX compliant, WebHDFS compatible, and has no file size or account size limits. In short, working with ADLS is not much different than working with HDFS. Your Data Lake developers will appreciate that. How long does it take to deploy? About 15 minutes, once you log into your Microsoft Azure portal. Now, I don't want to pretend that it’s that easy – most enterprises will be deploying more than just ADLS. Active Directory configurations need to happen to allow for secured data locations and networking infrastructure. For example, Azure Express Route needs to be installed to allow secure communication between your data center and Microsoft Azure. Those processes can sometimes take weeks to complete. But, while the networking infrastructure is being deployed, you can be working with a cloud-based Data Lake. Simpler encryption options like Site-to-Site or Point-to-Site VPN can be quickly configured for temporary use. So, yes, to fully operationalize the Data Lake in the cloud, you’ll need to plan for several weeks. But, to begin a proof-of-concept, you only need to plan for a day, or maybe two, to deploy a Data Lake infrastructure capable of housing all the data you need. Elastic Scale In our fictitious example, we estimated 100TB of data storage, then applied some multipliers to account for unknowns, data replication, and working space. Our final number came to 720TB of needed storage. What if our initial estimates were wrong? What if our multipliers were way off? With an on-premises solution you’re probably going to be locked into the hardware that you purchased. Getting budget approval for 12 servers doesn’t mean you can go out and buy 6 more because you underestimated – and vice versa – no one is going to pat you on the back because you over-estimated your storage requirements and are only using 20 percent of the expensive infrastructure that you just deployed. With cloud solutions, like ADLS mentioned above, you only pay for what you use. What exactly does that mean? Well, if you upload 1TB of data during month 1, you’ll pay for 1TB of storage use for month 1. That’s it. If it takes 24 months to hit the target of 200TB, you only pay for the monthly storage that you use as you climb to that final figure. Another plus with ADLS? The replication factor is set behind the scenes – with our on-premises solution example, WE had to account for the replication factor. With ADLS, we don’t have to. IF we upload 100TB of data, we pay for 100TB of data, not 300TB. With cloud solutions, we don’t have to make a special request to increase storage limits, or ask for forgiveness when we grossly over-estimate what we think we need. We only pay for what we use. In many ways, it’s a simpler model. Pricing and Cost Structure It’s no secret that cost is one of the major reasons that many organizations are looking to move to cloud platforms. The days of 3 to 5-year hardware refreshes are growing slim. New projects, like building a Data Lake, that require massive hardware investments are closely watched, and serious questions about the value of purchasing, maintaining, and depreciating that hardware is being very closely measured. As IT becomes more and more of a cost center, it is becoming more effective to treat the cost of “doing IT” as an operational cost, rather than a capital expense. So, just how much does the cloud cost? Let’s use our 100TB estimation as a benchmark and dig deeper. When we start to estimate cost, one of the major differences between on-premises and cloud platforms is exposed. With an on-premises solution, our storage layer and our compute layers are directly related. With Hadoop clusters, (one of the most common platforms to implement a Data Lake) each compute node uses locally attached storage. Each of the nodes' storage aggregates to the total cluster capacity. With cloud platforms, however, storage and compute are separated into different services. ADLS only provides the storage layer. When it comes time to apply compute to the data, there are multiple engines available, and each of them can access the data in a Data Lake. Because of this difference, it can be a bit difficult to align estimates for common comparison. For the example below, we will assume that the solution requires the ability to store 720TB of data maximum, and have 150TB of the raw data stored. We'll also assume that we are using comparable compute platforms. For the on-premises Data Lake (single environment), we'll need the following components: Component Estimated Price 2x Hadoop Management Nodes $30,000 (approx. $15,000 each) 30x Hadoop Data Nodes - 720TB capacity, 450TB (150TB x 3 replicas used) $300,000 (approx. $10,000 each) Network Components (intra-cluster switches) $7,000 Hardware Support Contract (Yearly) $35,000 Hadoop Support Plan (Yearly) $45,000   Assuming an even depreciation rate of hardware over 5 years, the approximate monthly cost for an on-premises Data Lake solution is $12,283. Let's compare that with the monthly cost for a cloud platform solution (single environment) hosted with Microsoft Azure: Component Estimated Price Azure Data Lake Store (150TB used, unlimited capacity) $5,700 HDInsight Cluster (10 compute nodes, used for average of 75 hrs / week) $3,450 Express Route (direct Fiber connection to Azure Data Center) $820 Enterprise Support $1,000  For a comparable cloud solution, the estimated monthly cost is $10,944. Please note that this pricing doesn't include any volume discounts that might be available, and is based on public Pay-As-You-Go pricing, not enterprise pricing - which is often offered at a cheaper per-unit rate in agreement for meeting consumption targets. You can see here, that the cloud solution isn't \"pennies on the dollar\" when compared directly to the on-premises solution, but it is cheaper. Additionally, the on-premises solution does not include hidden costs, such as those required to run a data center (environmental, utilities, staff, etc.) The on-premises version is also much less reactive to change. Adding more storage to an on-premises cluster requires more capital expenditure (buying more cluster nodes). Adding more storage to an Azure Data Lake simply comes with a higher monthly bill based on the amount of storage being used. Think Cloud First Implementing a Data Lake is not a small task, but it can reap huge rewards in the increased availability of data, open new doors for new analytics to drive company growth, and enable data analysts to be more efficient. Going cloud first means that you can take advantage of implementation efficiencies, deploying the Data Lake infrastructure faster. You'll be able to take advantage of elastic scale, only paying for the storage that you're using without worrying about maximum storage limits. And you'll be able to take advantage of simplified cost structures, focusing on operational spending versus capitalized costs, with the ability to control the operational spending over time. Have questions about implementing a Data Lake or just want to learn more? Contact us! BlueGranite will be happy to help you make an informed decision on the best solution for your environment."
"217" "In nearly every meeting I've had with customers to talk about building a Data Lake, a single question has been asked: \"How do I prevent my Data Lake from becoming a Data Swamp?\" The answer is organization. Hard work, governance, and organization.  Data Lakes Aren't Inherently Organized It's true that Data Lakes are, in some ways, nothing more than a big block of storage. There is nothing preventing a Data Lake from becoming a mess of disorganized files. This is why, when you first implement a Data Lake, you will need to design some basic organizational structures up front. Before you even begin loading data, you should divide your Data Lake into the following four main areas. Staging The Staging area of your Data Lake is a place for data to rest as it's moving from a source system to another area. It is primarily used by data ingestion systems as a landing zone for new data. It may also be used as a place for minor processing, such as merging many small files into in one larger file for storage in the Raw layer. Raw or Persisted The Raw or Persisted area of your Data Lake is where data is kept indefinitely in its raw format. It may have undergone some minor processing, such as combining many small files into a few larger files, but for the most part the data has remained untouched. One of the biggest benefits of a Data Lake is the ability to keep a copy of stored data in its original, raw format. Agility is a big selling point for the Data Lake – since we don’t always know every use case for the data, it’s often difficult to know exactly what shape it should be in for future solutions. By making a copy of the original data available at all times, we can be assured that we'll be able to apply new formatting, processing, and cleansing should the need arise. Analytic or Curated In this section of the Data Lake, the data has been heavily processed. Sometimes it is aggregated and stored in a star schema-like format to conform with different reporting and analysis tools. The Curated area of your Data Lake should be one of your most governed locations. Since the data living here has been heavily processed, it will also be under a high level of scrutiny. It should be protected just as much as your Data Warehouse, because this is the area that most users will have access to and will use to read appropriate data assets. The Curated area will also serve as the home for your online Data Warehouse archive. By storing a copy of your star schema in the Curated area, you can easily run queries similar to what we would ask of the Data Warehouse. Additionally, we can use technology such as SQL Server PolyBase to merge the results of live Data Warehouse data and online archived data in the Data Lake with a single query. Sandbox The Sandbox is designed to be used by deep analysts and scientists as an unmanaged area. Data can be added here by any user, and doesn't require a governance process, although we suggest that users self-govern their own data. This area is not designed to be used by the general populace for reporting, but rather by a much smaller user group focused on experimenting and exploring new data concepts and solutions. The sandbox is unique to the Data Lake. It is uncommon for a user community to have unfettered access to a database server and be allowed to store any data in any shape. But with a Data Lake, that access is happily granted. Because we can granularly apply security to a Data Lake, this ensures the Sandbox users won’t be able to adversely affect other sections of the Data Lake. Here is a review of the four main areas of the Data Lake and how each area relates to User Access, Governance, and what level of Data Processing the data undergoes:   Staging Raw Analytic Sandbox User Access None Limited Full Read (by Role) Read / Write Governance None Basic Full Minimal but User Managed Data Processing None Minimal Heavy Minimal based on use Further Organization Beyond the high-level organization patterns mentioned above, it’s also important to keep each section of the Data Lake organized. The organization pattern chosen will vary based on the needs of your business case, but I’d like to propose the following template. This template will not only ensure continued organization and easier governance, but it will also provide a level of self-documentation. By looking at the folder structure, it will be easy to understand what area of the business this data applies to, where it came from, and its general use. Most data in the Data Lake can be organized into its use case, and all data comes from a source system. Beyond that, there will be specific formats for each data source. Therefore, following a directory structure that looks similar to the following example is a good starting point. Nearly all compute engines that work with a Data Lake are designed to read data per folder, so it is important to ensure that data in a single folder is all similarly shaped. With this assurance, it is possible for the data transformation process to fail due to incorrectly defined data structures. For example, it is possible to create a Hive table that is pointed at a single file, but it is much more common to set the location of a Hive table to be an entire directory of files. If we define a table that describes a 4-column pipe-separated file, and one of the files in the directory has 9 columns, we’ll get unexpected data returned from user queries and data transformation routines. Ensure Successful Implementation When implementing your first Data Lake, planning your organization pattern should be one of the first tasks you complete. I've suggested in this article that you should segregate your Data Lake into four main areas: Staging – for data ingestion and movement Raw – for storing original copies of data in its raw format indefinitely Curated – for data storage that is heavily transformed, perhaps into a star schema for analysis Sandbox – to enable analysts and scientists the opportunity to work with, collect, and transform new data without the need for heavy controls Within each of the separate areas, it is important to apply logical organization by Business Area, Source System, and dataset format. Individual considerations for each dataset can be reviewed and integrated as needed. Continued success with a Data Lake will rely on reviews of the organizational format and continued governance of the data stored in the Data Lake. If you still have questions or are looking for more information on Data Lakes, contact us, and we will be happy to help get your project going in the right direction!"
"218" "At BlueGranite, we work hard to deliver high-quality blog posts to the community every week. This week, I thought it would be nice to do a quick recap of my top 5 favorite posts of all time, in no particular order. Below you will find helpful topics ranging from data lakes to Power BI, plus more.   Top Five Differences Between Data Lakes & Data Warehouses  I’m starting off the list with an oldie but a goodie - not only is this article the most popular post out of anything we have ever published, but it is also one of my favorites. It perfectly highlights what a data lake and a data warehouse actually are, in addition to pointing out important differences. If you are wondering which approach you should use at your organization, it depends on a lot of factors. The technologies you are using, the stage of your data warehouse and/or data lakes project, plus organization goals will all play a role in finding the right solution for you. For more information and help with understanding data lakes and data warehouses, check out our free eBook: Understanding Data Lakes in a Modern Data Architecture.   Effectively Planning a Power BI Rollout Number two on the list focuses on Power BI and some of the difficulties organizations face when rolling it out to their teams. Planning an effective rollout can be involved, and should take every user type into account. I’d urge you to consider the following four key steps outlined in this post when starting your Power BI project: Create a branch of IT dedicated to exploratory reporting Map users to deployment modes Map Power BI features to deployment modes Set adoption targets per deployment mode and monitor usage By following these steps accordingly, your firm will be more likely to be successful using Power BI and taking full advantage of its capabilities. Although it takes a lot of planning and forethought, your patience will be rewarded when the time comes to deploy. If after reading you are still looking for more help, try checking out the recorded webinar: Planning a Power BI Rollout.   Demo Day: Deriving Dimensions in Power Query I couldn’t make a list of my favorite posts without including our Demo Day Series. The most recent demo highlights how to derive dimensions using Power Query, also known as Get and Transform in Excel. The demo is a great example of how you can shape data into a clean and convenient form for analysis. In just a few minutes, you can learn how to: Import data and identify columns, making dimensions Isolate distinct values from the column(s) you identified, and number them with Add Index Join your data to dimensions using text columns for matching Expand matching ID values, remove the corresponding text columns, and make a neat, compact fact table With the educational video, you will be able to quickly analyze your data and gain access to valuable information for your organization – not bad! Built-in Analytics: Why R Matters to a SQL Server Professional Fourth on the list is a post focused on how R Services in SQL Server makes it easier for users to operationalize high-performance Big Data analytics with traditional, unstructured, or hybrid data sources. Since this was published, R’s open-source platform has grown rapidly in popularity, although it is sometimes difficult to scale to Big Data applications. Consider checking out the video toward the end of the article for some insight on how a common Big Data application of credit risk scoring uses R Services in SQL. Since credit risk usually needs to be scored in real-time and is based on very large data sets, having the model close to the data can really speed time to value. 5 Important Steps When Migrating to your Scaled-Out Data Warehouse Last on the list, but certainly not least, is an article on the important steps you should consider when migrating to a scaled-out data warehouse. In this post, we discuss how appliances like APS or platform solutions, such as Azure SQL Data Warehouse (DW), remove the need to build your own scaled-out infrastructure. However, they do still need planning when migrating an existing single-server DW to a distributed format. Five important steps include: Knowing how your data will be distributed Testing your reporting applications Checking your procedural code for incompatibilities Preparing to modify your ETL processing Identifying your highest priority pain points Migrating your data can be a challenging journey – sometimes it will seem quite simple and easy, but other times you might feel like you are relearning everything you already know about date warehousing. But don’t worry, as BlueGranite can be there to help you build a plan and ensure your scaled-out DW is a success. What’s up next? Stay tuned for industry news, information on Microsoft product announcements, and insights from our team on how to get the most out of your data analytics tools – all you need to do is subscribe to our blog and you will be sure not to miss out on any exciting information. Additionally, consider checking out the BlueGranite webinar series where we discuss new topics and share insights each and every month with the community. On February 23rd, we will be featuring Data Lakes and Data Warehouses, covering how to tell the difference between them and what pitfalls to avoid when implementing each solution.   "
"219" "In any organization, data is housed in many locations and in many formats. Nowadays, many business analytics tools have the native ability to import from almost any data source imaginable. For example, Microsoft Power BI has the ability to get data from over 60 different sources, including many common file types and more esoteric data storage software.  From flat files to databases to proprietary file formats, we'll take a look at how simple it is to use these data sources in R. Many times, as you’ll see, all it takes is the installation and use of a simple package. This includes many proprietary formats such as SAS and Microsoft Excel. The steps below will take you through each format and help you to master importing and exporting data!    Reading and Writing Common Flat Files Base R R comes equipped with the ability to read and write many common text-based flat files including .csv files, .tsv files, and more. For example, let’s say I have a file called mydata.csv in my data folder. R has a function, read.table, and its children functions, read.csv, and read.delim.     #read.table works in the following manner: \"read.table(file, header = FALSE, sep = \"\", quote = \"\\"'\",...\". csvinput <- read.table(\"data/mydata.csv\",header = TRUE, sep = \",\",quote = \"\\"\")  #Alternatively, using \"read.csv\"\" will assume the separator is a comma, which may save some time. #We used the quote parameter to tell R that there are some strings in the file surrounded by double quotes.     Now, let’s say that you have done some data manipulation and calculations and you want to export your data back to your data folder. This is just as simple as reading things in.      #write.table works in the following manner: \"write.table(x, file = \"\", col.names = TRUE, append = FALSE, quote = TRUE, sep = \" \",...\". #Alternatively, using \"write.csv\"\" will assume the separator is a comma, which may save some time. #We use the append parameter to tell R to overwrite the file rather than just tack on rows to the end of the old version. write.table(input,file=\"data/output.csv\",col.names = TRUE, append = FALSE, quote = TRUE,  sep = \",\")      [read.table documentation] readr In the realm of tidyverse, there is a package known as readr that can handle importing flat files, too. This package has multiple functions to handle different flat file types. The readr package runs a bit faster than the base R functions and handles factors and dates a bit better, too. It also supports seven file formats with seven read_ functions: read_csv(): comma-separated (CSV) files read_tsv(): tab-separated files read_delim(): general delimited files read_fwf(): fixed-width files read_table(): tabular files where columns are separated by white-space read_log(): web log files      #install.packages(\"tidyverse\") or #install.packages(\"readr\") library(readr) #All of the readr functions work in this way: readr_csv(\"filename\").  input <- read_csv(\"mydata.csv\") #readr assumes the filetype and delimiter by the function you choose. #For example, when you use readr_tsv, the package assumes it's looking for a .tsv file and that the delimiter is a \"\t\".      [readr package documentation]   Excel-ling with Importing and Exporting Similar to how we just imported a .csv, we can use almost identical functions to the previous section to import Microsoft .xls and .xlsx Excel files. There are many packages in the R universe that can do this. Here, we'll take a look at two: openxlsx and readxl. First, you’ll need to install one of the packages. After it's installed, it is practically the same syntax as before. openxlsx     #install.packages(\"openxlsx\") library(openxlsx) #read.xlsx works in the following manner: \"read.xlsx(xlsxFile, sheet = 1, startRow = 1, colNames = TRUE, detectDates = FALSE,...)\". input <- read.xlsx(\"data/mydata.xlsx\")  #You can use the detectDates parameter to tell R that there are dates in the file and to convert them to date strings rather than numbers.     After you work with the imported data, writing back to Excel files is also possible in the same manner as before.     library(openxlsx) #write.xlsx works in the following manner: \"write.xlsx(x, file, asTable = FALSE,...)\". write.xlsx(input,\"data/output.xlsx\")     [openxlsx package documentation] readxl     #install.packages(\"readxl\") library(readxl) #readxl works the same way as the openxlsx package: read_excel(\"filename\"). input <- read_excel(\"data/mydata.xlsx\")     [realxl package documentation]  But What About My SAS Files? Fear not! There are plenty of packages in the R universe to read in your SAS .sasb7dat files and other statistical software files as well. To start, the sasb7bdat package will do the trick. sasb7dat     #install.packages(\"sas7bdat\") library(sas7bdat) #read.sas7bdat works in the following manner: \"read.sas7bdat(filepath)\". input <-  read.sas7bdat(\"data/mydata.sas7bdat\")     [sas7bdat package documentation] haven The tidyverse realm has a package to open SAS, SPSS, and Stata data as well. This package is known as haven.     #install.packages(\"haven\") library(haven) #read_sas works in the following manner: read_sas(\"filepath\");. input <-  read_sas(\"data/mydata.sas7bdat\")     The haven package can also write SAS files:     library(haven) write_sas(input, \"output.sas7bdat\")     [haven package documentation] Remember that the haven package works on SAS, SPSS, and Stata. So, this may be easier than installing different packages for each different statistical software's dataset. Currently, haven supports: SAS: read_sas() reads .sas7bdat plus .sas7bcat files and read_xpt() SAS transport files (version 5 and version 8). write_sas() writes .sas7bdat files. SPSS: read_sav() reads .sav files and read_por() reads the older .por files. write_sav() writes .sav files.  Stata: read_dta() reads .dta files (up to version 14). write_dta() writes .dta files (versions 8-14).   Data in a Database RODBC Using the package RODBC, we can connect to a SQL-based database (such as Microsoft SQL Server) and run queries against it.     #install.packages(\"RODBC\") library(RODBC)  #odbcDriverConnect works in the following manner: \"odbcDriverConnect(\"driver={DB Type}; server=servernamer; database=databasename; trusted_connection=true\", uid = \"username\", pwd = \"password\")\" connection <- odbcDriverConnect(\"driver={SQL Server}; server=mysqlserver; database=finance; trusted_connection=true\", uid = \"admin\", pwd = \"password1234\")  #sqlQuery works in the following manner: \"sqlQuery(odbcDriverConnect String, query)\" input <- sqlQuery(connection, \"select * from mydatatable\")     This also works with Azure SQL Server, as well as Microsoft SQL Server on an Azure Virtual Machine. You just have to open each server up to external connections. This can work with Oracle, MySQL, and other databases as long as your machine has the correct ODBC drivers installed. You can even write data back to a database. This comes in handy when you’ve changed or added to data (especially in predictive analytics). Instead of extracting the data out of R and then loading it into the database, you can do it from within the R console.     #install.packages(\"RODBC\") library(RODBC)  connection <- odbcDriverConnect(\"driver={SQL Server};server=mysqlserver;database=finance;trusted_connection=true\", uid = \"admin\", pwd = \"password1234\") #sqlwrite works in the following manner: \"sqlwrite(odbcDriverConnect String, dataframe, tablename=\"table\")\" sqlwrite(connection, input, tablename=\"outputtable\")      [RODBC package documentation] Data Online Many public datasets live on the web. There’s no need to download these to your local machine when you can just import them directly in R. This is especially useful when you need to reshape or manipulate data before it’s useful to you. So, you can just import the data from the web, do whatever you need to do with it, and then write the useful data to your local machine. RCurl Using the RCurl package, we can connect directly to data using its web address. You will just use the url function in combination with any other read.* function listed earlier.       #install.packages(\"RCurl\") library(RCurl)  input <- read.csv(url(\"https://www.mywebsite.com/data/mydata.csv\"))      [RCurl package documentation] Getting Help  Some apprehension to using R comes from users not understanding how to get data into and out of the system. As you can see, importing and exporting are far from difficult. With just a few, simple (and notably very similar) commands, you can pull in data from any source you like. There are thousands of packages available for R, many of which can do very similar functions to what have been shown here. These are some of the most popular, but each package has its strength. You can search through the CRAN package list for access to many more packages. After you install any of the packages referenced here, you can view the documentation by adding a ? before the package name or function name, as shown below.       #install.packages(\"haven\") library(haven) #Get help on the entire haven package or list out the functions in the package. ?haven #Get help on the syntax and options of the read_sas function. ?read_sas      Still looking for assistance? BlueGranite can help! We can work with you and your team to help you learn more about R through our instructor-led training featuring data science lessons and topics using Microsoft R Server for enterprise-class, big data analytics. Contact us today for more information.   "
"220" "The inaugural RStudio conference was held a few weeks ago, January 11th-14th in Orlando. In case you couldn't be there, here are some of my insights from the event.  First off, it was a fantastic event! I’ve been a long-time fan of the people and products of RStudio, not to mention that gathering with a bunch of like-minded data science gurus in Orlando in January (I live in Colorado) was just plain fun. The vibe was terrific – during most sessions we were encouraged to get to know the people sitting next to us. At some events, this might lead to awkward silence (crickets, coughing, et al.) – but here it was hard to get order back in the room – lots of whooping and hollering about R and excellent networking. Hadley Wickham was a keynote speaker discussing tidyverse. They rented out the Wizarding World of Harry Potter at Universal Studios just for us. And… free hex stickers. And oh, yeah, we did some serious learning. The conference had a curious schedule with optional training Wednesday and Thursday, and conference general sessions Friday and Saturday. For Wednesday and Thursday, I chose to attend the ‘Master the tidyverse’ training with Garrett Grolemund. Other sessions were ‘Master R Developer’ with Hadley Wickham, and ‘Intermediate Shiny’ with Joe Cheng. I was 100% satisfied with the tidyverse training, and the feedback I heard was similar for the other sessions. Garrett was a great trainer and had well-organized materials for hands-on exercises. The course focused on elements in his newly released book with Hadley, R for Data Science (the best book I’ve seen for learning R; available at Amazon and O’Reilly). The tidyverse is a collection of packages that have a common data representation for doing the most important data science tasks, such as data access, data manipulation and tidying, functional programming, technical documentation, and managing models. Maybe most importantly, it provides a paradigm for more readable code that can be applied throughout the data science process. Everyone remembers the story of Little Bunny Foo Foo, right? The bottom box was written with the tidyverse approach! The general sessions on Friday and Saturday had tracks like Shiny, scalability, tidyverse, R Markdown, and a few others. After each day’s keynote, there was a choice of two, 2-hour tutorials followed by a series of 25-minute presentations aligned to the track themes. I really liked this idea of a bigger chunk of time for major topics, and then having lots of choice among the shorter sessions. You can see all the topics at the conference page. RStudio has promised links to all the presentation materials eventually, so keep an eye on that page or follow @rstudio on Twitter. Sadly, some of the tutorial sessions were less effective as the internet was very spotty – I don’t think the hotel was ready for a big group of techies hogging bandwidth. The other feedback for improvement I left was to include a better description of each session and the target level of experience; in the beginner-friendly sounding ‘Building Dashboards with Shiny’ the first discussion was about tweaking the blah blah blah parameters for… and I was lost. Other highlights for me included a spotlight on the new RStudio Connect publishing platform, learning more about R Notebook workflows, and the day 2 keynote from about Finding and Telling Stories with R by Andrew Flowers from FiveThirtyEight (they now have their own CRAN package, too, with code and data behind the stories). Hopefully this whets your appetite for attending next year. Registration opened in July 2016, and training seats and rooms at the conference hotel went quick. Can’t wait to see you at rstudio::conf 2018!"
"221" "As my colleague David Eldersveld pointed out in a recent blog post, doing data in the cloud can seem daunting.  There's a growing list of cloud providers to compare from, and each has their own set of tools, services, and even lingo.  There are also all-new concepts to learn, like infrastructure-as-a-service and platform-as-a-service.  So much to learn and evaluate, and so little time to do so (because if you aren't in the cloud, your competitor is... right?)  I'd like to share some advice that I give every client who is thinking about taking on a cloud data and analytics initiative (and even some who aren't thinking about it yet, but should be), so that they can navigate the hype and challenges of the cloud and have a greater chance of success and adoption.   Think Big, Start Small We see clients have the most success and best adoption with a cloud data endeavor when they think big, but start small.  I usually refer to it as \"tip-toeing\" into the cloud.  This generally means to approach the cloud data landscape with big ideas about how and where it can benefit your organization, but to also carve out one or two smaller, manageable, very specific use cases to get started.  It's very difficult to succeed if you are trying to boil the whole ocean or lift-and-shift your entire infrastructure and processes into the cloud all at once.  Below are just a few of the advantages for thinking big, but starting small with your cloud data initiative. Faster Time-to-Market One of the general promises of the cloud is that less infrastructure procurement, server configuration, and software configuration should yield faster time-to-market.  When operating in a platform-as-a-service model, you can often be up and running with a Hadoop cluster or a SQL database in just minutes.  Pair that with a manageable (but meaningful!) use case where you are only standing up the services necessary for the use case, and you can likely be up and running with a viable solution in just a couple of weeks (not months). Lower Risk Starting with a smaller use case also helps to mitigate risk.  In most cases, you only pay for the cloud resources that you use, which means no hardware or software license commitments up front.  If you spend a few weeks building something in the cloud and you decide you don't like it, you can just tear it down.  In this case, you only incur costs for services during the few weeks you were developing.  This is drastically different from buying and provisioning physical hardware and a big expensive license only to find a few weeks into it that the platform was not right your organization.   Model for Greater Adoption Driving interest and adoption for a project or initiative that seems to be taking months and months to implement can be difficult.  People begin to lose excitement and even question the value of the initiative. However, a project that shows results and value in just a few short weeks gets people excited and wanting to do more.  This is also true for cloud data and analytics initiatives.  Building out a smaller, more manageable use case that creates value in just a few weeks can be huge for your bigger cloud strategy. The results of this foundation project can be demonstrated to other stakeholders and business areas which will lead to greater buy-in, adoption, and funding for future initiatives. Building for Long-term Value Finally, starting small doesn't mean you are abandoning your big-picture ideas and long-term vision.  In fact, it's quite the opposite.  When architected appropriately, the smaller project is simply laying the pipes (both architecturally and organizationally) for more data flows and use cases.  Nothing you build should be throw-away.  Think about both the cloud services you stand up and the use cases you build out as building blocks that will continue to interact with each other, working towards a bigger, long-term cloud data strategy and architecture. Let us help If you are interested in starting a cloud data initiative, but you aren't sure where or how to get things going, let BlueGranite help.  We can work with you to develop that long-term, big-picture cloud data strategy, and help you to focus in on 2 to 3 good use cases to get started.  Once we define a few candidate use cases, we can help you \"quickstart\" your cloud initiative with a 2 to 5-week engagement where we will implement one or more use cases and help mentor your team in the best practices necessary to continue to develop effectively in the cloud.  Doing data in the cloud doesn't have to be scary.  Remember to think big, but start small, and you will be more likely to be successful in your cloud data and analytics venture."
"222" "We hear lots of people these days talking about prototyping with Power BI. It sounds kind of cool. But what exactly is it? What are we trying to accomplish with it and how do we support the effort?  Prototyping can be defined as the process of sampling an idea by creating a functional mockup which can inform the solution scope and viability of an investment. In the world of Power BI, we can prototype in two main forms: Strategic prototyping: Power BI efforts that inform blueprinting of IT-driven projects with a long-term vision. In other words, we can use Power BI to “sample” reports and actively perform requirement discovery on behalf of IT developers to get a better idea of what needs to be built as a production asset. Tactical prototyping: Involves exploratory attempts to test a short-term solution, satisfying a request typically driven by a business owner and outside of IT development cycles. The need for prototyping arises as users don’t always know what they want. When asked which data a report should contain, it is typical to hear users answer: “I don’t know” or “all the data.\" Unfortunately, neither of these answers give enough guidance to walk on solid ground when developing production-ready reports. That is why we talk about requirement discovery instead of requirement gathering. In a general sense, we prototype the report interface (friendliness and usability), the data model, and calculated business logic, as well as the data completeness and readiness. Structured prototyping sessions – as a phase of the development cycle – can be scheduled and actively managed to ensure the discovery process is moving forward. During prototyping, we have tolerance for lack of complete adherence to best practices – given the prototype itself is not a production-ready asset. We also see a high amount of “throw-away” work where, iteratively, we fail quickly and cheaply to realize gain. Companies I talk to are interested in the idea of prototyping, however, they often will ask: “Which team should lead the prototyping effort and how?” In a recent blog post, I mentioned how IT departments should create what Gartner calls a “Mode 2” team to increase the success of Power BI deployments. This team has a narrow focus: short-term solutions and exploratory efforts. Sound a lot like prototyping? Yes! To efficiently discover requirements, companies need to build a team structure in which prototyping can take place.  “Mode 2” teams can deliver the most value when: They are a full-time, dedicated team which is separate from the Enterprise BI team. By designating a team to perform prototyping duties as a full-time job, this team is legitimized and can actively pursue short-term goals without any sense of guilt. In contrast, if you only add up the prototyping duties to a traditional IT team focused in long-term solutions (“Mode 1” team, as per Gartner), prototyping may suffer – bogged down by a need to switch focus from delivering value as quickly as possible, to applying all practices and procedures applicable to traditional IT development. Remember, we prototype because we don’t yet fully know the requirements, so those teams need a lot of flexibility. There is a clear agreement about what is meant by “short-term solution.\" With this agreement, it becomes easier to channel requests to either Mode 1 or Mode 2 teams. A large manufacturing company I consulted recently decided to define short-term solutions as any project that has an estimated development time frame of less than 3 weeks, including interviewing Subject Matter Experts (SME) and iterating. Even with a short-term focus, they belong to the IT organization. Although many business Super Users also perform prototyping functions, their focus is not the same (business Super Users are those that can perform Power BI data prep, modeling and reporting independently). Both groups can be SMEs and technically savvy, however, business Super Users care about solving the problem whereas Mode 2 team members care about the approach to solving the problem. As an example, Mode 2 team members might ask “are we following good visualization practices?” or “Did we leverage any workable DAX pattern?” etc.Ultimately, the Mode 2 team is a conduit for work that can end up being adopted as Corporate BI, hence they are on the look-out for opportunities for extensibility, maintainability, and conformity to standards. However, given their focus is to “sample” solutions, they are normally willing to purposefully step away from recommended patterns to deliver value quickly while understanding the trade-offs.Notice that under this context, Mode 2 teams are in alignment with business Super Users, which are sometimes negatively referred to as Shadow IT. Given their shared interest, their organization can benefit from rewarding their collaboration over short-term solutions. This offloads the Mode 1 team from prototyping efforts, allowing them to collaborate with business leaders to work on long-term solutions. There is an assigned Team Leader who can help prioritize. As with any team, a leader can help establish priorities. It is common to see business users get excited after a tactical prototype has been developed. This excitement often helps build a healthy queue of prototyping requests. Keep in mind that tactical prototypes are not yet solutions until a business Super User adopts them (and ownership is transferred – see below). Finally – they must be structured to avoid solution ownership. This is a critical piece of the puzzle. Owning solutions signify they would also provide technical support and comply to service level agreements. That, again, would bog them down in a direction that decreases agility.Instead, they must constantly look to push prototypes to either business or IT adopters. When Mode 1 IT adopts the prototype, it transforms into a blueprint for development. When a business Super User adopts the prototype, it becomes a tactical solution.In this way, a structured process can squeeze the most value out of prototyping efforts to influence Corporate BI deployments, making them more accurate, or Self-Service BI deployments to deliver quick wins. Given Power BI Mode 2 teams play such a central role in the ownership transfer process (their work ultimately gets adopted by other teams), they can also assume monitoring functions, helping to track usage and create adoption strategies. To be effective, they need access to Power BI audit logs to understand consumption and sharing patterns. As you may expect, their continuous activity would deepen their expertise in Power BI. With this deep technical knowledge and focus on short-terms wins, this team becomes the center of Power BI evangelization. If you are looking to learn more about Power BI, or want to address how to effectively take advantage of the tool for your organization, contact BlueGranite today!"
"223" "In today's self-serve Bl world, it's common to need to analyze or mash up data from a huge variety of sources. Sometimes data arrives in a nice, clean and structured form, but it may also be delivered in a very basic flat file extract or single database tables. Power Query (also known as Get and Transform in Excel 2016) is a great way to shape data into a clear and more convenient form for analysis.   You may have to take a flat file data extract, perhaps pulled from some source database or a public data set, and shape it into a small star composed of a fact table surrounded by related dimension tables. In a Demo Day video below, I've taken a slice of Microsoft's new World Wide Importers SQL Server 2016 sample data and made a mock data extract, stored in a single CSV file, to emulate this scenario. Making Dimensions and Facts After downloading the sample data or finding a data sample of your own, follow along in Power BI with the short video below to learn how to: Import your original data and identify columns to make into dimensions. For each of those dimensions, make a copy of the table that isolates the distinct values from the column(s), and numbers them with Add Index. Join the original data to the dimensions using the text columns for matching. Expand the matching ID values, and remove the corresponding text columns, to make a neat, compact fact table. Analyze!                                    15:38             15:38                Twitter      Embed      LinkedIn                Speed 1x  0.5x 1x 1.25x 1.5x 1.75x 2x     Quality Auto  Auto 1080p                              About WistiaReport a problem      Thanks for reporting a problem. We'll attach technical data about this session to help us figure out the issue. Which of these best describes the problem? Choose oneVideo plays but frequently stuttersVideo has poor qualityVideo fails to playOtherAny other details or context?     CancelSend   message                        For more information or to learn more about analyzing data, contact BlueGranite today! We offer onsite, hands-on, instructor-led training to help business and IT teams take full advantage of self-service BI and analytics."
"224" "If the thought of working with a cloud data platform seems daunting, it doesn’t have to be. Many of the common tasks that you perform as part of traditional on-premises solutions extend to the cloud—and then some! Through Microsoft’s Cortana Intelligence Suite, a set of services based in the Azure Cloud, you can take advantage of the latest enterprise data technologies with minimal startup time.  For example, how do you move data into the cloud, and what can you do with it once it is available there? Microsoft’s answer for data orchestration is Azure Data Factory. Data Factory allows you to move data into Azure as well as helps facilitate analysis and transformation of your data with other Azure services. As with a musical orchestra, which the term data orchestration might evoke, there are many sections like percussion and woodwinds that need to work well together to make extraordinary music. Different instruments have their own parts just as different Azure services have their own functions. Each instrument, however, relies upon the conductor to help manage tempo and volume to make the collective score sound good. Data Factory acts as an orchestra conductor, rather than a one-man band. It does not do all of the work itself, but it helps different instruments in the Cortana Intelligence Suite collaborate together to help make your solution a good one. With this analogy, we are only scratching the surface of what the Cortana Intelligence Suite is capable of.  Data Factory is just one of the tools that you can utilize among many others that are available. If you would like to know more about how Azure Data Factory fits into the Cortana Intelligence Suite, BlueGranite is partnering with Microsoft to offer free, hands-on workshops across the US. In the workshop, you will learn about the various components that make up Cortana Intelligence and how they may fit into your solution architecture. You will also discover how to create an end-to-end solution utilizing data orchestration, machine learning, data visualization, web apps, and more! Wondering if a free BlueGranite hands-on workshop is coming to your own area? Want to request one for a location near you? If so, contact us today."
"225" "Everything, Including the Kitchen Sink A data lake is a persistent raw archive of any potentially actionable data.  The philosophy really is “everything, including the kitchen sink.” This means that a data lake will archive data from many different business systems and non-traditional sources, including sensor data, logs, image data, streaming data, and audio or video data.   An ambitious data lake may also include information from external sources, such as weather, traffic, or stock market data.  But that’s not all!  A data lake won’t just store the current version of a record or file; it will also retain every revision it can get.  By capturing everything, undiluted, a business will be able to answer the questions of today and the new questions of tomorrow. That raw material, though, is a whole mess of data.  It requires gobs of storage, gobs of processing power, and gobs of connectivity to continually archive all this data as it is generated.  For this whole hog process, a traditional relational database will not scale easily into the petabyte range, nor does it effectively store or consume unstructured data.  At this scale, you’re looking for a platform with near infinite scalability paired with elastic storage and processing power. An Organized Mess Despite the name, the mess of data in a data lake need not be total chaos.  In fact, it should be an organized mess.  There’s metadata that can be captured in the data ingestion process that does not transform the data, but will give the data additional context.  At the very least, data can then be categorized by its source and the date it was captured.  This enriches the data and gives it some level of organization without contaminating its raw nature. Multiple Tools Have Many Uses With a variety of data sources and types, there are an array of tools to get the job done.  For ingesting relational data alone, there’s at least half a dozen tools.  Broadly speaking, there are three types of data to be ingested: batched data, streaming data, and binary data. Much to my dismay, there is no one tool that is ideally suited for all three types. A true data lake at an enterprise may wind up using two or three (or more!) ingestion tools for dozens of data sources.  Orchestrating that aspect alone is a significant task, but that giant mess of data is the raw material for insights now and into the future. What to Do with All that Mess Here’s the problem with a data lake (stop me if you've heard this already):  It’s raw data.  It’s a mess.  To get insight out of it, you need to make sense of that mess and integrate it into something coherent, which might sound like a data warehouse.  And for some enterprises, it can be little more than a massive primary staging layer.  For others, it can be a data science playground. With a data lake feeding a data warehouse, adding new items to the warehouse is merely a matter of sourcing the required information from the data lake.  The data will already be available and ready to go.  In fact, it may even be possible to make a virtual data warehouse as a layer of views on the data lake itself.  It adds much more agility to a data warehouse. Additionally, a data lake is not only for feeding data warehouses.  It can become a one-stop shop for data science efforts too.  By capturing everything, there is a treasure trove of insights that may be hidden in the data lake.  Machine learning, text analysis, image recognition, and other processes will have the gobs of data they need.  It can open new insights about the workings of a business and audit conventional wisdom about your business processes. In the data driven world of today and tomorrow, having all your business data available to gain a competitive edge is a must, not an option. For more information on data lakes and data warehouses, check out this blog post to learn about the differences. If you are planning your data lake and need help getting started, contact BlueGranite today for insights into the right solution for your firm."
"226" "Note: At the time this post was published, Power BI Premium had not yet been released by Mircosoft. For more information on Power BI Premium, please follow this link.  Using Microsoft Power BI can bring in a lot of benefits and added value to your organization, but is it worth it to purchase Pro licenses? There are many features available in the “freemium” version of Power BI, however certain features require a retail subscription, currently $9.99 per user, per month.  Whether you decide to take advantage of the Pro features or not, Power BI is still a very robust tool. It will allow you to transform and clean up datasets, create visually appealing reports to grab users' attention, and tell stories about your data. You can also publish and share dashboards through the Power BI cloud service, as well as utilize built-in or custom visuals that are downloadable for free and will integrate nicely with Microsoft R. Before getting into the specifics of each license type, one important factor to note when trying to decide if paid subscriptions are right for your firm is that if a file uses any Pro features, any user that interacts with that file will need a Pro license. Power BI report authors must be mindful of which users they plan to activate when rolling out their reports.  If, for example, an author intends to implement an automated data refresh (a Pro-only feature, explained further below) they should ensure their users have access to Pro licenses. In general, Pro features will include:  Scheduled data refreshes using a gateway Live querying (rather than importing data) Group workspaces   Organizational content packs Streaming data at higher volumes Storage of >1GB for all imported data  What are the benefits to using Pro? Scheduled Data RefreshesIn the free version of Power BI, users do not have the ability to schedule data refreshes, so any deployed data models would need to be manually refreshed to display the most up-to-date source data. You might be thinking “I only have one data source, so this isn’t an issue,” but depending on how often your data needs to be refreshed, this could turn into a big headache for you and your team. As an example, when one or more data sources need to be refreshed every hour, or even once a day, it would make much more sense to purchase a Pro version and have the refreshes done automatically, saving you time and money in the long run. Additionally, without scheduled refreshes, teams might not always have access to the most recent versions of data, leading to misinformed decision making and longer lead times. Live QueryingThe live querying feature is a means by which reports can be built in Power BI that work directly against an on-premises data source. By using gateways (a Pro feature), if a company has a SQL datamart they have already built, or perhaps have concerns about data in their data models sitting in the cloud, this could be the route they might wish to explore. Sharing Data Between License TypesWhen it comes to sharing data models and dashboards, there are quite a few different options available between free and Pro licenses, as described below: Group workspaces can be a great added benefit when taking advantage of Pro features. They allow a Power BI author to create and manage a group, adding one individual at a time. Within group workspaces, users (with proper permissions who are configured to do so) are also able to save reports and make modifications, then upload an updated version for the rest of the group to see. In addition to editing permissions, you can classify read-only users, and prevent them from making modifications. Content packs allow Power BI authors to share with entire security or distribution lists managed by IT, only if said lists are being pushed into the cloud through Azure Active Directory. With a free license, you can easily consume content packs (that do not contain Pro features). However, if you want to create a content pack, you will need a Pro license. Direct dashboard sharing comes with either license type, but like organizational content packs, the dashboard should not contain any Pro features if you are sharing with an individual that has a free license. Additionally, with direct dashboards only one dashboard can be shared at a time, while content packs can include many dashboards/datasets. Row Level Security (RLS) is a Pro feature which offers a convenient way to leverage one dataset with a variety of users, each having different permissions to access the data within it. As an example, this would be helpful when sharing data across various lines of business, or sharing parts of reports with a sales team.  As you can see, there are many options for sharing and collaborating on datasets. The way that your teams plan to consume and share reports will help you to determine which roles and departments should have which license type. High-Volume StreamingIf you are planning to use the Power BI Streaming application program interface (API), this is a developer-centric feature that comes with a Pro license. It will require some knowledge of C# code to fully take advantage of this feature, and allows users to stream data at higher volumes compared to a free license. Increased StorageAdditionally, the free version of Power BI caps users at a total of 1GB of compressed storage space for reports, datasets, dashboards, etc. With a Pro license, each user gets a grand total of 10GB of storage, although the individual size limit for any one file published to Power BI remains at 1GB. The extra 9GB of space can be hard to pass on depending on the size of your user base and how heavily your team uses Power BI. When looking at the cost of implementing the Pro service over the free service, the benefits greatly outweigh the price per month. Reasonably, your organization's expectation should be to provide a Pro license for every active user of Power BI to fully take advantage of the technology. Without the ability to schedule data refreshes, any deployed data models would need to be manually refreshed in order to display the latest source data, and that time saved can prevent a world of future headaches. So which one is right for you? In the end, the free version of Power BI is mostly targeted toward individual use and a few team scenarios. If you are planning to implement the tool for your corporate organization, Pro is definitely worth the cost. To learn more about Microsoft Power BI pricing, check out their page for helpful notes on describing the feature differences between the free and Pro versions of Power BI. If you are thinking about rolling out Power BI to your organization, or need help in doing this effectively, register for our upcoming webinar where our Power BI expert, Javier Guillen, will be discussing how to roll out the software to organizations with a step-by-step process designed to minimize risk."
"227" "Early in 2016, Microsoft announced the public preview of R Tools for Visual Studio (RTVS), an add-on R environment that integrates into Visual Studio. Unfortunately, in the R universe, the dominant R IDE is R Studio, and the odds of Microsoft displacing R Studio are slim. So why create R Tools for Visual Studio?   Even with core similarities and the progress that RTVS has made since it was released, R Studio is still clearly a winner when it comes to functionality. In its current preview, RTVS has a similar user interface to R Studio (if you use the Data Science Settings). Writing and running R scripts, viewing plots, or using and publishing R markdown is about all that you do from a language perspective in RTVS. Admittedly, that could be enough for many R users, but that is where comparisons end. Once you get into the realm of writing Shiny apps, building your own R packages, and more, R Studio shines where RTVS has not even begun to try to compete. Additionally, many critical developers and data scientists are driving the wider adoption of R work and have connections to the continually improving R Studio IDE as well as R Studio’s evolving suite of R packages. I do not see RTVS versus R Studio as a David versus Goliath scenario where RTVS will eventually supersede R Studio, or even try to. Rather, in the right circumstances, providing the core functionality and a similar user interface to the more dominant IDE would allow users to easily use one or the other depending on their project needs. Here are three reasons it may benefit you or your team to consider R Tools for Visual Studio for Microsoft-oriented analytics or data science projects. Ultimately, each reason comes down to convenience. 1. Code snippets targeted toward Microsoft R ServerIf your organization is using or considering the use of R in the context of Microsoft R Server (MRS) or SQL R Services, RTVS may be a good choice because of conveniences like the built-in code snippets. R Studio supports snippets, but it is nowhere close to Visual Studio’s built-in snippets. Microsoft has an interest in driving people toward MRS while R Studio does not. It makes sense to include proprietary snippets in RTVS to help with adoption and use (RTVS also includes a variety of snippets for open source R). Whether you are just getting started with MRS or an established user, the built-in snippets (shown below) are extremely convenient for various MRS and open source functions.  2. Incorporate R projects as part of a broader Visual Studio solutionMany Visual Studio solutions end up being a collection of individual projects. More often than not, these projects are logically joined by virtue of being part of the same business solution, but each one can incorporate different components or languages. For example, you may architect a solution that involves separate projects for loading data­­ with Azure Data Factory, analysis with R, a front-end C# web app, etc. Rather than keep your R code siloed off in a separate solution, unite it with the rest of your code for development and source control. 3. Terrific IntelliSenseAs with snippets, R Studio supports code completion, but it does not rival Visual Studio’s IntelliSense. Unless you are a skilled R developer, you might share in my experience. With R, I regularly find myself looking through Help to get syntax or function-specific information. Part of it is because I’m not using R all the time (so I forget things), and part of it is because I use a variety of languages and technologies (so I often end up confusing my syntax). I often rely on the use of “?” or “??” to search R, or I perform external web searches. With RTVS, I was immediately aware of how much less I relied on external help or syntax checks while writing R code, and the reason was Visual Studio’s superior IntelliSense, as shown below.  As mentioned, these three reasons should not convince people to stop using R Studio, as that was not the goal. However, if the circumstances are right, it could benefit you or your team to consider R Tools for Visual Studio. Particularly if R is only a small component in a more extensive architecture, if you are already a “Microsoft shop”, or if you are using Microsoft R Server – the benefits of rapid and convenient development in RTVS can be greater than using R Studio alone. If you need help implementing RTVS, or have questions about the other benefits that come along with the tool, contact BlueGranite today and we will be happy to help."
"228" "As the year is coming to an end and winter weather has thoroughly frozen over BlueGranite’s home office in West Michigan, we’d like to take a few moments to look back and reflect on our growth and accomplishments in 2016, as well as share some annual company updates with our community. This was an exciting year when it came to celebrating 20 years of business, increasing our team, and growing client relationships, plus much more.  Updates to get excited about In November BlueGranite celebrated a milestone – being in business for 20 years. Although our offerings have transformed over time, we have continued to grow, doubling in size over the past 8 years. This was made possible through our dedicated team, deep client relationships, and our community. We are looking forward to celebrating many more years to come! As mentioned, our team has grown and continues to expand. We now have team members in 11 states that reach as far west as Denver, across the Northeast, Midwest, and Southern regions. Next year we plan to develop further in areas across the U.S., getting closer to our clients. Additionally, thanks to our team’s growth, we are fortunate to have a diverse set of deep client relationships, with over 70% of our business coming from existing clients. Not only has BlueGranite grown internally, we have also grown externally. In 2016, we continued to stand out as one of the largest Microsoft partners specializing in data and analytics solutions in the U.S. As our partnership has continued to grow, we have also expanded our service offerings in Advanced Analytics, including work with Microsoft R Server Training and Azure Machine Learning. Power BI was our fastest growing service last year, and we expect that to continue to gain popularity in the future. Top blog posts this year In 2016, we had some awesome content come from our writers! Check out some of our top blog posts below to learn more about some of the most popular topics to come up in the industry. Top 3 Reasons to Upgrade to SQL Server 2016 Reporting Services, by Jason ThomasIf you are looking for reasons to upgrade from the SQL Server Reporting Services (SSRS) 2008 R2 rollout, this is a great post to highlight the changes that came with the 2016 update. There have been many notable improvements, tool integrations, and added features that make it worth the consideration to upgrade. Power BI with ArcGIS Maps Closes a Critical Gap, by David EldersveldAfter the announcement of ArcGIS Maps for Power BI, more advanced mapping capabilities were made possible. This tool finally closed a gap in Power BI and now can make comparable and visually appealing maps that help satisfy numerous customer requests. Built-in Analytics: Why R Matters to a SQL Server Professional, by Andy LathropEarlier this year when R Services was announced, it became easier than ever to operationalize high-performance Big Data analytics with traditional, unstructured, or hybrid data sources. This blog post does a great job summarizing some of the exciting new features that were added, giving users more powerful capabilities. New Microsoft Power BI Governance and Deployment Whitepaper, by Erik RollTwo BlueGranite team members published a whitepaper on Power BI Governance. This post reviews 3 approaches to Power BI delivery, including Business-Led Self-Service BI, IT-Managed Self-Service BI, and Corporate BI. The published whitepaper is also available for download through the above link. Visualizing Your Data Made Easy with Power BI and R, by Mike CornellThis blog post explains details of Power BI, specifically a web publishing component that allows for sharing via multiple mediums. There is also a short video demonstration on how easy it is to incorporate R into your BI experience. Most popular content The most popular webinar of the year was Effective Communication through Data Visualization, presented by Meagan Longoria and Jason Thomas. By viewing the recorded session, you will be able to learn more about how data volumes are continuing to grow, making visualizations that are much more meaningful and important for proper communication. This webinar highlights how to transform boring tables into visually appealing reports, with examples using Microsoft Power BI. Our top solution brief for the year was Billion-dollar Retailer Forecasts Future by Adding Modern Reporting to ERP. BlueGranite was able to work with a large clothing retailer to help in adding reporting functionality for its enterprise resource planning (ERP) system. Through this project, we worked with the client to gain better business insights, which led to informed decision making and cost savings. What a great year! 2016 was a great year for BlueGranite in terms of growth, but it wasn’t just thanks to our company. From all of us at BlueGranite, thank you for your support this past year – we couldn't have done it without our community, client, and team member relationships. Wishing everyone a wonderful holiday season and happy new year!"
"229" "In Leo Furlong’s and Merrill Aldrich's recent post, Options to Scale Your SQL Server Data Warehouse, they described different scenarios for growing your data warehouse beyond a single-server solution. While appliances like APS or platform solutions like Azure SQL Data Warehouse (DW) remove the need to build your own scaled-out infrastructure, they still require solution planning to migrate your existing single-server DW to a distributed format.   1. Know how your data will be distributed This is the most important step to plan for when moving to a scaled-out DW solution. Data distribution is a key step in the linear performance scale that you can achieve in a distributed environment. Distribution is very important because an incorrect distribution key between two or more tables will result in the need for data shuffling between nodes in the database. The shuffling that can occur is very expensive because in this operation, data is physically being moved between disks, file groups, or in some cases, even physical nodes. While this data is being shuffled, no processing is happening – meaning your query stalls while the data is being moved around. Therefore, it is important to design your distribution patterns in such a way that you will minimize the amount of shuffling that will happen. In both cases of APS or Azure SQL DW, data is distributed on a single column in a table. You specify this column at the time the table is created, and it cannot be changed. APS supports three types of table distribution: REPLICATE – a copy of the entire table is stored on each distribution of the distributed database. NOTE: Replicated tables are currently only supported in APS. Azure SQL DW does not support replicated tables.ROUND_ROBIN – the data is distributed row by row among the distributions in the distributed database.HASH – a single column is chosen as the distribution key, and rows are distributed to head distribution based on values in the chosen column. In a production solution, you should NEVER use ROUND_ROBIN. It will never result in a distribution that avoids shuffling. Since the data is distributed row by row, even simple queries will result in a significant amount of data shuffling. This leaves two types of distribution patterns that are acceptable for a production solution. REPLICATED tables should be used for tables that are less than 5GB in total size, and aren't updated that often. Command examples of good candidates for replicated tables are smaller dimension tables in a star schema. This type of replication works well for smaller tables because any joins to these tables can be satisfied on each distribution without any shuffling, as all of the data is available. For your larger tables, you'll be using the HASH distribution, which means that you must pick an appropriate distribution column. There are a few factors to consider when choosing your distribution key: Skew – One key factor in choosing the distribution key is how much skew will be created. Skew happens when more data exists on one distribution than the other, or when there aren't enough unique values in the column to distribute data to all distributions. For example, a column containing Yes/No values is not a good distribution key, since the data will only be distributed to 2 distributions, instead of the 16+ distributions that are available.Changing Values – Once a row has been written to the table, the value in the distribution key column CAN NOT change. To change that value in the column you would need to delete the row and reinsert it. Therefore, it is a best practice to choose a distribution key column whose values do not change after being written.Join/Group by Usage – For many tables, there is a column that is often used in Joins to other large tables. This column is generally a good candidate for a distribution key – as long as it doesn't break the two previous best practice rules. For example, the OrderHeader and OrderLine tables are always joined by OrderNumber. The OrderNumber column would be ideal for the distribution key, because all rows in the OrderHeader table would be able to join to their corresponding rows in the OrderLine table without shuffling. The HASH algorithm used on both tables is the same, and since the columns match, we know the rows will live in the same distribution. For more information on how APS and Azure SQL DW works with distributed tables, check out this short Developer Introduction by James Rowland-Jones, Principal Program Manager for Azure SQL DW at Microsoft. 2. Test your reporting applications While both APS and Azure SQL DW work with standard SQL Server ODBC connections, it's still a good idea to test your reporting applications during a proof of concept or trial period. Some reporting applications write their own SQL statements to issue to the database. While not common, it is possible that the reporting application issues a statement that isn't compatible with APS and Azure SQL DW's DSQL interpretation. It's a good idea to test your most used reports against the system to ensure that the query issues are compatible. Also, some reporting applications use non-standard SQL server drivers. Understanding exactly how your reporting system connects to the appliance will be very important prior to migrating your DW solution over. If you don't yet have your APS appliance installed, we've helped past customers run a proof of concept in partnership with a Microsoft Technology Center (MTC). Many of these MTC locations have an APS appliance installed, and we can help schedule time on that appliance for a customer to do a test migration, and ensure that their current reporting systems will integrate with the appliance. If you're considering moving to Azure SQL DW instead of the on-premises route, Microsoft is currently offering a 1-month free trial – which is a perfect opportunity to shake out any major connectivity issues that you may face. 3. Check your procedural code for incompatibilities Although migrating to APS or Azure SQL DW doesn't require a complete relearn of query and procedural programming, DSQL does have some important differences between TSQL that can have an impact on your current data processing and querying solutions. While there are migration tools to help migrate a DW's table schema and data from a single-server approach to a distributed one, there doesn't exist a solution to convert incompatible procedural code – that still must be done manually. Some common incompatibilities are: Variable assignment using the SELECT @variable = {statement} format. All variables need to be assigned using a SET statement. CTE expressions in UPDATES and DELETES are not supported. CTEs are supports for SELECTS and INSERTS. Temporary tables need to be modified to include a distribution scheme appropriate for the operation being performed. Also, existence must be checked when using IF OBJECT_ID() IS NOT NULL to determine the table already exists. In APS/Azure SQL DW temp tables exist for the entire session, not just the query context. ANSI joins are not allowed in UPDATE or DELETE statements. Implicit joins are acceptable, but often it is required to use a temp table to join the data together to be updated prior to running the statement. SELECT…INTO is not supported in DSQL, instead the CREATE TABLE AS SELECT (CTAS) statement is used to perform the operation of creating a new table as the result of a select statement. The MERGE statement is also not supported in DSQL, so instead, a solution using CTAS can be implemented and is described in this Microsoft article. 4. Be ready to modify your ETL processing One of the biggest development hurdles to cross when migrating to a scale-out DW solution is what do with your current ETL processes. Often ETL is written in a tool like SSIS or Informatica. While APS/Azure SQL DW supports both tools, and even ships with special destination adapters to maximize data loading performance, it's often more efficient to translate your ETL to stored procedures that run natively on the appliance. If you rely on SSIS or Informatica to do your data cleansing and transformation, you are not taking advantage of the distributed processing engine of APS/Azure SQL DW. Often, ETL processing time is an important reason to move to a distributed solution, and therefore, it's important to understand how to maximize that performance. As you are migrating your ETL processes to DSQL stored procedures, a few common design patterns should be paid attention to: Consider staging data to the appliance with dwloader (APS) or PolyBase (Azure SQL DW). These tools are optimized for inserting bulk data into the environment, and are often the fastest way to get staging data loaded – often surpassing the 2TB/hour rate. Processing that currently makes use of UPDATE statements may be more efficient with the CTAS and RENAME OBJECT commands to build a new copy of the table and replace it. This will also reduce the amount of object locking occurring, and result in reduced downtime for user queries. Use PARTITION to quickly replace volatile data (such as current month transactions that are updated often) or archive data to other tables quickly. Table partitioning in APS/Azure SQL DW is a simpler implementation than that in SQL Server and is a bit easier to manage during reoccurring ETL processing. Don't let the magnitude of ETL migration scare you away from moving to a distributed processing environment. Often, your current ETL processes can be repointed to insert directly into the new system with minor modifications – a lift and shift approach – then, a phased project approach can be implemented to migrate your ETL to take advantage of the distributed processing paradigms. 5. Identify your highest priority pain points You've probably made the decision to move to a scaled-out DW solution because you JUST WANT THINGS TO BE FASTER! And, good choice, your solution likely WILL be faster, much faster, than you could achieve with your single-server solution of yesteryear. However, the road to get to the faster solution will be one that could last as little as a few weeks, or as much as 2 years, depending on the complexity of your data warehouse. You can also expect and plan for continued optimization and new development after that. So, knowing you can't tackle everything at once, it's important to optimize your highest pain points first: Is your user report performance the most important aspect of your decision? If so, then taking advantage of existing ETL processing, and focusing on distribution design and query performance should be the efforts of your first phases of solution migration.Are your current ETL processes running so long that users are negatively affected? If this is the strongest pain point for you and your team, then it's a good idea to start tackling those long-running ETL processes and get them translated to DSQL ASAP. Your user reports will continue to run faster against the distributed environment, but you can optimize those later, after your ETL issues are sorted out. While distributed processing is a great solution to scale issues, it's not magic. Every process you run in a distributed environment will not be faster – you'll run into situations where your new environment is not as efficient for a specific process as your old one. You'll need to plan for those types of issues and realize that you can't tackle everything at the same time. Having a priority plan of attack will help to show the quick value of your new solution implementation. You're not alone in this journey Moving to a distributed processing environment really is a journey. In some ways, it will feel painless – just a simple connection modification and your reports are now working! But in others, it will feel like you're relearning everything you know about data warehousing. You're not alone! We've worked with many clients on this path, and we've found that it helps to have an experienced team on your side. If you're ready to take your first steps, contact us today and we'll help you build a plan to ensure that your scale-out DW is a huge success."
"230" "It’s hard to talk about machine learning without someone bringing up Microsoft R or R-Studio. Although it has been around for over two decades, R has recently gained immense popularity by entering the big data landscape. With Microsoft’s release of SQL Server 2016, we now have an enterprise data warehouse directly integrated with one of the most diverse and open source analytical engines available. However, R is far more than just an analytical engine.  Capabilities The demographic for R is truly based on Data Scientists or quantitative people, but it’s not limited to just algorithms thanks to its ability to use custom packages developed by a large community of users. Packages can be as simple as a single function or as involved as complex modules able to communicate over different protocols and deliver highly refined visualizations. Business Intelligence Tool? When I was first exposed to R (about 10 years ago) I honestly didn’t know where to start outside of the usual tutorials on ingesting and manipulating data. After trying different packages, I concluded that I may be able to leverage this tool to do just about anything. Testing that hypothesis, I built a data visualization tool for email marketing campaigns, but this wasn't just for graphs. I found a package that could pull maps from Google Maps based on coordinates and then overlay markers representing customers treated with a campaign. Additionally, different colored markers were added for those who converted in weekly time frames. The final piece was to use an animated GIF program to combine the images for viewing on the web. Again, R is very versatile. API Integration Once R was made accessible from T-SQL, it was time to solve a long-standing difficulty I’ve had with data integration. Application Program Interface (API) is everywhere and it’s very common that those who have built one will also engage with it; however, to a database developer, it’s not common at all. If you wanted to get data from an API into the database, you would have to reach out to an application developer using .NET / Java / Python etc. to build a custom interface and deliver the data into the database. So, as fun as that sounds, building a stored procedure to handle it for you is much easier and faster. Example API XML Integration The following example shows a simplified approach for building a data integration with an API and parsing it into a table structure. A common architecture would include a web tier, data tier, and multiple people to develop and deploy a solution. However, I’m going to show you a single query that will deliver the current weather report for Grand Blanc, MI into a table. *NOTE: Before you attempt to use this query, take a moment to review Known Issues for SQL Server R Services. My experience did not go very well until I read the section on “Remote compute contexts blocked by firewall in SQL Server instances running on Azure virtual machines” which also applies to installs on local machines. *REQUIREMENTS: SQL Server 2016 and access to the machine to install addition R packages. To use the code below, you will need to install the R packages “xml2” and “XML”. Instructions on the procedure can be found here. The API used is free with registration which is required to get an API key to access the data and can be found here. The code is broken up into 3 parts, first declaring variables, like so:     DECLARE @Rscript NVARCHAR(MAX),     @API_URL NVARCHAR(MAX)     Next, we need to assign our API URL to a variable and the R code necessary to call and parse the data. Your code might look something like this:     SET @API_URL = 'http://api.openweathermap.org/data/2.5/weather?zip=48439,us&APPID=<enter your API key here>&mode=xml'SET @Rscript = N'library(xml2);library(XML);weather.doc <- read_xml(\"'+@API_URL+'\");weather.xml <- xmlRoot(xmlParse(weather.doc));city.id <- xpathSApply(weather.xml, \"//*/city\", xmlGetAttr, \"id\", NA);city.name <- xpathSApply(weather.xml, \"//*/city\", xmlGetAttr, \"name\", NA);city.lon <- xpathSApply(weather.xml, \"//*/city/coord\", xmlGetAttr, \"lon\", NA);city.lat <- xpathSApply(weather.xml, \"//*/city/coord\", xmlGetAttr, \"lat\", NA);city.country <- xmlToDataFrame(getNodeSet(weather.xml, \"//*/city/country\"));city.sun.rise <- xpathSApply(weather.xml, \"//*/city/sun\", xmlGetAttr, \"rise\", NA);city.sun.set <- xpathSApply(weather.xml, \"//*/city/sun\", xmlGetAttr, \"set\", NA);temp <- xpathSApply(weather.xml, \"//*/temperature\", xmlGetAttr, \"value\", NA);temp.min <- xpathSApply(weather.xml, \"//*/temperature\", xmlGetAttr, \"min\", NA);temp.max <- xpathSApply(weather.xml, \"//*/temperature\", xmlGetAttr, \"max\", NA);temp.unit <- xpathSApply(weather.xml, \"//*/temperature\", xmlGetAttr, \"unit\", NA);humidity <- xpathSApply(weather.xml, \"//*/humidity\", xmlGetAttr, \"value\", NA);humidity.unit <- xpathSApply(weather.xml, \"//*/humidity\", xmlGetAttr, \"unit\", NA);pressure <- xpathSApply(weather.xml, \"//*/pressure\", xmlGetAttr, \"value\", NA);pressure.unit <- xpathSApply(weather.xml, \"//*/pressure\", xmlGetAttr, \"unit\", NA);wind.speed <- xpathSApply(weather.xml, \"//*/wind/speed\", xmlGetAttr, \"value\", NA);wind.speedname <- xpathSApply(weather.xml, \"//*/wind/speed\", xmlGetAttr, \"name\", NA);wind.gusts <- xpathSApply(weather.xml, \"//*/wind/gusts\", xmlGetAttr, \"value\", NA);wind.direction <- xpathSApply(weather.xml, \"//*/wind/direction\", xmlGetAttr, \"value\", NA);wind.directioncode <- xpathSApply(weather.xml, \"//*/wind/direction\", xmlGetAttr, \"code\", NA);wind.directionname <- xpathSApply(weather.xml, \"//*/wind/direction\", xmlGetAttr, \"name\", NA);clouds <- xpathSApply(weather.xml, \"//*/clouds\", xmlGetAttr, \"value\", NA);clouds.name <- xpathSApply(weather.xml, \"//*/clouds\", xmlGetAttr, \"name\", NA);visibility <- xpathSApply(weather.xml, \"//*/visibility\", xmlGetAttr, \"value\", NA);precipitation <- xpathSApply(weather.xml, \"//*/precipitation\", xmlGetAttr, \"mode\", NA);weather.number <- xpathSApply(weather.xml, \"//*/weather\", xmlGetAttr, \"number\", NA);weather.value <- xpathSApply(weather.xml, \"//*/weather\", xmlGetAttr, \"value\", NA);weather.icon <- xpathSApply(weather.xml, \"//*/weather\", xmlGetAttr, \"icon\", NA);lastupdate <- xpathSApply(weather.xml, \"//*/lastupdate\", xmlGetAttr, \"value\", NA);OutputDataSet <- data.frame(city.id, city.name, city.lon, city.lat, city.country, city.sun.rise, city.sun.set, temp, temp.min, temp.max, temp.unit, humidity, humidity.unit, pressure, pressure.unit, wind.speed, wind.speedname, wind.gusts, wind.direction, wind.directioncode, wind.directionname, clouds, clouds.name, visibility, precipitation, weather.number, weather.value, weather.icon, lastupdate)';     The last step is to execute the R script and define our Result Sets, like so:     EXEC sp_execute_external_script     @language = N'R',     @script = @Rscript     WITH RESULT SETS(([city.id] VARCHAR(100), [city.name] VARCHAR(100), [city.lon] VARCHAR(100),[city.lat] VARCHAR(100), [city.country] VARCHAR(100), [city.sun.rise] VARCHAR(100), [city.sun.set] VARCHAR(100), [temp] VARCHAR(100), [temp.min] VARCHAR(100), [temp.max] VARCHAR(100), [temp.unit] VARCHAR(100), [humidity] VARCHAR(100), [humidity.unit] VARCHAR(100), [pressure] VARCHAR(100), [pressure.unit] VARCHAR(100),[wind.speed] VARCHAR(100), [wind.speedname] VARCHAR(100), [wind.gusts] VARCHAR(100),[wind.direction] VARCHAR(100), [wind.directioncode] VARCHAR(100), [wind.directionname] VARCHAR(100), [clouds] VARCHAR(100), [clouds.name] VARCHAR(100), [visibility] VARCHAR(100), [precipitation] VARCHAR(100), [weather.number] VARCHAR(100), [weather.value] VARCHAR(100), [weather.icon] VARCHAR(100), [lastupdate] VARCHAR(100)))     The R code consists of 5 parts, starting with loading the libraries needed to handle the XML as well as the API call. Curl is the package enabling the communication and will not have to be loaded directly as it gets loaded with the other packages. Here is step 1:     library(xml2);library(XML);     Step 2 makes the API call and reads the XML into a variable:     weather.doc <- read_xml(\"'+@API_URL+'\");     Step 3 parses the XML wrapper and leaves you with the root element and remaining body:     weather.xml <- xmlRoot(xmlParse(weather.doc));     This is the data returned and contained in “weather.xml”:     <current>- <city id=\"4994320\" name=\"Grand Blanc\"><coord lon=\"-83.63\" lat=\"42.93\" /> <country>US</country> <sun rise=\"2016-12-02T12:48:04\" set=\"2016-12-02T22:00:35\" /> </city><temperature value=\"275.49\" min=\"275.15\" max=\"276.15\" unit=\"kelvin\" /> <humidity value=\"80\" unit=\"%\" /> <pressure value=\"1016\" unit=\"hPa\" /> - <wind><speed value=\"5.7\" name=\"Moderate breeze\" /> <gusts value=\"8.2\" /> <direction value=\"280\" code=\"W\" name=\"West\" /> </wind><clouds value=\"90\" name=\"overcast clouds\" /> <visibility value=\"16093\" /> <precipitation mode=\"no\" /> <weather number=\"804\" value=\"overcast clouds\" icon=\"04d\" /> <lastupdate value=\"2016-12-02T14:16:00\" /> </current>     Step 4 parses each element or attribute of the XML into individual variables:     city.id <- xpathSApply(weather.xml, \"//*/city\", xmlGetAttr, \"id\", NA);city.name <- xpathSApply(weather.xml, \"//*/city\", xmlGetAttr, \"name\", NA);city.lon <- xpathSApply(weather.xml, \"//*/city/coord\", xmlGetAttr, \"lon\", NA);city.lat <- xpathSApply(weather.xml, \"//*/city/coord\", xmlGetAttr, \"lat\", NA);city.country <- xmlToDataFrame(getNodeSet(weather.xml, \"//*/city/country\"));     Step 5 combines all of the individual data frames into a single data frame using the expected SQL OutDataSet variable, as seen below.     OutputDataSet <- data.frame(city.id, city.name, city.lon, city.lat, city.country, city.sun.rise, city.sun.set, temp, temp.min, temp.max, temp.unit, humidity, humidity.unit, pressure, pressure.unit, wind.speed, wind.speedname, wind.gusts, wind.direction, wind.directioncode, wind.directionname, clouds, clouds.name, visibility, precipitation, weather.number, weather.value, weather.icon, lastupdate)     The below example shows the expected output once executed. Summary R will continue to gain popularity in the Data Science field for its ability to model and forecast trends, but I hope to see more database developers start to explore its capabilities. This example was intended to give database developers a bridge between the power of a relational database management system and the exorbitant amount of data available through APIs. If you’re still looking for more information or need help with Microsoft R, contact BlueGranite today! "
"231" " I recently heard a joke that went something like this –     Maria: “What forms do you need me to submit?”     Insurance Rep: “Please fax your proof of insurance and a copy of the police report to the number listed      on your insurance card.”     Maria: “Can I email you those items? Faxing isn’t supported where I live.”     Insurance Rep: “Oh, where do you live?”     Maria: “2016”I'm starting to feel the same way about Cloud Analytics. If your organization, in 2016, doesn’t have a Cloud Analytics strategy, then you are unfortunately behind the curve.   A Cloud Analytics strategy is one that: Supports the use of platform-as-a-service tools to deliver data-based analytics to discover key business trends Predicts positive and negative business outcomes and suggests how to achieve and/or avoid them Drives actionable results with data-backed decisions In this article, you’ll learn why it’s important to have both an Analytics Strategy AND a Cloud Strategy, and why they should intersect. You need an Analytics Strategy An analytics strategy helps to define a plan for how your organization is going to implement data-driven support to produce better outcomes.  Without analytics, you’ll be forced to react to a poor-performing customer promotion AFTER the damage has already been done. But with analytics, and the ability to use them appropriately, you will be able to proactively detect that a promotion is likely to be poor-performing and make the necessary changes BEFORE it happens, effectively turning a negative situation into a positive one.  Which scenario would you rather be part of?Your analytics strategy doesn’t need to be complex and it doesn’t need to involve building a massive team of data scientists. It does, however, need to: Align with a defined list of business goals Identify an approach for data exploration within your organization Include tools for data exploration, analytic modeling, and data visualization Lead to a path that includes machine learning and predictive capabilities  Building your first analytics strategy should include the following plans:Define what answers you need to know BEFORE an action is made. Having a key set of questions and   answers that you’ll need are important to building an effective analytics platform. Design a plan for acquiring the data you will need to answer those questions. Analytics aren’t meaningful without the right data. Modern tools let you integrate curated data with external data sources – you’ll often find yourself looking for broader sets of data as you mature your analytics. Identify your measure of success in predicting the future. Knowing how close you’ve hit the mark is a   good indicator of success. While no analytic model is perfect, you need to know what “good enough” means for your organization’s solution. You need a Cloud Strategy Your organization doesn’t have to be in the middle of a full cloud migration to have a good cloud strategy.  Hybrid cloud implementations – those than involve keeping infrastructure resources on-premises while supplementing with cloud platforms – are a great approach to incorporating cloud agility and scalability into your overall strategy.In addition to the obvious benefits of increased scale and decreased cost, cloud solutions provide a high level of agility. Cloud or Hybrid Cloud-based projects that are a developed with a “solution first and infrastructure last” methodology are often able to be more nimble and can be quickly adapted to new business requirements and changing industry norms.A hybrid cloud strategy doesn’t need to include sweeping changes across your technology organization. Instead, it needs to focus on filling gaps that your current environment is not able to support.  It should include: Identification of what tools and capabilities you are lacking now that could be provided via a cloud platform. Identification and description of sources of data that can live in the cloud and, also, those which need to remain on-premises. Focus towards platform-as-a-service offerings that reduce the necessity of direct administration and maintenance. Inclusion of tools that provide capabilities that your organization currently does not possess. Three important considerations to include in your cloud strategy are: Build in support for both corporate-wide and departmental-focused deployments. The former requires strong integration with current infrastructure, but the latter can be more flexible and dynamic based on business needs. Major cloud platforms, like Microsoft Azure, are secure by design. Include data security as a key priority, but don’t let overarching security mandates hinder your progress. Identify key personnel who will own a specific cloud-based tool. Feature updates often happen quickly in the cloud, and having a single point-of-contact who keeps up on new features and updates is key to adoption success. Your Analytic Strategy & Cloud Strategy Should Intersect Analytics is a cutting-edge industry, with lots of new tools evolving quickly. Cloud platforms, like Microsoft Azure, are also cutting edge and enable a level of agility that is not possible with on-premises installations. This fact allows analytics and cloud to fit together very well.  For most of our customers, their analytics strategy and cloud strategy intersect at some level.Some factors causing the two strategies to intersect include: Many analytics tools are complex, with a large ecosystem of community-built packages. Large ecosystems require constant administration. Going to a cloud platform minimizes and/or removes administration requirements. As analytic models mature, the tools mature with them. Cloud technology moves quickly, with updates coming more often than on-premises architectures.  New features arrive weekly or monthly. Analytic models are subject to change as business requirements are refined, or as activities happen in the real world. Using a cloud platform imbues your analytic model with the agility it needs to react to an ever-changing workload. Analytic solutions built using a cloud platform generally benefit from a faster time-to-market than those built on-premises using traditional infrastructure. Because these solutions minimize the focus of acquiring and configuring infrastructure, projects are often “off the ground”, quickly leading to realized business value during the first weeks of a new project. Scaling your solutions doesn't require a budget council meeting. Cloud is hyper-scale. When you need to grow your solution from 100GB to 100TB, you don’t need to ask for a large briefcase of money. Operational costs are easier to budget for. Data Analysts and Data Scientists tend to work on the cutting edge. Cloud platforms also tend to stay on the cutting edge, meaning your analyst teams are always able to work with the latest technology. Integration between development cycles and deployment cycles are built into the cloud platform. Often, the deployment process is built into the development process of cloud platforms.  Your business analytics team will be able to manage their own release schedules and ensure that the right answers are always ready. What’s next? Join our Webinar Hopefully you’ve read this and are thinking “I really need to get that cloud analytics strategy started!” – or, maybe you have already started and just want to make sure that you’re going down the right path. In either case, BlueGranite is here to help you feel confident that you’ll be successful with an upcoming free webinar discussing the Microsoft approach to a Cloud Analytics strategy.In this webinar, you’ll understand why Microsoft has become a leader in enabling success in the analytics space over the last two years. You’ll learn about the methodologies that highly successful teams have implemented. Additionally, you’ll learn about the tools available in the Microsoft Cortana Intelligence Suite; a complete Cloud Analytics platform that includes the data management, data transformation, analytic modeling, data intelligence, consumption, and collaborating tools that are required to implement a well-designed Cloud Analytics strategy.Sign up for our upcoming webinar today!"
"232" "The industry has reached an inflection point where cheap hardware, ubiquitous connections, and powerful cloud platforms allow us to consume data from devices all around us. The Internet of Things (IoT) represents a compelling opportunity for organizations to gain operational insight and perform predictive analytics in real-time and at a grain previously unimagined. These capabilities allow businesses to improve efficiency, reliability, and agility.   From predicting and preventing product failures to rerouting flights to improve fuel efficiency, creative application of IoT capabilities represent a tremendous opportunity to gain a competitive advantage. Microsoft’s Azure IoT Hub is a hyperscale cloud solution that is easy to provision, use, and manage while providing functionality across the spectrum of IoT solution needs, from security to device management. Microsoft offers several ways to get started with its platform. Azure IoT Suite offers two demo scenarios to demonstrate the power of the Azure IoT hub. The code for both the predictive maintenance and the remote monitoring scenarios are available on GitHub for you to customize. Raspberry Pi boards are an inexpensive and useful tool to explore the features and functionality of Azure IoT Hub with a physical device. Microsoft, in collaboration with Adafruit, sells an IoT Pack for Raspberry Pi 3 that is a wonderful way to jumpstart your development efforts or to get started with an IoT proof of concept. Microsoft also published a handy getting started guide that shows how to set up a development environment and how to set up the Raspberry Pi board. The getting started guide is also useful if you already have a Raspberry Pi board and simply want to experiment with Windows IoT Core and Azure IoT Hub. Microsoft publishes Azure IoT Hub SDKs for C, .NET, Java, Node.js, and Python 2.7. The variety of popular languages gives you options to leverage your developer’s existing skills.  Windows IoT Core is targeted at .NET development, and Microsoft published a samples library that has code examples for everything from controlling a single LED to building a voice-controlled coffee maker. One simple and fun starter project is to monitor the temperature and humidity in your environment using a DHT 11 or 12 sensor and to display a real-time graph of the data in PowerBI.  You can find the wiring diagrams for both sensors here.  Once you have the Raspberry Pi board configured and the sensor wired correctly, you can begin programming the device to stream data to the Azure IoT hub.  You can download and install the Windows IoT Core Project Templates for visual studio and use the Background Application template as a simple starter project.  In order to get started quickly, I recommend Daniel Porrey’s Universal Windows Library for the DHT11/22 sensors, which will allow you to focus on the Azure IoT Hub capabilities and device to cloud communication rather than low-level device programming. Add the library to your solution and add a reference to it in your background application project.  Newtonsoft’s json library is also an invaluable tool to easily serialize and de-serialize messages to and from the cloud. The IoT Connector Client sample provides examples of how to connect to, send messages to, and receive messages from Azure IoT Hub.  Note that the sample assumes you are using the Trusted Platform Module (TPM) to secure connection information.  You can learn more about TPM and how to provision devices here.  The Raspberry Pi 3 does not contain a hardware TPM device, but rather uses a software emulated device. Your code should look something like this: public void Run(IBackgroundTaskInstance taskInstance){    deferral = taskInstance.GetDeferral();    TpmDevice device = new TpmDevice(0);    string hubUri = device.GetHostName();    string deviceId = device.GetDeviceId();    string sasToken = device.GetSASToken();    _sendDeviceClient = DeviceClient.Create(hubUri, AuthenticationMethodFactory.CreateAuthenticationWithToken(deviceId, sasToken), TransportType.Amqp);    InitGPIO();    readingTimer = ThreadPoolTimer.CreatePeriodicTimer(ReadingTimerTick, TimeSpan.FromSeconds(1));} Once you are connected, simply put the code to read the DHT sensor, serialize the data, and send the message into the Background Application’s main timed loop.  Your code may look something like this: private async void ReadingTimerTick(ThreadPoolTimer timer){    DhtReading reading = new DhtReading();    reading = await dht.GetReadingAsync().AsTask();    if (reading.IsValid)    {         float currentTemperature = Convert.ToSingle(reading.Temperature);        float currentHumidity = Convert.ToSingle(reading.Humidity);        temperature = currentTemperature * 9 / 5 + 32;        humidity = currentHumidity;        var telemetryDataPoint = new        {            time = DateTime.Now.ToString(),            deviceId = deviceId,            location = \"Office\",            currentHumidity = humidity,            currentTemperature = temperature        };        var messageString = JsonConvert.SerializeObject(telemetryDataPoint);        var message = new Message(Encoding.ASCII.GetBytes(messageString));        await _sendDeviceClient.SendEventAsync(message);    }} Build and deploy the project to your Raspberry Pi.  You can use Device Explorer to confirm that your application is sending data to the Azure IoT hub.  The final step is to use Azure Streaming Analytics Service to stream your data into PowerBI.  Configuring the stream to PowerBI is straightforward.  Simply choose Azure IoT Hub as a source and PowerBI as a destination.  A very simple example query is shown below.  Microsoft’s how-to document on building a real-time analytics dashboard contains detailed instructions on how to configure Streaming Analytics Service and PowerBI to visualize the temperature and humidity data in real time.  Azure IoT Hub is a unique offering that simplifies many of the difficult tasks involved with a large-scale IoT project’s implementation.  This article outlined a few possibilities to explore the features of the platform and to inexpensively begin experimenting with your own solution. To learn more about how BlueGranite could help you build and manage your Azure IoT Hub, contact us today!"
"233" "The Data Warehouse has been critical to business decision making for over two decades. As a Microsoft business intelligence expert and systems integrator, BlueGranite frequently encounters customers looking to maximize their existing SQL Server data warehouse's performance, or who want help planning a migration to a new version of SQL Server.   One of the common problems that BlueGranite sees working with customers in many different industries is an increase in data sources, data volumes, and lengthening ETL load times. Additionally, end users expect more functionality and split-second report/dashboard response times. The days of waiting two minutes for a report are over. In most cases, customers are looking for features, functionality, techniques, and even hardware that will allow them to meet increasing performance requirements. As a nine-year leader in the Gartner Magic Quadrant for BI, Microsoft has a vast array of solutions and products to meet the growing needs of the data warehouse. While many customers are aware of SQL Server 2016 and its earlier versions, Microsoft also has three additional product offerings for data warehouse solutions: Data Warehouse Fast Track, Azure SQL Data Warehouse, and the Analytics Platform System (details below). While SQL Server contains many BI features like Integration Services, Analysis Services, and Reporting Services in a single installation of the product, here we are focusing on the Relational database being used for data warehouse solutions.  SQL Server What is It: SQL Server 2016 – Best BI Platform for over a decade Microsoft SQL Server Enterprise is the most common data warehouse solution of the four listed above. It is asymmetric multiprocessing (SMP) database engine that is NUMA aware, which means it relies on the CPUs, RAM, and storage shared within a single server. When using SQL Server for a data warehouse, the customer must build their own hardware solution, then install and “configure” SQL Server on top of the hardware. It has industry-leading features including Clustered Columnstore Index for extremely fast, low IO, in-memory query processing. Key Considerations: Build Your Own Deployment – Lots of Options Customers can use a mixture of physical or virtual hardware and direct attached or shared storage via a storage area network. Most customers will configure SQL Server for data warehouse workloads using specific considerations and configurations for scan centric workloads (opposite of OLTP). While SQL Server has traditionally been installed and run on premises, customers can also run SQL Server in the cloud using Infrastructure as a Service (IaaS). SQL Server Enterprise comes with a plethora of BI features that can be used to create a cohesive solution. The speed of storage available to the server is vital to any SQL Server warehouse deployment, so many built SQL Server solutions take advantage of current PCI and flash-based storage technologies. Any design should factor in not only the capacity of storage but also its IO performance to deliver results at the required speed. BI Features: All in One or One for All SQL Server doesn’t put constraints on a customer’s ability to install and use multiple features on one server; this is a popular practice due to Microsoft only imposing licensing on a server basis (per core pairs) and not by feature or instance. Programming: Regular T-SQL Customers program their data warehouse solution using the robust T-SQL programming language that is expanded and made more robust with each version release of SQL Server. Scale Options: Scale Up Yourself Customers can “scale up” their SQL Server data warehouse by adding more horsepower (RAM, CPU, and faster storage). By using SQL Server in a scaled up environment, customers can expect to grow their data warehouse up to 10TB before needing to analyze other solutions. Key Features: CCIs & Much More SQL Server has page and row compression which allow the data to be compressed on disk so data retrieval is faster overall. Starting with SQL Server 2014, customers can also create Clustered Columnstore Indexes (CCIs) which convert data to columnar storage from row storage allowing even greater compression. CCIs have the potential to speed up data warehousing queries 100x. In-memory OLTP tables can be used for Staging tables to speed up ETL workloads. Batch mode also helps with performance as the engine can iterate on “batches” of rows at a time. For information on SQL Server 2016 performance features, Microsoft’s library of resources is a great starting point.   SQL Server Fast Track What is It: High Performance SMP SQL Server Appliance SQL Server Fast Track is a reference architecture methodology for data warehouse solutions where performance is balanced between all the components of software, hardware, and storage to remove bottlenecks. Microsoft and various hardware vendors have teamed up to use the architecture to create pre-configured, tested, and shipped appliance solutions for SQL Server data warehousing. This can eliminate most of the effort and expertise needed to engineer an optimal server solution from basic parts. Key Considerations: Time Saver – Off the Shelf Hardware & Software Deployment The reference architecture can be manually applied to a plain SMP SQL Server data warehouse solution, but customers get the most value in being able to pick an off-the-shelf solution for their desired performance level and price, as this can speed up solution deployments. When buying a Fast Track appliance, customers can expect the vendor to ship and configure the hardware and software to the customer’s on-premises data center. BI Features: Only SQL on the Appliance Because Fast Track is an appliance-based approach, there are some underlying restrictions. First, Fast Track appliances are only currently available on-premises. Second, while SMP SQL Server allows the customer to install features for BI, this is typically not supported on the appliance. Programming: Regular T-SQL Compatible Since Fast Track is just SQL Server Enterprise SMP, you can use regular T-SQL. Scale Options: Scale Up – Predetermined Scale Size Limit (5TB to 145TB) Fast Track is a scale up solution. Appliances are available in many different sizes (5TB to 145T) from many vendors. Since the appliance is already scaled up, they typically can’t be scaled up any further. If a customer exceeds their Fast Track size threshold, they typically must buy another one. Key Features: Fast Storage, All SQL Server Features Available The main benefits of Fast Track are screaming-fast performance (up to 10 GB/second read/write throughput), usage of plain T-SQL, and access to all the performance enhancements to SQL Server like CCIs. This means that customers can migrate their data warehouse solutions to Fast Track, leave their code unchanged, and obtain tremendous performance benefits. For more information on Fast Track, try checking out Microsoft’s website. Analytics Platform System What is It: SQL Server MPP Appliance The Microsoft Analytics Platform System (APS) is a SQL Server Enterprise-based data warehouse appliance created by Microsoft and hardware vendors that can scale to up to 6PB of data. APS is an Massively Parallel Processing (MPP) database engine, meaning that it is a set of multiple SQL Servers that are running in concert together. Key Considerations: Scale Out & Distribute Your Data APS is a scale out solution. APS fundamentally works by taking a big problem and breaking it up into smaller pieces. APS does this by implementing a control/compute topology, where one SQL Server functions as the brains of the solution (control), and other SQL Servers function as the brawn (compute) to perform parallel operations on data that has been distributed among the compute SQL Servers. BI Features: Only SQL on the Appliance Similar to Fast Track, APS does have some limitations, and it’s only available on-premises. APS is only on SQL Server; additional BI features cannot be installed on the appliance, though all BI features can use the appliance.  Programming: DSQL is a little different and may require conversion APS uses Distributed SQL (DSQL). DSQL is slightly different than T-SQL and customers typically have some code conversion (DDL & DML) that is needed before they can move their databases to APS. This conversion can be minimal or extensive depending on the customer’s coding practices. Scale Options: Scale Out in Scale Units Customers should look at APS when they have around 10TB of data and expect data growth. APS can scale out to as large as 6PB. APS scales with linear cost and performance because it can be expanded by adding additional compute nodes to the appliance (called adding Scale Units). Key Features: It’s Extremely Fast When using APS, customers can expect significant performance improvements in querying and loading data. Customer reporting queries can often be sped up by a factor of 100 and data can be loaded as fast as several TBs an hour. Because APS is SQL Server under the covers, there is nothing special needed to connect to it, so customers can expect a seamless transition of their reporting and semantic layer tools. For more information on APS, see Microsoft’s website. Azure SQL Data Warehouse What is It: SQL Server MPP in the Cloud Azure SQL Data Warehouse (SQL DW) is a Platform as a Service (PaaS) data warehouse service in the Microsoft Azure cloud. SQL DW is an MPP database engine relying on a control/compute topology using Azure SQL Database and Azure Blog Storage. SQL DW is not APS, but it is very similar.   Key Considerations: SQL DW is Scale Out PaaS – Infrastructure is managed by Microsoft SQL DW is a scale out solution like APS, and like APS, data is distributed to compute nodes for faster performance and scale. Since SQL DW is an Azure offering, it only runs in Azure and is not available on-premises. The offering is also PaaS, so customers don’t have to worry about on-premises or even cloud infrastructure as it is all managed by Microsoft.   BI Features: Only SQL On the Appliance SQL DW is only SQL Server like Fast Track and APS. While you can’t add BI features to the cluster, SQL DW can be used with all the SQL Server and Azure BI features. Programming: DSQL is a little different and may require conversion Like APS, SQL DW uses DSQL and has similar programming limitations. Scale Options: Elastically Scale in Minutes Customers should look at SQL DW when they have TBs of data. Using SQL DW, they can expect to scale out to as large as PBs. SQL DW can be elastically scaled out in seconds by changing the Data Warehouse Units (DWU – unit of scale performance) setting in the Azure portal. Key Features: Separation of Compute & Storage Customers can create a SQL DW cluster in minutes. Just like other Azure offerings, it is a pay-as-you-go pricing model so you are charged by the hour. Not only can SQL DW be elastically scaled out, Customers can even pause their instance which eliminates compute charges. Storage and compute are completely decoupled so when customers pause their cluster, their data is persisted. If you’d like more information about Azure SQL DW, please see Microsoft’s website. Scale Your Company with BlueGranite Whether you want to scale an existing data warehouse platform or migrate to a new one work, BlueGranite would love to help. Contact us today – we’re experts at helping organizations of all sizes navigate the many Modern Data Platform solutions and better understand the benefits of Big Data. If your curious how Modern Data Architectures can provide faster insights, please check out our free e-book, here."
"234" "Every company is increasing their use of data.  In addition to increasing the amount of data they use, companies are looking to reduce the latency from the source system to analysis.  This is where real-time analytics comes into play.  Real-time analytics, as the name suggests, is time sensitive and enables a decision with as little delay as possible.  The incoming data requires immediate analysis and is composed of analyzing data in motion and at rest.  This low latency concept is not new, but it is gaining popularity as the scalability of hardware becomes more economical to handle high volumes of data.   So, what can you do with real-time analytics? Instead of trying to list off every example possible (an endless list), let's jump into one example BlueGranite has recently come across.  This example demonstrates how to transform the manual process of reviewing requests into an automated analysis, delivering decisions in real-time. Problem The organization has a goal of reducing processing time of the large volume of incoming requests.  One of the firm’s bottlenecks is a manual process of directing requests to their respective specialized departments to handle.  Implementing a simple yet effective real-time analytical process provides a strategic solution. Solution Under the old process, a request would arrive, wait until someone was available, and then that individual would take the time to read through the request statements.  After processing the information, the individual would decide on what department should handle the request.  Under the new process, a request will arrive and immediately a text analysis function consumes the request statements, then classifies which department handles the request.   Benefits Two of the major benefits from implementing such a strategy addresses the organization's goal of reducing processing time.  One benefit is reducing the time spent deciding where to send the request.  In more tangible terms, the process to review an incoming request was expected to take around 15 minutes or less.  The text analysis function only takes a second to run, providing reliable time savings. In addition to the reduced review time, more resources can be spent handling the request rather than deciding which department needs to review the request. Another major benefit is reducing the overall latency time from the request to the department.  Any surge of requests could flood the system under the old process requiring manual direction.  In return, this would cause delays before the departments could even start processing the requests.  By using text analytics in real-time, the hardware can be scaled appropriately to handle any flood of requests and reduce the latency time by automatically executing at the arrival of a request. Getting Started The components in this system design can be kept at a minimum to maintain a simple and effective solution.  One way is to take advantage of SQL Server R Services by using SQL 2016.  This allows for a set of R commands to be encapsulated in a stored procedure to be executed immediately, as needs arise.  In this example, R commands use a trained SVM model which consumes the request statements to determine the most suitable department alongside the probability or confidence in the classification. To learn more about how BlueGranite can help move your analytics into real-time, contact us today!"
"235" "Microsoft Power BI is a versatile tool. It can be used for reporting, transforming, and modeling data. Additionally, it can be used over batched data loads or streaming datasets, can consume cloud or on-premise data, plus allow access through mobile devices or embedded into custom applications.   Due to Power BI’s varied features and capabilities, when organizations plan to roll it out there can be confusion as to who should use it and how. Is it purely a self-service BI tool meant for power users? Or could it be used by IT personnel, and if so how? Microsoft released a roadmap for its BI toolset about a year ago, in which Power BI was categorized as an appropriate tool for interactive reporting. This designation can help when planning how to use it, however the question of what type of users should be doing the work was left open. Should these interactive reports be created by business users? How does “shadow IT” fit in the picture? Unfortunately, purchasing software licenses is not, by itself, an effective rollout strategy. Without a managed framework, power tools can generate chaos and fractures in data management, leading decision makers to question reporting as a legitimate source of information. To avoid these risks when planning a rollout of Power BI, it is of critical importance to understand who the target users will be and how they plan to use it. Ultimately, success can be measured by user adoption. Listed below, you’ll find the four most important elements to plan for when purchasing and deploying Power BI licenses. 1. Create a branch of IT dedicated to exploratory reporting Gartner calls this “Mode 2” of a Bimodal IT organization. It is different from traditional IT in that it is focused on exploratory efforts and can cycle quickly to deliver wins.  In the context of a Power BI rollout, the Mode 2 team becomes the center of Power BI evangelism. It must be committed to a deep understanding of tool capabilities and appropriate use cases, as well as able to demonstrate value in short order. In that sense, this team is considered to have already adopted Power BI as a key reporting and analysis tool. Although it is possible for “Shadow IT” teams to perform this function, they typically don’t report to the IT organization, hence their focus is not on technology expertise but on solving business problems. Although tactical solutions can be useful, their narrow focus limits their reach. A Mode 2 Power BI team will instead focus in building a re-usable solution, constantly monitoring the environment and finding opportunities for consolidation. 2. Map users to deployment modes There are three distinct modes in which Power BI can be deployed in an organization: Corporate BI: Power BI is used by IT for prototyping and as a tool in the reporting toolset, specifically targeting interactive analysis. IT Managed Self Service BI: In this mode, BI or Mode 2 developers build and actively manage the data and semantic layer and business users develop reports. Although they may opt to build the semantic layer in Power BI, that is not a necessity. Business Users will build reports in Power BI Desktop or through the Power BI cloud service by connecting to published Datasets. Business Led Self Service BI: Power BI is used by business users to build end to end solutions. They can author data models independently from IT. There are also four types of users that can use Power BI: BI Developers: Normally operating within a Corporate BI context, they use Power BI alongside other reporting tools like SQL Server Reporting Services. Super Users: Able to create data transformations, data models and reports/dashboards in Power BI. The exist in Mode 2 teams (within the IT organization) and inside business teams. Power Users: Able to create reports/dashboards in Power BI but not data transformations and data models. Typically represented as business users. Consumers: Utilize content created on their behalf, mainly business users.  Adoption will look different depending on which deployment mode and user type is being targeted. Please note that is common for multiple deployment modes to co-exist at any given moment. The Power BI evangelizing group (Mode 2 team) is responsible for crafting a Power BI adoption strategy. To assist, it must categorize individuals in the different categories of deployment and user types. 3. Map Power BI features to deployment modes There are three main ways to share Power BI content with others: via Content Packs, Group Workspaces, or Power BI Embedded. It is also possible to share individual reports or publish to the Web. These last two will not be discussed here, given their application within a BI program is limited. Content Packs mold together data transformation scripts, a data model, and reports/dashboards into a single asset. Although users can make a copy of it and modify/add new reports (to their copy), this use case is discouraged; given that the copy can get out of sync with the original and there is currently no way to merge them. An appropriate use case for internally developed Content Packs is Corporate BI where the original author retains control over the model and reports, and the target audience are consumers that desire minimal setup complexity. Group Workspaces allow sharing with other users, which can also be group administrators, read-only users, or editors. One of the best uses for this feature is IT Managed Self-Service BI, enabling BI developers or super users to publish datasets over which power users can build reports and share with consumers. Business super users can also share datasets through workspaces using the same method within a Business Led Self-Service approach. Power BI Embedded can be used for rendering Power BI visuals within a custom application. This is a feature that IT will typically own development for, so it is appropriate for Corporate BI deployments. 4. Set adoption targets per deployment mode and monitor usage Finally, Power BI evangelist teams must set adoption targets for consumers on each of the three deployment modes. This part is not an exact science, but helps to measure expectations against reality and ideally provide a tangible way to confirm business value is being generated through Power BI.  Although Power BI has a built-in Usage Metrics Dashboard available to administrators (shown above), it can be useful to leverage Power BI audit logs to create more customized usage reporting. For example, narrowing to the last 4 weeks:  Monitoring usage should not be an afterthought; instead, it should be an assigned responsibility of a Power BI administrator within the Mode 2 IT organization. Looking to learn more about the many benefits of Power BI? Check out BlueGranite’s Four-Day, Instructor-Led Training, or contact us today to learn how we can help you."
"236" "At BlueGranite, we spend a lot of time learning all we can about new technologies; we spend a lot of time working with our partners, Microsoft and Hortonworks, to develop strategic programs to help our customers solve business problems; and we also spend a lot of time with our clients, helping them realize their goals.  As we approach the close of 2016 – a busy year in the data and analytics industry – we at BlueGranite have a lot to be thankful for. While we take time this Thanksgiving week to reflect on the bounties that we've been afforded and the relationships that we nurture, the staff here is also grateful for the growth of the analytics industry. I recently reached out to our team to find out exactly what they appreciate most. Merrill Aldrich:\"I'm thankful for new features in SQL 2016 like the CREATE OR ALTER syntax for stored procedures.\" While the CREATE OR ALTER syntax may seem like a small addition in Microsoft’s SQL Server 2016 Service Pack 1, it's a huge boon to SQL developers who have long struggled with awkward IF EXISTS DROP syntax when making a change to a stored procedure. Even more impactful when implementing DevOps, this new syntax will call for stored procedures to be modified or created with the same command, simplifying both development and deployment processes. New additions to SQL Server, like this one, show that Microsoft listens to users and developers alike, and strives to make its products great for the entire end-to-end chain. David Eldersveld: “I’m thankful for the opportunity to develop in a platform like Microsoft Azure. I can immediately feel productive without having to wait long or worry about infrastructure. Whether it’s a project related to SQL databases or SQL Data Warehouse, Hadoop with HDInsight, machine learning with Azure ML, or more, I can focus on the solution rather than challenges with the setup.” 2016 was a banner year for Microsoft Azure. The amount of growth was staggering, and the adoption rate with our customers matched that pace. The ability to use platform-as-a-service products as part of our customer's solutions has greatly simplified the build and deployment processes. The ability to shift away from the classic acquire-servers-and-license-them model has enabled an environment where we can help our customers get creative and implement their solutions in novel ways; avoiding the expense of needing to piggyback a solution onto a platform that may not be a great fit. Azure has enabled us to operate in an \"always select the right tool for the job\" mode of operation, which is a huge win for us, for our customers, and for Microsoft. Meagan Longoria: \"I’m thankful for the Tabular Model Scripting Language (TMSL) in the new SQL 2016 Tabular models. It’s easier to read, as well as to copy and paste. It facilitates quick metadata changes and makes code merges much easier. And it works with PowerShell and Biml!\" For many users of Microsoft SQL Server Analysis Services (SSAS), this feature may have gone unseen, but for advanced users like Meagan, it was a great improvement to the previous version's gratuitous use of XML. TMSL enables operations teams to easier manage multiple environment deployment options. The operative features offered with PowerShell, and scripting features enabled when partnered with Biml, are also a huge benefit to customers. The ability to script the development of package templates directly relates to a quicker (and hence cheaper) development cycle, allowing us to focus our expertise on the finer points of solution design rather than the repetitive tasks of scriptable development. This pushes our focus away from repetition and forward to unlocking the insights that are a huge benefit of TMSL.  Erik Roll: “I’m thankful for the Power BI community who share their great designs, build custom visualizations, and answer lots of questions on how to implement Power BI best practices.”   The Power BI community is a unique piece of treasure in the analytics landscape. The Power BI product team embraces the community and enables them to make Power BI a better product. Following an open-source pattern, custom visuals are created by community members and delivered through the Custom Visuals Gallery on powerbi.com. The newly introduced Data Stories Gallery features great ideas, experiments, and \"I didn't know you could do that\" examples from people across the globe. Our very own David Eldersveld has a featured example with his \"See and Say Power BI Report.\" The community is also engaged in supporting other Power BI users, and supporting the Power BI product team by offering, and voting on, suggestions for product improvement. The community is THE reason why Power BI is quickly becoming the leading tool for analytics consumption in today's data landscape. Steve Cardella: \"I'm thankful for Hortonworks DataFlow. It does a great job of managing data ingestion and transport.\"   We've used Hortonworks DataFlow on several of our customers’ projects and have been impressed with how much it simplifies the process of dealing with data in motion. Having a graphical WYSIWYG editor is a great addition to the Hadoop developer's toolkit. With HDF 2.0, we've enjoyed the increased security, and additional processor types, extending our reach into different customer systems even further. We love products that make our job easier, and HDF is one of those great products that make Big Data approachable. With HDF, we can focus more on solving the problem that the customer approached us with, rather than worrying about how to tie together various technologies and infrastructure. It's like a Big Data easy button, and that makes it one of our go-to tools.  Jim Bennett: \"I’m thankful for SQL Server 2016 SP1 because SSAS Tabular models are now NUMA aware.\"   SQL Server 2016 Service Pack 1 was an incredible gift that we are thankful for multiple times over. In addition to the developer improvements mentioned above by Merrill, we are also excited about SSAS performance improvements due to the SSAS Tabular engine being non-uniform memory access (NUMA) aware. Better memory management is key for performance in today's high-powered servers with their large memory capacity, and SSAS is all about high performance. Adding NUMA awareness to the SSAS Tabular engine increases its ability to more intelligently manage memory and provide better performance for end-user reporting and dashboards. While we’ve listed a few of the industry advances that have made our lives easier this year, we at BlueGranite are most grateful for you – our collaborators, clients, and partners. And we’re eagerly looking forward to the discoveries, advances and partnerships 2017 holds."
"237" "Data visualization is a science. Those of us who are enthusiasts can appreciate the power of this science. We know to place charts, titles, labels, etc. in their proper place and we know it’s not just to make things pretty, as there’s a lot more to it! Every element placed on the dashboard should help support the information that needs to be surfaced, but sometimes you might use eye candy to help reel in the audience or connect the them to the data. Eye candy are visual elements that look slick but can be difficult to interpret or just don’t add much insight. When used sparingly, and depending on the context, eye candy can add a nice touch to a dashboard. Some visualizations can benefit from eye candy when a little fun is allowed (usually not financial reports). From time to time, we may encounter a business dashboard loaded with eye candy (pie charts, word clouds, unnecessary images) and the person who created it was probably proud of it. These dashboards can be exhausting to use because of all the attention it demands, and the visual elements may distract from the significance of the data. The designer may be trying to showcase their visualization talents at the sacrifice of data insights or audience satisfaction. While eye candy charts can be fun to build, it should not be used as a main chart to showcase data.  When used carefully, they can be very effective in drawing attention to a dashboard. Below, you’ll find an Aster plot from the Power BI Gallery. The visual looks great, but users need to hover over the visual to gain information and it’s difficult to see the top 5 South American countries at a glance.   After downloading the workbook and creating a dashboard (shown below) with the same data, the Aster plot and map now draws attention to the dashboard. The Aster plot and map are both eye candy that serve different purposes. The map gives the audience a connection to the country and can help reinforce what this dashboard is about (South America). According to Joshua Sarinara, a neuroscientist at the Massachusetts Institute of Technology, “vision is the strongest and most influential in memory formation”. While most people probably have an idea on where South America is, having a map as eye candy helps them remember which country had the largest GDP and population even after they are no longer viewing the dashboard. This is the power of recognition and recall, when the brain is able to recognize a familiar topic and recall something about it. The Aster plot chart is additional eye candy that brings an audience deeper into the dashboard by inviting them to interact with it. Here, text is used to tip the audience on how the chart functions. By adding effective visualizations around the original graphic, it is more clear on how the top 5 South American countries performed and the information is easier to consume. Brazil is clearly the leader without needing to click, and the data source is the same, but the data is now presented with more effective visualizations that give quick insights. Bar and column charts can be very easy to read, compared to Aster plots that may require more work to build and decrease readability.   Eye candy can be helpful in drawing attention to a dashboard, but should be used sparingly. Use eye candy to pull your audience to the location of most importance but be weary of detracting from the importance of the subject or purpose of the dashboard. In the eye candy example, the map and the Aster plot chart both had importance and contributed to the purpose in a different way. When eye candy has a purpose, it won’t be considered chart junk."
"238" "Two of the more popular methods of uploading data to an Azure SQL Database are Azure Data Factory and SQL Server Integration Services (SSIS). SSIS is the old guard, the comfortable tool that DBAs and ETL experts know and love. In contrast, Azure Data Factory is the new contender, young, fast, agile. Or is it? There isn’t a wealth of knowledge out there comparing the two in terms of pure performance. Here, we’re going to attempt to put some real numbers out there.   Testing Notes Your mileage may vary, but it should give you some idea of how well the two platforms transfer data. That said, here’s a summary of the testing procedures. The local system was a SQL Server 2016 install with Azure Data Gateway running during all tests. SSIS ran on the same system as the database service. The computer showed no more than 50% utilization during the tests, so it was likely not a bottleneck. The network advertised upload speed was 10 megabits per second. Speedtest.net reported 12 megabits to my internet service provider. The dataset is a 3.7 million row NY Taxi Sample database (Download Link). The CSV is a little over 400MB. It was imported to a local SQL table. No transformations were performed in either version. The SSIS package used ADO.NET source and destination connections. The Azure Data Factory pipeline, likewise, directly copied from source to destination. Between tests, the Azure SQL Database table was truncated. Each configuration was tested once. The Results I recorded results at a variety of pricing tiers for the Azure SQL Database to test relative performance between SSIS and Azure Data Factory. My first execution tried transferring the data with the Azure SQL Database using the basic tier (5 Database Transfer Units or DTUs) in Azure Data Factory. It took almost an hour to transfer: 59 minutes, 34 seconds. In keeping tabs on the network graph, it was obvious that the Azure database was the bottleneck, as the network showed spikes about once every 5 seconds. I did not bother running SSIS at this level. I tried a head-to-head comparison at the S2 tier of Azure SQL Database. That gives you a 50 DTU capacity, theoretically about 10 times the speed of the basic tier. Instead, I got about 4 times the throughput. Azure Data Factory took 15 minutes, 43 seconds and SSIS took 14 minutes, 53 seconds. SSIS here took roughly 1 minute less. Surprisingly, SSIS was a little faster. I tried increasing the Azure Data Factory to the S3 tier with 100 DTUs. Both SSIS and ADF showed minor improvements, each lopping off about 3 minutes from their transfer time. ADF clocked in with 12 minutes, 42 seconds, while SSIS came in with 11 minutes, 55 seconds. Again, here SSIS pulled ahead slightly with about a minute’s advantage. Then, I upped the Azure SQL Database to the P4 tier with 500 DTUs. SSIS dropped by about half to 6 minutes, 20 seconds. ADF closed some of the gap this time with a copy time of 6 minutes, 39 seconds. We were seeing peak throughput around 11 megabits here, and less frequent dips. Here, we’re not hitting peak utilization on either source or destination, so it seems that it’s finally limited by network bandwidth here. Finally, I tried going to the P6 tier with 1000 DTUs. While we do finally see ADF beating out SSIS in this scenario, we’re clearly hitting the limits of our network connection. SSIS dropped by four seconds, to 6 minutes, 16 seconds and ADF dropped to 6 minutes, 3 seconds. At this price point, the P6 tier was hitting less than 30% utilization. Here’s a table to summarize the results: Tier DTUs ADF Time (m:ss) SSIS Time (m:ss) Basic 5 59:34 -- S2 50 15:43 14:53 S3 100 12:42 11:55 P4 500 6:39 6:20 P6 1000 6:03 6:16 Conclusions There are two things being compared here. One is between the two transport services, ADF and SSIS. The other is throughput as Azure SQL DB is scaled. It’s clear that both Azure Data Factory and SSIS benefit when the Azure SQL Database is scaled up to an appropriate level. The obvious recommendation, then, is that during loading, one ought to scale up the Azure SQL Database to something like 25-50 DTUs per megabit of network bandwidth. Conveniently, you can scale it back down when you’re finished loading. What is less obvious is which technology to use. It’s clear that network throughput will be the limiting factor when the target is appropriately sized. What is not clear is which technology uses that bandwidth most efficiently. Azure Data Factory seemed to scale slightly better with greater network bandwidth, but the testing was insufficient to determine a clear winner. At this point, then, the recommendation must default to whatever suits your needs most. If you have a rich SSIS depth of knowledge and utilization, then keep using that. However, if you need to capture data from IoT devices or other outside data sources, then Azure Data Factory might suit your needs better. If you’re on the fence, it would be best to test and compare results for your specific situation."
"239" "How can we manage rising healthcare costs? What can we do to make services accessible to all? How soon will we cure cancer? Whatever the topic, healthcare issues are becoming more important, gaining attention and creating controversy on both sides of America’s political aisle. Costing an estimated $3.8 trillion (2014[1]) annually in the United States, healthcare consumes nearly 20% of GDP. As the baby-boom generation retires and life expectancies lengthen, overall costs threaten to continue climbing. Improving care and accessibility while reducing expenses depends on unlocking insights that are buried in growing mountains of healthcare data. Because healthcare organizations are inherently science-based, they are particularly well suited to become data-driven in the long-term. As important as trusted technology is, planning and patience are also required to change organizational culture. A leading analytics firm, BlueGranite has collaborated with several healthcare clients to mine data to advance their goals for improved care and reduced costs.   Big Data Applications for Tumor Genomics – A large U.S. hospital network in the Southeast engaged BlueGranite to use Microsoft cloud technology to speed processing of vast quantities of tumor genomics data. Since the DNA of a tumor is a corrupted form of the patient’s own DNA, scientists can use gene sequencing on both the tumor and patient to find differences that may help describe the cancer’s behavior. Sometimes these gaps indicate that a different therapy should be used instead of the treatments applied normally for the anatomical location of the tumor. For example, one patient’s breast tumor might behave in ways more typical of another kind of cancer, like colon cancer. Treating this hypothetical breast cancer with appropriate therapies for a colon tumor may yield more promise. Microsoft Azure’s ability to scale processing environments on-demand and shut them down when finished helped BlueGranite create a solution that provides information to scientists much more quickly. This opens doors to provide custom-designed medicines for tumors’ genetic traits rather than their physical location, reducing the time to treat the tumor, expediting favorable patient outcomes, and reducing total cost of treatment. EMR Text Analytics – After launching an expensive and complex electronic medical records platform, a major hospital found that process, device, and culture changes were not seamlessly adopted by all users. Many physicians were frustrated with the unfamiliar screens and hardware, which hindered their ability to care for all their scheduled patients. It was suspected that key medical information needing consistent, accurate, and timely action (like a prescription order) was being put in free form text fields (the path of least resistance) rather than in the boxes on tabs designed for that data. While it became anecdotally evident that quality was at risk, it was difficult to gauge the extent of the problem or to address it. BlueGranite was invited to show how Hadoop on Microsoft Azure can ingest large quantities of unstructured text rapidly and hunt for key terms, such as names of medicines or trigger words like “critical”. Allowing the client to focus on instances where patients need specific action or where to offer more training helped to improve the quality of care, staff morale, and patient outcomes. For an example of analytics applied to electronic medical records, see this BlueGranite case study on EMR system add-ons to increase patient success. In addition, implementing health analytics can promote better quality control to improve operations and patient care outcomes. To see how a European hospital is achieving both of those goals while reducing costs and meeting regulatory requirements, see this Microsoft case study. Reducing the Cost of Care Delivery – One straightforward path to reduce the cost of healthcare is cutting the cost to deliver it. In recent years, a large, multi-location long-term care organization in the Midwest struggled to schedule enough nurses to cover all assigned patients. Labor costs for overtime were growing faster than revenue, jeopardizing success and viability. The client tasked BlueGranite to analyze its operational data and recommend ways to make actionable information more visible to management. Using Microsoft’s SQL Server Analysis Services and Reporting Services, BlueGranite quickly identified factors driving inefficient distribution of work, which led to more balanced workloads, improved morale, and reduced turnover. These improvements enabled the client to capture more than $1 million in annual overtime savings. In another example, BlueGranite used Microsoft’s cloud analytics capability to improve healthcare operations and reduce readmits, which otherwise might have generated costly fines for the client. Streamlining Government Reporting – In a different scenario, a client that manages many nursing homes in several states sought to streamline accountability by making it easier for patients to rate the care they receive, which also must be reported quarterly to the Center for Medicare & Medicaid Services (CMS). The client needed to be able to track employee history, hire and termination dates, job codes, census data, and other information associated with the care provided to each patient. Historically, CMS has conducted onsite testing at each facility, awarding each nursing home a rating from 1 to 5 stars to help families select the best facility. BlueGranite integrated data from multiple locations and systems to enable reporting in the precise XML format required by CMS, without manual data entry. BlueGranite worked to ensure that the system included all the necessary source system data checks and format cleansing to enable compatibility with the stringent government destination system. In this critical project, BlueGranite improved the accuracy and ease of patient quality of care reporting, reducing the client’s cost to meet this regulatory requirement. Make these stories your own - If you are interested in learning more, BlueGranite can help you explore how healthcare analytics can improve quality and accessibility of care and reduce costs. Here are more examples of suitable use cases: Analyzing procedure codes to reveal unbilled, earned revenue Analyzing procedure codes and prescriptions to target healthcare fraud, waste, and abuse Improving population health management by using predictive analytics to anticipate patient needs and help prevent hospitalizations Scanning social media sentiment of patient feedback to find best practices to share, or to mitigate malpractice litigation risk These are just a few examples of how BlueGranite’s INSIGHTS Framework can help both provider and payer organizations to Discover surprising nuggets of truth in their data, Create actionable ways to realize operational efficiencies, and Transform their organizational strategies and processes to deliver revolutionary results. Please contact us if you’d like to learn more or arrange a conversation where we can tailor our capabilities to fit your specific situation and goals. [1] Munro, Dan, “Annual US Healthcare Spending Hits $3.8 Trillion.” Forbes online. Feb 2, 2014. http://www.forbes.com/sites/danmunro/2014/02/02/annual-u-s-healthcare-spending-hits-3-8-trillion/#22ee7acb313d    "
"240" "It's helpful to provide documentation for an Analysis Services Tabular model so analysts and developers can easily understand the relationships, calculations, security, and other business logic embedded in the model. But it can be tedious to keep the documentation up to date with each update.  If you have been manually documenting your tabular model, I'd like to show you a more efficient way. Documenting your Analysis Service Tabular model doesn't have to be tedious since we can use Dynamic Management Views (DMVs) to help us gather the necessary metadata and make a refreshable report. Analysis Services Dynamic Management Views are query structures that expose information about local server operations, server health, and database metadata.   In the past we could use the MDSCHEMA DMVs to document our Analysis Services database. With SQL Server 2016 we have new TMSCHEMA DMVs that help us document the new 1200 compatibility level models. These DMVs are also available for Power BI models. By creating a Power Pivot or Power BI model, I can query data from the DMVs and transform that data into an easily browsed model to provide information about: The SSAS Database Connection strings Source queries for each table Tables Columns Hierarchies Measures Relationships Security KPIs Perspectives  In this Demo Day video, I demonstrate how I used DMVs to create dynamic documentation that can be updated to point at any SSAS 2016 (level 1200 compatibility) Tabular or Power BI model. I built two versions of the end report: one that uses Excel to provide simple documentation, and one that uses Power BI to allow for more interactive analysis of my Tabular model metadata.                                                        For more technical information on how I built the model to support this tabular model documentation, see my post on DataSavvy.me. If you would like to download and use the Tabular model documentation solution, please click the links below to download the Power BI file or Excel file. Power BI file - Tabular Model Documentor Template Excel file - Tabular Model Documentor Template If you would like help building or documenting your SSAS Tabular or Power BI model, please contact BlueGranite today."
"241" "Microsoft’s Azure SQL Database and Azure SQL Data Warehouse promises reliability, scalability and ease of management.  With a few clicks and a few dollars, the lowly workgroup-level server can grow to consume nationwide traffic.  Such a feat is a miracle of cloud computing, but to take advantage of it, the data needs to get there.  All the computing capacity in the world is useless if it doesn’t have access to the data.  This post is going to outline just a few ways you can get that data up in the cloud.    Migrating a solution to Azure is far more than the simple transfer of data to the cloud.  It requires selection of the target platform, a migration strategy, security planning, database compatibility testing, application testing and more.  In short, it's potentially a huge topic. Here are a few notes on what will and will not be covered. This article focuses on migrating to Azure SQL Database (v12) and Azure SQL Data Warehouse. It assumes that you can transfer your data in bulk, preferably in a downtime window. Some of these methods work in incremental transfers. However, live migration is not feasible with these methods. Any compatibility or security issues need to be ironed out before production transfer. Plus, you might want to investigate ways to optimize your data structures for the new environment, particularly if you’re migrating to Data Warehouse. There are four primary approaches to transferring the data, and each approach has multiple implementations.   Approach 1 - Data Tier Export/Import Approach The data tier export/import approach uses BACPAC files, which are essentially database backups that are compatible with Azure.  Both methods are of limited use, but their one-shot nature might be appealing for small datasets. Using a Migration Wizard PROS: Transfers database as a single unit.  If you can use it, it’s a real “easy button.” CONS: Requires compatible database, interactive only, no error handling APPROPRIATE FOR: Testing With the Azure SQL Database Migration Wizard or the in-preview Data Warehouse Migration Utility, you can easily migrate a database to Azure.  It will do all the work, including creating the database and database objects as well as transferring data.   It’s much like an automated backup and restore. However, this method is of limited use.  If your database does not pass the compatibility check, you will be forced to use another method.  Additionally, it cannot schedule a migration for later.  Instead, it transfers the backup while you wait.  Finally, it cannot be used incrementally. Because of its limitations, it should only be used for testing purposes.  Export/Import BACPAC files  PROS: Transfers database as a single unit. Can be integrated into batch scripts. CONS: Requires compatible database, limited error handling APPROPRIATE FOR: Simple and smaller deployments The Migration Wizard uses these BACPAC files in the background, so many of the same limitations and recommendations apply here.  BACPAC files are essentially database backups that are compatible with Azure SQL Database.  When divorced from the Migration Wizard, they can be more useful for deployment.  PowerShell scripts can back up a SQL database to a BACPAC file, transfer it to Azure and deploy it to an Azure SQL database. Those scripts can be scheduled like any other PowerShell script.  Thus, it’s mostly useful for small databases whose data can easily migrate within the transfer window. Approach 2 – Direct Transfer The direct transfer approach copies data directly from the source SQL database to its analogue in Azure.  There are two different options for this approach, but neither significantly outperforms the other.  The limiting factor is ultimately the network connection, so the choice comes down to other factors.  When loading the data, be sure to scale the target database to maximize throughput. SSIS Direct Transfer PROS: Flexibility, error handling, scheduled, incremental transfers, BIML support CONS: SSIS is not part of all SQL Server installations, cannot stage to Blob Storage without Azure Feature Pack APPROPRIATE FOR: Most deployments (up to Gigabytes) SSIS is a tool that many DBAs and ETL experts know and love.  Because Azure SQL Database and Azure SQL Data Warehouse use native SQL Server connections, the existing SSIS tool set can be used to migrate the data to Azure as if it were migrating to any new SQL Server installation.  BIML can simplify development of these loading packages immensely.  There are no options, however, for bulk compression of the data.  That caps the size of any single data transfer.  However, it can incrementally transfer the data over days or weeks, if necessary. It’s a good general-purpose solution for all but the largest of deployments. Azure Data Factory (Direct Transfer) PROS: Scheduled, incremental transfers, BIML support CONS: Error handling is limited, requires Microsoft Data Management Gateway APPROPRIATE FOR: Small to medium deployments Azure Data Factory is the closest analogue to SSIS in Azure’s platform.  It is used to coordinate data transfers to or from an Azure service.  ADF pipeline definitions can also be built with BIML. Azure Data Factory can easily handle large volumes.  However, because it can so easily add a staging step as an intermediary, it’s only recommended for small to medium deployments when using a direct transfer. Approach 3 – Staging in Azure Blob Storage This approach breaks the deployment into two steps: staging and load.  The staging step migrates your data from your server to Blob Storage, and the load step gets it from staging in Blob Storage to its destination.  It’s useful for larger deployments. Staging SSIS with Azure Feature Pack PROS: Flexibility, error handling, scheduled, incremental transfers, BIML support CONS: SSIS is not part of all SQL Server installations, requires additional Azure Feature Pack, not appropriate for landing in Azure SQL DB APPROPRIATE FOR: Large deployments to Azure SQL Data Warehouse With SSIS in a staged approach, it is only appropriate to use with PolyBase to land in Azure SQL Data Warehouse.  But it does make an effective tool for the use case.  It can use data flows to stage the data in Blob Storage and issue queries against Azure DW to use PolyBase external tables.  Again, BIML makes automating package development much easier. Azure Data Factory PROS: Scheduled, incremental transfers, BIML support CONS: Error handling is limited, requires Microsoft Data Management Gateway APPROPRIATE FOR: Large deployments All of the previous comments about ADF apply here.  Adding a staging step in Blob Storage to an ADF pipeline is easy.  Its biggest draw here is that it’s the only tool here that natively stages in Blob Storage and loads Azure SQL DB or DW in a single pipeline. If you’re staging to Blob Storage over a network, this is the easiest one to set up and use. Azure Import/Export Service PROS: Data throughput, does not use network resources CONS: Only useful for large batches of data, lag, complexity of export/import APPROPRIATE FOR: Huge deployments The Azure Import/Export Service allows a customer to ship hard drives to an Azure datacenter, where the contents are loaded to Blob Storage within the datacenter itself.   This is potentially faster than most any network connection can handle, so long as the volume of data is extremely high. Here, a user would export the database contents to flat files using bcp or SSIS packages. They’d transfer the files to hard drives and ship them to Microsoft. For less than the recommended 20 TB data transfers, it’s less useful.  There’s shipping and loading lag, fragmentation of the process, and less control.  However, the decreased load on networks means that Microsoft recommends this method with volumes greater than 20 TB. Load PolyBase PROS: SQL queries, transfer speed CONS: Does not work with Azure SQL DB APPROPRIATE FOR: Azure DW When using Azure Data Warehouse, PolyBase is the fastest way to import data from Blob Storage. PolyBase can read the flat files as tables, making import just 2-3 SQL queries per table.  It’s best paired with scripting to automate the process. Approach 4 – Staging in a SQL Server Azure Virtual Machine PROS: Transfer efficiency CONS: Multi-step process APPROPRIATE FOR: Large to huge deployments Network bandwidth within data centers is much greater than bandwidth to the Internet at large.  For huge SQL databases, it may be desirable to transfer a compressed backup of the data to a temporary SQL Server VM in Azure.  There, you can transfer the data to an Azure Database using internal bandwidth, resulting in a net speedup. It’s useful for larger deployments only. You can transfer the database backup files using any file transfer mechanism, as the endpoint is another Windows (virtual) machine.  You can use ftp, sftp, Windows file sharing, or even the Azure Import/Export Service.   Once the backups are restored in the VM, any of the direct transfer methods are applicable. Wrapping It Up We’ve gone through four approaches to transfer your data into an Azure SQL Database or SQL Data Warehouse.  This outlines appropriate solutions for all sizes of transfers, from the smallest to the largest.  When in doubt, use the tools you’re most comfortable using.  What you might potentially lose in transfer efficiency is often gained back in speed of development and QA resources.   Please contact us for more details or if you're looking for design or delivery support for your Azure DB or Azure DW project.  Be sure to check out our Azure Data Platform Workshop for an on-site session with your team, including an assessment and roadmap for migrating your data to the Microsoft Cloud. Other helpful resources for Azure SQL include: Migrating SQL Databases to Azure eBook Cloud Analytics with Power BI and Azure Azure SQL Data Warehouse: Cloud Ready Big Data Solution  "
"242" "People often look at sales as a numbers game – more meetings, more calls and more effort means more sales, right? Unfortunately, not always. But there is a more efficient way to build success. If we evaluate and prioritize our pipeline, we can identify clients that we are best positioned to help. In this digital age, effective use of data and analytics can redefine the way we think about the sales numbers game. Instead of doing more – with less effective methods – using both historical and predictive data can help your organization decrease the frequency of its calls, meetings and presentations, while winning more opportunities and satisfying more clients.  Getting Real with Value-based Relationships In working with sales data and systems across many industries, we have found there are ways to improve efficiencies in the sales process, because, as they say, “time is money.”  Often, too much time is spent on prospects that don't have potential for high long-term value. Using data and analytics, companies can uncover insights to more accurately target high-value opportunities that are also ideal for the business.    Authors Mahan Khalsa and Randy Illig call these high-value opportunities “value-based relationships” in their book “Let’s Get Real or Let’s Not Play - Transforming the Buyer/Seller Relationship.” The notion of value-based relationships disputes the ‘do more to achieve more’ mentality. The most rewarding relationships for you and your company are those with long-term value. Mastering metrics that drive both long-term value and fit can help your organization invest its efforts in the right areas. In other words, sales and marketing can finally become more science than art. Determine Customer Lifetime Value We can use historic data from our pipeline to determine Customer Lifetime Value (CLV). Amy Gallo defines this concept in Harvard Business Review’s “How Valuable Are Your Customers?” She explains it as the amount of profit your company can expect to generate from a customer for the time the person (or company) remains with you (e.g., x number of years). At its core, CLV is the present value of all future streams of profits that a customer generates over the life of the relationship with your firm. Since we have data on existing accounts, we can start to establish CLV by examining historic data. What are the past wins and losses? What is the average size of those transactions, including minimum and maximum deal size? Writing for “Entrepreneur,” ActionCOACH founder Brad Sugars recommends estimating lifetime value by plugging actual or estimated (if you’re in the planning stages or just starting out) numbers into the following equation: (Average Value of a Sale) x (Number of Repeat Transactions) x (Average Retention) – (Acquisition Cost) An important step in this process is to include the acquisition cost – the cost you’re willing to spend per customer knowing that you’ll take a loss on an initial project. Assess Customer Fit From there, evaluate customer relationship sustainability. Examine the potential customer’s long-term vision and your ability to help them meet that goal with your products or services. Is it a customer your company will provide value to for months, or years?  For service-based firms, is it an engagement that will keep your team interested and motivated to deliver value?   Then, determine how the customer fits within your organization’s culture. Cultural compatibility means the client shares your values in working together.  This can encompass how team members will work together, how decisions are made, and how work gets done daily.  Are tasks and activities tangible, or intangible?  Respectful work environments, including team members and their attitudes, can be intangible, while workflows and processes are tangible.  Considerate work environments and attitudes include the willingness to share necessary information and embrace flexibility to achieve a common goal. For example, is value placed on the result, or on micromanaging tasks? Culture also includes interpersonal communication – open dialogue can lead to high-performing teams, while a lack of it creates roadblocks. Albeit intangible, this can be scored. Align Vision, Needs and Experience Long-term vision is vital in determining client fit and creating high lifetime value. As described in the aforementioned “Let’s Get Real or Let’s Not Play,” Khalsa and Illig champion the concept of starting with the end in mind.  That means understanding how an opportunity can ultimately impact your customer’s business, and yours. It means looking past the initial sale and assessing the likelihood of future revenue and alignment of vision and needs. Determining suitable opportunities requires clearly aligning expectations, capabilities and experience.  A well-defined beginning and end with clear expectations helps mitigate risk. Consider the customer’s specific needs and your company’s capabilities and experience in meeting those needs. If it is a new product or service, there is high risk of failing the customer. These opportunities should be given a lower rating in your pipeline. A product or service that has been delivered to multiple customers has a much greater chance of success, and should be scored with a high rating. Aligning needs and experience is a crucial step in determining which opportunities deserve the most attention. Let the data do the work.   Transform Your Pipeline As we at BlueGranite have implemented this process, we have found that lifetime value, customer fit, and alignment of needs can be quantified to create an overall score for each pipeline opportunity, and ultimately your accounts. Keeping your pipeline “real” using data and analytics can help you beat the sales numbers game and focus your efforts where you will reap the most reward.  Questions or comments? Please contact us today."
"243" "Data science brings the prospect of solving complex business problems with math. Along with that comes more advanced application development that blends rules-based decision patterns with intelligence like machine learning. This is no easy task, as it requires the confluence of the right data, the business will, and varied skill sets like developer, statistician, business analyst, data engineer/architect, and project manager. Further, no one's happy unless this can be done at scale and speed, and applied to many areas of the business. No wonder global tech consultant Capgemini finds that only 27% of organizations describe their Big Data initiatives as \"successful\".  Lots has been said about ways to organize data science teams to maximize effectiveness. I’ll put the bottom line up front: Research giant Gartner finds there is “no overall best practice”. Rather, they contend that the important components of “sharing best practices, procuring management buy-in, and launching and managing cross-functional analytics projects” can be achieved using a variety of structures. Although there’s no definitive answer on organization, it’s worthwhile to explore some advantages and disadvantages of different options. Additionally, an excellent data science process can help overcome some of the biggest organizational hurdles. And because organizational structure can and should change as analytic capability matures, establishing process as a main concern can offer a foundation of consistency along the journey. There are many excellent articles (see references below) on how to organize data science/business intelligence teams, and it’s worth reading the details. But for the most part, it boils down to a decision about how centralized (aligned with IT) or de-centralized (aligned with business or functional units) the teams are. Advanced analytics activities, both historically and currently, most commonly take place close to focused challenges like marketing mix or manufacturing optimization, where smart math/science folks work closely with business managers to develop ad hoc or custom solutions. This close association has the advantage of business intimacy and responsiveness, but often lacks scalability to operational systems, the means for knowledge sharing with other groups, and consistency between projects. Some of you might know this as “the Wild West” model, where data governance and documentation is on the outside looking in and everyone is doing their best to get things done with whatever tools they have. The more IT-aligned model advocates a centralized, shared-services model for data science projects. Teams with this organization often realize benefits from closer proximity to operational systems and data stores, and can provide more cross-functional and consistent assets from working on a variety of problems. The disadvantage with this more top-down approach is the distance from day-to-day contact with business challenges, longer wait times (usually), and trying to deliver solutions that may have disparate requirements for modeling and reporting in each department or functional area. Other hybrid approaches include creating a separate Advanced Analytics group outside of IT and lines of business (LOBs), or a mix of groups both in IT and in LOBs. Guidance for choosing which organization fits best include whether an analytics center of gravity (i.e. Chief Analytics Officer, Analytics Center of Excellence, etc.) currently exists, how projects or groups are funded, the urgency for analytics innovation, and the quality of existing data science assets. This last element about current asset condition can be quite broad, but we’ll focus on issues like collaboration between groups and consistent delivery as we discuss process considerations. As advanced analytics becomes more operationalized through intelligence in applications, data science practitioners need to learn and adopt better project and knowledge management processes that are standard in traditional IT development. Data science has well-known methodologies like CRISP-DM that provide solid guidance on major project life-cycle elements like modeling and evaluation. But they lack details on artifacts that should be produced, such as project templates and utility code, that can greatly increase productivity. Microsoft, with established tools like Visual Studio Team Services for software development, has developed an attractive, integrated process for advanced analytics called the Team Data Science Process (TDSP) that I believe can be a tremendous asset. Here’s a slide from Microsoft’s recent (recorded) presentation about TDSP at their Data Science Summit:     In Microsoft’s own words about TDSP, “The key feature is a set of (free) git-based repositories with templates providing a central archive with a standardized project structure, document templates, and utility scripts for all projects, independent of the execution environment, to allow scientists to use multiple cloud resources as needs dictate. We use Visual Studio Team Services (VSTS) to manage team tasks and execution cadence, control access, and maintain repositories containing work items.” Learning the git system for source control is probably the biggest productivity boost I’ve acquired in the last year; I think all analytics professionals can benefit. Here’s a look at the git repositories structure in TDSP:     Not only has Microsoft provided the structure and templates for project management artifacts in the repositories, it’s thrown in lots of valuable data science and cloud provisioning utilities as well: a data report script in R for interactive exploratory data analysis, a script for automatically exploring different models, and scripts for automatically connecting to Azure data sources and standing up virtual machines. A final important point is that tasks assigned to individuals in VSTS can be linked to git branches so that project management items, code, and versioning are all tracked together – pretty cool! This article has attempted to go broad, not deep, on the topics of analytics team organization and process. I’d encourage interested readers to follow the links to learn more. We are in the early days of understanding the optimal way to organize powerful predictive analytics capabilities; if you feel like it’s a bit out of control, you’re not alone. Hopefully this discussion provides some useful ways to get even more out of your analytics teams! To get started with Advanced Analytics for your organization, consider BlueGranite's Advanced Analytics Workshop. Links: Capgemini: Cracking the Data Conundrum: How Successful Companies Make Big Data Operational Gartner: Organizational Principles for Placing Advanced Analytics and Data Science Teams Gartner: Create a Centralized and Decentralized Organizational Model for Business Intelligence Accenture: Launching an Insights Driven Transformation PWC: Data and Analytics: Creating or destroying shareholder value? Microsoft TDSP documentation"
"244" " If a consumer provides feedback in an online review, a survey, through social media or other means, are you in a position to capture that data? If so, how do you determine whether it is valuable? Is it potentially time-sensitive? Can you use insights about the data to adjust business processes or improve customer relations? By employing text analytics, you can help answer questions such as these using a wealth of data that does not initially fit into a tidy, structured format.  Getting unstructured data into a format that is more effective for analysis is a critical first step toward finding value that could help your business. It is no longer difficult or costly to retain large volumes of text data, and it is no longer time-intensive to analyze all of the text that you encounter from customers, prospects, or other sources. The ability to decipher and classify text rapidly might even provide tangible results by helping to prevent customer churn, cut costs, or add additional revenue.  Text analytics helps build more structure and metadata around initially unstructured text. By adding additional structure, it is possible to derive further value.  What follows are three simple ways to add structure to what starts as a string of text. These solutions provide the building blocks for more detailed analysis, classification, and automation later on. In a future post, we will consider how these three basic techniques to bring structure to text data can be easily employed in a generic analytics solution, but for now, let’s lay the foundation. 1. Isolate Key Words­ Whether you want to analyze a short phrase or an entire document, individual words make up the structure of your text. In the context of natural language processing or text analytics, you may also encounter the term token or hear about tokenizing your data. For a simple definition, think of tokens as words. Tokenizing is simply the process of splitting a body of text into individual words. From a language as well as an analytical standpoint, some words carry more importance than others, and it is helpful to isolate not only individual words but determine those that act as key words. More advanced classification has to start somewhere, and finding the key words or phrases within sentences puts you on the path to discovering more through subsequent techniques such as word matching, frequency counts, and other types of analysis. 2. Determine Topics Another way to add structure to your text involves categorizing it by its subject matter. Depending on your data source, you may already know the general content. For example, you might be able to assume that a product review or targeted survey contains opinions pertaining to that product or survey topic. In many cases, however, the subject of a customer interaction may be a mystery until it is read. For instance, contact through social media could be particularly vague when a user tweets about your company. Until you review the text, you might not know why the customer is contacting you or whether it is a compliment, a complaint, or something else. Once you have a topic or topics, you can better categorize your data for storage or analysis. 3. Measure Sentiment Another common way to add value to your text involves gauging the tone. Sentiment analysis has been popular and widely accessible for a few years, and there are many solutions for measuring sentiment. Outputs will sometimes appear as a predetermined classification such as positive or negative, but if possible, it is ideal to use raw sentiment scores in a numeric format. With a score, you can make better comparisons as well as determine what is positive or negative based on your own criteria and assumptions. You can also obtain an understanding of the distribution of sentiment and see if there are any noticeable outliers. When working with sentiment analysis, however, be aware that there are different methodologies that can contribute to widely different numeric scores. Some techniques rely on matching the words in your text to a pre-scored list of words. This lexicon-based approach is fast but heavily depends on the judgment of the word list creators. Other techniques incorporate machine learning and provide scores based on a training data set rather than simple word matches. Scales differ as well, so initially be wary when using a sentiment score without first knowing more about it. A score of “1” might be considered highly positive using a technique that only provides a range from 0 to 1. A separate method might provide scores ranging from 0 to 5, and that 1 suddenly is not as positive. For any solution, consistently measuring sentiment is key. Getting Started Fortunately, you do not have to be an expert in natural language processing to add more structure around your text data. One tool that we have employed at BlueGranite is Microsoft’s Text Analytics service, which is part of their suite of intelligence application programming interfaces (APIs) called Cognitive Services. Not all of the APIs are valuable for a data and analytics project, but some of the language-oriented APIs can be easily leveraged as part of an analytics solution. Using Microsoft’s Text Analytics service, any developer can send text to the API and receive a list of key phrases, topics, and sentiment in return. In addition, we utilize technologies such as R, Azure Machine Learning, and Azure HDInsight for text analytics. Regardless of the tools used to help with text analytics, the goal is greater insight into what was previously indecipherable – at least without an abundance of manual work. Depending on your data source, you might have incidental details about your text, such as what date and time it was written, the author’s location, and other attributes. Even with those attributes, you miss out on what the text ultimately says and the true value it might have. A basic understanding of your data through text analytics provides the key to unlocking that value. To learn more about what BlueGranite can do with your text data, contact us today!"
"245" "With the recent announcement of ArcGIS Maps for Power BI and the collaboration between Esri and Microsoft, more advanced mapping capabilities are coming to Power BI. Microsoft has partnered with a leader in the GIS industry and will bring state-of-the-art geospatial analysis to its users this Fall. For any Power BI user or developer, these new capabilities are something to be excited about. They also shore up a key area where Power BI had some noteworthy feature gaps with competitors.  Crucial Mapping Needs While there are many existing strengths in Power BI, the ability to build appealing maps has been elusive until now. My first encounter with maps in Power BI occurred in early 2015 before the product became generally available. I wanted to take a map of Chicago that I had built in Tableau and rebuild it in Power BI. At the time, Power BI maps did not offer demographic underlays, different background layers, and a host of other functionality that Tableau had. Over time, some workarounds or alternatives appeared using custom visuals or R visuals that were simply not as functional or convenient to use. Compared to a competitor like Tableau, the gap in mapping capabilities when using or demoing Power BI was often awkward.  The diagram below shows a Tableau map (version 8.3 at the time) on the left with a current Power BI map on the right.  The NEW Power BI + ArcGIS Maps Solution With the coming ArcGIS Maps for Power BI, that gap is now closed. There are some features that the Power BI maps will have that Tableau does not and vice versa, but incorporating ArcGIS Maps is a colossal step forward for Power BI.  Take a look at how a map of Chicago will now appear in Power BI with ArcGIS Maps.   With functionality like clustering* and heat maps native to Power BI as well as conveniences like selecting data based on shapes in your reference layer, it would not be bold to say that Tableau will have some catching up to do. The main point, however, is that you can now make comparable and visually appealing maps in both products that help satisfy numerous customer requests. Here is a short list of what ArcGIS Maps for Power BI will offer when it becomes available this fall: Four basemaps including a dark and light canvas Compared to the current multi-colored map background, minimizing streets and having an appealing base layer allows you to place more focus on your data rather than worry about visual distractions.  Map themes including size, size and color, heat map and clustering Traditional map styles are enhanced by the ability to add heat maps as well as the option to aggregate individual points into larger circular \"clusters\" as you zoom out.  Optional reference layers that can appear under your data that offer a large variety of current 2016 demographic estimates It appears that there are only United States demographic layers such as income, population, and a few more native ones. You will have the ability to search ArcGIS for additional layers though.  Increased formatting options for size, color, transparency, and more As one example, the current map offers no control over minimum bubble size, which often leads to overlap when there are dense clusters of points.  For more detail, read about ArcGIS Maps for Power BI from Esri and Microsoft. Finally, thank you to both the Microsoft and Esri teams that helped put this collaboration together. A significant gap between Power BI and competing software has been addressed, and there is a bright future for plotting location data in Power BI. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more. ---------------------------------------------- *Tableau recently released a great feature called “clustering” in version 10. This feature allows users to use its algorithm to automatically group—or cluster—data. This is distinct from geospatial clustering, which aggregates points into a more visible group as you zoom out on a map.  "
"246" "Many companies are currently working to transform their traditional data warehouse systems into modern data architectures that address the challenges of today's data landscape. These innovative systems are designed to give companies a competitive edge. With huge amounts of historical, operational, and real-time data, combined with the new and ever-improving tools to analyze, model, and mine data, businesses have a lot of power at their fingertips. Access to that data is helping forward-thinking companies find ways to outperform and out-innovate their competition. An Analytics Sandbox is one of the tools that’s helping them succeed.  The amount of time that it takes a company to turn their data into knowledge is critical. Traditional enterprise data warehouse (EDW) and business intelligence (BI) processes can sometimes be slow to implement and do not always meet the rapidly changing needs of today’s businesses. When efforts made to speed up delivery cycles have limited success, businesses may take things into their own hands. This promotes the propagation of spread-marts and poorly built data solutions. This is where the concept of the Analytics Sandbox comes in. What is an Analytics Sandbox? An Analytics Sandbox is a separate environment that is part of the overall data lake architecture, meaning that it is a centralized environment meant to be used by multiple users and is maintained with the support of IT.  Here are some key characteristics of a modern Analytics Sandbox: Key Characteristics The environment is controlled by the analyst Allows them to install and use the data tools of their choice Allows them to manage the scheduling and processing of the data assets  Enables analysts to explore and experiment with internal and external data  Can hold and process large amounts of data efficiently from many different data sources; big data (unstructured), transactional data (structured), web data, social media data, documents, etc. The concept of an Analytics Sandbox has been around for a long time. Data warehousing pioneer Bill Inmon and industry expert Claudia Imhoff have been evangelizing about the idea since the late 1990s, although the co-authors referred to it then as “Exploration Warehousing” in their 2000 book by the same name. They even include the concept on many of their well-known Corporate Information Factory diagrams (see the yellow database objects).   Unlike Inmon and Imhoff's Exploration Warehouse though, which only got data from the EDW, a modern Analytics Sandbox will commonly pull data from all layers of the data lake. It acts mainly as a playground for data scientists to conduct data experiments. As shown in the Modern Data Architecture, it resides in the lower levels of the data lake because it consumes a lot of raw/non-curated data. It may even end up feeding the EDW at some point.  Advantages of an Analytics Sandbox There are many advantages to having an Analytics Sandbox as part of your data architecture. Perhaps most significant is that it decreases the amount of time that it takes a business to gain knowledge and insight from their data. It does this by providing an on-demand/always ready environment that allows analysts to quickly dive into and process large amounts of data and prototype their solutions without kicking off a big BI project. In other words, it enables agile BI by empowering your advanced users. Another major benefit to the business and IT team is that by giving the business a place to prototype their data solutions it allows the business to figure what they want on their own without involving IT. When they decide that a solution is adding business value, it becomes a good candidate for something that should be productionized and built into the EDW process at some point. This saves both teams a lot of time and effort. Could your business benefit from having an Analytics Sandbox? Interested in learning more?   Please contact us today."
"247" "In Part I of this 2-part series, The Hammers, I made the case that attitude and aptitude are a business intelligence practitioner’s most critical tools. I’ve heard many folks suggest more technical skills (T-SQL, data visualization), as well as technical business skills (data modeling, business value/ROI calculations) as the hammer – or tool that becomes synonymous with the profession – of BI. And while technical skills are certainly critical, the pace of change in this space is just too rapid for any one technical skill to top the list. I’m intentionally omitting technology specific skills in the interest of making this more universal (across technical tool sets) as well as less given to the ceaseless march of time (every time you blink, a new technology comes out that changes how we interact with data!)  In addition to the right attitude and aptitude to be a successful BI practitioner, there are a handful of other critical skills that will make life easier, and help set up a candidate for success. These range from the fairly basic and teachable (effective, professional written and oral communication) to the more abstract and difficult to assess (the ability to tell stories with data). Here are some valuable skills that BlueGranite looks for and continues to develop: Effective Communication – Effective business intelligence practice requires equal parts successful communication, translation and technical implementation. In order to succeed, one must be able to really listen, identify both overt and covert messages, and address these interactions with effective oral and written communication. The impact here goes beyond what common sense might supply – effective communication is likely more important than technical skill (though not by much). Performance within a Deadline / Minimally Viable Solution – If you are not familiar with the Minimally Viable Solution or Product (a different take on MVP), it bears deeper understanding. The nature of BI projects requires flexibility and fluidity, and truly must embrace agile methodologies by allowing for continuous (or at least frequent) feedback from stakeholders, and working towards the MVS/MVPs allows for tighter development loops and a greater chance of ensuring that VALUE has been delivered within the time frame specified. Will it have every bell and whistle? Nope. But will it help ensure you don’t miss deadlines, foster tight, engaged feedback loops and prevent wasted time in features that sound good during requirements gathering, but don’t really deliver on the promise? More often than not. Attention to Detail – Little things can make big differences, and given the nature of BI – essentially providing greater access and visibility into data and its integrations and correlations – these can really make or break the success of a business intelligence initiative. It is often small details in conversations or existing assets (reports, Excel workbooks, etc.) that help us identify the bits of encapsulated business logic that need to be considered in order to move forward. There are often things left unsaid that need to bubble to the top and expedite appropriate follow ups or research. The basics of attention to detail can be taught, but the nuance only comes with time and experience. Analytical Feasibility – In other words, we can’t avoid the elegant truism of GIGO, aka garbage in garbage out. A BI practitioner needs to constantly assess the analytical feasibility of their current stakeholder’s expectations. Successful BI leads to more questions and increased appetite for consumable data. But because the visibility and appetite is new to the stakeholder, many associated domains have not had fully developed data quality assessments or programs associated with them. If the data hasn’t been accurately captured at the right grain, no amount of desire for a downstream metric is going to solve the problem. In most cases, we can take this new-found appreciation for data and do a better job collecting consistent, accurate data moving forward. In a few, rare cases, we can put in some significant effort to rebuild a portion of history with the right amount of data to compute a metric, but this is almost always more work than the stakeholders anticipate. And there you have it. At BlueGranite, these are the types of tools we look for, support and foster in our consultants. While not every team member will pull top marks in every discipline or attribute, they represent a common pattern that denotes the right skills for a team to consistently deliver successful data and analytics projects. Our team members are the drivers of BlueGranite’s success, and our recipe can work for BI teams across different organizations. Of course, there is also the small issue of having the right amount of skill with the selected technologies (in our case those include SQL Server, Power BI, Hadoop, R, Azure, etc.), but those topics are explored in other articles. Interested in learning more?  Please contact us today!  "
"248" "Master data management (MDM), the creation of a single reference point for the whole of a company’s critical information, is increasingly becoming a necessity. Master data governance (MDG) helps ensure the safety of those assets across the organization. While master data management may not be the most exciting of data management solution principles, when coupled with master data governance it can create major business value for an organization Microsoft’s Master Data Services (MDS) in SQL Server 2016 can be an excellent technology platform to implement MDM and MDG solutions. Originally released with Microsoft SQL Server 2008 R2, MDS has come a long way over the years. We recently finished a project with an enterprise retail customer who was evaluating MDS as a platform and home for master and reference data needing migration from a legacy mainframe environment.  We evaluated MDS across several different candidate data models and here is what we found. What Worked Rapid data model creation and data loading We were able to quickly create data models and load data using several different methods including batch loads via Extract, Transform and Load (ETL), bulk imports through the Excel add-in, and singular imports through the application interface. Data integrity enforceable using multiple mechanisms There are many ways to enforce data integrity in the models.  We were able to use a combination of data types, domain-based attributes and filters, synched entities, and a robust business-rules matrix to enforce data integrity.  Notification emails were sent to model administrators when the data was out of line. Ability to track data history in the application and easily view it MDS provides the capability to see the history for all members of any entity within models.  All history is stored by default, but you can configure MDS to purge history after a configurable window to enhance performance of large models.  Robust security framework We were able to work in multiple roles including those of Data Owners, Data Stewards, and Data Subscribers.  For each role, we were able to specify distinct permission sets on models, entities, attributes, and even member slices (only able to see data in a certain department as an example).  Price!  The price is fantastic – it’s FREE when using MDS in an existing SQL Server 2016 Enterprise Edition instance.  You’ll want to analyze that the extra overhead of MDS on your servers isn’t going to push it over the edge. What Was Clunky Notifications can be overwhelming While notification emails from MDS is a great feature, many groups found the volume of emails overwhelming, especially in DEV or QA where the Data Stewards are working through the data, cleaning it up, and executing new and existing business rules repeatedly.  We found that it was best to limit the audience of notifications to those who really need to take action on the data.   Approvals feel awkward Approvals via changesets are a new feature of MDS in SQL Server 2016.  The capability allows the creation of a light workflow approval process within MDS.  The feature is much appreciated and of value, but it definitely feels a little awkward to work through and it takes some getting used to.  We’ll spare you the finer details, but we ultimately decided to use these features in certain scenarios and appreciated them once we mastered them. What’s Missing Going into this proof of concept, we informed the retailer that Master Data Services lacked certain capabilities regarding data quality integration, golden records management and advanced workflow creation, as we detail below: Truly Integrated Data Quality The ability to perform data cleansing, address, email, and contact verification via built-in web service connectors right in the MDM console – not via another application.  Golden Records Management The capability to match, de-duplicate, and harmonize records from multiple source applications with MDM to create a single truth to your master data.  Advanced Workflows The ability to create advanced, multi-step, and/or sophisticated workflows that incorporate multiple actors in order to complete data entry, update, or cleansing operations.  Microsoft does offer some of these functions through its Data Quality Services (DQS) product.  While some of DQS’s capabilities are integrated into Master Data Services, some are standalone.  At BlueGranite we recommend evaluating Profisee when a customer needs a full-featured master data management solution.  Profisee is a bolt-on MDM solution that leverages MDS as a backend platform and it provides a very rich and fully capable MDM, data quality, and workflow capability. MDS was a success! Overall, the evaluation successfully proved SQL Server 2016 Master Data Services’ business value and the company should be moving forward with implementation.  This customer is aware of what MDS is missing but, for the data models in scope, the evaluation proved that the missing capabilities weren’t an issue.  With the releases of SQL Server 2016, Master Data Services is better than ever.  There are several new features that make MDS worth reevaluating or even implementing if you haven’t looked in a while.  Some of the new features are: Advanced hierarchies including Recursive and Many-to-Many hierarchies Domain Based Filtering – the ability to have chained domain-based attributes Entity synching for sharing entities between models Approval workflows via a new feature called “changesets” Improved user interface (UI) and overall application performance – including redesigned UIs, data compressions, enhanced logging options, purging soft deletes, and faster data imports More robust security capabilities Business rules UI is simpler and easier to use Ability to create custom indexes We’d love to help you evaluate whether Master Data Services can benefit your organization. Contact us and we'll help you get started with your own evaluation of SQL Server MDS for your enterprise master data management needs.  "
"249" "If you’re a carpenter, there are tools you likely use on every job, for every task. At its most basic, your toolkit probably holds a couple of screwdrivers, a saw, a tape measure, and always, a hammer. That essential tool is never far from your hand, and it’s almost synonymous with carpentry. Well, successful business intelligence has a similar set of absolutely crucial tools; tools you will lean on every day of every project, and in nearly every task within those projects. At first pass you might say ‘writing SQL queries’ or ‘creating compelling visuals’, but I’d argue that it’s something a little more fundamental.  I contend that attitude and aptitude are the hammers of successful BI practitioners. As an executive responsible for building out BlueGranite’s world-class team of data and analytics consultants, I’m constantly balancing our need to attract and retain senior talent, with that of developing and training new consultants. As the demand for data and analytics solutions skyrockets, many organizations are facing similar challenges – whether they are working with new recruits engaged to lead up BI initiatives, or current team members (e.g., developers, DBAs, business analysts) transitioning to this innovative, exciting field. At BlueGranite we designed a Junior Consultant program to help our new recruits deliver value across a wide spectrum of data and analytics projects. The planning process for the program really drove home that attitude and aptitude are essential to, or the hammers of, business intelligence success. Here’s why: First, let’s start with attitude. Successful business intelligence has always been a mix of sharp business acumen, communication, and technical know-how. This means that successful candidates for these roles need to stand out in thought and action. Stereotypical ‘business users’ and ‘IT staff’ both have their fair share of surface area to be skewered in Dilbert, and a successful BI practitioner should leave that way behind.  Perhaps no two qualities are more important than curiosity and flexibility, and both of those qualities represent a dynamic attitude. Straddling the space between the IT process and the dynamic demands of the business, we must be aware that we are working in an area not dictated by standard operating procedures, guidelines and best practices. And while the industry does, in fact, have best practices, much of the skill involved in successful BI projects is knowing how and when to deviate from those practices in order to deliver value. BI practitioners are not afraid to try something new – whether it’s a new technology, technique or project approach – and must be willing to continuously learn from mistakes, as well as critically assess successes. This challenging environment will frustrate or burn out those who don’t approach it with the right attitude. This magnifies the importance of aptitude. In addition to having the right attitude, and a desire to try new things and learn, successful BI practitioners must have the aptitude to actually DO the learning, and to do it quickly. This includes understanding business drivers and patterns, as well as technical solutions and features. A BI expert may be supporting a marketing team, and need to quickly understand the process and flow of the team’s social media and inbound marketing initiatives, and then rapidly put these concepts into the context of new data visualization technology that itself has new features and techniques to apply. That same BI expert may be assigned to help drive data modeling in the finance department next, and must rapidly pivot to grasp the nuts and bolts of how financial data is recorded and accessed, and how to help provision it for the data-hungry audience who already live and breathe in Excel. In short, being quick on your feet and capable of rapidly adding to both domain knowledge and technical capability is a requirement, and for many folks, this can be off-putting. Professionals who build a wealth of knowledge in one area and build success as an expert (either domain or technical) can feel frustrated by this demand, and may look to move into an IT or business-labeled position. But those whom are equipped with the right attitude and demonstrate a sharp, innovative aptitude are well their way to success in the business intelligence world. Stay tuned for our next installment, Part II: The Supporting Cast, where I’ll dig in to a series of slightly less sweeping but no less important skills that are crucial for a world-class business intelligence practitioner. Interested in learning more?   Please contact us today."
"250" "These days, gaining customers, keeping them and attracting new ones often requires more than a great product. The most successful organizations use customer intelligence to forge relationships, hone offerings and improve decision making. Increasingly they are turning to social media, online comments, call logs, product reviews and blog posts to glean insight – all nuggets in a goldmine of potentially useful customer information. Traditionally, analyzing such data sources was costly, difficult and time-consuming. The once-manual process of reading through these different mediums also left a wide margin for human error. Enter text analytics, a machine learning activity using big data that makes extracting value out of these virtual mountains of unstructured data possible for everyone. Until recently, it was solely the realm of specialized marketing service providers.   Text analytics can turn a mass of words into structured content – think data tables and rows – that can then be studied in multiple ways, including to evaluate customer feedback, recommend the right product, optimize marketing and even tweak manufacturing. If you can capture it, you can analyze it. And if you can analyze it, you can use it to your advantage. One of the easiest ways to do this is through Microsoft’s enterprise-grade cloud computing platform, Azure. The comprehensive, managed-services solution comprises all of the components needed for text analytics, including information management and storage, intelligence services and machine learning, and reporting. We’ve created a brief demonstration showing some of the text analytics capabilities of Azure cloud services with Cortana Intelligence, Microsoft’s big data and advanced analytics suite. You’ll see an example of a customer intelligence report on Microsoft Power BI, a self-service analytics system that includes a web publishing component allowing sharing via multiple mediums – including phone, web browsers, tablets, desktops and laptops:                   Questions about Text Analytics in Azure?                     As you can see in the video above, Azure has simplified the once-difficult and specialized task of gauging customer sentiment, making the technology accessible to all. Interested in exploring how text analytics can benefit your organization? Let us help! Contact BlueGranite today. To see multiple examples of Azure text analytics solutions or to learn to use the platform to create solutions, including recommended architectures, please view our free recorded webinar “Overview of Text Analytics with Hadoop, R, and Microsoft Azure” now."
"251" "Power BI is able to connect to all sorts of different data sources (and more are being added every month), and as my colleague Josh Fennessy pointed out in a recent blog post, there's no shortage on different design patterns for implementing a big data cloud solution with HDInsight.  This can make figuring out the best way to visualize your big data sources from HDInsight using Power BI a little challenging.  Below, I will try to outline some common approaches to using Power BI with HDInsight.  Refresh from Hive Tables/Queries  If you are using a Hadoop cluster in HDInsight, one way you might use Power BI to connect to your data is with Hive tables.  Hive provides a logical layer for you to extract the data from.  Using the Microsoft Hive ODBC Driver, you can import entire Hive tables into Power BI or write Hive queries to import data directly into Power BI.    Importing from Hive tables/queries provides a familiar SQL-like feel and syntax, and you are able to use the whole gamut of modeling capabilities within Power BI Desktop.  It is important to understand, however, that data refreshes with this method can often be slow as a Hive job will be executed on your cluster before transferring the data.  Being that this method imports data directly into the Power BI model, you may also be limited in the size of the data that you can work with, as the model will have to fit into the memory of your machine and/or the file size limits of the Power BI service.  Your cluster will also need to be up and running at the time of a Power BI refresh so that Power BI can see/access any Hive objects. Refresh from Flat Files  Another way to import data from HDInsight into Power BI is by connecting to flat files in either Blob or the Data Lake Store.  In this scenario, you would use HDInsight to process your data and write the resulting curated or aggregated data into text files (CSV, TAB, etc.)  Unlike refreshing from Hive tables, refreshing from flat files would allow you to only run your HDInsight cluster while processing data (deleting a cluster when it is not active can help save money in Azure consumption) as the resulting text files would still exist in Azure storage even when the cluster is deleted.    Data refreshes into Power BI are also likely to be faster when pulling directly from flat files versus using the Hive ODBC Driver referenced above.  You will still run into the same model size restrictions as when using Hive tables/queries, as you are still importing data directly into a Power BI model. The previous two methods for using Power BI with HDInsight required importing data into a Power BI model, thus limiting the size of your model to the constraints of your computer or the Power BI Service. The next two methods will utilize DirectQuery in which your data will stay at the data source, thus removing the model size limitations discussed before. DirectQuery with Spark  The first method is connecting directly to tables in a Spark cluster.  With this method, you will process your data in Spark and then put the resulting data into tables on the cluster. Power BI can then use Spark SQL to interactively query the tables in Power BI's DirectQuery mode.  Again, the key advantage here is that the data stays at the source, which removes the need to schedule Power BI refreshes and worry about Power BI model size.  A downside to this approach is that in order for Power BI to connect to a table in Spark, the cluster must be running, which likely means the cluster is on all the time (depending on when your users will be using the reports/model).  This can sometimes be more expensive than only turning your cluster on for processing data and deleting it when you are done (as with the flat files method).   DirectQuery from Azure SQL DB  The final method we will look at is DirectQuery using Azure SQL Database (DB).  In this approach, similar to the flat files approach, you would process your data in your cluster, but write the resulting curated and/or aggregated data to tables in Azure SQL DB (or Azure SQL Data Warehouse).    As with the flat files approach, this allows you to delete your cluster when you are done processing data (which helps control costs) as the processed data will still reside in the SQL DB when the cluster is gone.  However, unlike the flat files approach, Azure SQL DB can be highly optimized and is well suited for DirectQuery in Power BI.  This means that you will not have to worry about data refreshes at the Power BI level.  When connected to Azure SQL DB with Power BI using DirectQuery, you also still have a lot of additional modeling capabilities included in Power BI, like creating new measures, calculated columns, and relationships between tables.  The downside to this approach is that you will need to set up and configure an Azure SQL DB service, and that service will likely need to run all the time (depending on when your users will be using the reports/model).  Also, while DirectQuery virtually removes any limitations on the Power BI model size, you still have to be concious of the size, performance, and cost of your Azure SQL DB instance. While there are likely other ways to work with HDInsight using Power BI, these are some of the more common scenarios we have run into and deployed.  Hopefully this helps in planning your architecture for visualizing your big data in the cloud. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"252" "Biml is the operating system for business intelligence. It is a domain specific language that describes BI artifacts: tables, SSAS databases, ETL, and more. The XML tag system provides for the expressiveness of the language, but the true power comes from the native ability to integrate .NET libraries in the generation of Biml objects. This is called BimlScript. With just a few lines of .NET code, C# or VB.NET, you can connect to a database, web service or Excel workbook and use that metadata to drive the generation of your BI artifacts. As an architect, I find Biml indispensable for delivering consistent, repeatable, cost-effective BI solutions.  Consistency The wealth of features available in BimlScript for solving problems within the Microsoft Business Intelligence space is staggering. Just looking at SSIS, there are many different ways to solve the same ETL problem. As a simple example, how many times have you seen one person use an OLE DB connection manager and another use an ADO.NET connection manager for the same database? One person uses the OLE DB Command object for updates while another pushes changes to a staging table and uses the MERGE statement.   While there is no wrong way to eat a Reese's, there are certainly better ways to solve problems within the SSIS space. We've already talked about why Good Data Integration Patterns Are Good For Business.  Organizations can quantify the cost of a new project but rarely do they look at the cost of on-going maintenance and support. A three-month development project might require care and feeding for five to seven years, with maintenance costs far exceeding the original project cost. If the maintenance and support developers have to spend time getting up to speed because each SSIS package is a unique snowflake, that is going to have a severe impact on their efficiency. How many versions of SQL Server do you need to support? The same Biml code that generates a SQL Server 2016 SSIS package can be used to generate a SQL Server 2005 package. Until the 2016 release of SQL Server, you might have had 4 different SSIS projects with similar packages, each of which you had to open in a different version of BIDS/SSDT. In these situations, we tend to leave older versions behind as we update logging procedures and implement better integration patterns on newer systems. To address all of that, it is our duty as architects to ensure we deliver consistent and efficient solutions. Biml is one way to ensure this happens. Instead of handing a junior developer a thick tome of all your best practices, codify it with Biml. This is my pattern for a Type 2 Slowly Changing Dimension, here is how we handle unknown members. You simplify the developer's life as they only need to ensure they have the correct metadata assembled and your project is successful because you know what your team is delivering before they begin coding. Metadata driven approach Metadata is data that describes other data. Within your organization, you already have implicit metadata repositories and you likely have explicit ones as well. Do you have a relational database? Your RDBMS exposes functions and views that allow consumers to know what catalogs, schemas, tables, views, and relationships exist. Further, they expose the columns, data types, and nullability of those fields. That’s a rich metadata store ripe for consumption. If you use extended properties on your databases, tables and columns, now you've got a hybrid metadata store. What about explicit repositories? In organizations that still use the waterfall methodology, it is common to have detailed lineage describing each data element. Do you have a Kimball Bus matrix? In data warehouse projects, often there is an Excel spreadsheet with source and target mappings. Be honest, how often does that document get touched after the first draft? In many organizations, it becomes stale once the electrons have dried. Change that behavior, make the metadata work for you! Instead of updating that document as one of the last things to happen in a project, use the metadata defined in the various source systems as well as the spreadsheets to drive the creation of your BI artifacts. Biml allows you to take a documentation driven approach toward your development. Use five to ten lines of .NET code to drive the construction of BI artifacts based on the definitions within the metadata store. That's powerful. As a simple example, we needed to export a few hundred tables for a PolyBase proof of concept. We defined a database connection, used the GetDatabaseSchema method in Biml to generate an in-memory model of the database, and then created an SSIS package for each object that met our criteria. That took about 30 lines of Biml to define the pattern and 10 lines of .NET code to drive the automation, which was about an hour's worth of work to create all those SSIS packages. Cost effective There are always limiting factors in projects but the biggest two are time and money. The Biml and BimlScript features you need for an effective ETL framework are free to download and free to use. All you need is one of the free Visual Studio/SQL Server Data Tools add-ons, BIDS Helper or BimlExpress, and you'll be up and running with your Biml development. Because the solutions you're going to build are consistent and they're built on metadata, the cost of developing solutions is going to decrease because you're using established and tested patterns against accurate representations of sources. But what if time is of the essence? Biml can still help you. There are advanced features in the paid product that allow you to get up and running quickly. If you already have patterns that you want to use as a template, you can reverse engineer an existing SSIS package into Biml and go from there. Adopting this approach not only improves development speed and consistency, but we take on less technical debt. And because we can make updates to multiple packages at once, we can more easily relieve technical debt as we fix bugs and find ways to improve our solutions. Biml at BlueGranite We have used Biml as an accelerator for delivery at a number of clients across different industries. We are an official Varigence Consulting Partner and would love to talk to you about how you can leverage Biml to become more effective in your delivery of Business Intelligence. Like what you read and want to know more? Sign up for our Biml webinar on September 8. Other Biml Posts and Resources https://www.blue-granite.com/blog/good-data-integration-design-patterns-are-good-for-business https://datasavvy.me/2016/06/30/bimlscript-get-to-know-your-control-nuggets http://billfellows.blogspot.com/2016/08/biml-reverse-engineer-database.html https://varigence.com/Biml  "
"253" "There is no shortage of choices when it comes to designing an architecture for HDInsight projects.  There are multiple cluster types, multiple operating systems, multiple storage options, deployable applications -- it's a maze of choices that can often be overwhelming.  In this article, I'll provide clarity into the decisions that go into designing the appropriate HDInsight architecture.  First, let's cover some HDInsight basics. Is HDInsight that much different from designing an on-premises cluster? Whether your Hadoop cluster is on-premises or in the cloud, it contains two main resources: compute resources to process jobs, and storage resources to hold data.  In an on-premises cluster, the storage and compute resources are combined into the same hardware tying them together.  With HDInsight the storage is wholly separated from the compute resource.  This is a very important distinction of HDInsight.  It means that I can completely turn off the compute portion of the cluster and the data will remain accessible.  Because of this major distinction, it allows me to architect HDInsight solutions much differently than those designed with traditional on-premises Hadoop clusters. With on-premises clusters, I have to design them based on the amount of data that is planned to be stored, processed, and consumed during normal usage.  With HDInsight, however, I design the environment based on the usage of the cluster.  Additionally, I can schedule the HDInsight cluster compute resource to only be available during the time that scheduled jobs need to execute.  Since HDInsight clusters are primarily designed for the type of compute usage that is needed, it's common practice to create multiple compute clusters to meet the needs of different jobs.  The next decision point is knowing what type of cluster to create.  With multiple clusters, I can cater the architecture and design to match the exact requirements of the jobs that are going to be run. What is the right type of HDInsight cluster to create? HDInsight supports 4 main types of workloads:  Workload   HDInsight Cluster Type   ETL/ELT   Hadoop   Data in Motion / IoT   Storm   Transactional Processing   HBase   Data Science / Advanced Analytics   Spark -or- R Server with Spark    Because HDInsight is a platform-as-a-service offering, and the compute is segregated from the data, I can modify the choice for the cluster type at any time.  Multiple clusters connected to the same data source is also a supported configuration. A typical project has the following sample processing requirements: Several hours of processing each night to prepare data for daily reporting Additional hours of processing either weekly or monthly to close fiscal cycles Development environments for analysts to build / test statistical models An on-premises cluster would have to grow pretty large in order to encompass all of these use cases, but with HDInsight I can create multiple transient clusters to meet the business requirements.  Here is a visual example of how HDInsight clusters can be managed to control the compute costs incurred by the platform.  When I design HDInsight projects, I don't worry about building a single cluster that meets every need.  I take a look at what's needed by the job requirements, how long the jobs will run, and then design a right-sized compute cluster that will complete that job.  Other jobs may have different requirements, and therefore may require a different cluster type and compute size. What about data storage? Above, I explained how data and compute are physically separated in HDInsight, and that wasn't an exaggeration.  When I create an HDInsight cluster, I also specify one or more Azure Blob Storage accounts to store data that the cluster will access. Azure Storage accounts are the default storage location for data processed by HDInsight. Azure Data Lake Store (ADLS) is a new storage offering from Microsoft that is another option for storing data.  ADLS is fully distributed, and like Azure Storage, ADLS keeps your data separated from compute, and allows for data access whether the cluster is running or not.  Major benefits that ADLS has over Azure Storage Blobs include: True distributed file system optimized for parallel processing jobs Security model integrated with Azure Active Directory No file size or account storage limits In both cases, multiple clusters can reference the same storage, so data can easily be shared between processing units and business teams.  Right now, ADLS is a preview technology.  This means that you’ll need to use Azure Blob Storage for your default cluster storage. At BlueGranite, we’ve done significant testing with ADLS and are excited about its performance and security model. When it graduates to General Availability (GA) it will be our recommended storage platform of choice for Big Data projects with HDInsight. Now for the big question, Windows or Linux? Linux. Snarky answers aside, at BlueGranite we support the idea of deploying Hadoop on Linux, especially for HDInsight. Linux is more widely supported in the Hadoop community and since HDInsight is based directly on Hortonworks HDP, it makes sense to deploy clusters on the most widely supported operating systems.  Other benefits of running HDInsight on Linux include: Full support of Hadoop user interfaces including Ambari and Jupyter Direct remote access into cluster nodes via SSH -- Windows-based HDInsight clusters are limited to pre-scheduled Remote Desktop into the master head node only Access to advanced features of HDInsight including Spark and R Server Updates to HDInsight are also delivered first to Linux, followed several weeks later by updates to Windows-based clusters.  With a faster update cycle, clusters running on Linux will be more secure, and more current with the latest patches, and features. So, how do I pick the right HDInsight cluster? Designing an HDInsight environment is very different from designing an on-premises Hadoop environment.  The best cluster configuration may very well be multiple compute clusters, running at different times, designed to handle different workloads.  The separation of compute and data is key to this architecture functioning as well as it does.  Keeping the data in a separate location from the compute cluster opens up a number of possibilities not available with on-premises hardware. When choosing the cluster type, keep in mind the targeted workloads for each configuration, and remember to strongly consider Linux as your choice of OS, even if your team doesn't have Linux support.  HDInsight is a platform-based offering, so it doesn't have heavy-handed operating system administration requirements. While you'll have to store your data in Azure Storage for now, keep a close eye on Azure Data Lake Storage. The POSIX-compatible filesystem and distributed nature will be very powerful in the future as it matures into general availability. If you have questions or need assistance with an HDInsight or advanced analytics project, please contact BlueGranite."
"254" "Many business decisions need to be made with incomplete information, and require managers to plan for uncertain outcomes. If implementing a new inventory policy has the promise of reducing holding costs across the enterprise by $10 million over the next 2 years, what’s the chance the savings will be exactly $10 million? Well, it’s about zero. Would a manager’s decision to move forward with the initiative change if they were told there’s an 80 percent chance of saving more than $10 million? What about only a 50 percent chance? How about 10 percent? Incorporating measures of uncertainty can be tremendously helpful for decision makers. Unfortunately, many analytic techniques produce very deterministic, or single point, estimates (e.g., “Our model predicts exactly $10 million in savings!”). In a recent project, we applied fuzzy logic to a common machine learning technique – clustering – to provide probabilistic information for inventory management.   For many organizations, inventory management policies can vary by groups of products that are similar. This could mean they come from the same distribution center, have similar sales patterns, require the same cost or effort to produce, etc. But across many product attributes, it can be challenging to define what makes products “similar.” Applying an inappropriate policy, as might happen when associating an item with the wrong group, can be tremendously expensive, especially when inventory or stock-out costs are high. The machine learning task of clustering is a common way to find hidden structures in data, namely which data points are more closely related to certain points than others – or groups. Examples in industry include customer segmentation for targeted marketing, image processing and recognition, and patterns of gene expression in bioinformatics. The objective in this project was to use clustering to help identify groups of similar products across several identifying characteristics, and fuzzy logic to pinpoint items that may not currently belong strongly to any one particular group. This was done in support of a supply chain management group that wished to get deeper insight into product relationships for inventory policies. By having a prioritized list of items that do not strongly associate with one particular group, managers can easily keep an eye on a subset of products that may hold higher potential savings, or require a hybrid management policy. To illustrate an application of fuzzy clustering, we created an example using the classic Iris data set and performed analysis using the R statistical programming language. The Iris data consists of 150 observations, and 4 features. In a supply chain context, these might represent average periodic sales volume, holding cost, stock-out cost, and shelf life. We’ll set the number of clusters (K) to 3. The values have been normalized since they are on different measurement scales. The R code is here.  Figure 1 shows the original values in 2-dimensional principal component space (a technique to represent multiple attributes in a compact number of dimensions). We can see a clear separation of values along the left side. But what about the products on the right? How can we group them?  Figure 1 Figure 2 illustrates the 3 K-means clusters. Using a distance calculation, K-means performs multiple iterations to calculate the best group for item membership. Using fuzzy K-means, we also have the benefit of understanding the degree, or probability, of cluster membership. Larger dots have a stronger membership; smaller dots have a weaker membership. Notice that dots near the perimeter of clusters are smaller; there is less certainty to which cluster they belong.   Figure 2 Figure 3 adds a fuzzy cluster according to a given membership threshold. In this case, any item in clusters 1 or 3 that does not have at least a 60 percent membership degree in its primary cluster falls into the “fuzzy” category; these are items we want to watch more closely – they may be special cases or likely to move from one cluster to another.   Figure 3 Figure 4 shows the item numbers of the fuzzy cluster and a summary table, where we can more closely inspect the items in the fuzzy cluster as well as all other products according to their cluster membership. Using the fuzzy clustering results, as well as a discovery tool like Power BI, would allow an even deeper dive into the data.   Figure 4 For those of you wondering about the smaller dots on the perimeter of all the clusters, these items have small membership degrees because they may represent outliers. The ‘fclust’ package used in this example also has a function to detect and create an ‘outlier’ cluster for anomaly detection! Fuzzy clustering represents an excellent way to visualize statistical groupings of data and also highlight items of interest that might need special attention. R, a platform designed especially for analytics, makes this easy. For more information on advanced analytics, be sure to check out the helpful resources on our Microsoft R landing page and our webinar on text analytics.   If you have questions or need assistance with an advanced analytics project please contact BlueGranite today."
"255" "HDInsight, Microsoft's open-source, Big Data platform on Microsoft Azure, has come a long way since its 2014 introduction. It was one Azure’s first available platform offerings.  Many of our customers took an early look at the product, and frankly, were underwhelmed.  But multiple improvements since the initial release have not only made HDInsight a great product, it’s now our Big Data processing platform of choice.  Below, we share some of the new features that have pushed HDInsight to the top of our analytic processing list.    HDInsight on Linux  One of the biggest complaints with the first edition of HDInsight was that it was delivered on Windows. This meant that we didn't have access to all of the Hortonworks Data Platform (HDP) features – Ambari included.  Because connectivity to the cluster was limited, Remote Desktop being the only option, development was challenging.  Now that HDInsight is delivered on Linux – as Hadoop should be – Big Data in the cloud is better than ever.  The cluster can be accessed via Ambari in the web browser, or directly via SSH.  Additionally, since HDInsight is based on the Hortonworks Data Platform, it follows HDP's release schedule. Updates to HDP are delivered on Linux first, followed behind by Windows.  Updates to HDInsight are also delivered to Linux first.  Additionally, premium versions of HDInsight that include advanced tools like Apache Spark or R Server are only based on the Linux platform.  Customized Platform Configuration with Script Actions  HDInsight is a platform-as-a-service offering. What does that really mean? It means that the installation, configuration, and administration of the Hadoop platform is not performed by the Azure customer. Microsoft makes sure that the cluster is operational upon deployment and that it stays that way while running.    The cluster is deployed with a predefined configuration, and customizations are not recommended to be made directly on the cluster. Why is this? Cluster nodes can be redeployed at any time while the cluster is running, and any configurations made directly on a node can be lost.  However, we often need to make customizations to the HDInsight cluster. To make those configurations, we use Script Actions. Script Actions are Bash scripts that make modifications to nodes within the cluster. These scripts can target head nodes, worker nodes, zookeeper nodes, or any combination of the three.  Common configurations include:  Installation of new software on the cluster Modification of software configuration Pre-loading data Script Actions can be defined and applied when the cluster is originally created, or at any time while the cluster is up and running.  Better Control Over Cluster Performance  HDInsight has always been an elastic platform for data processing.  Since its introduction, adding new nodes to HDInsight has been an easy process.  In today's platform, it's even more scalable. Not only can nodes be added and removed from a running cluster, but individual node size can be controlled.  We have the ability to define head node virtual machine size and the worker node size. This means that the cluster can be highly optimized to run the specific jobs that are scheduled.  In addition to CPU and Memory specifications, new storage options also allow for more control over the data processing performance of the cluster. HDInsight has always supported Azure Blob Storage, and economical cloud storage platform. For data processing workloads that demand higher performance, HDInsight also supports Azure Data Lake Store, which is optimized for parallel processing workloads.  ADLS is great for parallel processing tasks because data stored there is split into chunks, replicated, and distributed across storage clusters. While the cost for ADLS is higher, it allows for processing customization based on the required performance characteristics of the jobs.  Development Options Abound  In its initial form, there were many options for developing HDInsight processing jobs.  Today, however, there are really great options available that enable developers to build data processing applications in whatever environment they prefer.  For Windows developers, HDInsight has a rich plugin for Visual Studio that supports the creation of Hive, Pig, and Storm applications.  For Linux or Windows developers, HDInsight has plugins for both IntelliJ IDEA and Eclipse, two very popular open-source Java IDE platforms.  HDInsight also supports PowerShell, Bash, and Windows command inputs to allow for scripting of job workflows.  For data scientists, HDInsight includes Jupyter. Jupyter is a notebook-based development environment that allows for integration of code and content.  When code and content come together, they create a living document that updates with data.   Closing Thoughts  At BlueGranite, we've had our thumb on the pulse of the Big Data industry for several years. Our customers are looking to implement Big Data solutions that don't require a heavy-handed administration effort to see business value. Many of our customers are moving to the cloud to optimize their IT infrastructure. For these reasons, our customers are embracing HDInsight and Windows Azure-based data platform tools.  We offer a 3-day Big Data Bootcamp that can help your team get up to speed with HDInsight, learn how to use the power of parallel processing to build advanced analytic models at Big Data scale, and learn how to deliver analytics to business users through the use of power visualizations.  Contact us today to learn more.   "
"256" "Design patterns may sound like a technical concept that only developers care about, but the presence (or absence) of good design patterns for moving and integrating your data can have a significant business impact. In software engineering, a design pattern describes a recurring and useful solution to a common problem in a particular context. A good data integration developer can look at existing code and abstract it into patterns.  As we go through our careers we build up (mental or documented) libraries of design patterns for solving common problems. We build hundreds of processes to copy data from a table in a database and insert it into a table on another database. After a while we determine what we consider to be the best way to complete this task, and we reuse our optimized logic the next time we need to complete a similar task.  Good SSIS Design Patterns Decrease Total Cost of Ownership  We use SQL Server Integration Services (SSIS) to exchange data between mission critical transactional systems and to populate data in data warehouses that our organizations depend upon to make informed decisions. When our ETL isn't running smoothly, we are slowing down the business.  Different organizations, and even different people within organizations, may solve the same problem in different ways. If a data warehouse team allows development to be a free-for-all they end up with a mess of a solution that is difficult to understand, difficult to support, and difficult to enhance. Employee morale decreases when everyone is too bogged down to make progress and achieve goals. If SSIS developers communicate and discuss design patterns, they are likely to create better solutions because they can incorporate the best ideas from each contributor. If the team commits to employing consistent design patterns across people and projects, they can create a well-oiled machine. Developers don't have to spend the time and effort to re-think how to solve a common problem if they already have a design pattern for a solution. This results in decreased time to delivery.  And over time, it facilitates better estimates of time and effort because the team has solved similar problems before in a similar manner, and they know how long it took them. When SSIS project and package designs are consistent, it's easier to more thoroughly test the packages before putting them into production. Required testing time is decreased and data quality is improved. If the same design pattern is used over and over again, the last SSIS package to use it benefits from all the other packages that came before it because they helped to improve the pattern over time.   Support, maintenance, and integration costs are usually underestimated when calculating total cost of ownership. When the support team is working with consistently designed SSIS projects, they are more effective at troubleshooting because they know where to look when someone encounters an issue. They don’t have to dig through obscure properties or wait on the developer who wrote it to become available to answer questions. Decreased response time and issue closure statistics show the team working efficiently.  DBAs and teams whose applications touch the data created by the SSIS packages get used to the consistency of the work and can better plan for their interactions with the resulting data and database. This makes security and performance tuning easier.  Consistent design patterns can help business analysts and data scientists gain an understanding of how their data is populated and refreshed over time. They understand what questions to ask when analyzing their data or requesting new data integration efforts. And they can feel more confident in data quality because of the improved testing performed before release to production. The decrease in support and maintenance cost creates more capacity to add value through new data or new capabilities to analytics solutions that improve analysis and decision making in the organization. Please contact us to discuss how BlueGranite can help you create and implement good SSIS design patterns that decrease your total cost of ownership and enable you to build new BI capabilities."
"257" "For those who aren’t familiar with the business analytics workhorse that is Microsoft Power BI, the self-service analytics system includes a web publishing component that allows sharing via multiple mediums – including phone, web browsers, tablets, desktops and laptops. The flexible business intelligence suite is known for its rich visuals. Thanks to R, the powerful, open-source programming language, creating and supplementing those visuals just got easier. R, which scripts advanced analytics and graphics, works with Power BI in two ways – it can be used as a data source and to create visualizations.   Typically, users might import a CSV file, connect to a SQL Server data source, or an analysis services data set to access the information needed to create charts. R can serve the same purpose. By treating an R script as a source, Power BI users can take any data frame (similar to a table) created in the script and make it available as a data set that can be used for multiple purposes, including modeling and creating graphics, performing analytics, and data predictions. Users can even create a data frame of predictions and ultimately tie them into their Power BI workflow. The simple click of a button captures the R script data sets and their IDs. Just import them into a blank Power BI canvas and execute the script. The user can then instantly create visuals based on that information, or integrate the information with other data sets. Creating visuals is just as easy. In Power BI’s Visualizations panel, users can open the R editor to import code, drag and drop fields that they want to use within R script, and Power BI will visualize and render the data. If the user changes filter selections, Power BI will re-run the R code based on the context of the filter selected, automatically updating the visuals, switching effortlessly from line graphs to bar charts, and allowing the seamless integration of R and Power BI visuals, in an interactive, user-friendly experience. Watch this brief video tutorial below to see how easy it is to incorporate R into your Power BI experience.                        0:03             0:03               Speed 1x  0.5x 1x 1.5x 2x     Quality 720p  360p 720p                          About Wistia   0:02     message                          For more information or to learn more please contact BlueGranite today.  Be sure to check out our Microsoft R landing page for more articles and helpful resources. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"258" "SQL Server Integration Services (SSIS) is a data integration tool used in companies both large and small. Graphical SSIS development tools reduce the need to learn a new coding language in order to extract, transform, and load your data. But organizations that use SSIS find that the lack of abstraction and inability to reuse code cause inconsistencies in package design as well as tedium and repetition for SSIS developers. The Constant Struggle As an example, imagine you have added a new project management application to your environment, and you must now add that application’s data to your data warehouse in order to report on project progress and costs. If you follow the traditional model of keeping a current copy of the source data in your data warehouse staging area and then transforming that data to populate facts and dimensions, SSIS developers now have the tedious job of manually creating the staging and dimensional model tables as well as all of the SSIS packages needed to populate them. While this job is repetitive, it still requires attention to detail. Developers must ensure they don't omit any columns needed from the source data and that they add the appropriate audit columns when creating the tables. And they must make certain they are following the correct SSIS design pattern, which may include logging and error handling in addition to a specific way of handling data for each task (staging, populating dimensions, populating facts).  It's not convenient to copy and paste SSIS tasks and components because they are so tightly coupled to the data they reference. Even good developers deviate from patterns while performing repetitive manual work simply due to human nature, forgetting an audit field here or violating a naming convention there. Perhaps a developer devises a better method for error handling as the project starts. The new SSIS packages might employ this new error handling method, but it is difficult to justify the effort required to update all existing SSIS packages when there are outstanding tasks that provide more business value. So you accept that the packages are inconsistent and move on. Six months later, it is discovered that an edge case wasn't covered in the new error handling method, so each package must be updated again. Team members get bogged down supporting this inconsistent solution instead of adding new data and functionality that advances the organization’s analytics capabilities. A Robust Solution This is a common scenario for which there is now a good solution. Business Intelligence Markup Language (Biml) is a programming language for creating business intelligence solutions in the Microsoft stack. It uses XML in combination with small nuggets of C# or VB code to automatically create SQL scripts, Integration Services packages, Analysis Services databases, and other objects. Varigence, the maker of Biml, offers free tools that create relational database objects and SSIS packages that are no different than the same objects created by hand.  Biml provides a way automate SSIS design patterns. This reduces the time required to complete a data integration project, and it helps employ consistent design patterns within and across projects. Re-generating multiple packages after making a change to a design pattern takes just a few minutes, so small changes to several similar packages are no longer a significant effort. Automating SSIS design patterns allows teams to work more efficiently. Senior developers can stop solving the same problems over and over again. Instead, they can solve them once, automate the solution, and move on to new and interesting challenges. Junior developers still learn good development practices with Biml, but they require less supervision to create quality output in a shorter amount of time. SSIS developers that prefer typing code over the drag-and-drop interface of SQL Server Data Tools now get a better way to work in addition to the automation capabilities. Consistent use of design patterns improves testing and support, too. The reuse of your code across projects exposes it to more scenarios and helps you identify issues sooner. While you should always test your SSIS packages, you can feel more confident about the tenth project in which you use a Biml pattern because the previous nine projects tested and improved upon it. Additionally, team members in a support role can more quickly diagnose issues because they know where to look in a consistently designed SSIS solution. DBAs can also appreciate the consistency of SSIS solutions as they collaborate with developers on security and performance tuning.   Automating SSIS development with reusable patterns and code can improve time to delivery, maintainability, and correctness of your data. This in turn allows your data integration team to complete more projects with high business impact. We would love to help you explore how using Biml to automate your SSIS development can benefit your organization. Contact us today to schedule a call with our team."
"259" "There are data visualizations all around us. It’s easy to place value when judging a graphic display on appearance alone – pretty colors, fancy charts, cool pictures – but if the visualization doesn’t give quick insights that aid decision making, it’s really not effective. Identifying and removing chart clutter reduces visual “noise”, allowing the audience to focus on the information. In “The Visual Display of Quantitative Information” Edward Tufte describes such noise as “elements in charts that are not necessary to comprehend the information represented in the graph.” Common chart clutter items include: 3-dimensional effects Dark gridlines (use soft gray gridlines or eliminate gridlines when possible) Overuse of bright, bold colors Unnecessary use of all uppercase text (uppercase text is only necessary when calling attention to an element)   In the images above, the top chart comes from the USDA Economic Research Service. The bottom chart contains the same data but has less chart clutter, making it easier to read. Here are the things that were changed: Remove 3-D effect: A 3-D chart showing 2-D data doesn’t add value to your charts but does add noise and makes the chart harder to read. Use softer color tones except where you want to draw attention. Bright colors can be tiring to look at. Be mindful of your color-blind audience and use tools like the Coblis Color Blindness Simulator to determine whether your visual is color-blind friendly. Pivot the chart: Use the horizontal bar chart when you have category labels that are many characters long so that users don’t have to tilt their heads to read the chart. Choose an appropriate bar width: Make bars skinnier to use just enough white space so that the bars are slightly thicker than the white space between them. When the bars are too close together, your brain will naturally try to evaluate the area vs. length. Remove the x-axis header and add data labels. By adding data labels, you can easily see that the national average is 2.4%. Since we added data labels, the x-axis would be redundant and thus may be removed. A final change made to the bottom chart improves readability; sort the bars in descending order. By sorting, readers of the bottom chart can easily see that food price inflation is highest in St. Louis, Seattle and Atlanta and lowest in New York, Anchorage and Pittsburgh. Try for yourself to see which chart is easier to read. A good practice after creating a chart is to take a step back, identify unnecessary elements (chart clutter), and remove them. Repeat this process until nothing else can be removed because every element of the visualization has a purpose and supports the objective of the visualization. For other data visualization tips and tricks, be sure to check out BlueGranite’s recorded webinar Effective Communication through Data Visualization."
"260" "Dashboards can be used to communicate a dense collection of information efficiently on a single canvas. Your audience has a limited amount of time to monitor key metrics to get a quick status and identify anything that needs attention. The attention span of the average human has gone from about 12 seconds in 2000 (when mobile phones became mainstream) to about 8 seconds today – a second less than a goldfish – according to a 2015 study. Following data visualization design principles is key to making your dashboard easily consumable. A poorly designed dashboard can make your eyes jump all over the screen. While it won’t give you much insight, it may cause a headache. In the Western world, we read from top left to right, then zig-zag down left and scroll right again (in a Z-pattern). Understanding where the audience’s eyes will start and travel next allows you to guide them through your dashboard.  The purpose of the dashboard above is to monitor a wireless communications company’s top operations goals. Another approach to dashboard layouts is to use quadrants of importance. This corresponds well to the Z-pattern. Quadrant 1 (Q1): This is where to place the most important information – that which requires immediate attention. Reducing Subscriber Acquisition Cost (SAC) is the highest priority for the company, so the critical information related to its current SAC is placed where the eyes automatically look first. Quadrant 2 (Q2): Q2 is an extension of Q1 but with data from a different level of detail or different category. This is usually something that needs to be compared to or evaluated with Q1 or that supports Q1. In the above dashboard, every chart in the top row gives more detail about the cost of a new customer (lowering in importance as we move right). Quadrant 3 (Q3): Q3 contains less important information that is still related to Q1 to achieve the overall purpose of the dashboard. This dashboard shows how Average Revenue Per User (ARPU) is performing in Q3, and the charts that follow the Z-pattern to the right support the visual to the left. Quadrant 4 (Q4): This is the quadrant where you include visuals that have value (otherwise they wouldn’t appear in your dashboard) but are less emphasized. In this case, we have included charts that display the underlying measures that affect ARPU. There is also a dashboard layout concept called the golden triangle, which indicates where the most important items of the Z-pattern should be placed. The golden triangle is the Q1-Q3 (top-left) corner. It is where the eye focuses the most and where your audience’s attention needs to be. Positioning a number or a chart in Q1 that should be in Q4 disrupts the flow of the dashboard. It’s like reading the ending of a book first. Place visuals in supporting order so that each visual makes sense and guides the user on a journey to insights. Please contact us if you'd like to set up a call to talk about design best practices.  In the meantime, check out our webinar on Effective Communication through Data Visualization. "
"261" "Many data visualization practitioners have been enjoying the weekly Makeover Monday series on the VizWiz blog where Andy Kriebel (a Tableau Zen Master and certified trainer) posts a link to a publicly available data visualization, gives it a makeover to improve the design, and invites the community to do the same. This is a great way to understand and practice good data visualization habits. It’s also a great way to see how other data viz designers think. In explanatory data visualization, the designer is trying to catch the audience’s attention and communicate a message. Organizations of all types often struggle to do this because the people providing the data or analysis have yet to develop the data visualization skills necessary to achieve these goals.  A colleague pointed out some data visualizations in an article from a regional paper in our area that offered a great opportunity for improvement. They accompanied an article that discussed enrollment metrics and demographics in Kansas public universities and colleges in the 2014-2015 academic year. The article made interesting points about the proportion of the population represented by each ethnicity compared to the proportion of enrollees in Kansas public higher education institutions. It also examined trends for each ethnic group over the last 5 years. The end of the article focused on enrollment by ethnic group at the University of Kansas (the largest university in the state). The charts were created by the Kansas Board of Regents and republished in the article.  While the article contained great statistics, the graphs didn’t seem to support the story effectively.  The first graph in the article used a 3-D pie chart with 9 different slices to show the ethnic makeup of enrollees at all Kansas Board of Regents institutions.    There are some opportunities for improvement with this chart: The pie chart doesn’t support the message of how enrollment by ethnicity is trending over time. The bold, dark blue color assigned to the white category stands out most even though that is not the focus of the story. 3-D pie charts obscure the area and angles of data, making them difficult to read. A pie chart with 9 slices is difficult to read, especially when the slices are not ordered in descending size. The labels cause the reader to move her eyes back and forth between the pie slice and the label in order to take everything in, and the label text is a bit small and difficult to read. There is a general lack of visual appeal with this graph. This data visualization violates some data visualization principles that come from cognitive psychology. Most people can retain only about 4 chunks of information at a time and preattentively distinguish about 8 colors at a time. The pie chart is problematic as readers attempt to interpret and retain the data from 9 slices. Seeing an opportunity, I used the publicly available source data to perform a chart makeover. In keeping with the spirit of Makeover Mondays, I time-boxed myself to an hour. A lot can be done in an hour with a good data visualization tool and an understanding of data visualization principles. An initial attempt resulted in a line chart.  Since the headline and much of the article focused on change over time, a line chart was a good approach. Nine lines would have been too many to be easily interpreted, so the white category was removed and ethnicities with less than 3% of the total in 2015 were grouped with the unknowns into the “Other/Unknown” category. This is a common practice used to clarify the visual and support the intended message. In this case the message is that enrollment of minority ethnicities has increased over the last 5 years. The blue and gray color scheme makes it easier to focus on the important information. The least important lines are gray, while the most important lines are blue. The axis labels and graph title are gray so the reader’s eyes are drawn towards the important lines. The lines on the chart are directly labeled, so there is no need to go back and forth between the lines and a legend. The labels are a larger font size than in the original visual so they are easier to read. A second makeover attempt resulted in a combination chart.  This visual focuses on the current year by showing the proportions in a line chart, but it also provides the trend over time through sparklines with the change from 2010 to 2016 as a number beside it. All text is in gray to draw attention to the data in the charts. A simple blue color scheme is used to reduce clutter and focus attention. Bars are directly labeled for a clean look. Since the article mentions specific numbers from the current year, this visual still supports the story while showing a different focus. Bars don’t have the same limitations as pies and lines, so all 9 ethnic groups can be shown without creating a cognitive burden for the reader. Both charts look more modern and have more visual appeal than the original. They also do a better job of reducing clutter and focusing attention to the important points in the visual. The biggest difference between the makeovers and the original is the intentional design choices made to better communicate the intended message. The best tip for those new to data visualization comes from Storytelling with Data: \"Don't let your design choices be happenstance; rather, they should be the result of explicit decisions.\""
"262" "I still remember completing my first “real” dashboard in early 2012. I was so proud of it until I tried to show it off to a colleague using my laptop. I had originally created the dashboard using Tableau on my 24” Dell monitor with the dashboard size set to “automatic”. In doing this, I had unwittingly created a super-sized dashboard that looked terrible after it was squeezed down to fit inside my laptop’s much smaller screen. As someone who has always considered design to be as important as finding the right mix of Key Performance Indicators (KPIs), I want my dashboards to be consistently rendered, and that includes minimizing the number of devices that need scroll bars to view the full dashboard. After researching the pros and cons of the “Range” size setting, a fixed set of dimensions seemed to be my only viable choice.  When it came time to pick the new dimensions, the decision was heavily influenced by my recently purchased iPad 2. Although tablet adoption by the business community was only starting to take off in 2012, it was easy to envision the tablet becoming a key method for business data consumption in the coming years. So I chose the iPad Landscape (1020 x 765) preset as the new default size. From a prediction standpoint, if I was right, I would be future-proofing my existing work, as well as preparing for a time when tablet design requirements could become commonplace. On the other hand, if the prognostication turned out to be overly optimistic, I simply didn’t see any downside to learning how to build mobile-friendly dashboards. With the new default chosen, the next step was to recreate my tabloid-sized dashboard inside the much smaller dimensions (see picture below). From the start, it was obvious I would not be able to keep all of the original content. The difference in canvas size was simply too great. I would have to decide what content to keep and what to discard as I rebuilt the dashboard.   Looking back now, I couldn’t have asked for a better start to my initial foray into dashboard building. For most new dashboard builders, the initial mindset is not one of “how little” but “how much” data can be packed inside. Among new (and more than a few not-so-new) users, this “how much” mindset all too often manifests itself in an oversized dashboard crammed full of charts and text. With a larger canvas, it is easy for a screen full of empty space to become a distraction and cause one’s focus to shift from quality to quantity. But the decision to use the iPad Landscape preset pushed me in the opposite direction. Working with a smaller canvas, I almost always find myself with a surplus of great content rather than a deficit. Having a surplus is always the preferred outcome. With a surplus you have to evaluate all of the content and then decide which mix provides the most value to the user. As you incorporate client feedback and develop more content, the evaluation process has to be repeated. If you want to add something new, something old has to be removed. In order to make the right determination each time, you have to keep the dashboard’s goals front and center, think like a user and, when necessary, challenge a client’s assumptions. If all of this sounds like Dashboard 101, it is. This approach, this mindset, should be part and parcel of every dashboard project regardless of canvas size. This is how you build an effective dashboard. In practice, consistently following this approach is rarely easy. And it only becomes harder as you push the mix of content from good to great. It can become such a challenging exercise towards the end of the development process that it's easy to lose sight of the intial fun and excitement of dashboard building. Simply put, building a really effective dashboard isn’t all fun. It’s also hard. By “hard”, I’m not necessarily referring to the extra effort needed to create a complex calculation or an awesome looking chart or graph. For a lot of dashboard builders, this kind of “hard” work is part of the fun – the fun of building something cool. It's “hard” in the sense of having to maintain a dispassionate mindset every time you evaluate the content mix and then having the wherewithal to discard that awesome-looking chart if you determine that a table, or even a single number, would convey the KPI in a faster, more effective manner. And this is the crux of my smaller is better argument. With a larger canvas, the temptation is greater to save the cool chart that doesn’t add as much value. But all you have done is needlessly clutter your dashboard and turned the chart into a distraction. Every item on your canvas requires cognitive processing power from your users, and that is a limited resource. A smaller canvas should actually make it easier (but no less frustrating) to let go of that awesome chart since the replacement table (or number) would require less space and may make it possible to add something new to the dashboard. As you struggle to make everything fit, it is now easier for effectiveness to trump eye candy. While it may be quicker to increase the canvas size and avoid difficult conversations with users by putting in every chart they request, everyone gains by having conversations about priorities, key metrics, and how users employ the information in your dashboard. When your users only have a limited amount of time and attention, every pixel counts. You want to create a dashboard that quickly communicates a clear message of what needs attention or requires action, which means it should be free of distractions. As Antoine de Saint-Exupery said, “Perfection is achieved not when there is nothing more to add, but when there is nothing left to take away.\" If you’d like to learn more about how to improve your organization’s data visualization, join us May 17 for our Effective Communication through Data Visualization webinar. If you have other questions or would like to schedule a call, please contact us today."
"263" "Analyzing historical data trends can minimize risk and maximize opportunity, allowing leaders to make proactive rather than reactive decisions. This improves productivity. The predictive analytics (PA) area of data mining does this by using facts to predict future events and trends.  Predictive analytics isn’t magic. It’s a method of collecting and analyzing statistical data and correctly condensing the variables. The process can significantly improve decision making and associated costs. In a large, expensive operation, even a one-percent savings is substantial. Preventative Maintenance Maintenance schedules are often based on statistics that predict minimum failure. We typically initiate maintenance when we reach the minimum expected time of failure. For example, we change the oil in our cars based on mileage, rather than testing oil quality. It would be impractical to test our oil because changing it is an eventual necessity and relatively inexpensive. The frequency that it’s recommended is based on an educated guess as to when maintenance should be done. For manufacturers, using a similar maintenance approach for their much more expensive machinery can keep equipment in service longer, decreasing expenditures and increasing profits. Data mining can identify patterns that lead to potential manufacturing equipment failure. It can also reveal defective products and determine factors influencing the success or failure of the process. The predictive analytics fault detection and failure prediction process relies on data collected during the normal operation of equipment, including temperature, vibration, and deflections.  A Real-World Example – Polyester Manufacturing Polyester production is one arena where predictive analytics has been successfully employed. Polyester, a melt-spun fiber, is heated, forced through spinnerets, and cools upon hitting the air. The machinery used for this process varies greatly among manufacturers. In this example, the equipment consists of ovens with up to 30 spinnerets each. The oven heats the spinnerets while they spin fiber. The process rapidly degrades the spinnerets, requiring regular rebuilds. Because these machines run very hot, they must be shut down 24 hours before initiating maintenance. The lost productivity every time a machine goes down is necessary, but can be minimized. Additionally, spinneret rebuild costs 10 times as much after failure than if it’s done before failure. PA can aid in creating a maintenance schedule that minimizes the costs associated with downtime. The Benefits of Predicting Probabilities While we don’t know specifically when a machine will fail, we can predict probabilities. By using data to pinpoint what variables can identify failure, we can develop a maintenance schedule. Then we can pull the equipment just before it’s expected. We want to pull equipment at the ideal moment where expense is minimized between the probability of failing and the cost of downtime. In statistics, the term “expected value” refers to the value of something multiplied by its probability. Over time, this is what the value (cost in our case) will be. The longer we run the equipment before maintenance, the lower the maintenance cost. This also equates to a higher probability of failure which increases that expected value. This is where we use predictive analytics to minimize our risk.  The Data One oven has about 30 spinnerets. Each spinneret has about 300 data points capturing temperature, acoustics, forces, etc. These points capture data every minute. In just one factory running 30 ovens, this equates to almost 400 million data points daily, which is wonderful for predictive analytics. If we were to try to use this data for reporting, there would be a host of other problems but that’s another story. Suffice it to say that this data is significantly aggregated. These collected data points are used only for trend analysis. In predictive analytics, this is about as close to real-time as it gets. Significant Savings By collecting this data and employing predictive analytics, one pilot factory changed its fixed run-time of 10 days to a PA calculated run-time of 14 days. This amounted to a 2.7 percent increase in gross production, at virtually no extra cost after the initial project, and resulted in a $7 million annualized return. By using this data to prevent failure, one would expect that there would be more unplanned failures. There were. At roughly $70K annualized, this increase was insignificant compared to the increased production. Additionally, trends in equipment failure helped to identify parts that were involved in early failures more than is statistically significant. This data was used to permanently remove the problem equipment from the line. This also reduced operating cost over time. We’d love to help you explore the possibilities of predictive analytics. Please contact us today!"
"264" "BlueGranite is delighted to announce that my colleagues, Javier Guillén and Melissa Coates, have published a new whitepaper called Power BI Governance and Deployment Approaches. Developing a culture of analytics is not an easy endeavor, with no one-size-fits-all roadmap. There are an incredible number of cultural factors surrounding the people, processes, and technology which affect user adoption and overall success. In the whitepaper, we propose three ways to approach delivery of business intelligence solutions:  These three delivery approaches do not require an exclusive either/or decision. Multiple approaches can be successfully employed simultaneously in the same organization, even within the same functional area, based on the level of business user involvement and control. Gartner frequently refers to a variation of this as Bimodal BI. However, a key departure from Bimodal BI is that we define two forms of Self-Service BI in the above chart. Frequently we hear the term ‘Managed Self-Service BI’ to denote what we are terming ‘IT-Managed Self-Service BI.’ From client engagements, we have observed Self-Service BI taking place in radically different ways. Therefore, we have opted to make the distinction between ‘Business-Led’ and ‘IT-Managed’ with respect to Self-Service BI. This distinction hinges entirely upon who owns and supports the data layer and semantic layer. For instance, as soon as a business user combines a non-governed data source (ex: industry data or demographics data) with ‘sanctioned’ internal data, we consider the solution to fall under ‘Business-Led Self-Service BI.’ Categorizing a solution as led by the business (as opposed to owned by IT), has implications in defining service level agreements (SLAs), integration points with existing processes, and opportunities for extensibility. The second half of the whitepaper focuses on the tactical deployment options offered by Power BI (whereas the first half emphasizes strategy). There are numerous ways to implement Power BI based on your organization’s business and technology objectives. Though early releases of Power BI placed it squarely in the realm of small self-service BI projects, Power BI has since evolved to become capable of playing a part in delivery of corporate BI solutions as well. Utilizing a tool like Power BI does require making some key decisions, which are discussed throughout the second half of the whitepaper. Though it is evolving at an unprecedented pace, Power BI is still a young tool. As such, system capabilities to govern and manage Power BI are still maturing. In this whitepaper, we offer suggestions for tracking system usage, security, performance, and data sources with respect to governance. Practical suggestions regarding features available as of April 1, 2016 are included when possible in the whitepaper. We believe that governance activities can, and should, be conducted not just by IT but also by business units who own solutions which fall under ‘Business-Led Self-Service BI.’ The whitepaper concludes with a sample roadmap which provides an outline of what tasks are involved, as well as whom is responsible, for each phase of delivery. Note that the phases of delivery, for each deployment mode, correlates to the phases introduced in Section 2. We hope you enjoy the whitepaper, available for download from the Power BI documentation site, and find the ideas useful for your organization. At BlueGranite we are well-versed with how all of the Microsoft BI reporting tools can be used to deliver value and solve problems. Please contact us if we can help you with your BI and analytics solutions. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"265" "With almost 50 years’ experience helping and healing others, Life Care Centers of America is a premier national provider of nursing and long-term care, as well as inpatient and outpatient short-term rehabilitation. The organization has prided itself on putting patients first since its beginning as a single Tennessee care facility in 1970.  Though today it helps more than 1.5 million patients at locations across the country, unsurpassed resident care remains the organization’s top priority. To ensure this focus carries into the future, the company embarked on a multiyear journey with BlueGranite to modernize its operations.  Join LCCA’s Mark Gooch, vice president of business intelligence and process improvement, and our own Eric Wozniak, a BlueGranite principal and leader, at 1 p.m. Thursday, April 21 for a free discussion of ways data can propel an organization toward a successful future. The pair will explore how data and analytics have improved LCCA business operations and patient care and in what ways predictive analytics and mobile will improve the future quality of care for patients of this major skilled-nursing provider. Register now at www.blue-granite.com/webinars and discover what data can do for you."
"266" "According to the Microsoft BI roadmap released October 2015, there are now four primary report types as outlined below:       Additionally, for our four primary report types, we have two main areas emerging for report delivery and collaboration: The following video walks through an overview of the report types and delivery options:                        5:07             5:07                Embed      Twitter      Facebook                Speed 1x  0.5x 1x 1.5x 2x     Quality 540p  360p 540p                          About Wistia   4:35     message                          Note that the information presented here is as of early April 2016, and things are evolving and changing rapidly. The most current version of the full diagram shown in the video can be found at the top of this page: http://www.sqlchick.com/presentations/. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"267" "Introduction to Solr Apache Solr is a free and open source search engine.  It provides ultra-fast search against structured, semi-structured and unstructured data.  Its cloud-enabled mode allows for massive search indexes scaled and replicated on a Hadoop cluster.  It forms the search backbone used by companies such as Best Buy, Sears, eHarmony and more. In fact, it’s used by 90% of Fortune 500 companies. Now that you know what Solr is, you might question how a search engine would be used in a BI infrastructure, but the very things that make it an excellent search engine also make it a potent data store for analytic use.  After all, search engines are nothing more than specialized databases.  In this article, we’ll outline a few BI applications of Solr.   Scenario 1: Text Analytics with HR Hiring managers often have to go through piles of resumes just to find a dozen or so to interview.  If you’re in HR, you’re going through resumes ad nauseam.  Whereas traditional databases have limited text processing capabilities, Solr is much better suited to analyze and filter resumes submitted for job openings. It’s a natural use case for Solr. It can be fed native documents, including PDF, Word, XML or plain text and integrate those into its index.  With its development as a search engine, it can easily process the unstructured text.  It can extract key words and phrases, perform language detection and transparently deal with differing word forms.  After a hire, periodic reviews can be combined with keywords, key phrases and other metadata extracted from the source resume to form a predictive model, which can then be used in later hiring processes. Scenario 2: Spatial Analytics with Strategic Planning When a store chain grows from a local to a regional endeavor, new locations better serve existing customers and attract new ones.  A strategic planner has to make the age old decision of a business storefront: location.  Here, too, Solr has specialized features that can help.  Its geospatial features allow the strategic planner to plot existing and potential customers on a map, and easily incorporate distance into the ranking of each potential location. Likewise, when visualizing customer purchases, its grouping features can quickly break down customers by distance traveled, amount purchased, or number of visits.  Scenario 3: Log file Analytics with Manufacturing Manufacturing operations track parts assembly as they enter the inventory until they leave the line fully assembled.  All the machines on the assembly line record log entries.  They might post entries with different structures.  That line might be one of dozens or hundreds.  With that volume, you need very efficient and scalable ingestion and search.  Solr can operate in SolrCloud mode, scaling to nearly infinite volume.  Combined with an ingestion tool like Apache Nifi, Solr can index extremely high volumes of data. Because it is first a text processing engine, it can deal with vagaries of structure, searching the general text or extracting them into appropriate structures as the entries are indexed.  It can power responsive dashboards showing production rate, defect rate, etc.  They can be filtered by date range, batch, product line, location or even by keyword. Solr can usually handle these filters in near real-time.  Will Solr 6 Provide Analytics for Anything? Solr 6, due in the first half of 2016, introduces a new SQL query engine.  That allows it to be a data source for, well, just about anything.  SQL opens up a new world of complex queries AND makes it available to a much broader audience already familiar with SQL.  It might just be a new day for Solr. If you have any additional questions, please reach out to us on our Contact Us page.   "
"268" "Heavy users of SQL Server Reporting Services (SSRS) haven’t had much incentive to upgrade since the 2008 R2 rollout – until now. The release of SQL Server 2016 changes everything. Microsoft has made major improvements to the latest version of its report-generating software. Here are three great reasons to consider upgrading: 1) Behind-the-scenes improvements The background investments in SSRS 2016 have created a truly modern reporting tool. Boasting a new HTML5 renderer, Reporting Services now displays consistently and correctly across modern browsers including Edge, Chrome, Firefox and Safari. The new rendering engine no longer relies on quirks mode, used by a few older browsers, and now also supports the current version of Microsoft .NET Framework 4. Microsoft’s modern makeover of the report builder interface gives it a streamlined look. SSRS 2016 now supports high DPI (dots per inch) helping to present content in an easily readable way, reducing eye fatigue and thereby allowing for increased productivity. Printing is also easier, since SSRS replaced the former ActiveX experience with a modern, PDF-based capability that works across a matrix of supported browsers, including Microsoft Edge.  2) A superior roadmap and well-defined tool integration Though other vendors may offer tools for reporting or data work, Microsoft continues its tradition as an end-to-end Business Intelligence leader. Microsoft offers a wide range of tools for Extract, Transform and Load (ETL), Analytics, and Reporting, from enterprise as well as self-service perspectives. Previously, the wide range of tools raised questions and confusion in users, and clear use directions weren’t officially provided. With SQL 2016, Microsoft has posted a strong reporting roadmap (read about it in detail here). The many improved abilities of the newly integrated tools include the option to pin a Reporting Services or Excel visual to a Power BI dashboard, and to store Power BI and Excel files in the new SSRS web portal. To summarize, the path forward focuses on the four main report types: Paginated reports built with SQL Server Report Builder or SQL Server Data Tools Interactive reports built with Power BI Desktop Mobile reports built with Mobile Report Publisher Analytical reports and charts created with Excel Check out the following video from Yaroslav Lukomskiy to learn how to create mobile reports using SQL Server 2016 Mobile Report Publisher:                          8:55             8:55               Speed 1x  0.5x 1x 1.25x 1.5x 1.75x 2x     Quality 720p  360p 720p                             About WistiaReport a problemCopy link and thumbnail      Thanks for reporting a problem. We'll attach technical data about this session to help us figure out the issue. Which of these best describes the problem? Choose oneVideo plays but frequently stuttersVideo has poor qualityVideo fails to playOtherAny other details or context?     CancelSend   message                          You can also read more about Power BI from BlueGranite's team here. 3) New features There are also a ton of new features in SQL Server 2016, including: Subscription improvements in SSRS, including enable/disable/pause subscriptions, changing subscription owners, adding subscription descriptions, etc. New chart types – Tree Map and Sunburst Custom parameters panel to add rows and columns to change the panel layout PowerPoint rendering and Export Support of SharePoint 2016 (however, note that some of the new features might not be available in SharePoint integrated mode. This article is a good read if you are interested in learning more about the tradeoffs between native mode and SharePoint integrated mode) Reporting Services web portal – an updated, modern portal that incorporates KPIs, Mobile Reports and Paginated Reports. This portal can be customized with an organization's logo and colors by using a branding pack. You can also search for KPIs and connect them to another view to see details. Report embedding – embed mobile and paginated reports in other web pages, and applications, by using an iframe along with URL parameters. The vast improvements in the latest version of SSRS haven’t gone unnoticed. Gartner has positioned Microsoft as a leader in the BI and Analytics platform. SQL Server 2016 Release Candidate (RC2) came out Friday. Take it for a spin by downloading from the link below: SQL Server 2016 RC2 Mobile Report Publisher Preview When you’re ready to upgrade/migrate and experience the magic of SSRS 2016, have a look at the following article for best practices. We’d love to help you explore the many ways SSRS 2016 can benefit your organization. Contact us today to schedule a call with our team."
"269" "SQL Analysis Services 2016 (SSAS) models can be a key component in a robust semantic layer strategy. They give users the capability to run fast ad hoc queries and benefit from metadata like hierarchies, KPIs, and security while simplifying the query experience through drag & drop reporting tools. SQL Server Analysis Services 2016 adds new functionality to strengthen your semantic layer. The 3 features below are the ones I have found particularly interesting to increase alignment with users, and increase the overall value of your SSAS environment:  #1 Easier implementation of analytical semantic layers SSAS Tabular 2016 provides more than 50 new functions to simplify business calculation development. Things that used to be hard (like calculating quartiles or computing rate of return) are now easy with DAX percentile and ProductX functions, among others. Additionally, many-to-many calculations can now leverage built-in functionality, reducing the amount of time required to develop sophisticated data sources. #2 Increased user satisfaction via faster reports & analytical queries Many small yet impactful enhancements have now been added to optimize measure execution in Tabular data models. Features like reduction of query traffic can boost speed of your SSAS-based reports. More sophisticated scenarios, like conditional many-to-many calculations, also benefit from additional optimizations in the DAX language. Another area that can offer large performance improvements is the new support for parallel partition processing in Tabular. Many companies have daily SLA standards for when models must be online with refreshed data, and this feature can increase compliance given model data will be available to users faster. #3 Improved Data Governance capabilities SSAS 2016 offers a lightweight approach to monitor your SSAS environment, through the use of a friendly user interface to manage extended events. This is an important feature that can facilitate an understanding of how deployed models are being used, for example. As companies develop BI governance programs, extended event management can help in understanding SSAS usage and ultimately user adoption. If you are currently considering how SSAS 2016 may play a role in your environment, let us know if we can assist with evaluating it within the context of your organization’s data needs. We can help determine if the new features/enhancements make sense for you and assess the impact to your production models. Do you have any additional questions about SQL 2016 and Analysis Services?  Please contact us today and we'd be happy to set up a call with you."
"270" "Those of us, who are now decent with Tableau, can probably remember the days when we first started using Tableau. We connected to a data source and started dropping dimensions and measures from the left to the right and not really sure what would eventually appear on the right as we were dragging. The suspense of moving the mouse, the frustration of the unexpected results, and then finally…once we watched a few videos and figured out how the tool works, the gratification and pride of an awesome dashboard.  Getting started can sometimes be a challenge though, especially if you are new to Self-service BI tools. Do you know the difference between a blue pill and a green pill? What happens when you add a green pill to the color shelf? Pills, cards, and shelves are the foundation of Tableau visualizations. If you are new to Tableau or have been playing around with it but not quite sure how to get the view to look the way you want, we have a video for you!  In this video, I'll help you understand pills, cards, and shelves so instead of wondering \"Why did Tableau do THAT?\" you'll be in better control of what you want Tableau to do.                            4:52             4:52               Speed 1x  0.5x 1x 1.25x 1.5x 1.75x 2x     Quality 720p  360p 720p                             About WistiaReport a problemCopy link and thumbnail      Thanks for reporting a problem. We'll attach technical data about this session to help us figure out the issue. Which of these best describes the problem? Choose oneVideo plays but frequently stuttersVideo has poor qualityVideo fails to playOtherAny other details or context?     CancelSend   message                        Hopefully you are having fun with data visualizations! Being part of the Tableau community, we are compassionate people who love helping our fellow vizzers. The goal of this blog is to help users starting off with Tableau get better ground. In the video above I've few things I learned along the way that should help getting started much easier. When connected to a relational database that is slow because it might not be optimized or well indexed, try using an extract while you are building. Tableau allows you to use a live connection or extract connection for the data source. Extracts are saved locally which increases performance because Tableau doesn’t have to go back to the live source to pick up data. Once you are done building, turn the live connection back on so your data refreshes automatically. Tableau has a collection of videos to help you get started. The Quick Start page has a list of the more useful videos or articles.  Use the community! If you get stuck, use the community forum. You can ask any Tableau question and if people can help, they will. It’s free! Usually, you will be asked to package your workbook so those helping you can see what you are having trouble with. Once you become an expert, you can return the favor by answering questions on the forum! Looking for a solution specific to your needs? Contact BlueGranite to learn how we can help you move your business forward. Interested in Tableau?  Click here for a free trial of Tableau Desktop. Visit other great Tableau blog posts from the BlueGranite team:  http://www.blue-granite.com/blog/topic/tableau.   Or join us for our upcoming Data Visualization webinar.  Click here to register."
"271" "The 2016 release of SQL Server promises an amazing benefit to organizations of all sizes with its Stretch Database offering. Stretch Database allows companies to use on-premises instances of SQL Server to \"stretch\" their data into the Microsoft Azure SQL Database. It gives the user control of what's extended into the cloud, whether stretching an entire database, selecting tables or just choosing a subset of data within a specific table.   Why adopt Stretch Database? Here are five simple reasons. Cost  Projects in the financial services sector commonly specify the need for seven years of data retention. Actual usage suggests that having one year's worth of readily accessible data will satisfy more than 90 percent of all the questions the analysts were asking. Imagine if you could remove 6/7 of the data in your storage area network (SAN). Next year's budget won't require adding additional storage, as you'd be able to more effectively use your current resources to solve today's problems without the burden of carrying yesterday's data debt.  Introduction to Cloud Adoption  Many organizations are nervous about using cloud resources. Look at Stretch Database as a low-risk opportunity for testing externally hosted data. All of your existing infrastructure remains intact and, if you decide this set of data wasn't a good fit for the cloud, you migrate it back down - no harm, no foul.  Scalability  Azure SQL Database allows users to pay for performance, so if an \"S0\" level is sufficient for day-to-day needs, then companies pay only for that tier. If, at year end, you realize you're going to need more performance to address processing needs, it's a literal click of the button to get greater performance out of your stretched database. The beautiful part of this is when you're done processing, you step back down to your normal processing tier.  Performance  The Stretch Database is a form of partitioning, but, unlike SQL Server's table partitioning, there's no need to restructure tables to make it work. In the financial services example, if six years' worth of data is in Azure storage and one year is stored locally, a query asking for all the transactions in the past 30 days immediately eliminates 85 percent of the data from being searched. If that data were only stored on-premises, then you would need to have created and maintained a suitable index to get that level of performance.  Seamless Integration Easily the best part of adopting Stretch Database is the work you don't have to do. The changes to the database are transparent to the users. Your data is either stretched or it's not. There are no code changes required to calling applications, extract, transform and load (ETL), data visualization or your backup strategy. All the tools you're using now will continue to work without the need to write, test, and implement specific code for data stretched into the cloud.  The 2016 release of SQL Server is another great step toward hybrid data solutions. One that optimizes your local investment while leveraging the Azure platform for its stability, security and scalability. Interested in learning more? Check out the video below from Steven Cardella for a demonstration on how to configure SQL Server 2016 Stretch Database.                                                          Want to learn more about SQL Server 2016?  Pleaes contact BlueGranite today to discuss how we can help."
"272" "In today’s data-rich world, analytics mastery is a business must. But the rapid evolution of business intelligence can seem dizzyingly complex.  The state of business intelligence, analytics, and data warehousing is undergoing a major transition.  Though much simpler than their current counterparts, the BI and analytics frameworks of yesteryear were often massive, rigid, and slow to react to change. Today’s analytics environments require just the opposite – flexible, dynamic and fast – and there are multiple options to accomplish this. Successfully navigating the many choices is tricky and mistakes are often costly. BlueGranite’s Melissa Coates and Javier Guillén can help. The pair offer the benefit of their more than 25 years’ combined experience in the upcoming discussion: “Trends in BI and Analytics: A Practitioner's Perspective.”   Join the conversation at 1 p.m. March 17 to talk about what’s current in the business intelligence realm, and get the inside track on effective applications. Coates, a BlueGranite solution architect, and Guillén, a company principal, will delve into the many facets of modern analytics, including traditional and self-service BI, self-service data exploration and modeling, data science, internal versus external data, and more. As the duo discusses advances in analytics, they also will share proven business intelligence strategies that can quickly drive value while incurring minimal technical debt.  Register now at www.blue-granite.com/webinars and let us help you discover ways to successfully leverage the latest technology for BI and analytics!"
"273" "We have all seen how pretty Tableau dashboards can be and we want to jump right in and start making beautiful dashboards to get those ooohhs and aaahhs. The truth is, getting the data in the correct format usually takes longer than building the entire dashboard itself!  Before you start, you need to understand a few things about your data so that the visualization part will be much more fun.  Here are three tips to get your data ready for Tableau:   #1 Understand Your Data Think about the message you are trying to communicate and what types of visualizations you will need to accomplish this task.  Do you need raw (row-level) data to produce scatter plots to find outliers?  Say I wanted to find the correlation between sales and profit. I wouldn’t want sum of sales and sum of profit, I would want every sale and every profit (all rows).  Or maybe you need sales data by territory and month?  Then you will need to take the data to the grain of territory and month.  Most commonly we need to know if your data is at a yearly, monthly, or daily grain.  Source Data: Tableau Result:   In the table above, I want to see how my sales is performing against my quota.  Sales is at the level of day while quota is at the level of month.  Since they are at different levels, you would have to get monthly sales in order to compare it to the monthly quota.  So you need to get your data to the level of month to compare sales against quota. Source Data: Tableau Result:  #2   Flatten Your Data The best data structure for Tableau data sources is flat.  Tableau likes to have everything in one flat table.  Denormalize your data into a flat structure and take it to the lowest grain that you will need for your dashboards.  In my experience, it’s better to flatten the data outside of Tableau (SQL Server or Excel) and then bring it in as a data source. #3 Blend Your Data When you can’t flatten your data, you need to blend.  Using the same example from the Level of Detail description, you can also get the data to the right level if you blend.  You take the Sales data:  And blend it with Quota data:   But here you must specify the relationships so they join correctly.   Then you simply drop a pill from Sales and another pill from Quota and turn on the relationship by clicking on the join symbol next to Quota Month.  Now see how the numbers calculate correctly because they are at the same level of detail.  I once worked on a project where we had budget information that was at the grain of Year, forecast information that was at the grain of Month, and sales data that was Daily.  Naturally, I thought I could have everything in one table and Tableau would just know not to sum up my budget information for each month since I had Year as a field, but that was not the case.  In the end, I had to roll my sales data up to Month, then separate my budget and forecast information into two tables and blend them back together.  With this approach, I was able to see how my daily sales performed by month and how my monthly totals were performing against my annual budget. Closing Thoughts Do not be afraid to create data sources with similar data at a different grain.  This is a good thing to do in Tableau in a case like this.  When you build it like this, name your fields carefully so that you know which fields belong to which tables.  You will often need to bring in duplicate fields to create relationships so it’s best to keep track of which fields you are using in your worksheets.  Take the time to get your data right before you jump into building the dashboards.  Once you have all three of these steps completed, you can get to the fun part of visualization!  Looking for a solution specific to your needs? Learn how BlueGranite can help you move your business forward. Interested in Tableau?  Click here for a free trial of Tableau Desktop. Visit other great Tableau blog posts from the BlueGranite team:  http://www.blue-granite.com/blog/topic/tableau "
"274" "Exciting news! Microsoft continues to show its leadership in advanced analytics with the upcoming release of SQL 2016 featuring R integration. This functionality, known as R Services in SQL Server, makes it easier than ever to operationalize high-performance Big Data analytics with traditional, unstructured, or hybrid data sources – providing a key link between data science and the database.  R is an open-source platform for statistical modeling, and has grown rapidly in popularity everywhere – from academia to enterprise – as a powerful tool for predictive analytics. However, with its design as a single-threaded, in-memory application, R has traditionally had difficulty scaling to Big Data applications. Models developed and tested in R often have to be ported to a more production-ready platform like .NET to handle the workload and data environment, seriously delaying time to value. But Microsoft’s recent acquisition of Revolution Analytics, the leading provider of commercial software and services built on top of R, has led to exciting developments in what’s possible with in-database analytics. With R Services in SQL Server 2016, Microsoft enhances (and maintains complete compatibility with) open-source R by offering multi-threaded and parallel computing capabilities, as well as in-memory and disk-based data management. Including it in the same service as SQL Server means processing analytics in-place, eliminating or reducing data movement and management latencies. The combination of offering enhanced R processing capabilities and bringing the models closer to the data really enable high-value Big Data applications like predictive maintenance (Internet of Things), IT log analytics, customer sentiment and behavior analysis via social media, targeted marketing/customer segmentation, improving healthcare outcomes, and more.  Further, other exciting new features of SQL 2016 like PolyBase data querying technology allow you to easily tackle hybrid data scenarios. For example, bring together unstructured data in SQL Server from Hadoop with other on-premises or cloud-structured sources and perform analytics on that data.  R in SQL Server not only brings powerful capabilities to the database, but offers another important option in Microsoft’s ‘write once – deploy anywhere’ approach for analytics agility across platforms. As seen in the Code Portability Across Platforms image above, Microsoft R Server (aka R Services in SQL 2016) can be deployed in Hadoop, the warehouse, locally in workstations, or in the cloud. I would also add that there are a nice variety of ways to consume the results, such as Power BI, Excel, or web service. All of these options signal a strong commitment from Microsoft not only to data science, but the R platform. If you haven’t already done so, now is a great time to learn more about moving from traditional BI to a true system of insight with advanced analytics, and R can be a key component!  The video below demonstrates a common Big Data application of credit risk scoring using R Services in SQL. Credit risk often needs to be scored in real-time, based on very large data sets. This is where having the model close to the data really speeds time to value.                                           Learn more about SQL Server 2016 at the BlueGranite landing page: http://www.blue-granite.com/sqlserver Learn more about R Services in SQL Server 2016: https://www.microsoft.com/en-ca/server-cloud/products/r-server/ If you have additional questions about predictive analytics using R in SQL Server 2016 please contact us we'll respond as soon as possible.  "
"275" "Microsoft recently held a Power BI Best Report Contest, and we are proud to announce that BlueGranite’s Senior Consultant, Jason Thomas, took first place!   Congratulations to Jason for his excellent work! To date, Microsoft has held two Power BI competitions, and BlueGranite is in the unique position among Microsoft partners to have had employees win prizes in both. In fact, in the latest contest, two of the Top Ten finalists were from BlueGranite. After a period of public voting, Jason won overall with his creative look at basketball data.  Unlike the Best Visual Contest last October, which focused on coding and chart design (and where a BlueGranite employee took third prize), Jason’s goal for the current Best Report Contest was to design content that told a story about his data. Although all ten of the finalists had creative entries, Jason’s report stood out for his combination of superb design and technical expertise.  Power BI Creativity and Innovation Let’s take a look at the intriguing report behind the reigning Power BI MVP. There are a few aspects to Jason’s story that I wanted to highlight: Subject Matter Jason designed his report’s story around shot statistics for Stephen Curry, the NBA’s 2015 MVP. While the typical use for Power BI is for business data, the choice of basketball data shows how Power BI can be extended for other purposes. This is an entertaining use of Power BI and highlights how the software is a capable tool for broader data visualization. It is not simply another business application. Innovation The report stands out for its use of layered visuals. Jason showcased his Power BI talent by combining images with charts. Most noticeably, this occurs with the display of shot locations overlaid on the basketball court. The team logos also stand out. Other combinations are subtle but add great value to the final theme, such as the use of a grey dotted grid under some of the numbers. Design Looking at the report, you would have no indication from the theme that this is a report that was created in Power BI. The design is polished, and it does not contain any of the default colors that typically identify Power BI content. Jason did not try to place too much content on a single page. He also humbly incorporated feedback from a few colleagues. It shows the care and attention that he paid to his overall design, and the report stands out as a result.  Below is a live version of Jason's report, or view it full screen here:    New Power BI Functionality - Publish to Web What made this contest and showcase possible? Microsoft ran its Best Report Contest in conjunction with a new Publish to Web feature. This allows anyone to embed a Power BI report into a website without requiring a login to view the content. In addition, for private company data, the Power BI team also released functionality to directly share a report to users outside of your organization. Why is this new functionality important? You are now free to build reports in Power BI and distribute your content to a wider audience. As web capabilities have grown, we have progressed from static visualizations to interactive ones online. If you read news articles or see other types of posts, these often include the ability to interact directly with a report to enhance your experience or tell a better story. Or, perhaps you simply need to share a report privately with your prospects or clients. Whether it is to enhance your brand, for customer service, or for any other use, you can put your content more easily in front of external viewers. This combination of direct sharing and publishing to the web opens up numerous possibilities for Power BI. As an example, BlueGranite recently published a portfolio of reports in Microsoft’s Partner Showcase. Interact live with our sample reports for Finance, Healthcare, and Retail. We're excited to help our clients utilize these new capabilities! Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"276" "As Melissa Coates showed us in her recent article, Top 3 Reasons to Upgrade Your Analytics Environment to SQL 2016, the upcoming release is huge! It includes a large number of new features, many of them enabling deep analytics and integration with Big Data solutions. One of the features it aligned with Big Data is Polybase.  Polybase is included with the SQL 2016 Enterprise Edition.    Now, Polybase isn't exactly new. It was actually developed to be used alongside SQL Server Parallel Data Warehouse (PDW) in the Analytics Platform System (APS) appliance. This is, however, the first time many enterprise customers will be introduced to Polybase, as APS is more of a niche product as opposed to the broad adoption of SQL Server. What is Polybase? Polybase is a feature of SQL Server that bridges the gap between SQL and Hadoop.  Simply put, it allows a SQL engineer to write a standard T-SQL query that can reach into a Hadoop cluster and return data. It's a very powerful tool for organizations that are currently building, or evaluating, a Data Lake environment.  Here are 5 reasons why you should be excited about SQL Server 2016 with Polybase: Do Big Data without Learning New Tools If that isn't the biggest selling point for Polybase, then I don't know what is!  Seriously, this is a very powerful feature of Polybase. It allows data analysts to use the very commonly known T-SQL, in a very commonly used development environment -- SQL Server Management Studio -- to query data stored in a Hadoop cluster.  No Java or MapReduce required! Flexible Storage Options Data queried with Polybase doesn't reside on your SQL Server. It is persisted on external storage.  Polybase provides a couple of options here: HDFS on Hadoop - The most common distributed Hadoop file system, HDFS, is a great place to store all of your Big Data.  HDFS is a distributed, resilient, redundant file system designed to store exabytes of information -- a massive amount (5 exabytes could encompass all the words ever spoken on Earth in any language, according to the What's A Byte website.)  Polybase can reach directly into HDFS and return data from Hadoop alongside your SQL Server data.   Windows Azure Blob storage - If you don't have a Hadoop cluster, you can still take advantage of Polybase's ability to query external data. Simply place your data in Windows Azure Blob storage and provide Polybase with the location information. Scalable Performance Management Polybase does a pretty good job overall of managing the performance of remotely executed queries. It automatically shifts between modes of transporting data to SQL Server for native processing, and remote execution on the Hadoop cluster (when run in Hadoop connectivity mode). However, some situations arise where the developer would like more control over the performance management. Polybase allows this with full predicate push down. With this mode enabled, Polybase will generate a native MapReduce application that will be executed through YARN on the Hadoop cluster.  This mode will allow long-running jobs to take full advantage of parallel processing, with minimal data movement across the wire to SQL Server. Additionally, when extra horsepower is needed, Azure SQL Data Warehouse can easily be scaled to 2x, 4x, 8x, or even more processing power!  The environment stays up the entire time it is being scaled, so no downtime is requried.  Scaling works the other way also -- when you don't need that much oomph, scale it back to save money. Aligned with Enterprise Security  Security is always an important concern with any major data management project.  Polybase supports with Kerberos negotiation through the Hadoop cluster to ensure that queries will only touch data the logged in user is allowed to see. Standard SQL Security is included as well, which means you'll have object-level access to secure your data.  Transparent Data Encryption (TDE) is also supported to ensure that in the rare case someone who shouldn't places their hands on your data, they won't be able to make sense of it. Platform Support Polybase includes support for both on-premises and cloud-based unstructured data storage platforms.  Currently, Polybase supports the following Hadoop clusters: Cloudera CDH 4.3 Cloudera CDH 5.1 Hortonworks HDP 1.3 (Windows/Linux) Hortonworks HDP 2.1 (Linux) Hortonworks HDP 2.1 (Windows/Linux) Hortonworks HDP 2.2 (Windows/Linux) In addition to supporting Hadoop clusters, Polybase also supports the following cloud-based solutions: Windows Azure HDInsight (Use the appropriate HDP version) Windows Azure Blob storage Windows Azure Data Lake (Future support) When Polybase is connected to Windows Azure Blob storage, it cannot perform predicate push down, as there is no underlying processing framework attached. To learn more about Polybase, and get started on your own testing, please check out Leo Furlong's video Getting Started with SQL Server 2016 and Polybase                                  Polybase is only one of the many new features in SQL Server 2016. Sign up for BlueGranite's blog updates today to be notified of new articles related to SQL 2016, data analysis, and other important data-related topics."
"277" "At BlueGranite we are very excited about the release of SQL Server 2016 because it represents a meaningful step forward in the technology capabilities for integrated delivery of business intelligence, data warehousing, analytics, and data science. To highlight three of the most compelling new capabilities: 1. Integration with R Services opens up an immense number of possibilities for deep data analysis, data science, and custom data visualization. R Services allows a data scientist to do what he does best, alongside a database administrator to operationalize, secure, and oversee the analytics solution. 2. Polybase facilitates accessing Hadoop data structures using familiar SQL Server tools and language. This significantly lowers the entry point for starting to work with Big Data because it can more easily integrate different types of data. It also works with your favorite reporting tool such as Power BI or Reporting Services. 3. Last but not least, new security capabilities via Row-Level Security, Always Encrypted, and Dynamic Data Masking can be game-changers when it comes to safeguarding data. Having these features available natively as part of SQL Server will reduce the need for some customized solutions, freeing up developer time to create features of more value.    For more information, check our Overview of Features for Data and Analytics for an overview of the most relevant new data and analytics capabilities in SQL Server 2016, including why each item has significance in the area of data management. If you have additional questions about SQL Server 2016 we'd be happy to help.  Please contact us today and we'll setup a call.  "
"278" "Microsoft Power BI’s potential spans industries. This cloud-based business analytics suite offers a one-stop shop to harness, view and examine company data. Our latest demos illustrate three different ways to bring information to life in the worlds of money, healthcare and retail.   By using Power BI to analyze portfolio performance, finance professionals can track the rate of return, as well as how allocations change over time. Momentum indicators – including the stochastic oscillator and moving average convergence divergence (MACD) – help technical analysts evaluate when it may be appropriate to make additional trades for the securities in their portfolio. However, insights into historical performance, as well as where stocks are trending, can benefit anyone looking to evaluate and manage their trades. Our demo below shows how.                                                                                                                                                                                                  For healthcare providers, in our demo video below we illustrate how Power BI can evaluate hospital inpatient admissions and discharge data to reveal what’s driving hospital admissions and the associated costs.  One potential use for this information? To create solutions aimed at reducing the cost per hospital admission.                                                                                                                                                                                                  In retail, effective promotion plays a major role in product awareness and sales. Power BI can gauge whether promotions are working, and how well, as our retail example below demonstrates. Analysts can also view past store performance to gain insight into how discounts for different item categories and geographies may have improved overall sales. This information is certainly useful for anyone looking to improve sales lift when planning their next promotion.                                                                                                                                                                                                    Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"279" "Tableau is a great tool for analysis and reporting. Through Tableau, BlueGranite has created a report showing customer-level metrics across a wide variety of programs and participation levels. This allows us to give our clients an in-depth understanding of their customers. This report has become so popular, our client wanted a way to render monthly PDF versions for every one of its thousands of customers. Creating those individual PDFs quickly became an arduous task. The client needed an automated way to run PDFs without having to redesign or redevelop the report.   BlueGranite implemented a flawless solution using tabcmd, a little-known utility that comes with Tableau Server. This utility allows a user or automated process to access Tableau Server and perform actions, just as a user would interactively through the web client. In addition, we used Microsoft’s shell-scripting language, PowerShell, to build a process around the automated rendering and saving of the report. We first created a simple Tableau report showing the list of active customers in the client’s database and their associated IDs. The PowerShell script then logs into Tableau Server using tabcmd, runs the Customer List workbook and exports the list of customers to a CSV (comma-separated values) file. Then the PowerShell script reads in the newly created customer list file and runs the regular customer report for each of the customer IDs and instructs Tableau to save it as a PDF. Tabcmd allows the script to pass values from the CSV file into the report through the use of Tableau workbook parameters.  While this approach worked, we found that running each report one at a time was taking too long. PowerShell has the capability of running multiple threads at the same time, so we implemented this technique to call the tabcmd utility in a concurrent fashion. With some testing, we determined the right number of tabcmd instances to run concurrently. This technique reduced the processing time to be able to run the process over a weekend and have all the reports ready Monday morning. The end result: Easy, automated, individual monthly customer reports in PDF format – exactly what our client was looking for. If you are looking for a solution specific to your needs, let’s talk about how BlueGranite’s experienced team can help you move your business forward.   Interested in Tableau?   Click here for a free 14-day trial of Tableau Desktop.  "
"280" "By now, most companies understand the need for Business Intelligence.  Whether it’s general reporting or advanced analytics, upper management has a never-ending desire to turn data into actionable information.  Companies have the data, the hardware, the software, the analysts and even the programmers, but the end product is never as good as anticipated and delivered much later than promised.  So what’s the problem?  It could be a lack of Business Intelligence leadership and organization.  Considering the speed with which technology has evolved over the years, this shouldn’t come as a surprise. Rather than racing to keep up, businesses should take a step back, devise a clear vision, road map and strategy.  The first step in that strategy should be making sure the right team is behind the effort.   Assembling an All-star Team Look to the world of sports for clues in drafting a winning Business Intelligence team.  A functional football team can’t be built with three top quarterbacks and no good running backs.  BI teams also must be structured with balance.  An ideal BI team combines various technical talents and skill sets.  Having a team of three highly skilled senior database developers may seem like a good idea on paper, but several unintended consequences can arise.  The potential for power struggles over how things should be coded is one unintended consequence of stacking a BI group with senior-level skills.  Senior developers have adopted patterns and techniques that may clash with those of their fellow senior counterparts. An effective teambuilding approach is to stagger ability, pairing a senior and one or two junior to mid-level developers.  This allows the senior to set direction and focus on complex issues while the junior handles more routine tasks.  Not only is this cost effective, but it also provides a healthier team balance and growth opportunity for the junior roles. Stacking a team with senior developers can also ultimately lead to high employee turnover.  This results in lost productivity. Even more damaging may be the loss of domain knowledge specific to a company.  This intangible asset often goes unrecognized until it is too late and the individual has already left the building. While staggering ability, it is also a good idea to stagger skill sets.  Business Intelligence has become very dynamic, demanding both database and programming ability.  Having a mix of team members who can ‘throw’ and ‘catch’ the ball is paramount. An important goal in building a team is finding people that work well together.  The old adage, “There is no ‘I’ in ‘TEAM’” comes to mind.  In other words, personality and the individual’s ability to collaborate with others often trumps his or her ability to perform a technical task.  Technical skills can always be learned over time but behavior is hard to change. Managing the Effort Once the team is in place, the next step is to ensure it is properly managed.  Having a strong manager at the helm is vital to high performance and team success.  This individual acts as a liaison between the team and the rest of the company, hence needs to be carefully chosen.  A good BI manager must embrace both hard skills to work with the team and soft skills to work with management across the company.  Being able to code brings instant credibility and respect among the team, while being business savvy is just as important. Wayne Eckerson, former director of research at The Data Warehousing Institute, calls those leaders who can straddle the divide between business and IT “purple people” because they “are neither blue (i.e., business) nor red (i.e., IT), but a combination of both.” Sometimes the strongest Business Intelligence managers can be found within an existing team. According to research by Matthew Bidwell, an assistant professor at Wharton, “workers promoted into jobs have significantly better performance for the first two years than workers hired into similar jobs.” Ultimately though, the goal should be to hire the right fit for the role, whether from within or without. No matter what stage of development your business intelligence effort is in, if you are looking for direction, BlueGranite can help. Learn more here or contact us today."
"281" "2015 was a landmark year for Big Data. Hortonworks had its first year as a publicly traded company, acquired Onyara, and released Hortonworks Data Flow.  Cloudera expanded its worldwide footprint and was appointed to Deloitte's 2015 Technology Fast 500(TM) for Fastest Growing Companies in North America. Microsoft expanded its cloud offerings beyond HDP and HDInsight, and now offers elastic parallel processing engines like Azure Data Lake and Azure SQL Data Warehouse. It's hard to imagine a year bigger than 2015 in the Big Data world, but I predict that 2016 will surpass the previous years in many ways.  Here are my Top 5 Big Data Trend predictions for 2016.   Spark will overtake MapReduce. Without a doubt, if you've been paying attention in the Big Data industry, Apache Spark is on your radar. You are probably even using Spark for some of your interactive data processing.  Apache Spark has so many improvements over MapReduce, it's not a hard prediction to make that Spark will take over as the processing engine of choice in many Hadoop clusters. It's such an easy prediction to make, that IBM made that bet last year, announcing a $300 million investment into Apache Spark over 3 or 4 years.  With such a large investment into Databricks, we can expect to see some great new features coming out this year.  In fact, we've already had a new major release. Apache Spark 1.6 was released on Jan. 4, 2016.  This release included new performance enhancements, as well as new Data Science/Machine Learning functionality.  From a deployment perspective, all of the three major Hadoop vendors, Hortonworks, Cloudera, and MapR, are now including Spark in their distributions: Hortonworks HDP 2.3 Cloudera CDH MapR Converged Data Platform In addition to the improvements that Databricks is adding to Apache Spark, community projects are also investing into the platform.  Many common Hadoop applications are already supporting Spark as an execution engine, or have plans to in 2016: Apache Hive has supported Spark Execution since version 1.1 Sigmoid currently has forked the Apache Pig source and created a project called Spork and also has a current development to enable Spark as an execution engine in the main source branch. There is a community project to run Sqoop on Spark currently in development. With all of the development activity around Spark, the next 12 months should prove to be very exciting! Ingesting data from any device will be easier than ever before. Big Data and the Internet of Things (IoT) are inseparable now.  You just can't have one without the other.  That being said, until recently, it was actually pretty difficult to build a true streaming application with Hadoop.  You have to be pretty good at Java to use Storm; and Flume only takes you so far. It was a good start, but to be truly enterprise ready, Hadoop needs more. 2015 started the momentum, and we'll see more of it in 2016.   For example, in 2015 Hortonworks acquired Onyara, who was the main contributor to Apache NiFi.  Shortly thereafter Hortonworks Data Flow (HDF) was released. HDF is a huge step in the right direction for IoT implementations. For the first time, we got to see a vendor supported, open-source IoT GUI.  In addition, the HDF team went to work immediately on new features, like log tailing -- meaning HDF will soon support many of the same functions that Flume supports. MapR is another great example of a major Hadoop vendor with eyes on IoT.  MapR has taken a close look at Hadoop and realized that the 50 million file limit (per namenode) in HDFS isn't sufficient when dealing with the potential trillions of files generated by sensor devices.  In response, MapR has developed MapR-FS, a distributed file system without the limits of HDFS. Finally, cloud solution providers have been laser focused on IoT as well.  In late 2015 Microsoft released the Azure IoT Hub; a scalable, fully parallelized, single-point deployable infrastructure designed for collecting, sorting, and analyzing millions of points of data from all over (and sometimes off) the globe.  Cloud platforms, like Microsoft Azure, are greatly disrupting the Big Data field in a good way.  With this release, Microsoft made what should be (and is) a very complex infrastructure deployment, as simple as completing a wizard and waiting a few minutes.  This kind of innovation allows Big Data developers to focus on what is really important, putting together the code to make things go and not worrying about the status of various services in a Hadoop cluster. With as much effort and investment as Hadoop providers have put into IoT, it's definitely going to remain a strong Big Data field in 2016. With tools like Azure IoT hub, and Hortonworks Data Flow, we'll be able to implement these solutions more easily than ever before. Data Governance and Security will become forefront in 2016. There is no question that Enterprise wants to adopt Hadoop.  I talk to customers every week that want to know what it takes to get started.  There is often a hurdle to leap over though, and that hurdle is generally related to data governance and security.  Enterprises are nervous, and rightly so, that managing petabytes of data will be an unruly task. With so many applications living and breathing in a Hadoop cluster, is it possible to know that all of the data is correctly secured? It's a big concern, and frankly a valid one. While data governance and security are two different topics, when designing a solution they often go hand in hand.  This year, we are going to see more investment into both topics, and simplified implementations for enterprise. In 2015 Hortonworks open-sourced Apache Falcon and it is now included with HDP (version 2.2+). Falcon is a data governance tool that allows developers, administrators, and data stewards to define rules for data ingestion, data access, and data lifecycles. It also includes auditing, so if any funny business happens, administrators will be able to track down the source.  Hortonworks has a large list of planned features for 2016, and I think we'll see some huge improvements to Falcon. By the end of this year, adoptions will increase, and organizations will be more confident in the data governance story for Hadoop. Cloudera answered the request for Data Governance with Cloudera Navigator (CN).  CN is a fully implemented data management tool that Cloudera ships with Cloudera Enterprise.  It includes a vast array of features including data lineage, data auditing, search indexes, data policy definition, and will even help administrators understand how data is being used and suggest performance improvements to data models.  In addition to the data management aspect, CN also provides encryption at rest and in motion, to ensure that any data captured by third parties will not be useable.  While CN isn't open source, and is only available on the Cloudera Distribution, enterprises who use Cloudera are very excited for the future of this product. Security is another extremely important topic for enterprise customers. In the Hadoop landscape, security most often means configuring Kerberos delegation.  I know, I know, no one likes configuring Kerberos. Good news!  There are new projects to help remove that pain. Apache Knox is an authentication framework that encapsulates Kerberos around the cluster and creates a centralized environment to manage user access. When paired with Apache Ranger, which provides data and process level securables, administrators need no longer question who has access to the Hadoop cluster.  Throughout 2016, we will see improvements to both of these tools, making them easier to implement, and more secure in their operation. 2016 will prove to be a big year for Enterprise Hadoop. I'm positive that customers will become confident in the security and governance of their Hadoop implementations during the next 12 months and beyond. Cloud processing will replace or augment a record number of on-premises solutions. They say \"the cloud is just someone else's computer\", well, this 'someone else' has a way better computer than I do!  It's been great watching the cloud, specifically watching Windows Azure grow over the last year. There has been a lot of very fast growth, and new tools introduced, like: Azure HDInsight with Spark - See above why Spark is so important to today's Big Data landscape. Azure supports it with their Platform-as-a-Service Hadoop offering, HDInsight. Azure Data Lake - Think of this like cloud-based HDFS. It offers scalable, redundant storage, with a job processing framework (Hive/Pig/Spark) attached.  It can also be used as the storage layer for HDInsight. Azure Data Warehouse - For your structured data querying needs, ADW is a highly scalable, MPP, relational database. Azure ML - Machine Learning in the cloud! Azure ML is a great tool. It's a graphical approach to machine learning and includes a bunch of templates to get started quickly. It also allows the data scientist's model to be turned easily into an on-demand web service, ready to be included in any application. Microsoft isn't the only company that is innovating in the cloud; Amazon has been busy at work too: Amazon Machine Learning - Was introduced in April of 2015. It's similar to Azure ML above, and is based on the same tools Amazon Data Scientists use to analyze and predict our shopping habits. Amazon Supports Spark in EMR - EMR, or Elastic Map Reduce, is Amazon's PaaS Hadoop offering.  In 2015, they added Spark support. I told you Spark was a big deal! AWS IoT - Just in time for the holidays, Amazon released AWS IoT for general use. AWS IoT allows developers to easily connect [millions of] devices to collect, store, and analyze data. Like most cloud applications, it's highly scalable, and relatively easy to set up. 2015 was the year we were introduced to dozens of new tools for analyzing Big Data.  2016 is the year we implement them.  Nearly every customer I talk to is considering cloud to be an important part of their IT vision.  If they aren't considering it as part of their vision yet, they are evaluating it.  I predict this year we will see a HUGE uptick in cloud deployments. We'll still have on-premises work too, but I'm putting my money on cloud and cloud hybrid solutions. Nimble BI tools will outpace monolithic platforms. I've been deep into SSAS for many years.  For the last three years, I've remained a staunch defender of multi-dimensional analysis. I think it's still an important tool.  I can't ignore the fact that its implementation rate has dropped.  Sure, we still have many customers using it, but more often than not, our customers are looking at nimble tools like Microsoft Power BI or Tableau. Microsoft Power BI was re-released in 2015 as a cloud-based product that has revolutionized how customers are thinking about Microsoft BI.  It is based on PowerPivot and SSAS, and includes a rich designer, the ability to create custom visualizations using D3, and a powerful data modeling tool.  In addition, it has hooks to on-premises data sources, so you don't have to worry about storing data in the cloud.   In 2015 we saw fast and furious updates to the product, often at a weekly pace.  Some of these improvements enabled Power BI to connect directly to on-premises and cloud-based Big Data environments.  These new features, when paired with the data governance features above, create a powerful self-service BI platform that we'll see implemented many times this year. Tableau, too, has had has a very big year. They released Tableau 9.0 at the beginning of the year.  New features in version 9.0 included new connectors to on-premises and cloud-based Big Data sources.  They also added support to import data directly from statistical tools, like R. This is a HUGE feature for data scientists.  New performance enhancements help to analyze more data than ever before. Additionally, Tableau released Vizible, a mobile-based, self-service tool that enables analysts to take their Big Data with them, whereever they are. Microsoft Power BI and Tableau enable business users to be agile, flexible, and empowered to create the right visualizations for their team.  In 2016, we'll see a lot of growth in this space. Paired with the new tools we have with Hadoop, business decision makers will be more informed than ever before. If you're interested in discussing these or other predictions, please feel to contact BlueGranite."
"282" "The Microsoft Power BI team was fast and furious in 2015, and there are no indications they are slowing down in 2016. If you haven’t checked out Power BI V2 since it was first released last summer, you might want to take another look. Many features have been added and updated since then. Based upon the release schedules since July, it seems there are 3 separate release cycles for Power BI: The Power BI Service (PowerBI.com) gets weekly updates. The Power BI Desktop tool gets monthly updates. The Power BI mobile apps get monthly updates.  Get the Latest Information on New Features If you have been using the new version of Power BI, you might have noticed a new feature and wondered how long it’s been there or what other features have been added. You can likely find the answers to those questions on the Power BI documentation site. It has a “What’s New” section for Power BI Service, Power BI Desktop, and Power BI Mobile Apps that lists the changes and the related blog posts that contain detailed explanations. If you follow the Power BI Blog, you’ll see the feature update blog posts as they are published, in addition to posts about new content packs, custom visuals, and successful use cases. Influence What New Features Will Be Added to Power BI You can be more than just a consumer of Power BI. You can help shape the product by sharing your ideas with Microsoft and other Power BI users. You can log ideas, see what suggestions other users have submitted, and vote for suggestions at Ideas.PowerBI.com. The Power BI team responds to these ideas and lets us know their status. If there is a feature that would increase adoption or usefulness of Power BI for you or your organization, you can log it and be notified when it’s under development and complete. What Are Some New Features I Should Check Out? Many great features have been released in the last couple of months. Check out the video to see some of my favorites. These include support for R scripts and visuals, better control for visual layout in reports, Quick Insights, and the ability to pin Excel ranges and charts to dashboards.                          5:38             5:38               Speed 1x  0.5x 1x 1.25x 1.5x 1.75x 2x     Quality 540p  224p 360p 540p 720p 1080p                             About WistiaReport a problemCopy link and thumbnail    Thanks for reporting a problem. We'll attach technical data about this session to help us figure out the issue. Which of these best describes the problem? Choose oneVideo plays but frequently stuttersVideo has poor qualityVideo fails to playOtherAny other details or context?     CancelSend   message                        For an overview of the concepts and capabilities available in Power BI, check out Melissa Coates’ post on BlueGranite's blog. Please contact us if you'd like to setup a more detailed discussion about Power BI and the updates we described in this article.    Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more."
"283" "Harnessing data can enhance and optimize a company’s workforce by defining goals, measuring successes and pointing to problems. The challenge for most companies is knowing where to begin and what to measure. Unfortunately, there is no “one size fits all solution.” I believe that the best measures are unique to each company, depending on their individual needs.   Workforce analytics is a process — one that is continuously advanced by improving problem solving, measurement, research methods and technology. While the starting point may be different for every company, the key to success is the same: beginning the journey. Creating a workforce analytics roadmap starts with implementing actionable measures that have a high impact potential. That means identifying what factors may lead to behavioral or mindset changes. Those changes could ultimately lead to advancements in policies, procedures or practices to increase a company’s success. The outcomes of the analytical process drive discussion, equipping companies with the tools needed to make informed decisions to solve business problems. Strategic Understanding Do employees know why customers and clients seek the company out? What makes it unique or desirable? A company can best determine whether its workforce has a strategic understanding of its direction by way of a survey. Having team members ask themselves “Am I sufficiently informed about our company’s plans?” is a great survey jumping off point. Analyzing the baseline and long-term trend of this measure – by department, team and location – can provide tremendous insights. Educators and authors Mark A. Huselid, Brian E. Becker, and Richard W. Beatty (2005) have done significant research around the impact strategic understanding has on company performance. Creating alignment and transparency for what is important can do wonderful things. It is also very actionable – low scores mean better and more initiatives around alignment and understanding. Defining Success Does the company have a clear agreement and understanding of what it focuses on to measure success? If so, are employees aware of it? In a conversation with the owner of multiple successful coffee franchises, I asked what defines his success. \"I call or text at the end of the night to ask about total cups sold,” he answered. “There is no better measure of our sales team's performance.” I attempted to dig a little deeper, asking about the margins of different types of coffee and other products. That only adds unnecessary complexity, the owner said. If we start doing that, he added, we lose focus, we end up pushing things on our customers they don’t want, and we end up losing customers. For this organization, sharing the simple metric of \"cups sold\" defines success. If that metric is met, all other traditional metrics, such as margin and cash flow, fall in line. Knowing whether your workforce understands the company definition of success can help build alignment and focus in the organization. Visible Success Measures The third step in the journey toward effective workforce analytics is to make the strategic measure visible. Once employees understand the company's strategy and can define its success, broadcast that definition to your workforce. This is often done by breaking the success metric down by team, department and shift, and displaying it in a common area. This measure needs to be the one thing everyone looks at the end of the day. Great analytics departments help to make sure that happens. They frequently enable transparency on these metrics using dashboards. These traditionally offer a single version of the truth for the entire organization. A more recent trend is to embed business intelligence inside of customer relationship management, enterprise resource planning, or line-of-business applications so that the metrics are front-end-center to the team. Once your workforce understands what makes the company unique and what makes it successful, it’s time to make sure they understand their individual role. Employees need to know how they personally help drive business success. Putting it all Together A superior workforce analytics strategy must measure and analyze the leading indicators that impact success – indicators that are unique to each organization. The ongoing analytics process starts by prioritizing the items you can take action on, and defining what can potentially impact the business and its success. From there, those metrics should be shared with the team, personalized and frequently measured to see where a business is in relation to its goals. Remember, workforce analytics is a process in which many things are changing. However, we can improve as we undertake the process and start learning. I would enjoy hearing your thoughts. Derrick McIver is a professor of management at the Haworth College of Business at Western Michigan University. He can be reached at derrick.mciver@wmich.edu. If you enjoyed this blog post, be sure to check out other BlueGranite articles relating to workforce analytics: Demo Day: Human Resources Analytics Insights Through Mashup Strategic Analytics & BI: How to Maximize Value for Your Organization Demo Day: Better Hiring and Retention with Predictive Analytics References Huselid, M. A., Becker, B. E., & Beatty, R. W. 2005. The workforce scorecard: Managing human capital to execute strategy: Harvard Business Review Press.  "
"284" "These days, it seems almost every data and analytics presentation, webinar or whitepaper highlights the current explosion in data production. As an analytics professional, I find this growth exciting. It represents an opportunity to help clients extract more value from their data and to make better business decisions; to improve their ability to take advantage of market opportunities, to serve customers, and to advance their mission. But for organizations experiencing this growth, the data deluge can create uncertainty and breed frustration.   A potential client recently summed it up this way: \"Our organization is data rich, but information poor. We're creating more reports than ever before, and yet we're still not able to give the business the information they need, in the format that they want, when they want it. We're missing opportunities to enable our business because we just can't keep up.\" Does this sound familiar?Here's a simple approach to move forward:AssessTake stock of your situation. Where are you succeeding? Where are you challenged? What do your end users tell you about your reporting and analytics systems? Are they using the system? If so, which reports or dashboards and how often? User satisfaction and user adoption are two metrics to measure the success of your reporting and analytics initiative. Do you have the right systems in place to capture the data you'd like to report against and analyze? Do you have a robust and flexible data platform that can grow with the changing needs of your business? Do you have a team that can design and build analytics solutions that are compelling and effective in facilitating better decision making? Do you have a plan to support this initiative long term?EnvisionWhat does “good” look like to your organization? The answer reveals clues about where to focus your efforts. In some cases, “good” is simply a reliable system that provides trustworthy data – a \"single version of the truth.\" In other cases, for an organization further down the maturity curve, “good” might mean adding more advanced capabilities, like machine learning to analyze large data sets to identify patterns and/or provide predictions.ActMake your data and analytics initiative a first-class citizen in the organization by fixing the problems and eliminating the negatives. Bad data breeds mistrust, and slow reports and dashboards frustrate end users. Prioritize your efforts and take steps to address these issues.Raise the bar and change the culture. Help decision makers see the benefit in making fact-based choices. Provide easy-to-use, relevant solutions and watch your initiative grow.From there, create a cross-functional team to steer and shape the initiative. Measure success and reinvest in new capabilities to derive even more value from the modern business's most important asset: data. And lastly, evangelize. When you experience success, share it and celebrate it. Companies that embrace a strategic plan to harness and utilize new data gain a leading edge. No matter what stage of development your data culture is in, we can help. BlueGranite leverages the latest technology to create a modern data platform customized to your needs. Contact us to learn more. "
"285" "Tableau's new Vizable app, introduced late last month, gives the saying \"crunching numbers\" a whole new meaning. Users of the free iPad tool can simply pinch, swipe and drag their data in interactive graphs. Vizable provides a good touch-optimized experience, but it has a limited set of features and a different audience from Tableau Mobile.  Vizable paints pictures of information trends and relationships, much like the multiplatform Tableau Mobile, but this app was specifically designed for touch gestures.   Robert Kosara, a research scientist at Tableau Software, says the beauty of Vizable is that it makes “complex features very approachable.” Vizable accesses data from Excel (.xls or .xslx) or CSV files. However, data must adhere to a specific format to work well with the app. Tableau recommends users check to make sure data is in an optimal format prior to getting started. What I like so far: The ease – if you get a spreadsheet via email, you can simply tap and hold on the screen, sending the spreadsheet to Vizable to start graphing. The detail – Vizable recognizes a date and builds a hierarchy so that you can get a year view and keep drilling down to the day and back up. File sharing – Once you build a view you can share it via email, instant message or social media. What I'm not so crazy about: Limited graphics – Users of Tableau Mobile will find Vizable lacks its rich visual variety, displaying only bar, line, and table graphs. But it is easy to switch views between the three. The Excel or CSV file must be in a format that's consumable by Vizable. Tableau is trying to make data more accessible to the average person, and it seems Vizable is doing that. Whether you're an executive trying to quickly make sense of a data-rich spreadsheet or a health fanatic trying to up your game, you can easily see the bigger picture with just a few clicks in Vizable. Try exporting data from a Fitbit activity tracker to get the big picture on your health. The free tool can be downloaded from the App Store and includes sample files for practice. For those looking to make sense of large volumes of data at an organizational level, we at BlueGranite leverage the latest technology to create a modern data platform customized to your needs. Contact us to learn more.  Interested in learning more about how to create great data visualizations? Check out this blog post from our team, Improving Data Visualization Effectiveness. If you'd like to try the regular version of Tableau for yoursef, click here for a free 14-day trial of Tableau Desktop.  "
"286" "It's no secret that Big Data is big news. Every day there are new articles about how organizations are making use of previously untapped streams of data to gain new insights and make sweeping changes in the way they do business.  Technologies like Hadoop are at the center of this wave of change. Big data technologies have the potential to change (and indeed already have changed) the way we think about data management and analytics. The downside to these technologies is that they can be hard to learn and manage. When considering a big data program, organizations are faced with the challenge of acquiring new infrastructure and skill sets to support it. This can have the effect of either extending the timeline and cost or stalling the project altogether. If an organization doesn't think it has enough data or doesn't have a vision of where the big data project will lead, it may never become a reality.  Cloud-based implementations have some advantages in this regard. By using a cloud services provider, you can change your infrastructure investment from capex to opex. You can start small and scale as needed, and you may be able to take advantage of a managed big data solution to reduce the need to acquire new skill sets. For example, Microsoft's HDInsight service is a full Hadoop implementation offered as a service. However, you still need some Hadoop knowledge to use it, and it does require some management. If your goal is to simply focus on managing and analyzing lots of data, you want to get going quickly, and you don't want to build a Hadoop skill set (at least not right away), Azure Data Lake might be for you! Azure Data Lake is a new set of services recently made available in public preview. They are designed for massive scale, highly performant parallel processing and are purely services. This means you only pay for what you use and the services scale dynamically to meet your requirements. For more details about Azure Data Lake, see \"6 Key Features from Microsoft's Azure Data Lake\". In this short video, I'll walk through setting up an Azure Data Lake account, load some sample data, perform some data manipulation to analyze multiple files in different formats, and finally visualize the results with Microsoft Power BI.                             Plays 0                     If you are considering a big data project, we can help! Please contact us today for a more in-depth discussion about Data Lakes and what they mean for your data and analytics strategy.  "
"287" "Retailers still can’t read the minds of consumers, but leaders in the industry are using big data solutions as the next best thing. Especially when it comes to deciphering the elusive demands of the millennial market.   Eric Thorsen, general manager of Consumer Products and Retail at Hortonworks, says \"Most organizations I speak to are stymied by figuring out how we can get inside the brain of millennials - that mysterious category of 18 to 34 year olds. They are very much focused on their phones, really working with screens and devices.  What is fascinating about that is the data coming out of it.\"Companies who utilize big data can get closer to their consumer by capturing, evaluating and analyzing the data that is being created from smartphones and social media.  According to Science Daily, a full 90% of all the data in the world has been generated over the last two years. Mobile, social, local and regionalized data - “it’s exploding, it’s unprecedented and it’s unusual data types - it’s not managed by traditional data warehouse and business intelligence solutions,\" Thorsen said.While this data creates huge challenges, it also presents major opportunities for the corporate world.Traditional structured systems aren't capable of capturing this constant and copious river of unstructured data Thorson talks about - including clickstream, geolocation, Web data and the Internet of Things: beacons, sensors and video cameras.Thorson says the data doesn't fit in traditional repositories. That’s where big data comes in. Forward-thinking industry leaders are using big data platforms to run their businesses, gain consumer intimacy and to drive revenue and margin. Once you know your customer, you can better serve their needs.  A big data solution allows companies to combine all customer touch points - purchases, social media, emails, banners, devices, essentially anything collecting or distributing data - to understand a customer’s engagement with the entire brand.One of the myriad ways retailers are using the platform is by gauging the franchise-wide value of promotions. For example, decision makers take the data generated by giving away a cup coffee - is the customer also buying a sandwich or a salad, or are they taking the coffee and running with it - to determine future strategies. For more information please contact BlueGranite, or watch a recording of our Big Data Webinar hosted together with Hortonworks.   For an example of how big data can be used in retail, check out our Demo Day on Promotions Effectiveness.   "
"288" "A key technology challenge facing organizations today is finding a way to deliver timely and relevant analytics to an audience that is frequently on the go.   Products abound for distributing dashboards, reports, or even data-driven alerts to users at their desktops or laptops, yet these days many decision makers spend a substantial portion of their working time away from their desks, in meetings with colleagues or customers, or even just traveling.  So why don’t typical mobile analytics solutions meet this need?   A common attempted solution is to send emails containing links to browser-based reports to mobile devices, but this often requires multiple logins or constrains users to suboptimal feature sets, so users often opt to wait until later to take a closer look. According to BI Scorecard, “BI adoption as a percentage of employees remains flat at 22%, but companies that have successfully deployed mobile BI show the highest adoption at 42% of employees.”   Since it is possible to deliver insights on the spot in a compelling format, we can avoid the potential costs of delaying a critical decision due to poor mobile information delivery. A second problem is that in our Bring Your Own Device era, users’ selection of mobile devices is broad and diverse, so organizations seeking to provide analytics anywhere must support multiple mobile platforms, which can be complicated and costly.  According to Gartner, “90% of enterprises will have two or more mobile operating systems to support in 2017.”   With the proliferation of reporting tools, it is important to find the best-fit solution that can leverage analytics investment by deploying capabilities across multiple mobile platforms. BlueGranite’s business intelligence and analytics architects have explored these questions exhaustively and have identified and documented best practices for implementing a leading, full-featured mobile analytics platform that supports iOS, Android and Windows Mobile. "
"289" "OK, maybe I am overly excited, but Azure Logic Apps are really cool.  Pulling in external data for our clients' data warehouse or analytics applications used to be time consuming.  Microsoft has released a new solution on Azure that greatly increases the speed of building and deploying our solutions requiring external data sets.  The possibilities are limitless - someone even automated watering their lawn with Logic Apps! While I am not going to automate watering my lawn, I do care about my clients and their businesses. I want to help them increase sales, reduce costs and understand risks.  Combining data sets, both internal and external, can help provide the insights I need to solve these challenges. Automating external data sources can be painful, and this is where Azure Logic Apps can help.   Adding Business Value with External DataMany companies and organizations are now blending their traditional sales and CRM data with external data sources to create more meaningful solutions. For example, sales data can be blended with historical and forecast weather data to find out how weather patterns impact sales in individual stores throughout the day. If I can find real patterns in the data, I can plan customer service levels in my stores to handle those patterns - whether that means an increase in staff to handle peaks or a reduction to save labor costs. Similar solutions could be created with social media data like Twitter, Facebook, and Yelp to address customer sentiment issues. Pulling External Data Made EasyAzure Logic Apps makes it easy to pull in data from external data sources which are usually provided by RESTful application programming interfaces (APIs). Traditionally, developers had to create .NET applications in Visual Studio to consume APIs, and it was time consuming and challenging to manage. With Logic Apps, developers can do this with a user friendly drag-and-drop GUI (they can write custom code directly in Azure if needed). Additionally, Logic Apps come with many pre-developed connectors like HTTP, Twitter, SQL Server, and Azure Blob Storage that are immediately available via the Azure Market place. At BlueGranite we've already started using Logic Apps with our client projects.  In the video below I'll provide a technical walkthrough and give you a quick demonstration of what Logic Apps can do for your big data, advanced analytics, or IOT project.                                   Click for sound                       Logged In to Wistia Report a ProblemOpen video in WistiaView stats in WistiaCopy link and thumbnail                                 What's Next?In my next blog post, we'll explore working with JSON files and uncover easy ways to add JSON file data to SQL Server. In the meantime, if you would like to learn more about using Logic Apps to pull in external data sources, contact BlueGranite today.   To learn more about watering your lawn with Logic Apps, visit this post from Microsoft's Channel 9."
"290" "Power BI V2, which went into General Availability mid-2015, has evolved significantly in the last several months. With an ever-increasing number of capabilities and integration points, Power BI has grown to become an ecosystem.  As with any business intelligence system, a Power BI implementation requires planning and preparation. A few things to consider when you are getting started include: Decision on using Power BI Desktop, Excel, or both for authoring content Integration with Office 365 unified groups and Active Directory groups Options for organizing content by subject area and/or user security boundaries Securing access to content via file-level and row-level security Options for distributing and sharing content Data source types and usage of embedded datasets and/or direct connectivity Managing the frequent release cycles In the following video tour, we will briefly walk through the concepts and capabilities available in Power BI as of early November 2015. Hopefully it will familiarize you with the functionality available and provide you with some things to consider when planning or evaluating a Power BI project.                        5:13             5:13               Speed 1x  0.5x 1x 1.5x 2x     Quality 1080p  360p 1080p                          About Wistia message                          Note that the video above was recorded November 2015. Power BI capabilities, as depicted in the end-to-end diagram, have evolved since that point in time. The most current version of the diagram displayed above can be found at the top of this page:  http://www.sqlchick.com/presentations/. Check out our Demo Day video on Hospital Patient Admits for a real-world example of how Power BI can be used to gain insights.  Please contact us for more information about how Power BI can be utilized in your organization. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more."
"291" "Many mainstream topics and issues surrounding better healthcare revolve around providing better patient care, improved patient health, and reduction in overall costs of care. The ability to effectively glean insights from hospital admits data can lead to actions that tackle all 3 of these issues. In the video below, we use Microsoft Power BI to analyze hospital inpatient admits and discharge data from the state of New York. The demo takes us from state level analysis and leads us all the way into insights about individual physicians at specific hospitals. In doing so, we utilize some of the latest features and visualizations in Power BI including Tree Maps, the ability to easily drill through visualizations, the ability to use custom colors based on fields and values, the ability to quickly and easily share your findings with peers, and the ability to ask natural-language questions about the data and get instant answers in the form of visualizations that you can then add to your analysis.                                                                                                                                                                                                           Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more."
"292" "Traditionally in Data Warehouse and BI projects, data processing is done in batch mode.  ETL processes run for a set amount of time, generally during the night. When everyone arrives at the office in the morning the data is updated, reports are current, and everyone can move on with their day.  In today's world of constant information, the way we consume and process data is changing.  Data is being generated faster and faster, and in larger quantities.  We no longer talk in terms of \"yesterday's numbers\", we want to know what's happening right now.  The Internet of Things, or more recently, the Internet of Anything, is taking enterprises by storm and it's changing how we need to approach data projects. Move Over Batch Processing; Hello Stream Consumption In the world of sensor data, we deal with streams of information. Streams have two main features Streams are continuous Streams don't have a defined endpoint. Data is continuously transmitted. It is not possible to define a time of day that makes sense to process the stream. It must be always processed.   Streams travel in a single directionInformation moves from a sensor to an endpoint.  A separate stream of information may move from the endpoint to another sensor.  A single stream will always move in only one direction. Our data processing framework for a single stream needs to only know about the source and endpoint of the stream.  It will not have to deal with the possibility of data moving in an unknown direction.  Because streams tend to move massive amounts of data and never end, we need a new way to interact, collect, aggregate, and analyze that data.  It's not enough anymore to just act upon data at rest; we have to react to data in motion. Why Reaction instead of action In the traditional world of data architecture, batch ETL processing was sufficient. Our workloads were well defined. They often had an endpoint -- the end of the business day -- transactions could be easily grouped up and moved into a data warehouse. In today's sensor driven world, analytics often need to happen more often than just at the end of the business day. Reactive programming methodologies give us that platform we need to consume and process continuous streams of data, without missing data generated by the stream during an ETL processing window.  The Reactive Programming Manifesto tells us that reactive programs should be: Responsive - The data processing framework should respond to an event in a timely matter if it needs to.  When a sensor measures a value outside of defined limits, it should notify the operator immediately. Not at the end of the day.  Not all data architecture projects will require responsive alerts.   Resilient - The data processing framework should be resilient to failure. If a listening node fails, another should take over. If a listening process on a single node fails, it should not take down other processes that may be running on that node.  Hadoop and YARN provide the framework to support resiliency in our stream processing applications.   Elastic - The data processing framework should respond to workload demand incresase or decreases.  When more streams of data are added, the system should detect this extra traffic and spin up more cycles to allow the stream to be processed.  Hadoop and YARN provide the framework to do this.   Message Driven - The data processing framework should be message-based. These systems are inherently fast, as messages can be queued and delivered with little overhead.  They are also fault tolerant as the transmission of the messages are handled by a different sub-system than is handling consumption of the stream. In order for our Modern Data Architecture to support stream processing, we need to have the right tools that can handle directed and continuous currents of data. Some of these tools are system based, others are framework based, allowing data developers to build the right solution for the problem with source code. Hadoop includes Reactive tools Modern versions of Hortonworks' HDP already include tools than can be used to create Reactive designed data frameworks: Apache Flume -- is a stream consumption tool that can be used to passively listen to a stream and save information in a variety of formats. When run in cluster mode, it will automatically scale to increased workloads.   Apache Storm -- is an event driven stream consumption framework that allows robust applications to be created.  Not only can Streams be consumed, but actions can also be taken in response to events that are happening in the stream.  Apache Storm applications are commonly associated with the Lambda Architecture, which says that we can use a single stream to both react to and archive, without impact on the stream itself. Apache Kafka -- is a distributed messaging system that allows for guaranteed transmission of data from source to subscriber.  Kafka is often used when the stream throughput is more than a single machine can handle, and delivery AND order of the data needs to be guaranteed.  Kafka is commonly used in conjunction with frameworks like Apache Storm to provide message based stream processing frameworks.   Spark Streaming -- is a micro-burst stream processing framework that allows for very short intervals of stream consumption. While not truly event driven, it is often times quite sufficient to poll the stream at a certain short interval, say 2 seconds, rather than consuming every event that happens Spark Streaming will poll the stream at the defined interval and allow for robust applications to be built that process that interval's data. Hortonworks DataFlow -- HDF is a new tool that provides a web-based graphical interface to design IoT architectures (see my first impressions here.) DataFlow is based on Apache NiFi and is a great new way to process streams of data.  Not only does it contain nearly 100 data processors for working with all sorts of data ranging from REST APIs to sensor created data, it also provides a robust security model to ensure that only the right users can modify the flow architecture.  In addition, it provides a number of reporting options that allow DevOps teams to monitor their IoT solutions.  It is one of the first open source fully comprehensive IoT platforms available and is a very exciting product to keep an eye on.  Where do we go from here? As always, in the data management world, there are many options to design a solution. An Architecture Design Session (ADS) is a great way to take a high level look at the features of the project. Working with an expert during the ADS will help to define what tools should be used during project development.   Please contact us today for more information or to schedule a call."
"293" "Listening to what your data is trying to tell you can have a profound impact on your business. Learn how quick-serve restaurants and retailers are using open source Apache Hadoop to heed their data’s message when you join the 60-minute “Leveraging Big Data to Drive Consumer Demand” webinar at 11 a.m. Oct. 27. In collaboration with our partner Hortonworks, we'll explain how big data gives quick-serve restaurant and retail organizations the knowledge they need to drive demand, capture wallet share and build consumer loyalty. Hortonworks’ Eric Thorsen, general manager of Consumer Products and Retail, will clarify how industry front runners in retail, consumer packaged goods and food service are gaining an empowering edge using Hadoop. BlueGranite's Solutions Architect Scott Faculak, will join Eric to detail how to quickly tune into your data to impact business performance and improve customer loyalty.  Join us by registering for the webinar today at BrightTALK. For more information on how Hadoop can help you, please contact BlueGranite."
"294" "With the announcement of the Custom Visual Gallery and packaged visualizations for Power BI, Microsoft has made a great leap in the evolution of analytics software. As a result, the balance in business intelligence and analytics may shift away from innovating around a common set of vendor-supplied visuals to creating a more flexible user experience. Rather than focus on limitations, customers can now focus on possibilities.   While still an early response to the news, here are four ways custom visuals may have a much broader impact: Separates Power BI from the Competition Power BI now allows developers to create visualizations and display data in a way that is not currently available in other BI software. For many web developers, it can be done with their existing skills and with minimal effort. While it is too early to gauge its reception, perhaps built-in custom visuals may start a trend among competitors. It should be noted that other mainstream BI vendors have attempted to highlight integration with D3.js or other JavaScript libraries, but this typically involves embedding or framing standard BI content in a separate website. Customization is then employed on top of the core visual. In contrast, with Power BI, D3 is at the heart of the visual and not layered on top of it. Whether deployed in Power BI Desktop or in the PowerBI.com Service, it becomes much easier for users and developers to take advantage of the new capabilities. Empowers Users Through the Gallery Users are already comfortable with the gallery/store/extension format. It has become a proven way to deliver extended functionality for Windows and other operating systems, browsers, MS Office, and many other platforms. While not an innovative concept in and of itself, the Gallery solves the issue of how to provide additional visualizations in Power BI without having to worry about bloat in the core product. Microsoft provides a set of key visualizations, and users and organizations are empowered to choose complementary ones based on their own unique needs. For some, that may mean trying everything in the Gallery, and for others, nothing. Proprietary visuals can also be packaged as a \"pbiviz\" file and distributed privately outside of the Gallery. Overall, it puts greater control in the hands of users. Energizes the Power BI Development Community Any software derives benefit from its champions and a community of enthusiastic users and supporters behind it. With the release of custom visualizations and their promotion through the Best Visual Contest, the Power BI team extended the audience. While business intelligence products are typically the domain of data analysts, BI developers, and business users; web developers can now be considered part of the fold. Custom visuals allows developers an opportunity to showcase and share their talent in a very visible manner, and everyone benefits as a result. Additionally, it is a big step forward for users who may have highly unique requirements that could not otherwise be met with the standard charts. Microsoft also minimizes its own risk by shifting visualization effort to independent developers. When it comes to how to best display data, it seems that everyone has an opinion. The recent Best Visual Content saw everything from a spartan and austere bullet chart to colorful fish swimming around (cue the jokes about data lakes). Is Power BI a business-centered product that should only offer core visualizations that align with expert opinions and research? Should Power BI go to the other extreme and offer more capabilities for data art and unlimited creativity? To a large extent, it no longer matters from a product perspective. The Power BI team can focus on continued product improvement in other areas and let the democratic nature of the Gallery sort out winners and losers for visuals. Popular or functional visuals will earn prominence in the Gallery, and unpopular ones will gradually fade into obscurity. Reflects Favorably on Cortana Analytics Finally, looking at the larger picture, Power BI is only one piece of a much broader ecosystem. Bringing the ability to easily create and share new visualizations into Power BI is a success for the Cortana Analytics Suite as well. For anyone not familiar with Cortana Analytics, it is Microsoft’s advanced analytics offering that bundles several Azure data and analytics services together—hardly “self-service BI”. Without minimizing all of the great capabilities of the other products in the suite though, Power BI will be the primary way that the majority of business users will interact with Cortana Analytics. For organizations employing Cortana, any strength or weakness in Power BI will reflect on Cortana as a whole. We are witnessing a strength here.  In conclusion, bringing the new visual capabilities and Gallery to Power BI adds critical value. For anyone who has tracked the rapid changes to Power BI throughout 2015, the team has not failed to deliver, but there are still gaps. As the “face of Cortana”, there should be pressure on the Power BI team to deliver enterprise-level software that can meet the needs of any organization. The new custom visuals in Power BI represent a move toward the greater level of control over user experience that many organizations desire. All in all, the ability to create and share independently developed visuals is a welcome innovation not only for Power BI, but hopefully for analytics software in general. It’s great to see Microsoft leading the way here. Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more."
"295" " First, a Little Bragging In July, Microsoft announced that Power BI would be open source and developers would have the ability to add custom visuals to the product. To promote this, Microsoft recently held the first Power BI Best Visual Contest. Lots of great entries were submitted but today we are thrilled to announce that BlueGranite's own David Eldersveld won third prize for his Hexbin Scatter Plot!    Check out the video of David's entry below. I love this entry because it takes a commonly used type of chart that can be difficult to interpret and gives it a new layer of meaning by visually grouping related data points.  Our heartfelt thanks and congratulations go out to David for his great work!                        2:51             2:51               Speed 1x  0.5x 1x 1.5x 2x     Quality 720p  360p 720p                          About Wistia message                        Why I think this is important The trend for the past several years has been for customers to want to leverage out-of-the-box capabilities of their BI tool to support reporting and analysis. Tools that allowed for a high-degree of customization were often seen as too complex for the average business user. Unfortunately, the result of this restriction often meant that the results didn't exactly meet the customer's expectations. Either we could make it look exactly like they wanted at the expense of interactivity or we could make the result interactive but couldn't give them the look and feel they wanted. What's more, ISV's can't keep up with the demand for new visuals and capabilities in their products.  Visuals in Power BI are built on the very popular D3.js library. What this means is that you can leverage the power of D3 to embed new and highly customized visualzations directly in to their Power BI solutions without having to code complex web applications. They become drag-n-drop objects just like all the other built in options. The ability to extend the capabilities of the product with an easily accessible, open source interface opens up tons of possibilities. When faced with a challenging need, advanced users (or their development teams) can now build exactly the right visual for their needs, practitioners can build portfolios of their own work to differentiate them from their peers and a widely accessible, open source community that leverages a well known and powerful platform means that support and examples are only a search and a click away. I can't tell you how excited I am by this capability in Power BI. I think it gives Microsoft a differentiator in the visualization market that their competitors are going to need to respond to. I can't wait to see more and more great work from David and all of the others who are blazing this trail. How to get started Read about Power BI Custom visuals on PowerBI.com Download the Power BI code from Git Hub Learn about TypeScript Read the Getting Started documentation   Is your organization deploying Microsoft Power BI? BlueGranite offers hands-on, instructor-led training at your facility to help both business and IT teams adopt Microsoft Power BI for self-service business intelligence and analytics. Up to ten attendees will receive expert-led guidance through a complete set of hands-on labs and training modules. After the training, attendees will be able to acquire data, build data models, and create visualizations quickly and easily with Microsoft Power BI.  Click here to learn more.  "
"296" "On October 1st, 2015 Hortonworks announced the release of DataFlow, it's newest product in the field of stream processing. DataFlow is \"powered by\" Apache Nifi.This is a very exciting announcement. DataFlow is purported to change the way we deal with IoT and data stream consumption. Let's see if it lives up to the hype. Since the announcement, I've been able to spend a couple of hours with the product.  I have spent a fair amount of time testing different stream processing strategies, so when this product was released to the public, I was ready to jump in and see how it can help. One of the challenges I've faced when working with streaming data is simply trying to figure out which tool to use.  There are so many applications and frameworks to consume streams, it's quite confusing to know which way to turn.  Some of the existing applications are easier to use than others. Some are just application frameworks and require a fair to vast knowledge of OO programming methodologies to fully implement. With Hortonworks DataFlow, it appears that there is finally a tool directed to data analysts for stream consumption.  Here are my initial impressions of the new product. Installation and Setup Like many open source projects, it's possible to download the source code for Hortonworks Data Flow, build the binaries manually, and deploy the solution. Hortonworks does provide a set of built binaries in both Zip and tarball format -- which is nice. Installation was really quite easy. Simply uncompress the file to a user directory, and run the script provided. The script accepts a couple of different execution options ranging from background execution, to service installation. Total time spent downloading and getting Hortonworks Data Flow stood up and running in my test environment? About 15 mins. Seriously! 15 mins.  That's fantastic in my book. User Environment  I have a lot of discussions with customers, peers, friends, and community members about Hadoop. One of the most common questions is \"Is there a GUI I can use, or do I have to use the command line?\"   Often, my answer to that question is not well received (read: NO), but in this case, I think I'll be able to surprise everyone.   Hortonworks DataFlow is 100% based on a GUI. Finally! Again, yes, DataFlow is 100% GUI based and runs in a web browser. So far, I've had no issues with it running in Safari or Firefox. It's responsive, provides great tooltips and information popups, and is quick to respond. I think a lot of enterprises out there are going to be excited to see that they can finally build complex data flows without have to be XML experts. Usage At time of this writing, I've only spent about an hour or so actually using DataFlow to do any tasks.  I haven't even read much of the documentation yet.  That being said, I've already created a flow that connects to twitter, downloads tweets related to Hortonworks, and Hadoop, and Pizza, and The Martian, and  saves them in HDFS.  Yes, all of that work, without reading an ounce of documentation. Disclaimer: I'm not suggesting that you shouldn't read the documentation. You *should* read the documentation. I'm reading the documentation as soon as I'm done writing this article. I think this is a pretty big leap forward for Hadoop/Big Data here -- let me reiterate. In less than two hours, I've installed a product, built a data flow to collect thousands of bits of information from a major social media network, and have them stored in my very own cluster computing environment. WITHOUT READING ANY DOCUMENTATION.   What about the other stream consumption tools I've used? I definitely had to read the documentation (I'm still reading the Storm documentation) -- and it definitely took me longer than one hour to get a full Twitter consumption task working the first time. Score some big points for GUI based programs here. I'm glad to see an intuitive interface, and I think a lot of others will be also. What does all of this mean? At first glance, I think DataFlow can be a BIG game-changer in the world of IoT and stream consumption.  The interface is great so far; It can run in clustered mode; It can interface with Ambari for metrics monitoring; and it was originally built by the NSA -- who doesn't know more about collecting tons of data and doing massive amounts of processing than the NSA? I'm really, REALLY excited about DataFlow now that I've gotten my hands on it.  Watch the BlueGranite resources page for more information as I and the BlueGranite team dig deeper.  Where to learn more If you want to learn more about Hortonworks DataFlow, here are some resources: BlueGranite Resources page -- we'll be continuing to post resources related to Hortonworks DataFlow and other Big Data tools. Hortonworks - The DataFlow home page has links to product documentation and will likely have hands-on lab content in the future.  Apache Nifi - Product Documentation and up-to-date application development guides. This the documentation that we should all be reading. The Final Word We at BlueGranite are ready for IoT with tools like Hortonworks DataFlow. We want you to be ready too. If you are trying to wrangle massive amounts of real-time data and are looking for a team of experts to help guide you down the path, contact us today.   In the meantime you may enjoy other articles on our blog, including my recent post on the Lambda Architecture, or this article on Data Lakes and the Modern Data Platform by our CTO, Chris Campbell.  "
"297" "Organizations today are experiencing data growth for many reasons. LOB applications are growing due to the growth of the business, there are new data sources used in analysis and decision making, unstructured data sources are making it into the mix, and many companies are even expanding into the IOT sphere of solutions. Additionally, firm data is coming from both on premise and the cloud as companies move to cloud business solutions.   To handle this new complex world of data, companies have traditionally solved these problems by scaling up hardware or buying an appliance. Both approaches are acceptable, but the needed upfront investment can paralyze the business as the business value they hope to gain is unvetted. Enter Azure SQL Data Warehouse How does a company deal with these challenges? Enter Azure SQL Data Warehouse (Azure SQL DW). Azure SQL DW is a massively parallel Big Data store in the Microsoft Cortana Analytics Suite of products. It is used as a storage and compute engine for large data volumes and is purposefully built for traditional data warehouse and analytic workloads. The product is made available as a Platform as a Services (PaaS) offering in Microsoft Azure, which means that customers don't have to worry about managing infrastructure, software configuration, or even complex licensing contracts. Some of the key solution differentiators of Azure SQL DW are: Separation of Compute and Storage – one of the key benefits is the built in separation of compute and storage. While the separation of the two components seems nebulous, this important distinction allows consumers to manage and pay for compute (CPUs) and storage (disk space) independently and without long term commitments. Elastically Scale Your Solution and Even Pause – customers can elastically grow or shrink the computing power of the solution with resizing taking effect in seconds. The solution can even be paused when it is not in use, so customers aren’t charged for compute cycles when they aren’t needed. Data Loading with Familiar Tools – your IT group doesn’t need to learn new tools to use the solution. Data can be loaded into the solution using a variety of popular tools like SQL Server Integration Services (SSIS), SQL Server Bulk Copy Protocol (BCP), Azure Data Factory, and PolyBase. Consume Data with Your Reporting Tool – consuming data is easy; you just use SQL. This means that popular reporting tools like Power BI, Reporting Services, Datazen, and even 3rd party tools like Tableau and Qlik can consume data from Azure SQL DW. Build and Deploy with Visual Studio – query and develop your solutions using familiar development tools like SQL Server Data Tools inside of Visual Studio. Monitor Usage and Performance with the Azure Portal – customers can monitor the usage, storage, compute utilization, and can even create custom alerts using the monitoring capabilities deployed in the Azure Preview Portal.In the following BlueGranite Demo Day video I'll show you how to use Azure SQL Data Warehouse combined with Microsoft Power BI for a high-performing cloud-based analytics environment.  Check out other BlueGranite blog posts to learn more about Cortana Analytics Suite, including this post on HDInsight and Azure Machine Learning,  or this post on Spark on HDInsight to learn about a new interactive data exploration experience within Cortana Analytics. If you would like to learn more about Azure SQL Data Warehouse or Cortana Analytics Suite please contact BlueGranite today. "
"298" "There's a true challenge in today's data-rich environment between requests for up-to-the second data, and the time it takes to process, cleanse, and verify that data. For enterprise business intelligence and data warehousing systems, the Holy Grail is to produce a single version of the truth. Unfortunately, that single version of truth comes at the highly coveted price of time. It's no secret that it takes considerable time to process and manage large amounts of data. In today's real-time society, key decision makers and data scientists feel the pressures of delivering decisions and analytics based closely on real-time trends.  How do we avoid making crucial decisions based on yesterday's trends? Enter the Lambda Architecture. Introducing the Lambda Architecture The Lambda Architecture offers a novel way to bridge the gap between the single version of the truth and the highly sought I-want-it-now, real-time solution.  By combining traditional batch processing systems with stream consumption tools a version of symmetry can be achieved. The Lambda Architecture does this with three main components: The Batch Layer This layer is familiar to many students of data warehousing. It is the ETL layer; the master data layer; the data warehouse. What goes in here is correct. It is the truth. We trust this layer. The Speed Layer This layer is the new kid on the block. It's fast and loose. New data that is created is nearly instantly stored in this layer. It is not clean. It might not even be right; but it's now. It's current. We only trust this layer on the surface. The Serving Layer This is the mediator. The calm one. The Serving Layer accepts queries, and decides when to use the Batch Layer, and when to use the Speed Layer. It prefers the Batch Layer – the trusted layer. If you ask for the up-to-the-second data, it will return that too. It's the bridge between what we trust and what we want right now. How Is this Achieved? To implement a Lambda Architecture is to implement a number of parallel or consequent projects. The Batch Layer is typically implemented using traditional data warehousing tools and served via traditional data warehousing databases. In Apache Hadoop terms, this could mean Oozie workflows to process the data, and Apache Hive or Cloudera Impala to answer queries. This layer is built using a predefined schedule, giving enough time for processing – usually once or twice per day. Updates to the Batch Layer include ingesting those pieces of data currently stored in the Speed Layer. The Speed Layer is typically implemented using a stream processing framework. Apache Storm, Kafka, Flume, or Spark Streaming are common examples of tools used to consume data at the rate of creation. This layer offers minimal data processing and generally is simply a \"lift 'n shift\" of the streaming data into a central repository. Apache HBase or Cassandra are common platforms for storing data in the Speed Layer. The Serving Layer is a bit more complicated in that it needs to be able to answer a single query request against two or more databases, processing platforms, and data storage devices. Apache Druid is an example of a cluster-based tool that can marry the Batch and Speed layers into a single answerable request. Why doesn't everyone do this? Frankly, it's complicated. Setting up a true Lambda Architecture requires a large number of hardware and software resources. It requires different codebases for each layer, often using completely different technologies. Changes to the code introduce complexities to the code often at three or four times that of a traditional data warehouse architecture. Often times, the extra cost of implementing a complex architecture like this does not outweigh the benefit of being able to use data slightly newer than standard data warehousing systems. OK, I know I need this. What next? BlueGranite specializes in best practices of data management. A good place to start is an Architecture Design session. These sessions are typically spread over two to three days, during which time our architects strive to understand your data, to understand your requirements, and help to build a high-level roadmap of implementation. From there, our consulting services will help to implement a pilot project to prove the architecture's benefit in your organization. We work in a phased approach to help you realize return on investment during the implementation cycle, not just at the end. Contact us today to learn more."
"299" "Do your analysts typically know where to locate the data they need? When they locate a data source, do your analysts definitively know how the attributes and metrics are defined? Based on our experience working with customers, the answer to both questions is often no. Domain knowledge is typically gained over time through trial, error, and experience. And, it can be all too easy to lose this tribal knowledge when an experienced staff person leaves the company.  One of the best things we can do for our BI and Analytics programs is to introduce a data catalog. A data catalog can be of assistance with self-service BI, operational BI, corporate BI, analytics, and data science efforts in the following ways: Enable data discovery. By registering a data source in the data catalog, analysts don't have to waste time asking around or searching for what data is available and where. Visibility of particular sources can be constrained to a particular user base to enforce security requirements. Single catalog for all enterprise data sources. Most organizations have multiple data platforms which have evolved over time to handle structured and unstructured data, on-premises and cloud data. By registering data into one catalog which exposes data from a variety of locations, we can save analysts significant time and effort hunting for data. Reduce duplication of effort. When existing data sources are registered and discoverable, this reduces the risk that an analyst will recreate data assets which already exist. Standardization not only saves time, but also reduces the risk for error and the need for reconciliations. Repository for common definitions, tips, and advice. Metadata is how we refer to \"data about data\" and can be exposed via a metadata repository. When terms are ambiguous (such as defining customer churn), documenting and annotating data definitions can significantly assist analysts with understanding the data and creating accurate reports. We often like to say that BI initiatives are dependent not only upon technology, but on people and processes as well. Introducing a data catalog is highly dependent on all three components: Technology > People > Process. In this short video, we focus on the Technology portion with a brief tour of the Azure Data Catalog.   If you liked this article, check out our post on Azure Data Factory: Connecting Data to Insights by Mike Cornell, or contact us if you have any questions about Azure Data Catalog and how it fits into your overall BI and Analytics program."
"300" "In an earlier post, Mike Cornell of BlueGranite introduced Azure ML – Microsoft’s Machine Learning engine in the cloud. Mike showed you how you can build an experiment, then publish it for use in your self-service BI application. In this video, we’ll go one step deeper and show you how to connect your data stored in HDInsight.Why combine HDInsight with Azure ML?HDInsight provides the ability to work with massive amounts of data – billions of data points – using the power of distributed processing. HDInsight provides functionality to pre-process and shape that massive amount of data into a form that can be easily consumed by Azure ML.In addition, it provides a way to add structure to un-structured and semi-structured data allowing analysts working in Azure ML to quickly put together models and experiments. These models and experiments on unstructured data can provide insights to customer buying patterns, buyer sentiment, and forecasting predictions at a pace never before possible.With the ability to connect directly to HDInsight from the Azure ML workspace, gaining insights from data has never been closer at hand.  Connecting HDInsight Data with Azure ML     // <![CDATA[ wistiaEmbed = Wistia.embed(\"ztmq9oo61k\"); // ]]> Interested in learning how a Modern Data Platform using Hadoop Data Lakes might benefit your organization? BlueGranite's Whiteboarding Sessions are an on-site, hands-on discussion between your leadership team members and our Data and Analytics Strategists.  Request a session today."
